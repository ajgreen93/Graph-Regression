\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left( #1 \right)}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\dotp}[2]{\langle #1 , #2 \rangle}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\Perp}{\perp \! \! \! \perp}


\newcommand{\Linv}{L^{\dagger}}
\newcommand{\tr}{\text{tr}}
\newcommand{\h}{\textbf{h}}
% \newcommand{\l}{\ell}
\newcommand{\x}{\textbf{x}}
\newcommand{\y}{\textbf{y}}
\newcommand{\Lx}{\mathcal{L}_X}
\newcommand{\Ly}{\mathcal{L}_Y}
\DeclareMathOperator*{\argmin}{argmin}


\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Rd}{\Reals^d}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\begin{document}
	
\title{Graph Testing: Notes for the Week of 12/11 - 12/18}
\author{Alden Green}
\date{\today}
\maketitle

\section{Setup}

\paragraph{Data model.}

We are given two distributions, $P$ and $Q$, defined over compact set $\X \subset \Rd$, with the ability to sample from either one. Our goal is to test the hypothesis $H_0: P = Q$ vs. the alternative $H_a: P \neq Q$. 

Under the \textbf{binomial data model}, our sampling procedure is to draw i.i.d Rademacher labels $L_i \in \set{1, -1}$ for $i \in \set{1, \ldots, N}$, and then sample 
$Z_i \sim P$ if $L_i = 1$ and $Z_i \sim Q$ otherwise. Define $1_X$ to be the length-$N$ indicator vector for $L_i = 1$
\begin{equation*}
1_X[i] = 
\begin{cases}
1, L_i = 1\\
0 \text{ otherwise } 
\end{cases}
\end{equation*}
and similarly for $1_Y$
\begin{equation*}
1_Y[j] = 
\begin{cases}
1, L_i= -1 \\
0 \text{ otherwise } 
\end{cases}
\end{equation*}
and define $a = \frac{1_X}{N / 2} - \frac{1_Y}{N / 2}$. 

Under the \textbf{fixed label data model} we use the same data generating process as above, except fix $\mathcal{L}_X = \set{1, \ldots, N/2}$ and $\mathcal{L}_Y = \set{N/2, \ldots, N}$. Say that $L_i = 1$ for $i \in \mathcal{L}_X$ and $L_i = -1$ for $i \in \mathcal{L}_Y$, and call $\set{ X_1, \ldots, X_{\abs{\Lx}} } = \set{Z_i: i \in \mathcal{L}_X}$ and likewise for $Y$.

\paragraph{Graph.}

Form an $N \times N$ Gram matrix $A$, where $A_{ij} = K(Z_i, Z_j)$ for \textbf{kernel function} $K$. Let $G = (V,E)$ with $V = \set{Z_1, \ldots, Z_n}$ and $E = \set{A_{ij}: 1 \leq i < j \leq n}$. Take $L = D - A$ to be the (unnormalized) \textbf{Laplacian matrix} of $A$ (where $D$ is the diagonal degree matrix with $D_{ii} = \sum_{j \in [n + m]} A_{ij}$). Let $M$ be the number of non-zero entries of $A$. Denote by $B$ the $M \times N$ \textbf{incidence matrix} of $A$, where we denote the $i$th row of $B$ as $B_i$ and set $B_i$ to have entry $A_{ij}$ in position $i$, $-A_{ij}$ in position $j$, and $0$ everywhere else. 

\paragraph{Test statistics.}

We define our \textbf{laplacian smooth} test statistic. 
\begin{equation*}
T_2 = \left(\max_{\theta: \norm{B\theta}_2 \leq 1} a^T \theta \right)^2 = a^T \Linv a. 
\end{equation*}

\paragraph{Distances between probability measures.}

An \textbf{integral probability metric} (IPM) with respect to a function class $\F$ is defined
\begin{equation*}
\underset{f \in \F}{\sup} ~ \Expect{f(X)} - \Expect{f(Y)}
\end{equation*}
for $X \sim P$, $Y \sim Q$. 

Hereafter, we will assume $P$ and $Q$ are absolutely continuous with respect to Lebesgue measure, with density functions $p$ and $q$, respectively. Denote the \textbf{mixture density} by $\mu = \frac{p + q}{2}$. 

Denote the \textbf{gradient} of a function $f$ by $\nabla_x$. Then we can define the \textbf{Sobolev semi-norm} and \textbf{dot product}, $\norm{f}_{W_0^{1,2}(\X, \mu^2)}$ and $\dotp{f}{g}_{W_0^{1,2}(\X, \mu^2)}$, by
\begin{equation*}
\dotp{f}{g}_{W_0^{1,2}(\X, \mu)} = \int_{\X} \dotp{\nabla_x f(x)}{\nabla_x g(x)}_{\Rd} \mu^2(x), ~~ \norm{f}_{W_0^{1,2}(\X, \mu)} = \sqrt{\int_{\X} \norm{\nabla_x f(x)}^2 \mu^2(x) dx }
\end{equation*}

Let the \textbf{Sobolev space}, $W^{1,2}(\mathcal{X}, \mu^2)$, be 
\begin{equation*}
W^{1,2}(\mathcal{X},\mu^2) = \set{f: \X \to \Reals, \int_{\X} \norm{\nabla_x f(x)}^2 \mu^2(x) dx < \infty}.
\end{equation*}
and denote by $W_0^{1,2}(\mathcal{X}, \mu^2)$ the restriction of $W^{1,2}(\mathcal{X}, \mu^2)$ to functions which vanish at the boundary of $\X$. Note that $\norm{f}_{W_0^{1,2}(\X, \mu^2)}$ defines a semi-norm over $W_0^{1,2}(\X, \mu^2)$. Finally, let $B_W(\X, \mu^2)$ be the \textbf{unit ball} of $W_0^{1,2}(\X, \mu^2)$, meaning
\begin{equation*}
B_W(\X, \mu^2) = \set{f \in W_0^{1,2}(\X, \mu^2): \norm{f}_{W_0^{1,2}(\X, \mu^2)} \leq 1}
\end{equation*}

Now we can define the \textbf{Sobolev IPM}, $\mathcal{S}_{\mu^2}(P,Q)$ It is simply an IPM where the function class is the Sobolev unit ball with respect to $\mu^2$.
\begin{equation*}
\mathcal{S}_{\mu^2}(P,Q) \overset{\mathrm{def}}{=} \sup_{f \in B_W} \biggl\{ \Expect{f(X)} - \Expect{f(Y)}\biggr\}
\end{equation*}

We will show that the Laplacian constraint $\norm{B \theta}_2 \leq 1$ is very similar to the constraint $f_{\theta} \in B_W(X,\mu^2)$ for the right choice of $K$, over all Holder functions. 

\paragraph{Holder functions}
For mapping $f: \Rd \to \Reals$ and $\beta$ a positive integer, we say $f$ is a \textbf{$\beta$-Holder function} if there exists $C > 0$ such that for all $x, y \in \X$
\begin{equation*}
\abs{f^{(\beta - 1)}(x) - f^{(\beta - 1)}(y)} \leq K \norm{x - y}
\end{equation*}
Roughly speaking, this means the functions have bounded $\beta$ partial derivatives.

\section{DESIRED RESULTS}

\begin{theorem}
	\label{thm:convprob_of_T2}
	For bandwidth parameter $h >0$ and decreasing function $k(\cdot,\cdot)$, write
	\begin{equation*}
	K(Z_i,Z_j) = \frac{1}{h^m} k(\norm{Z_i - Z_j}^2 / h^2).
	\end{equation*}
	
	For Sobolev IPM $\mathcal{S}_{\mu^2}(P,Q)$ as defined above,
	\begin{equation*}
	\sqrt{T_2} \convprob \mathcal{S}_{\mu^2}(P,Q)
	\end{equation*}
\end{theorem}

\begin{proof}[Proof attempt of Proposition \ref{thm:convprob_of_T2}]
	Recall that, for incidence matrix $B$, 
	\begin{equation*}
	\sqrt{T_2} = \left(\max_{\theta: \norm{B\theta}_2 \leq 1} a^T \theta \right).
	\end{equation*}
	
	We expand $\abs{\sqrt{T_2} - \mathcal{S}_{\mu^2}(P,Q)}$,
	\begin{align}
	\label{eqn: bias_variance_expansion}
	\abs{\sqrt{T_2} - \mathcal{S}_{\mu^2}(P,Q)} & \leq \abs{\max_{\theta: \norm{B\theta}_2 \leq 1} \bigl\{a^T \theta \bigr\} - \sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} } \nonumber \\
	& + \abs{\sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} - \sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}(f) - \mathbb{Q}(f) \right\} }
	\end{align}
	
	\textcolor{red}{(The following statement would hold only if Proposition \ref{prop: sobolev_donsker} held over $B_W(\X, \mu^2)$, rather than over $B_W([0,1], \lambda)$ for $\lambda$ Lebesgue measure.)}
	
	By Proposition \ref{prop: sobolev_donsker}, the second term in the summand on the right hand side of (\ref{eqn: bias_variance_expansion}) is $o_P(1)$.
	
	\textcolor{red}{(The following statement would hold only if Proposition \ref{prop: convprob_regularization_functional} were uniform over $B_W(\X, \mu^2)$ rather than over the class of $\alpha$-Holder functions $\F_{\alpha}$)}
	
	Then, Proposition \ref{prop: convprob_regularization_functional} implies that for any $\epsilon > 0$, there exists $N$ such that for $n \geq N$, 
	\begin{equation*}
	\sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} - \max_{\theta: \norm{B\theta}_2 \leq 1} \bigl\{a^T \theta \bigr\} \leq \epsilon
	\end{equation*}
	with high probability. 
	
	To complete the proof, we will have to show that for any $\epsilon > 0$, there exists $N$ such that for $n \geq N$, 
	\begin{equation*}
	\max_{\theta: \norm{B\theta}_2 \leq 1} \bigl\{a^T \theta \bigr\} - \sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} \leq \epsilon
	\end{equation*}
	with high probability.
	
	
\end{proof}

\section{SUPPLEMENTAL RESULTS}

\paragraph{Empirical process over Sobolev classes.}

The following theorem is a stand-in; it handles only functions with domain on the unit interval, and is stated specifically with respect to Lebesgue measure.
\begin{proposition}
	\label{prop: sobolev_donsker}
	Let $\F$ be the set of all absolutely continuous functions $f: [0,1] \to \Reals$ such that $\norm{f}_{\infty} \leq 1$ such that $\int (f'(x))^2 dx \leq 1$. Then, there exists a constant $K$ such that for every $\epsilon > 0$,
	\begin{equation*}
	\log N_{[]}(\epsilon, \mathcal{F}, \norm{\cdot}_{\infty}) \leq K\left(\frac{1}{\epsilon}\right).
	\end{equation*} 
	
	Thus, the class $\mathcal{F}$ is $P$-Donsker (and $P$-Glivenko-Cantelli) for all $P$.
\end{proposition}

\paragraph{Regularization functional.}

\begin{proposition}
	\label{prop: convprob_regularization_functional}
	Let $\mathcal{F}_{\alpha}$ be a unit ball in the space of $\alpha$-Holder functions, and define $k(\cdot,\cdot)$ as in Theorem \ref{thm:convprob_of_T2}. For function $f \in \F_{\alpha}$, denote $f$ evaluated on the data, $\mathbf{f} = (f(Z_1), \ldots, f(Z_N))$. Then, there exists a constant $c$ depending only on $k$ such that for $\alpha \geq 3$ and a sequence $(h_n) \to 0$ such that
	\begin{equation*}
	\sup_{f \in \mathcal{F}_{\alpha}} \abs{ \norm{B \mathbf{f}_2} - \norm{f}_{W_0^{1,2}(\X, \mu^2)} } \convprob 0
	\end{equation*}
\end{proposition}





\section{PROOFS}



\end{document}