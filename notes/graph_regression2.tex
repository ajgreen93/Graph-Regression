\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{tikz}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}
\newcommand{\Lap}{{\bf L}}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Minimax-optimal nonparametric regression with graph Laplacians}
\author{Alden Green}
\date{\today}
\maketitle

\section{Introduction}
\label{sec:introduction}

In the random design nonparametric regression problem, we observe data $(X_1,Y_1),\ldots,(X_n,Y_n)$, where $X_1,\ldots,X_n$ are independent samples from a distribution $P$ with support $\Xset \subset \Rd$, and 
\begin{equation}
\label{eqn:random_design_regression}
Y_i = f_0(X_i) + \varepsilon_i
\end{equation}
with independent Gaussian noise $\varepsilon_i \sim N(0,1)$. Our goal is to perform statistical inference on the unknown regression function $f_0: \Xset \to \Reals$, by which we mean either (a) \emph{estimating} $f_0$ by $\wh{f}$, an estimator constructed from the data $(X_1,Y_1),\ldots,(X_n,Y_n)$ or (b) simply \emph{testing} whether or not $f_0 = 0$, i.e whether or not there is a signal present. 

In graph based nonparametric regression, the idea is to perform the above inferential tasks by first building a neighborhood graph $G_{n,r}$ which captures the geometry of $P$ and $\mc{X}$ in an appropriate sense, and then constructing an estimate $\wh{f}$ which is smooth with respect to the graph $G_{n,r}$. For a kernel function $K: [0,\infty) \to \Reals$ and connectivity radius $r > 0$, the neighborhood graph $G_{n,r} = ([n],{\bf W})$ is a weighted, undirected graph on vertices $[n] = \{1,...,n\}$, which we associate with the samples $\{X_1,\ldots,X_n\}$. The weight matrix ${\bf W} = ({\bf W}_{ij})_{ij}$ is an $n \times n$ matrix with entries
\begin{equation*}
\label{eqn:neighborhood_graph}
{\bf W}_{ij} = K\Biggl(\frac{\norm{X_i - X_j}_{\Rd}}{r}\Biggr),
\end{equation*}
and the degree matrix ${\bf D}$ is the $n \times n$ diagonal matrix with entries ${\bf D}_{ii} = \sum_{j = 1}^{n}{\bf W}_{ij}$.  Our star operator, the neighborhood graph Laplacian, can then be written as
\begin{equation}
\label{eqn:graph_Laplacian}
\Lap_{n,r} = \bf{D} - \bf{W}
\end{equation}
The two graph-based regression methods we will consider are \emph{Laplacian eigenmaps} and \emph{Laplacian smoothing}. Each method uses $\Lap_{n,r}$ in a different manner to construct an estimator of $f_0$. Letting $v_1(G_{n,r}),\ldots,v_n(G_{n,r})$ denote eigenvectors of $\Lap_{n,r}$, the Laplacian eigenmaps estimator $\wh{f}_{\LE}$ \citep{belkin2003} is given by 
\begin{equation}
\label{eqn:laplacian_eigenmaps}
\wh{f}_{\textrm{LE}} = \sum_{k = 1}^{\kappa} \Dotp{Y}{v_k(G_{n,r})}_n v_{k}(G_{n,r})
\end{equation}
where $\kappa \in \{1,...,n\}$ is a tuning parameter. (Here $\dotp{u}{v}_n = \frac{1}{n}\sum_{i = 1}^{n} u_i v_i$ for vectors $u,v \in \Reals^n$.) The Laplacian smoothing estimator $\wt{f}_{\LS}$ \citep{smola2003} is a penalized least square estimator, given by
\begin{equation}
\label{eqn:laplacian_smoothing}
\wt{f}_{\LS}= \min_{f \in \Reals^n} \Bigl\{\sum_{i = 1}^{n}(Y_i - f_i)^2 + \rho \cdot f^T \Lap_{n,r}^s f \Bigr\}
\end{equation}
where $\rho > 0$ acts a tuning parameter $f^T \Lap_{{n,r}}^s f$. Then, assuming~\eqref{eqn:laplacian_eigenmaps} and~\eqref{eqn:laplacian_smoothing} are reasonable estimators of $f_0$, the squared empirical norms
\begin{equation}
T_{\LE} = \Bigl\|\wh{f}_{\LE}\Bigr\|_n^2 \label{eqn:laplacian_eigenmaps_test}
\end{equation}
and
\begin{equation}
T_{\LS} = \Bigl\|\wt{f}_{\LS}\Bigr\|_n^2 \label{eqn:laplacian_smoothing_test}
\end{equation}
are in turn reasonable statistics to test whether or not $f_0 = 0$.

\begin{itemize}
	\item \textbf{Motivate graph Laplacians.} There already exist a vast number of methods for both the nonparametric testing and, especially, the nonparametric estimation problems. In fact, as we will see in Section~\ref{sec:problem_setup_and_background} some of these methods are actually quite analogous to the graph Laplacian estimation and testing methods we have just introduced. Nevertheless, methods such as~\eqref{eqn:laplacian_eigenmaps}-\eqref{eqn:laplacian_smoothing_test} have a deserved place in the nonparametric regression toolbox. Their advantages include:
	\begin{itemize}
		\item Simplicity.
		\item Computational efficiency and stability.
		\item Domain and density adaptivity.
		\item Easily generalizable.
		\item Practical success.
	\end{itemize}
	\item \textbf{Our contributions, short version.} For these reasons, there is a  substantial body of work assessing the statistical properties of graph Laplacian based methods. \textcolor{red}{(TODO): Include citations.} In this paper, we adopt a different perspective, and analyze~\eqref{eqn:laplacian_eigenmaps}-\eqref{eqn:laplacian_smoothing_test} through the minimax lens. We show that when the regression function $f_0$ is suitably smooth (by which we mean that it belongs to a first-order Sobolev ball; roughly speaking, that it possesses weak derivative $Df$ bounded in $L^2(\Xset)$ norm) both~\eqref{eqn:laplacian_eigenmaps} and~\eqref{eqn:laplacian_smoothing} are minimax optimal estimators, and both~\eqref{eqn:laplacian_eigenmaps_test} and~\eqref{eqn:laplacian_smoothing_test} are minimax optimal test statistics. We also show that under appropriate conditions, these estimators and test statistics can leverage additional assumed structure, either on the regression function $f_0$ or distribution $P$.
	\item \textbf{Organization.}
\end{itemize}

\paragraph{Notation.}
\begin{itemize}
	\item We write $\mathbb{Z}^d$ for $\mathbb{Z} \otimes \cdots \mathbb{Z}$, where the tensor product is taken a total of $d$ times.
	\item We denote by $\Leb^2(\Xset)$ be the set of square integrable functions $f:\Xset \to \Reals$.
	\item For functions $f,g$ which map from $\Xset$ to $\Reals$, we write $\dotp{f}{g}_n := \frac{1}{n}\sum_{i = 1}^{n} f(X_i) g(X_i)$, which we refer to as the \emph{empirical inner product}, and $\norm{f}_n^2 = \dotp{f}{f}_n$, which we refer to as the (squared) \emph{empirical norm}. For vectors $v \in \Reals^n$, we write $\norm{v}_p^p := \sum_{i = 1}^{n} \abs{v_i}^p$.
	\item For a graph $G$, we use $\Lap_G$ for the Laplacian of $G$. The matrix $\Lap_G$ is a square, symmetric, positive semi-definite matrix, with $n$ eigenvalues $\lambda_k(G)$ and eigenvector $v_k(G)$ pairs, defined according to
	\begin{equation*}
	\Lap_G v_k(G) = \lambda_k(G) v_k(G), ~~\frac{1}{n} \norm{v_k(G)}_2^2 = 1 
	\end{equation*}
	and ordered so that $0 \leq \lambda_1(G) \leq \ldots \leq \lambda_n(G)$. For convenience, we write $\Lap_{n,r}$ instead of $\Lap_{G_{n,r}}$. 
\end{itemize}

\section{Problem Setup and Background}
\label{sec:problem_setup_and_background}

Before we get to our main results, we will review some of the relevant literature on minimax optimal nonparametric estimation and testing of Sobolev functions. We will also briefly recall some (of the many) estimators and tests which achieve these rates: in particular the \emph{truncated series} and \emph{smoothing spline} methods. As we will see, Laplacian eigenmaps and Laplacian smoothing can be seen as graph-based, i.e. discrete, analogues to these ``continuuum'' methods.

\subsection{Sobolev spaces}
\label{subsec:sobolev_spaces}

\begin{itemize}
	\item \textbf{Sobolev spaces.}  Roughly speaking, Sobolev spaces contain functions $f \in \Leb^p(\Xset)$ with derivatives which themselves belong to $\Leb^p(\Xset)$. More formally, for given integers $s, p > 0$, the Sobolev space $W^{s,p}(\Xset)$ consists of all functions $f \in \Leb^p(\Xset)$ such that for each multiindex $\alpha = (\alpha_1,\ldots,\alpha_d) \in \mathbb{N}^d$ satisfying $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ belongs to $\mathcal{L}^p(\Xset)$. The Sobolev-$\{s,p\}$ seminorm $\seminorm{f}_{W^{s,p}(\Xset)}$ and norm $\norm{f}_{W^{s,p(\Xset)}}$ are then given by 
	\begin{equation*}
	\seminorm{f}_{W^{s,p}(\Xset)}^p = \sum_{\abs{\alpha} = s}\int_{\mathcal{X}} \Bigl|\bigl(D^{\alpha}f\bigr)(x)\Bigr|^p \,dx, ~~ \norm{f}_{W^{s,p}(\Xset)}^p = \sum_{k = 0}^{s} \seminorm{f}_{W^{k,p}(\Xset)}^p
	\end{equation*}
	and for a given $M > 0$, the corresponding ball is $W^{s,p}(\Xset, M) = \set{f: \norm{f}_{W^{s,p}(\Xset)} \leq M}$. In the special case when $p = 2$, the Sobolev space $W$ is a Hilbert space, and we adopt the usual convention of writing $H^s(\Xset) = W^{s,p}(\Xset)$ and $H^s(\Xset,M) = W^{s,2}(\Xset,M)$. We will confine our attention to this special case hereafter. 
	\item \textbf{Connections to sequence spaces.} The balls $H^s(\Xset,M)$ are closely connected with the \emph{Sobolev ellipsoids} $\Theta_s(M) \subseteq \ell^2(\mathbb{Z}^d)$, defined as
	\begin{equation*}
	\Theta_s(M) = \Bigl\{\theta \in \ell_2(\mathbb{Z}^d): \sum_{k \in \mathbb{Z}^d} \theta_k^2 \abs{k}^{2s} < M\Bigr\}
	\end{equation*}
	Let us assume that the domain $\Xset = [0,1]^d$. Letting be $\set{\phi_k}_{k \in \mathbb{Z}^d}$ be the tensor product trigonometric polynomial basis of $\Leb^2(\Xset)$, the space
	\begin{equation*}
	H_{\textrm{per}}^s(\Xset,M) = \Bigl\{\sum_{k \in \mathbb{Z}^d} \theta_k \phi_k: \theta_k \in \Theta_s(M) \Bigr\}
	\end{equation*}
	satisfies $H_{\textrm{per}}^s(\Xset,M) \subseteq H^s(\Xset,M')$ for a sufficiently large value of $M_1 > 0$. On the other hand, any $1$-periodic function $f \in W^{s,p}(\Xset,M)$ also belongs to $H_{\textrm{per}}^s(\Xset,M_0)$ for a sufficiently large value of $M_0 > 0$.
	\item \textbf{Zero-trace Sobolev spaces.} We will also be interested in the \emph{zero-trace} Sobolev spaces.
\end{itemize}

\subsection{Orthogonal series and smoothing splines.}
\label{subsec:continuum_methods}
When the regression function is assumed to belong to a Sobolev space, two approaches particularly well-suited for nonparametric regression are orthogonal series \textcolor{red}{projection}, and smoothing splines. The orthogonal series estimator $\wh{f}_{\OS}$ is given by
\begin{equation}
\label{eqn:orthogonal_series}
\wh{f}_{\OS} = \sum_{k: \abs{k} \leq \kappa} \dotp{Y}{\phi_k}_n \phi_k .
\end{equation}
where $\kappa \geq 0$ is a hyperparameter, and we recall that $\{\phi_k\}_{k \in \mathbb{Z}^d}$ is the tensor product trigonometric polynomial basis of $\Leb^2(\Xset)$. The smoothing spline estimator $\wt{f}_{\SM}$ is a variational estimator, defined as the solution to the following optimization problem,
\begin{equation}
\label{eqn:smoothing_spline}
\wt{f}_{\SM} = \argmin_{f} \sum_{i = 1}^{n} \bigl(Y_i - f(X_i)\bigr)^2 + \seminorm{f}_{H^{s}(\Xset)}^2
\end{equation}
where the minimum is taken over all $f \in H^s(\Xset)$.

\begin{itemize}
	\item \textbf{Connecting orthogonal series and Laplacian eigenmaps estimators.}The orthogonal series estimator $\wh{f}_{\OS}$ is closely connected with the Laplacian eigenmaps estimator $\wh{f}_{\LE}$. At a high-level, one can go from~\eqref{eqn:orthogonal_series} to~\eqref{eqn:laplacian_eigenmaps} simply by substituting $\{v_k\}_{k \in [n]}$ for $\{\phi_k\}_{k \in \mathbb{Z}^d}$. More deeply, just as the vectors $v_k$ are eigenvectors of the graph Laplacian $\Lap_{n,r}$, so too are the functions $\phi_k$ eigenfunctions of a particular differential operator $\Delta: C^2(\Xset) \to \Leb^2(\Xset)$, known as the \emph{Laplace} operator and formally defined as
	\begin{equation}
	\label{eqn:laplace_operator}
	\Delta g := -\mathrm{div}(\nabla g).
	\end{equation}
	In fact, it has been shown in \textcolor{red}{(references)} that when the radius $r \to 0$ as $n \to \infty$, the graph Laplacian $\Lap_{n,r}$ converges in an appropriate sense to a weighted version $\Delta_{P}$ of $\Delta$, with
	\begin{equation}
	\Delta_{P} g := -\frac{1}{p}\mathrm{div}(p^2 \nabla g).
	\end{equation}
	Along similar lines, \textcolor{red}{(references)} have established that for any fixed $k \geq 0$, as $n \to \infty$ the eigenvector $v_k$ will converge, again in an appropriate sense, to an eigenfunction $\phi_k$ of $\Delta_{P}$. The upshot is that when $P$ is the uniform measure, for an appropriate sequence of radii $r(n) \to 0$ and any fixed $\kappa \geq 0$ the estimator $\wh{f}_{OS}$ is in fact the limit of $\wh{f}_{\LE}$, in the sense that
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_consistency}
	\abs{\wh{f}_{\LE}(X_i) - \wh{f}_{\OS}(X_i)} \overset{P}{\to} 0~~\textrm{as $n \to \infty$.}
	\end{equation} 
	for each $i \in \mathbb{N}$.
	\item 
	\textbf{Connecting smoothing spline and Laplacian smoothing estimators.} In a similar vein, the smoothing spline estimator $\wh{f}_{\SM}$ has deep connections with the Laplacian smoothing estimator $\wh{f}_{\LS}$. Using the definition~\eqref{eqn:laplace_operator}, recursively applying integration by parts, and then invoking the aforementioned convergence properties of $\Lap_{n,r}$, it can be shown that \textcolor{red}{references} that for any $f \in C^{2s}(\Xset)$,
	\begin{equation}
	\label{eqn:pointwise_seminorm_convergence}
	\abs{f}_{H^s(\Xset)} = \int_{\Xset} \bigl(\Delta^sf\bigr)(x) f(x) \,dx = \lim_{n \to \infty} f^T \Lap_{n,r}^s f.
	\end{equation}
	By applying appropriation concentration inequalities, one can extend~\eqref{eqn:pointwise_seminorm_convergence} to hold uniformly over all $f \in H^{2s}(\Xset)$. While this does not imply a formal statement about the convergence of $\wh{f}_{\LS}$ to $\wh{f}_{SM}$ as in~\eqref{eqn:laplacian_eigenmaps_consistency}---note the difference between the domain of minimization in~\eqref{eqn:smoothing_spline} and the regularity required for~\eqref{eqn:pointwise_seminorm_convergence}---it nevertheless makes clear that the estimators $\wh{f}_{\LS}$ and $\wh{f}_{\SM}$ are linked.
	\item \textbf{Domain and density adaptivity.} \textcolor{red}{(TODO)}
\end{itemize}

\subsection{Minimax rates over Sobolev classes}
\label{subsec:minimax_rates}
One reason all of this is intriguing is that, as we mentioned previously, the continuum estimators $\wh{f}_{\OS}$ and $\wh{f}_{\SM}$---and test statistics based on functionals of these estimators--- have strong statistical properties. In particular, these estimators and tests are known to have error rates within a constant factor of (minimax) optimal.

\paragraph{Estimation rates.}
Under appropriate regularity conditions, the minimax estimation rate over Sobolev balls is 
\begin{equation}
\label{eqn:sobolev_space_estimation_minimax_rate}
\inf_{\wh{f}} \sup_{f_0 \in \mc{H}^s(M)} \Ebb\Bigl[\norm{\wh{f} - f_0}_{L^2(\Xset)}^2\Bigr] \asymp n^{-2s/(2s + d)}~~\textrm{for all $s$ and $d \geq 0$,}
\end{equation}
where the infimum is over all measurable maps $\wh{f}(X_1,Y_1,\ldots,X_n,Y_n)$, and the function class $\mc{H}^s(M)$ is one of $H^s(\Xset,M)$, $H_{\mathrm{per}}^s(\Xset,M)$ or $H_0^s(\Xset,M)$. Orthogonal series and smoothing spline estimators are two (of the many) estimators which certify the upper bound in~\eqref{eqn:sobolev_space_estimation_minimax_rate}. Concretely, if we are willing to assume
\begin{enumerate}[label=(P\arabic*)]
	\item
	\label{asmp:uniform_density}
	the distribution $P$ is the uniform probability measure defined on domain $\Xset = [0,1]^d$.
\end{enumerate}
then the orthogonal series estimator is minimax optimal over the Sobolev ellipsoids $H_{\mathrm{per}}^s(M)$, meaning
\begin{equation*}
\sup_{f \in H_{\mathrm{per}}^s(\Xset,M)}\Ebb\Bigl[\norm{\wh{f}_{\OS} - f}_{L^2(\Xset)}^2\Bigr]  \lesssim n^{-2s/(2s + d)}~~\textrm{for all $s$ and $d \geq 0$.}
\end{equation*}
An advantage of $\wt{f}_{\SM}$ over $\wh{f}_{\OS}$ is that it requires less stringent assumptions on the design density in order to be minimax optimal. Instead of Assumption~\ref{asmp:uniform_density}, we will merely assume that the density is bounded above and below.
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{1}
	\item 
	\label{asmp:bounded_density}
	The distribution $P$ is defined on domain $\Xset = [0,1]^d$, and admits a density $p$ with respect to the uniform (Lebesgue) measure on $[0,1]^d$. The density $p$ is bounded away from $0$ and $\infty$, that is
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty
	\end{equation*}
	for all $x \in [0,1]^d$.
\end{enumerate}
The assumption~\ref{asmp:bounded_density} is itself a standard but significant assumption; we will return to discuss it further at \textcolor{red}{a later point}.
Assuming~\ref{asmp:bounded_density}, the estimator $\wt{f}_{\SM}$ satisfies
\begin{equation*}
\sup_{f_0 \in H^s(\Xset,M)}\Ebb\Bigl[\norm{\wh{f}_{\SM} - f_0}_{L^2(\Xset)}^2\Bigr]  \lesssim n^{-2s/(2s + d)}~~\textrm{for all $2s > d$.}
\end{equation*}
The restriction $2s > d$ is a severe limitation of smoothing splines relative to  orthogonal series. It is worth emphasizing that this is a fundamental defect of the smoothing spline estimator, as opposed to any flaw in the theory; when $2s \leq d$, the optimization problem~\eqref{eqn:smoothing_spline} is not even well-defined! \textcolor{red}{(TODO):} succinctly explain why.

\paragraph{Goodness-of-fit testing rates.} 

\begin{itemize}
	\item \textbf{Minimax hypothesis testing.} In the nonparametric regression hypothesis testing problem, we ask for a test function (formally, a Borel measurable map $\phi: \mc{X}^n \times \Reals^n \to \{0,1\}$) which can distinguish between the null and alternative hypotheses
	\begin{equation}
	\mathbf{H}_0: f_0 = 0, ~~\textrm{versus}~~ \mathbf{H}_a: f_0 \neq 0.
	\end{equation}
	We measure the performance of a test $\phi$ by its Type I error $\Ebb_0(1 - \phi)$, and Type II error $\mathbb{E}_{f_0}(\phi)$. It is well known that no test $\phi$ with controlled Type I error---say, $\Ebb_0(\phi) \leq 1/b$ for some constant $b > 1$---
	can have controlled Type II error---$\Ebb_{f_0}(\phi) \leq 1/b$--- for every $f_0 \neq 0$. As a result, it is standard to impose two kinds of restrictions on the alternative hypothesis class $\mathbf{H}_a$. The first is a smoothness condition on $f_0$: specifically, we will assume that $f_0$ belongs to a Sobolev ball $\mc{H}^s(M)$. The latter is a separation condition, enforcing a minimum distance between $f_0$ and $0$: specifically, we will insist that $\norm{f_0  - 0}_{\Leb^2(\Xset)} > \epsilon$ for some $\epsilon > 0$.
	\item\textbf{Minimax testing rates over Sobolev classes.} Once smoothness and separation conditions are established, the following becomes a natural question to ask: what is the smallest value of $\epsilon$ for which there exists a test $\phi$ with both small Type I error, and small Type II error over all functions $f_0$ which are smooth and separated from $0$ by at least $\epsilon$? This smallest separation $\epsilon$ is known as the critical radius, and in the case of the Sobolev classes it is given by
	\begin{equation*}
	\epsilon_n^{\star}\bigl(\mc{H}^s(M)\bigr):= \inf\biggl\{\epsilon: \inf_{\phi} \Bigl\{\Ebb_0(\phi) +  \sup_{\substack{f_0 \in \mc{H}^s(M) \\ \norm{f_0}_{\Leb^2(\Xset)} \geq \epsilon}} \Ebb_{f_0}[1 - \phi]\Bigr\} \leq \frac{2}{b} \biggr\} \asymp n^{-2s/(4s + d)}
	\end{equation*}
	for any $4s > d$, and $\mc{H}^s(M)$ one of $H^s(\Xset,M)$, $H_{\mathrm{per}}^s(\Xset,M)$ or $H_0^s(\Xset,M)$. On the other hand, when $4s \leq d$ the minimax rate is dimension-free,
	\begin{equation*}
	\epsilon_n^{\star}\Bigl(\mc{H}^s(M)\Bigr) \asymp n^{-1/4},
	\end{equation*}
	revealing a distinction between the estimation and goodness-of-fit testing problems.
	\item \textbf{Error rates of orthogonal series and smoothing spline-based tests.} 
	Just as $\wh{f}_{\OS}$ and $\wh{f}_{\SM}$ achieve minimax optimal estimation rates, tests using the plug-in statistics
	\begin{equation*}
	T_{\OS} = \bigl\|\wh{f}_{\OS}\bigr\|_{\Leb^2(\Xset)}^2,~~\textrm{and}~~T_{\SM} = \bigl\|\wt{f}_{\SM}\bigr\|_{n}^2
	\end{equation*}
	are minimax optimal for the goodness-of-fit testing problem over Sobolev balls (see \citet{ingster2009} for analysis of the former statistic, and \citet{liu2019} for analysis of the latter).
\end{itemize}

\subsection{Our contributions}
\label{subsec:our_contributions}

\begin{itemize}
	\item \textbf{Departure from what's already known.} At a high level, the connections between the graph-based methods~\eqref{eqn:laplacian_eigenmaps}-\eqref{eqn:laplacian_smoothing_test} and the continuum methods outlined in Subsection~\ref{subsec:continuum_methods} give reason to believe that the strong statistical properties of the latter are shared by the former. Making this rough statement precise is the content of this paper. In particular, we will prove the following results:
	\begin{itemize}
		\item \textbf{Minimax optimal estimation.}
		We show that with high probability, $\norm{\wh{f}_{\LE} - f_0}_{n}^2 \lesssim n^{-2s/(2s + d)}$ over all $f_0 \in H^1(\Xset;M)$. The same conclusions hold with respect to $\wt{f}_{\LS}$ when $d \leq 4$. 
		\item \textbf{Minimax optimal testing.}
		We show that with high probability, a test constructed using $T_{\LE}$ has non-trivial power whenever $f_0 \in H^1(\Xset;M)$ satisfies $\norm{f_0}_{\Leb^2(\Xset)} \gtrsim \max\{n^{-2s/(4s + d)},n^{-1/4}\}$. \textcolor{red}{The same conclusions hold with respect to $\wt{f}_{LS}$.}
		\item \textbf{Taking advantage of higher-order smoothness conditions.} Under appropriate boundary conditions on the function classes $H^s(\Xset)$ and additional smoothness conditions on the density $p$, when $s > 1$  each of the aforementioned conclusions hold for certain values of $d$.
		\item \textbf{Manifold adaptivity.}
		If $\mc{X} \subset \Rd$ is a submanifold of dimension $m < d$, each of the aforementioned rates hold with $d$ replaced by $m$.
	\end{itemize}	
\end{itemize}

\section{Graph Sobolev classes}
\label{sec:graph_sobolev_classes}

\begin{itemize}
	\item For $s,M > 0$, define the order-$s$ graph Sobolev ball to be
	\begin{equation}
	\label{eqn:graph_sobolev_ball}
	H^s(G_{n,r},M) = \set{f \in \Reals^n: f^T \Lap_{n,r}^s f \leq M}
	\end{equation}
	where we recall that the \emph{graph Sobolev seminorm} $f^T \Lap_{{n,r}}^s f$ is precisely the penalty term in~\eqref{eqn:laplacian_smoothing}. 
	To understand the relevance of the graph Sobolev ball to the analysis of our graph-based procedures, let us reason again by analogy with the continuum world. The statistical analysis of continuum-time methods such as Fourier series projection and smoothing splines rests on two facts about the continuum classes $\mc{H}^s(\Xset)$: first, the \textit{a priori} assumption that the regression function $f_0$ belongs to an appropriate Sobolev ball $\mc{H}(\Xset,M)$, and second that the asymptotic decay of the eigenvalues of the continuum Laplace operator $\Delta$ obey Weyl's law, meaning $\lambda_k(\Delta) \asymp \abs{k}^{2s}$ for $k \in \mathbb{Z}^d$. In this section, we will show that both these statements have appropriate analogues in the graph setting. Roughly speaking, we will show first that for functions $f$ belonging to $\mc{H}^s(\Xset, M)$, the evaluations $f = (f(X_1),\ldots,f(X_n)) \in \Reals^n$ satisfy the upper bound
	\begin{equation}
	\label{eqn:graph_sobolev_seminorm_asymptotic}
	f^T \Lap_{n,r}^s f \lesssim M(n^{s + 1}r^{s(d + 2)})
	\end{equation}
	and second that the eigenvalues $\lambda_k(G_{n,r})$ satisfy the lower bound
	\begin{equation*}
	\bigl[\lambda_k(G_{n,r})\bigr]^s = \lambda_k(\Lap_{{n,r}}^s) \gtrsim k^{2s/d} (n^s r^{s(d + 2)});
	\end{equation*}
	both statements will hold with high probability. These results elucidate why our graph-based procedures ``work'' as well as their continuum counterparts---in the sense of being statistically rate-optimal over (continuum) Sobolev classes. They also clarify when our analysis does and does not apply---in terms of various degrees of smoothness $s$ and dimensions $d$.
\end{itemize}

\subsection{Graph Sobolev seminorm}
We will state and prove three different results involving the graph Sobolev semi-norm $f_0 \Lap_{n,r}^s f_0$. In each case, we will derive that the asymptotic relation~\eqref{eqn:graph_sobolev_seminorm_asymptotic} holds with high probability, whenever $r \to 0$ sufficiently slowly as $n \to \infty$. The difference between the three results will be the assumptions we make, and the resulting rate of permissible decay for the radius $r$. 

\subsubsection{First-order graph Sobolev semi-norm.}
We begin by upper bounding $f_0^T \Lap_{n,r} f_0$ under the assumption $f_0 \in H^1(\Xset,M)$. This will be the easiest of the three Lemmas to prove, while still giving some sense of how the proofs develop in the more complicated cases. The upper bound will depend on the kernel $K$; we assume throughout that $K:[0,\infty) \to [0,\infty)$ is a $K_{\max}$-Lipschitz function supported on $[0,1]$, and without loss of generality that
\begin{equation*}
\int_{\Reals^d} K(\norm{z}) \,dz = 1.
\end{equation*}
\begin{lemma}
	\label{lem:first_order_graph_sobolev_seminorm}
	Suppose $\Xset$ is a Lipschitz domain, and that $p(x) \leq p_{\max}$ for all $x \in \Xset$. Then, for any $f \in H^1(\Xset,M)$, there exists a constant $C > 0$ which may depend only on $\Xset$ and $d$ such that
	\begin{equation}
	f^T \Lap_{n,r} f \leq \frac{C}{\delta} n^2 r^{d + 2} M^2
	\end{equation}
	with probability at least $1 - \delta$.
\end{lemma}

\subsubsection{General graph Sobolev semi-norm}

When $s > 1$, in order to obtain the desired upper bound on the graph Sobolev semi-norm we must make some additional assumptions regarding the behavior of $f_0$ near the boundary of $\Xset$, as well as on the smoothness of the density function $p$.
\begin{theorem}
	\label{thm:graph_sobolev_seminorm}
	For some integer $s > 1$, suppose that the density $p \in C^{s - 1}(\Xset,p_{\max})$ and $f \in H_0^s(\Xset,M)$, and additionally that the domain $\Xset$ is Lipschitz. Let $r = r(n)$ be a sequence of connectivity radii satisfying $r(n) \to 0$ and $r(n) \gtrsim n^{-1/(2(s - 1) +d)}$ as $n \to \infty$. Then for all $n$ sufficiently large, there exists a constant $C > 0$ which may depend only on $\Xset$ and $d$ such that
	\begin{equation}
	\label{eqn:graph_sobolev_seminorm}
	f^T \Lap_{n,r}^s f \leq \frac{C}{\delta} n^{s + 1} r^{s(d + 2)}M^2
	\end{equation}
	with probability at least $1 - \delta$.
\end{theorem}
It is worth pausing to make a few remarks.
\begin{itemize}
	\item \textbf{Bounded norm vs. bounded semi-norm.}
	\item \textbf{Markov's inequality, as opposed to exponential concentration.}
	\item \textbf{Simpler proof and stronger concentration under Holder assumption.}
	\item \textbf{Explain why~\eqref{eqn:graph_sobolev_seminorm} should be familiar, and a lower bound on $r$ expected.} The scaling (in $n$, $r$, and $M$) on the right hand side of~\eqref{eqn:graph_sobolev_seminorm} will be familiar to the informed reader. For a sufficiently regular function $f \in C^3(\Xset,M)$ and density $p \in C^1(\Xset,p_{\max})$, it is known \textcolor{red}{(references)} that $\frac{1}{nr^{d + 2}}\Lap_{n,r}f \to \Delta_pf$ for \textcolor{red}{$r \geq ???$}. \textcolor{red}{A recursive argument shows that when $f \in C^{2s + 1}(\Xset)$ and $p \in C^{2s}(\Xset)$ are sufficiently regular and $r$ is sufficiently large},
	\begin{equation*}
	\Bigl(\frac{1}{nr^{d + 2}}\Lap_{n,r}\Bigr)^s f \to \Delta_p^sf \Longrightarrow \frac{1}{n^{s + 1} r^{s(d + 2)}} f^T \Lap_{n,r}^s f \to \int_{\Xset} f(x) \cdot  \Delta_p^sf(x) p(x) \,dx \leq c M^2.
	\end{equation*}
	\item \textbf{Explain why the actual proof is more subtle.} In fact, the actual proof of Theorem~\ref{thm:graph_sobolev_seminorm} is rather more involved. Among other subtleties we highlight the mismatch, in the previous remark, between the number of smooth derivatives $f$ possesses ($2s + 1$) and the degree of the Laplacian operator $\Lap_{{n,r}}^s$ $(s)$; this is unsuitable for our purposes. A more careful analysis implies~\eqref{thm:graph_sobolev_seminorm}, but note well that~\eqref{eqn:graph_sobolev_seminorm} is only an upper bound rather than a statement about consistency. This is the price we pay for insisting that the assumed number of smooth derivatives match the degree of the Laplacian; fortunately an upper bound will be sufficient for our purposes.
\end{itemize}

\subsubsection{Graph Sobolev semi-norm under manifold assumption.}

\textcolor{red}{(TODO)}

\subsection{Graph Laplacian eigenvalues}

\textcolor{red}{(TODO)}

\section{Minimax optimal graph Laplacian methods}
\label{sec:minimax_optimal_graph_Laplacian_methods}

Let us start by assuming $f_0 \in H^1(P,M)$. In this section, we will establish that the estimators~\eqref{eqn:laplacian_eigenmaps} and~\eqref{eqn:laplacian_smoothing}, and the resulting test statistics~\eqref{eqn:laplacian_eigenmaps_test} and~\eqref{eqn:laplacian_smoothing_test}, achieve the standard minimax optimal estimation and hypothesis testing rates, respectively. Pleasingly, these results will go through with essentially no additional regularity conditions---additional, that is, to~\ref{asmp:bounded_density}---on either the regression function $f_0$ or the density $p$. The story will also be complete: we will obtain the ``right'' rates for all regimes--that is, all values of $s$ and $d$---for which we would expect to get these rates.

\subsection{Minimax optimal estimation}

Under appropriate conditions, the Laplacian eigenmaps estimator $\wh{f}_{\LE}$ achieves minimax rates over all $f_0 \in H^1(P,M)$ with high probability. The conditions we impose will be on the neighborhood graph $G_{n,r}$. As the construction of the graph is in the user's control, these assumptions are therefore mild.
\begin{enumerate}[label=(K\arabic*)]
	\item 
	\label{asmp:kernel_form}
	The kernel function $K(z) = \1\Bigl\{\norm{z}_2/r \leq 1 \Bigr\}$.
	\item 
	\label{asmp:kernel_radius_estimation}
	The radii $r = r(n)$ satisfies $r(n) \to 0$ as $n \to \infty$, and additionally
	\begin{equation*}
	n^{-1/d}\leq r(n) \leq M^{2/(2 + d)}n^{-1/(2 + d)}
	\end{equation*}
	for all $n$ sufficiently large.
\end{enumerate}

We use~\ref{asmp:kernel_form} and~\ref{asmp:kernel_radius_estimation} to ensure that the approximation error (bias) of $\wh{f}_{\LE}$ is of the correct order. On the other hand, bounding the estimation error of $\wh{f}_{\LE}$ -- both in expectation and in probability-- is straightforward. Together, bounds on these two sources of error imply Theorem~\ref{thm:laplacian_eigenmaps_estimation1}.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation1}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}. Suppose that $f_0 \in H^1(P,M)$, that $P$ satisfies~\ref{asmp:bounded_density}, and that neighborhood graph $G_{n,r}$ is computed with kernel $K$ and radius $r$ which satisfy~\ref{asmp:kernel_form} and~\ref{asmp:kernel_radius_estimation}. Then, for the Laplacian eigenmaps estimator $\wh{f}_{\LE}$ as given by~\eqref{eqn:laplacian_eigenmaps}, when $\kappa = M^{2/(2 + d)}n^{d/(2 + d)}$, the following statement holds: for all $n$ sufficiently large, there exist constants $c$ and $C$ such that
	\begin{equation*}
	\Bigl\|\wh{f}_{\LE} - f_0\Bigr\|_n^2 \leq \frac{C}{\delta} M^{2d/(4 + d)} n^{-2/(2 + d)}
	\end{equation*}
	with probability at least $1 - \delta -  Cr^{-d}\exp\{-cnr^d\} - \exp(-\kappa)$.
\end{theorem}

\begin{itemize}
	\item Discuss the choice of radius $r$ imposed by~\ref{asmp:kernel_radius_estimation}.
	\item Comment on the distinction between error rates as measured by the norm in $L^2(P_n)$ vs. $L^2(P)$.
	\item \textbf{Proof techniques}: to prove results we rely on both (a) classical bias-variance decomposition of orthogonal series estimators (see e.g. \textcolor{red}{Tsybakov}), and (b) pointwise and variational results on neighborhood graph Laplacian semi-norms and eigenvalues (see e.g. \textcolor{red}{Belkin04, Belkin08, GarciaTrillos18, Calder19})
\end{itemize}

Theorem~\ref{thm:laplacian_smoothing_estimation1} is analogous to Theorem~\ref{thm:laplacian_eigenmaps_estimation1}, except it deals with the Laplacian smoothing estimator $\wt{f}_{\LS}$ rather than the Laplacian eigenmaps estimator $\wh{f}_{\LE}$.   
\begin{theorem}
	\label{thm:laplacian_smoothing_estimation1}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}. Suppose that $f_0 \in H^1(P,M)$, that $P$ satisfies~\ref{asmp:bounded_density}, and that neighborhood graph $G_{n,r}$ is computed with kernel $K$ and radius $r$ which satisfy~\ref{asmp:kernel_form} and~\ref{asmp:kernel_radius_estimation}. Then, for the Laplacian eigenmaps estimator $\wh{f}_{\LE}$ as given by~\eqref{eqn:laplacian_eigenmaps}, when we set $\kappa = M^{2/(2 + d)}n^{d/(2 + d)}$ the following statement holds: for all $n$ sufficiently large, there exist constants $c$ and $C$ such that
	\begin{equation*}
	\Bigl\|\wh{f}_{\LS} - f_0\Bigr\|_n^2 \leq \frac{C}{\delta} M^{2d/(4 + d)} n^{-2/(2 + d)}
	\end{equation*}
	with probability at least $1 - \delta -  Cr^{-d}\exp\{-cnr^d\} - \exp(-\kappa)$.
\end{theorem}

\begin{itemize}
	\item \textbf{Proof techniques}: Because the solution to~\eqref{eqn:laplacian_smoothing} has an analytic form, we can rely on a bias-variance decomposition---as opposed to empirical process theory---to derive~Theorem~\ref{thm:laplacian_smoothing_estimation1}.
	\item A key difference between Theorems~\ref{thm:laplacian_eigenmaps_estimation1} and~\ref{thm:laplacian_smoothing_estimation1} is that the latter holds only when $d \leq 4$. This restriction is unsurprising, and reflects a fundamental drawback in (penalized) least squares estimators more generally. The underlying phenomenon is similar to that elucidated by the seminal papers~\cite{birge1993} and~\cite{birge1998}: when the dimension $d$ is large enough, the parameter space is too large, and (penalized) least squares approaches are no longer minimax optimal. It is interesting to note that we see a gap emerge between the graph- (Laplacian smoothing) and continuum (smoothing spline) estimator when $d = 3$: the former continues to be minimax optimal, whereas the latter is not even well-defined. We should also mention that in a related context (estimation over a $d$-dimensional lattice graph), \cite{sadhanala2016} require the same restriction $d \leq 4$. 
\end{itemize}

\subsection{Minimax optimal hypothesis testing}

Under appropriate conditions, the Laplacian eigenmaps test statistic $T_{\LE}$ has non-trivial power over all $f_0 \in H^1(P,M)$ that are separated from $0$ in $\Leb^2(P)$ norm by at least the minimax critical radius. These conditions include~\ref{asmp:kernel_form} and~\ref{asmp:smooth_density}, but not~\ref{asmp:kernel_radius_estimation}. Instead, we will require the following scaling of the neighborhood graph radius.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:kernel_radius_testing}
	The radii $r = r(n)$ satisfy $r(n) \to 0$ as $n \to \infty$, and additionally
	\begin{equation*}
	n^{-1/d} \leq r(n) \leq n^{-2/(4 + d)}
	\end{equation*}
	for all $n$ sufficiently large.
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing1}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}, with $d \leq 3$. Suppose that $f_0 \in H^1(P,M)$, that
	$P$ satisfies~\ref{asmp:bounded_density}, and that the neighborhood graph $G_{n,r}$ is computed with kernel $K$ and radius $r$ which satisfy~\ref{asmp:kernel_form} and~\ref{asmp:kernel_radius_testing}.
	Then for the Laplacian eigenmaps test statistic $T_{\LE}$ as in~\eqref{eqn:laplacian_eigenmaps_test} with $\kappa = n^{2d/(4 + d)} M^{4d/(4 + d)}$, the following statement holds true for all $n$ sufficiently large: there exist constants $C_1$ and $C_2$ which do not depend on $f_0$ such that if
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing}
	\norm{f_0}_{\Leb^2(P)}^2 \geq \frac{C_1}{\delta^2} M^{2d/(4 + d)} n^{-4/(4 + d)}
	\end{equation}
	then
	\begin{equation*}
	\Pbb \biggl(T_{\LE} \geq \frac{\kappa}{n} + \frac{1}{\delta}\sqrt{\frac{2\kappa}{n^2}}\biggr) \geq 1 - C_2\Bigl(\delta + r^{-d} \exp\{- c n r^d\}\Bigr).
	\end{equation*}
\end{theorem}
\begin{itemize}
	\item Explain why Theorem~\ref{thm:laplacian_eigenmaps_testing1} implies minimax optimality.
	\item Include results regarding the low-smoothness regime (i.e. the case when $d \geq 4$.)
\end{itemize}

\section{Higher-order smoothness classes}
\label{sec:higher_order_smoothness_classes}
The estimation and testing rates derived in Section~\ref{sec:minimax_optimal_graph_Laplacian_methods} reflect a bonafide curse of dimensionality: as  the dimension $d$ gets larger, inference quickly becomes (much) harder. As covered in Section~\ref{sec:problem_setup_and_background}, it is well known that by making additional structural assumptions---either on the regression function $f_0$, the distribution $P$, or both---one can derive improved minimax rates. In the following two sections, we show that under such assumptions, the estimators~\eqref{eqn:laplacian_eigenmaps} and~\eqref{eqn:laplacian_smoothing}, and test statistics~\eqref{eqn:laplacian_eigenmaps_test} and~\eqref{eqn:laplacian_smoothing_test} can in certain situations obtain the faster (known) minimax rates in the nonparametric estimation and testing problems.

In this section, we will assume that $f_0 \in H^s(P,M)$ for some integer $s > 1$. In other words, we will assume that $f_0$ has at least two smooth derivatives. 

\subsection{Minimax optimal estimation}
\label{subsec:minimax_estimation_higher_order}
Under the following three conditions, the estimation error of the Laplacian eigenmaps estimator $\wh{f}_{\LE}$ achieves the minimax optimal rate for any $f_0 \in H^s(P,M)$.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:kernel_radius_higher_order}
	\textbf{Neighborhood graph radius}:
	The sequence of radii $r = r(n)$ satisfy $r(n) \to 0$ as $n \to \infty$, and additionally
	\begin{equation*}
	n^{-1/(2(s - 1) + d)}\leq r(n) \leq M^{2/(2s + d)}n^{-1/(2s + d)}
	\end{equation*}
	for all $n$ sufficiently large.
\end{enumerate}
\begin{enumerate}[label=(F\arabic*)]
	\setcounter{enumi}{0}
	\item 
	\label{asmp:boundary} \textbf{Boundary condition}; The regression function $f_0$ is zero-trace. In other words, $f_0 \in H_0^s(P,M)$. 
\end{enumerate}
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{1}
	\item 
	\label{asmp:smooth_density}\textbf{$\beta$-Holder density}: The design distribution $P$ has a density $p \in C^{\beta}([0,1]^d;p_{\max})$ for some $\beta \geq 0$. 
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}. Suppose that $f_0 \in H_0^s(P,M)$, that $P$ satisfies~\ref{asmp:smooth_density} with $\beta = s - 1$, and that neighborhood graph $G_{n,r}$ is computed with kernel $K$ and radius $r$ which satisfy~\ref{asmp:kernel_form} and~\ref{asmp:kernel_radius_higher_order}. Then, for the Laplacian eigenmaps estimator $\wh{f}_{\LE}$ as in~\eqref{eqn:laplacian_eigenmaps}, when $\kappa = M^{2/(2s + d)}n^{d/(2s + d)}$, the following statement holds: for all $n$ sufficiently large, there exist constants $c$ and $C$ such that
	\begin{equation*}
	\Bigl\|\wh{f}_{\LE} - f_0\Bigr\|_n^2 \leq \frac{C}{\delta} M^{2d/(4s + d)} n^{-2s/(2s + d)}
	\end{equation*}
	with probability at least $1 - \delta -  Cr^{-d}\exp\{-cnr^d\} - \exp(-\kappa)$.
\end{theorem}

We note that the assumption~\ref{asmp:kernel_radius_higher_order} is rather mild, since the user can choose how to construct the neighborhood graph $G_{n,r}$. The assumptions~\ref{asmp:boundary} and~\ref{asmp:smooth_density} are more stringent. That being said, some restrictions on the boundary behavior of the regression function $f_0$ are typical in the analysis of orthogonal series estimators (see e.g. \textcolor{red}{Tsybakov}). On the other hand, restrictions on the smoothness of the density $p$ are also typical when analyzing orthogonal series estimators under a random design setup (see e.g. \textcolor{red}{Mukherjee16}).

\begin{enumerate}
	\item \textcolor{red}{(TODO)}: Comment a bit on the proof strategy; i.e., 
\end{enumerate}

\subsection{Minimax optimal hypothesis testing}
\label{subsec:minimax_testing_higher_order}

Under appropriate conditions, the Laplacian eigenmaps test statistic $T_{\LE}$ has non-trivial power over all $f_0 \in H_0^s(P,M)$ that are separated from $0$ in $\Leb^2(P)$ norm by at least the minimax critical radius. These conditions include~\ref{asmp:kernel_form} and~\ref{asmp:smooth_density}. Additionally, we will require the following scaling of the neighborhood graph radii.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:kernel_radius_testing}
	The radii $r = r(n)$ satisfy $r(n) \to 0$ as $n \to \infty$, and additionally
	\begin{equation*}
	n^{-1/(2(s - 1) + d)} \leq r(n)~~\textrm{and}~~ r(n) \leq n^{-2/(4s + d)}
	\end{equation*}
	for all $n$ sufficiently large.
\end{enumerate}
In one sense this is still a mild condition, in that the neighborhood graph radius is still a tuning parameter to be selected by the user. However, some elementary calculations show that when $d > 4$, it is in fact the case that $n^{-1/(2(s - 1) + d)} > n^{-2/(4s + d)}$; hence no choice of $r$ can satisfy~\ref{asmp:kernel_radius_testing}. As a result, our theory applies only when $d \leq 4$.

\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}. Suppose that $d \leq 4$, that $f_0 \in H_0^s(P,M)$, that
	$P$ satisfies~\ref{asmp:smooth_density} with $\beta = s - 1$, and that the neighborhood graph $G_{n,r}$ is computed with kernel $K$ and radius $r$ which satisfy~\ref{asmp:kernel_form} and~\ref{asmp:kernel_radius_testing}.
	Then for the Laplacian eigenmaps test statistic $T_{\LE}$ given by~\eqref{eqn:laplacian_eigenmaps_test} with tuning parameter $\kappa = n^{2d/(4s + d)} M^{4d/(4s + d)}$, the following statement holds true for all $n$ sufficiently large: there exist constants $C_1$ and $C_2$ which do not depend on $f_0$ such that if
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing}
	\norm{f_0}_{\Leb^2(P)}^2 \geq \frac{C_1}{\delta^2} M^{2d/(4s + d)} n^{-4s/(4s + d)}
	\end{equation}
	then
	\begin{equation*}
	\Pbb \biggl(T_{\LE} \geq \frac{\kappa}{n} + \frac{1}{\delta}\sqrt{\frac{2\kappa}{n^2}}\biggr) \geq 1 - C_2\Bigl(\delta + r^{-d} \exp\{- c n r^d\}\Bigr).
	\end{equation*}
\end{theorem}

\begin{itemize}
	\item \textbf{Test based on the Laplacian smoothing test statistic}. 
	\item \textbf{Low-smoothness regime}. Include results on minimax testing when $4s < d$.
	\item \textbf{Head off potential criticism}. Acknowledge frankly that there is a regime---when $4 < d < 4s$---where minimax testing rates are known, but for which we cannot prove that either of our test statistics achieve these rates.
\end{itemize}

\section{Manifold Assumption}
\label{sec:manifold_assumption}


\section{Discussion}

\begin{itemize}
	\item \textbf{Relationships between $s$ and $d$}. Comment on the different types of relationships between $s$ and $d$. (1) Those due to fundamental upper bounds on testing; (2) those due to ``defects'' in the proposed methods, i.e. largeness of the parameter space for Laplacian smoothing, and (3) those (potentially) due to ``defects'' in our proof strategy.
	\item \textbf{Proof techniques}. On the subject of our proof techniques, the careful reader will note that our proofs do not rely on convergence of the Laplacian eigenvectors $v_k(G_{n,r})$ towards eigenfunctions of a limiting (weighted) Laplace-Beltrami operator.
	
	\textcolor{red}{(TODO) Flesh the following out.} This is helpful for at least the following two reasons. First, proving convergence of eigenvectors is harder than proving convergence of eigenvalues, a fact which is reflected in the gap between the best known rates for the two problems. Second, because in order for our methods to be minimax optimal, orthogonal series estimators rely on projecting onto $\kappa$ basis functions, where $\kappa$ is a number growing in $n$. Thus we have an accumulation of error problem.
	\item \textbf{Possible extensions}.
\end{itemize}

\section{Simulations}
\begin{itemize}
	\item \textbf{Estimation MSE 1.} Plot $L^2(P_n)$ error as a function of $n$ and choice $f_0^{(n)}$ when $P$ is (close to) uniform, showing that Laplacian methods are competitive with their non-graph-based brethren.
	\item \textbf{Estimation fitted values.} Show fits for two regression problems, one where the distribution $P$ is assumed to be close to uniform, one where $P$ is dramatically non-uniform.
	\item \textbf{Estimation MSE 2.} Plot $L^2(P_n)$ error as a function of the distribution $P$, as we vary $P$ from being uniform to being very non-uniform.
	 
	\item \textbf{Testing power.} 
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width = .48\textwidth]{plots/graph_regression/mean_squared_errors/sobolev_MSE_by_density}
	\caption{}
	\label{fig:piecewise_cosine}
\end{figure}

\clearpage

\appendix

Many of our results follow the same general two-part proof strategy. First, we establish (estimation or testing) error bounds which hold for any graph $G$; these bounds are stated with respect to functionals of the graph $G$. We then analyze the behavior of these functionals with respect to the particular neighborhood graph $G_{n,r}$ and give high probability (upper or lower) bounds on these functionals. It is in this second step where we invoke our various assumptions on the distribution $P$ and regression function $f_0$.

\section{Graph-dependent error bounds}
\label{sec:fixed_graph_error_bounds}

Suppose we observe a fixed graph $G = \bigl([n],E\bigr)$ with Laplacian $\Lap_G$ and responses
\begin{equation}
\label{eqn:fixed_graph_regression_model}
Y_i = f_{0,i} + \varepsilon_i
\end{equation}
where $f_0 = (f_{0,1},\ldots,f_{0,n}) \in \Reals^n$, and the noise variables $\varepsilon_i$ are independent $N(0,1)$. The Laplacian eigenmaps estimator of $f_0$ on $G$ is then
\begin{equation}
\label{eqn:le_G}
\wh{f}_{\LE}(G) := \sum_{k = 1}^{\kappa} \biggl(\frac{1}{n}\sum_{i = 1}^{n} Y_i v_{k,i}(G) \biggr) v_k(G),
\end{equation}
where $(\lambda_1(G),v_1(G)),\ldots,(\lambda_n(G),v_n(G))$ are the eigenvalue/eigenvector pairs of $\Lap_G$. The Laplacian smoothing estimator of $f_0$ on $G$ is
\begin{equation}
\label{eqn:ls_G}
\wt{f}_{\LS}(G) := \argmin_{f \in \Reals^n} \biggl\{ \sum_{i = 1}^{n}(Y_i - f_i)^2 + \rho \cdot f^T \Lap_G^{s}  f \biggr\} = (\rho \Lap_G^s + \Id)^{-1}Y.
\end{equation}

In this fixed design context, the hypothesis testing problem becomes
\begin{equation*}
\mathbf{H}_0: f_{0} = (0,...,0) ~~\textrm{versus}~~ \mathbf{H}_a: f_{0} \neq (0,...,0)
\end{equation*}
and two test statistics for this purpose are
\begin{equation}
\label{eqn:le_ts_G}
T_{\LE}(G) := \frac{1}{n} \Bigl\|\wh{f}_{\LE}(G)\Bigr\|_2^2 
\end{equation}
and
\begin{equation}
\label{eqn:ls_ts_G}
T_{\LS}(G) := \frac{1}{n} \Bigl\|\wt{f}_{\LS}(G)\Bigr\|_2^2 
\end{equation}
We note that~\eqref{eqn:laplacian_eigenmaps}-\eqref{eqn:laplacian_smoothing_test} specialize~\eqref{eqn:le_G}-\eqref{eqn:ls_ts_G} to the case where $G$ = $G_{n,r}$.

\subsection{Error bounds for linear smoothers}
Let $S \in \Reals^{n \times n}$ be a square, symmetric matrix, and let $\check{f} = SY$ be a linear estimator of $f_0$. Both~\eqref{eqn:le_G} and~\eqref{eqn:ls_G} can be written in this form, and Lemma~\ref{lem:linear_smoother_fixed_graph_estimation} is therefore useful in analyzing them. Let $\lambdavec(S) = (\lambda_1(S),\ldots,\lambda_n(S)) \in \Reals^n$ denote the eigenvalues of $S$ and let $v_k(S)$ denote the corresponding \emph{unit-norm} eigenvectors, so that $S = \sum_{k = 1}^{n} \lambda_k(S) \cdot v_k(S) v_k(S)^T$. Denote $Z_k = v_k(S)^T \varepsilon$, and observe that $Z = (Z_1,\ldots,Z_n) \sim N(0,\Id)$. 

\begin{lemma}
	\label{lem:linear_smoother_fixed_graph_estimation}
	Let $\check{f} = SY$ for a square, symmetric matrix, $S \in \Reals^{n \times n}$ satisfying $\lambda_n(S) \leq 1$. Then
	\begin{equation*}
	\Pbb_{f_0}\biggl(\frac{1}{n}\bigl\|\check{f} - f_0\bigr\|_2^2 \geq \frac{10}{n} \bigl\|\lambdavec(S)\bigr\|_2^2 + \frac{2}{n}\bigl\|(S - I)f_0\bigr\|_2^2\biggr) \leq 1 - \exp\Bigl(-\bigl\|\lambdavec(S)\bigr\|_2^2\Bigr)
	\end{equation*}
\end{lemma}
\begin{proof}[\textbf{Proof of Lemma~\ref{lem:linear_smoother_fixed_graph_estimation}}]
	It holds that
	\begin{align*}
	\frac{1}{n}\bigl\|\check{f} - f_0\bigr\|_2^2 & \leq \frac{2}{n}\Bigl(\bigl\|\check{f} - \Ebb_{f_0}[\check{f}]\bigr\|_2^2 + \bigl\|\Ebb_{f_0}[\check{f}] - f_0\bigr\|_2^2\Bigr) \\ 
	& = \frac{2}{n}\Bigl(\bigl\|S\varepsilon\bigr\|_2^2 + \bigl\|(S - I)f_0\bigr\|_2^2\Bigr)
	\end{align*}
	Writing $\norm{S\varepsilon}_2^2 = \sum_{k = 1}^{n} \lambda_k(S)^2 Z_k^2$, the claim follows from the result of \textcolor{red}{Laurent and Massart} on concentration of $\chi^2$-random variables, which for completeness we restate in Lemma~\ref{lem:chi_square_bound}. To be explicit, taking $t = \norm{\lambdavec(S)}_2^2$ in Lemma~\ref{lem:chi_square_bound} completes the proof of Lemma~\ref{lem:le_fixed_graph_estimation}.
\end{proof}

On the testing side, the statistic $\norm{\check{f}}_2^2 = Y^T S^2 Y$ is a $U$-statistic of order $2$. Both~\eqref{eqn:le_ts_G} and~\eqref{eqn:ls_ts_G} can be written in this form, and Lemma~\ref{lem:linear_smoother_fixed_graph_testing} is therefore useful in analyzing them.
\begin{lemma}
	\label{lem:linear_smoother_fixed_graph_testing}
	Let $T = Y^T S^2 Y$ for a square, symmetric matrix $S \in \Reals^{n \times n}$. Define the threshold $t_b$ to be 
	\begin{equation}
	t_{b} := \norm{\lambdavec(S)}_2^2 + 2b \norm{\lambdavec(S)}_4^2
	\end{equation}
	Suppose the eigenvalues $0 \leq \lambda_{1}(S) \leq \lambda_{n}(S) \leq 1$. Then,
	\begin{itemize}
		\item \textbf{Type I error.}
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_typeI}
		\Pbb_0\bigl(T > t_b\bigr) \leq \frac{1}{b^2}
		\end{equation}
		\item \textbf{Type II error.} Suppose further that
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_critical_radius}
		f_0^T S^2 f_0 \geq 4b \norm{\lambdavec(S)}_4^2.
		\end{equation}
		Then
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_typeII}
		\Pbb_{f_0}\bigl(T \leq t_b\bigr) \leq \frac{4}{b^2} + \frac{8}{b \norm{\lambdavec(S)}_4^{2}} 
		\end{equation}
	\end{itemize}
\end{lemma}
\begin{proof}[\textbf{Proof of Lemma~\ref{lem:linear_smoother_fixed_graph_testing}}]
	We compute the mean and variance of $T$ as a function of $f_0$, then apply Chebyshev's inequality.
	
	\textit{Mean.} Writing $Y = f_0 + \varepsilon$, we make use of the eigendecomposition of $S$ to obtain
	\begin{equation}
	\label{pf:linear_smoother_fixed_graph_testing1}
	\begin{aligned}
	T & = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \varepsilon^T S^2 \varepsilon \\
	& = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \sum_{k = 1}^{n}  \bigl(\lambda_k(S)\bigr)^2 (\varepsilon^T v_k(S))^2 \\
	& = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \sum_{k = 1}^{n}  \bigl(\lambda_k(S)\bigr)^2 Z_k^2,
	\end{aligned}
	\end{equation}
	implying
	\begin{equation}
	\label{pf:linear_smoother_fixed_graph_testing_mean}
	\Ebb_{f_0}[T] = f_0^T S^2 f_0 + \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^2.
	\end{equation}
	
	\textit{Variance.} Starting from~\eqref{pf:linear_smoother_fixed_graph_testing1} and recalling the basic fact $\Var(Z_k^2) = 2$, we derive
	\begin{equation}
	\label{pf:linear_smoother_fixed_graph_testing_var}
	\Var_{f_0}[T] \leq 8 f_0^T S^4 f_0 + 4 \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^4 \leq 8 f_0^T S^2 f_0 + 4 \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^2
	\end{equation}
	where the second inequality follows since by assumption $\lambda_{n}(S) \leq 1$.
	
	\textit{Bounding Type I and Type II error.} The bound~\eqref{eqn:linear_smoother_fixed_graph_testing_typeI} follows directly from Chebyshev's inequality, along with our above calculations on the mean and variance of $T$.
	
	The bound~\eqref{eqn:linear_smoother_fixed_graph_testing_typeII} also follows from Chebyshev's inequality, as can be seen by the following manipulations,
	\begin{equation*}
	\begin{aligned}
	\Pbb_{f_0}\bigl(T \leq t_b\bigr) & = \Pbb_{f_0}\bigl(T - \Ebb_{f_0}[T] \leq t_b - \Ebb_{f_0}[T]\bigr) \\
	& \overset{(i)}{\leq} \Pbb_{f_0}\bigl(\abs{T - \Ebb_{f_0}[T]} \geq \abs{t_b - \Ebb_{f_0}[T]}\bigr) \\ 
	& \overset{(ii)}{\leq} 4 \frac{\Var_{f_0}[T]}{(f_0^T S^2 f_0)^2} \\
	& \overset{(iii)}{\leq} \frac{32}{f_0^T S^2 f_0} + \frac{4}{b^2} \\
	& \overset{(iv)}{\leq} \frac{8}{b} \Biggl(\sum_{k = 1}^{n}\bigl(\lambda_k(S)\bigr)^4\Biggr)^{-1/2} + \frac{4}{b^2}
	\end{aligned}
	\end{equation*}
	In the previous expression, $(i)$ and $(ii)$ follow since assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} and equation~\eqref{pf:linear_smoother_fixed_graph_testing_mean} together imply $\Ebb_{f_0}(T) - \frac{1}{2}f_0^T S^2f_0 \geq t_b$, $(iii)$ follows from assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} and the inequality~\eqref{pf:linear_smoother_fixed_graph_testing_var}, and $(iv)$ follows assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius}.
\end{proof}
\subsection{Analysis of Laplacian eigenmaps}

In Lemma~\ref{lem:le_fixed_graph_estimation}, we upper bound the mean squared error of $\wh{f}_{\LE}(G)$. Our bound will be stated as a function of $f_0^T \Lap_G^s f_0$--a measure of the smoothness the signal $f_0$ displays over the graph $G$--as well as the $\kappa$th eigenvalue $\lambda_{\kappa}(G)$.

\begin{lemma}
	\label{lem:le_fixed_graph_estimation}
	For any integer $1 \leq \kappa \leq n$ and any $s > 0$,
	\begin{equation}
	\label{eqn:le_fixed_graph_estimation}
	\frac{1}{n}\bigl\|\wh{f}_{\LE}(G) - f_0\bigr\|_2^2 \leq 2\frac{f_0^T \Lap_{G}^s f_0}{n\lambda_{\kappa}(G)^s} + \frac{10\kappa}{n}
	\end{equation}
	with probability at least $1 - \exp(-\kappa)$.
\end{lemma}
\begin{proof}[\textbf{Proof of Lemma~\ref{lem:le_fixed_graph_estimation}}]
	Note that if $\lambda_{\kappa}(G) = 0$ the claim is trivial, and we therefore only cover the case when $\lambda_{\kappa}(G) > 0$. 
	
	Letting $\wh{S} = \frac{1}{n}\sum_{k = 1}^{n} v_k(G) v_k(G)^T$, the estimator $\wh{f}_{\LE}(G) = \wh{S}Y$. The matrix $\wh{S} \in \Reals^{n \times n}$ is symmetric, and satisfies $\lambda_k(\wh{S}) \in \{0,1\}$ for all $k \in [n]$. The claim then follows from Lemma~\ref{lem:le_fixed_graph_estimation} upon noting first that $\norm{\lambdavec(\wh{S})}_2^2 = \kappa$, and second that
	\begin{equation}
	\label{pf:le_fixed_graph_estimation_1}
	\begin{aligned}
	\bigl\|(\wh{S} - I)f_0\bigr\|_2^2 & = \sum_{k = \kappa + 1}^{n} \bigl(f_0^T v_k(G)\bigr)^2 \\
	& \leq \frac{1}{\lambda_{\kappa}(G)^s}\sum_{k = \kappa + 1}^{n} \lambda_{k}(G)^s \bigl(f_0^T v_k(G)\bigr)^2 \\
	& \leq \frac{1}{\lambda_{\kappa}(G)^s}\sum_{k = 1}^{n} \lambda_{k}(G)^s \bigl(f_0^T v_k(G)\bigr)^2 \\
	& = \frac{f_0^T \Lap_G^s f_0}{\lambda_{\kappa}(G)^s}.
	\end{aligned}
	\end{equation} 
\end{proof}

In Lemma~\ref{lem:le_fixed_graph_testing}, we upper bound the testing error of a test based on $T_{\LE}(G)$.

\begin{lemma}
	\label{lem:le_fixed_graph_testing}
	Define the threshold $\wh{t}_b$ to be
	\begin{equation*}
	\wh{t}_b := \frac{\kappa}{n} + \frac{2b\sqrt{\kappa}}{n}
	\end{equation*}
	Then each of the following hold for any $\kappa \in [n]$ and any $b \geq 1$,
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $f_0 = 0$,
		\begin{equation}
		\label{eqn:graph_spectral_type_I_error}
		\Pbb_{f_0}\biggl(T_{\LE}(G) > \wh{t}_b\biggr) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} Suppose that
		\begin{equation}
		\label{eqn:le_fixed_graph_testing_critical_radius}
		\frac{1}{n} \norm{f_0}_2^2 \geq \frac{f_0^T \Lap_G^s f_0}{n \lambda_{\kappa}(G)^s} + \frac{4b\sqrt{\kappa}}{n}
		\end{equation}
		Then,
		\begin{equation}
		\label{eqn:graph_spectral_type_II_error}
		\Pbb_{f_0}\biggl(T_{\LE}(G) \leq \wh{t}_b\biggr) \leq \frac{4}{b^2} + \frac{8}{b\sqrt{\kappa}}.
		\end{equation}
	\end{enumerate}
\end{lemma}
\begin{proof}[\textbf{Proof of Lemma~\ref{lem:le_fixed_graph_testing}}]
	Letting $\wh{S} = \frac{1}{n}\sum_{k = 1}^{\kappa} v_k(G) v_k(G)^T$, our test statistic $T_{\LE}(G) = \frac{1}{n} Y^T \wh{S}^2 Y$. The matrix $\wh{S}$ is symmetric and positive semidefinite, with eigenvalues $\lambda_k(\wh{S}) \in \{0,1\}$ for all $k \in [n]$, so $\norm{\lambdavec(\wh{S})}_2^2 = \norm{\lambdavec(\wh{S})}_4^4 = \kappa$ and clearly $n \wh{t}_b = \norm{\lambdavec(\wh{S})}_2^2 + 2b \norm{\lambdavec(\wh{S})}_4^2$.  The desired result thus follows from Lemma~\ref{lem:linear_smoother_fixed_graph_testing}. To see that the conditions of this Lemma are met, note first that $\lambda_k(\wh{S}) \in \{0,1\}$ for all $k \in [n]$, and so it remains only to verify \eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius}. Invoking assumption~\eqref{eqn:le_fixed_graph_testing_critical_radius} along with the inequality~\eqref{pf:le_fixed_graph_estimation_1}, we obtain
	\begin{align*}
	f_0^T \wh{S}^2 f_0 & = \norm{f_0}_2^2 - \sum_{k = \kappa + 1}^{n} \bigl(f_0^T v_k(G)\bigr)^2 \\
	& \geq \frac{f_0^T \Lap_G^s f_0}{\lambda_{\kappa}(G)^s} + 4b\sqrt{\kappa} - \sum_{k = \kappa + 1}^{n} \bigl(f_0^T v_k(G)\bigr)^2 \\
	& \geq 4b\sqrt{\kappa}.
	\end{align*}
	Reiterating that $\norm{f_0}_4^2 = \sqrt{\kappa}$, we see that~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} is met, completing the proof.
\end{proof}

\subsection{Analysis of Laplacian Smoothing}
In Lemma~\ref{lem:ls_fixed_graph_estimation}, we upper bound the mean squared error of $\wt{f}_{\LS}(G)$.
\begin{lemma}
	\label{lem:ls_fixed_graph_estimation}
	For any $\rho,s > 0$,
	\begin{equation}
	\label{eqn:ls_fixed_graph_estimation_prob}
	\frac{1}{n}\bigl\|\wt{f}_{\LS}(G) - f_0\bigr\|_2^2 \leq \frac{2\rho}{n} \bigl(f_0^T \Lap_{G}^s f_0\bigr) + \frac{10}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G)^s + 1\bigr)^2}
	\end{equation}
	with probability at least $1 - \exp\Bigl(-\sum_{k = 1}^{n}\bigl(\rho \lambda_k(G)^s + 1\bigr)^{-2}\Bigr)$.
\end{lemma}
\begin{proof}[\textbf{Proof of Lemma~\ref{lem:ls_fixed_graph_estimation}}]
	Letting $\wt{S} = (\Id + \rho \Lap_G^s)^{-1}$, the estimator $\wt{f}_{\LS}(G) = \wt{S}Y$. The matrix $\wt{S} \in \Reals^{n \times n}$ is symmetric, and satisfies $\lambda_k(\wt{S}) \in (0,1)$ for all $k \in [n]$. The claim then follows from Lemma~\ref{lem:linear_smoother_fixed_graph_estimation} upon noting first that
	\begin{equation*}
	\bigl\|\lambdavec(\wt{S})\bigr\|_2^2 = \sum_{k = 1}^{n} \frac{1}{\bigl(1 + \rho \lambda_k(G)^s\bigr)^2}
	\end{equation*} 
	and second that
	\begin{equation*}
	\begin{aligned}
	\bigl\|(\wt{S} - I)f_0\bigr\|_2^2 & = f_0^T \Lap_G^{s/2} \Lap_G^{-s/2}\bigl(\wt{S} - \Id\bigr) \Lap_G^{-s/2} \Lap_G^{s/2} f_0 \\
	& \leq f_0^T \Lap_G^s f_0 \cdot \lambda_n\Bigl(\Lap_G^{-s/2}\bigl(\wt{S} - \Id\bigr)\Lap_G^{-s/2}\Bigr) \\
	& = f_0^T \Lap_G^s f_0 \cdot \max_{k \in [n]} \biggl\{\frac{1}{\lambda_k(G)^s} \Bigl(1 - \frac{1}{\rho\lambda_k(G)^s + 1}\Bigr) \biggr\} \\
	& = f_0^T \Lap_G^s f_0 \cdot \rho.
	\end{aligned}
	\end{equation*} 
	where we write $\Lap_G^{-1}$ for the pseudoinverse of $\Lap_G$.
\end{proof}

In Lemma~\ref{lem:ls_fixed_graph_testing}., we upper bound the testing error of a test based on $T_{\LS}(G)$.
\begin{lemma}
	\label{lem:ls_fixed_graph_testing}
	Define the threshold $\wt{t}_b$ to be
	\begin{equation*}
	\wt{t}_b := \frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k(G)^s + 1\bigr)^2} + \frac{2b}{n}\sqrt{\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k(G)^s + 1\bigr)^4}}
	\end{equation*}
	Then each of the following holds for any $\rho,s > 0$ and any $b \geq 1$.
	\begin{itemize}
		\item \textbf{Type I error.}
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_typeI}
		\Pbb_0\Bigl(T_{\LS}(G) > \wt{t}_b\Bigr) \leq \frac{1}{b^2}
		\end{equation}
		\item \textbf{Type II error.} If
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_critical_radius}
		\frac{1}{n}\norm{f_0}_2^2 \geq \frac{2 \rho}{n} \bigl(f_0^T \Lap_G^s f_0\bigr) + \frac{4b}{n} \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \Biggr)^{1/2}
		\end{equation}
		then
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_typeII}
		\Pbb_{f_0}\Bigl(T_{\LS}(G) \leq \wt{t}_b\Bigr) \leq \frac{4}{b^2} + \frac{8}{b} \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \Biggr)^{-1/2}
		\end{equation}
	\end{itemize}
\end{lemma}

\begin{proof}[\textbf{Proof of Lemma~\ref{lem:ls_fixed_graph_testing}}]
	Let $\wt{S} := (\rho \Lap^s + \Id)^{-1}$. The matrix $\wt{S} \in \Reals^{n \times n}$ is symmetric and positive semidefinite, and our test statistic $T_{\LS}(G) = \frac{1}{n}Y^T \wt{S}^2 Y$. The desired result thus follows from Lemma~\ref{lem:linear_smoother_fixed_graph_testing}. To see that the conditions of this Lemma are satisfied, we first note that since
	\begin{equation*}
	\lambda_k(\wt{S}) = \frac{1}{(\rho\lambda_k(G)^s + 1)}
	\end{equation*}
	and $\rho, \lambda_k(G) > 0$, it is evident that $\lambda_{n}(\wt{S}) \leq 1$.  Then, by assumption~\eqref{eqn:ls_fixed_graph_testing_critical_radius}
	\begin{equation*}
	f_0^{T} \wt{S}^2 f_0 = \norm{f_0}_2^2 - f_0^T(I - \wt{S}^2)f_0 \geq 2 \rho (f_0^T \Lap^s f_0) + f_0^T(I - \wt{S}^2)f_0 + 4b \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \Biggr)^{-1/2},
	\end{equation*}
	and along with the following calculations,
	\begin{equation*}
	\begin{aligned}
	f_0^T \Bigl(\Id - \wt{S}^2\Bigr) f_0  & = f_0^T L^{s/2} L^{-s/2}\Bigl(\Id - \wt{S}^2\Bigr) L^{-s/2} L^{s/2} f_0 \\ 
	& \leq f_0^T L^{s} f_0 \cdot  \lambda_{\max}\biggl(L^{-s/2}\Bigl(\Id - \wt{S}^2\Bigr) L^{-s/2}\biggr) \\ 
	& \overset{(i)}{=}  f_0^T L^{s} f_0 \cdot \max_{k} \biggl\{ \frac{1}{\lambda_k(G)^s} \Bigl(1 - \frac{1}{(\rho \lambda_k(G)^s + 1)^2}\Bigr) \biggr\} \\
	& \overset{(ii)}{\leq} f_0^T L^{s} f_0 \cdot 2\rho,
	\end{aligned}
	\end{equation*}
	we have that
	\begin{equation*}
	f_0^{T} \wt{S}^2 f_0 \geq 2b \biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \biggr)^{-1/2}.
	\end{equation*} 
	In other words condition~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} in Lemma~\ref{lem:linear_smoother_fixed_graph_testing} is met, and applying that Lemma completes the proof.
	
	(In the previous derivation: in $(i)$ the maximum is over all indices $k$ such that the eigenvalue $\lambda_k(G)$ is strictly positive; and $(ii)$ follows from the basic algebraic identity $1 - 1/(1 + \rho x)^2 \leq 2 \rho x$ for any $x, \rho > 0$.
\end{proof}


\clearpage
To prove Theorems~\ref{thm:laplacian_eigenmaps_estimation},\textcolor{red}{...}, we would like to apply Lemmas~\ref{lem:fixed_graph_estimation},\textcolor{red}{...} in the random design context, i.e. to the random geometric graph $G_{n,r}$. To do so, we must establish the following (high probability) bounds: (i) an upper bound on the graph Sobolev seminorm of the regression function $f_0 \Lap_{n,r}^s f_0$, (ii) a lower bound on the Laplacian eigenvalue $\lambda_{\kappa}(G_{n,r})$ as a function of $\kappa$, and (iii) a lower bound on the empirical norm $\norm{f}_{n}$ as a function of $\norm{f}_{L^2(P)}$. 

\section{Bounds on the graph Sobolev semi-norm}
\label{sec:graph_sobolev_seminorm}
We first bound $f_0^T \Lap_{n,r}^s f_0$ in expectation.
\begin{lemma}
	\label{lem:roughness_functional_expectation_sobolev}
	Let $\Xset$ be a Lipschitz domain and let $s \geq 1$ be an integer. Suppose that \textcolor{red}{$f \in H_0^{s}(P,M)$}, and that $P$ admits a density $p \in C^{s-1}(\Xset,p_{\max})$ for some constant $p_{\max} < \infty$. Let $\{r(n)\}$ be a sequence of connectivity radii such that $n^{-1/(2(s - 1) + d)} \leq r(n)$ for all $n$ sufficiently large, and $r(n) \to 0$ as $n \to \infty$. Then \textcolor{red}{for any $2$nd-order kernel $K$}, there exists a constant $c$ depending only on $p_{\max}, K_{\max}$, $s$, $d$ and $\Xset$ such that the resulting neighborhood graph Laplacian satisfies
	\begin{equation}
	\label{eqn:roughness_functional_expectation_sobolev}
	\Ebb\bigl[f^T L_{n,r}^s f\bigr] \leq c \cdot n \cdot (nr^{d+2})^s \cdot M^2
	\end{equation}
	for all $n$ sufficiently large.
\end{lemma}

\section{Bounds on graph eigenvalues}
\label{sec:graph_eigenvalues}
Lemma~\ref{lem:neighborhood_eigenvalue} establishes a scaling rate which holds for all eigenvalues $\lambda_{k}(G_{n,r})$, assuming appropriate regularity conditions on $P$.
\begin{lemma}
	\label{lem:neighborhood_eigenvalue}
	Suppose the distribution $P$ satisfies~\ref{asmp:bounded_density}. Then there exists constants $c$ and $C$ which depend only on $p_{\min}, p_{\max}$ and $d$, such that with probability at least $1 - C r^{-d} \exp\bigl\{- c n r^d\bigr\}$, for all $2 \leq k \leq n$
	\begin{equation}
	\label{eqn:neighborhood_eigenvalue}
	\lambda_k(G_{n,r}) \geq c \cdot \min\Bigl\{nr^{d + 2}k^{2/d}, nr^d\Bigr\}.
	\end{equation}	
\end{lemma}
As those readers familiar with Sobolev spaces will be aware of, the scaling $k^{2/d}$ is the \textcolor{red}{typical scaling of a Sobolev ellipsoid}, and is thus the scaling of the eigenvalues we desire. On the other hand, the second term on the right hand side of~\eqref{eqn:neighborhood_eigenvalue} is an artifact of the well known inequality $\lambda_n(G) \leq 2 \max_{i = 1,\ldots,n} \mathbf{D}_{ii}$--- in words, that the maximum eigenvalue of the Laplacian can never be more than twice the maximum degree of the graph. Doing some algebra, we may deduce the conditions on $k$ and $r$ under which the graph eigenvalue follows the desired scaling, i.e. $\lambda_k(G_{n,r}) \sim n r^{d + 2}k^{2/d}$. 

First, when $k = M^{2/(2s + d)} n^{d/(2s + d)}$, it holds that for any $r \leq M^{-2/(d(2s +d))} n^{-1/(2s + d)}$, 
\begin{equation}
\label{eqn:neighborhood_eigenvalue_1}
\lambda_{k}(G_{n,r}) \geq c \cdot nr^{d+2}{k}^{2/d}
\end{equation}
with probability at least $1 - C r^{-d} \exp\{- c n r^d\}$. Second, when $k$


\section{Proofs of Theorems}
\label{sec:proofs_theorems}
With the results of Sections~\ref{sec:fixed_graph_error_bounds}-\ref{sec:graph_eigenvalues} established, the proofs of our main theorems are quite straightforward.

\subsection{Proof of Theorem~\ref{thm:laplacian_eigenmaps_estimation}}
\label{subsec:laplacian_eigenmaps_estimation_pf}


\paragraph{\eqref{eqn:fixed_graph_estimation_sobolev_rate_2} holds with high probability.}

Since $\Lap_{n,r}$ is a positive semi-definite matrix, $f_0^T \Lap_{{n,r}}^s f_0$ must be non-negative. We may therefore apply Markov's inequality and conclude
\begin{equation}
\label{eqn:roughness_functional_probability_sobolev}
f_0^T \Lap_{{n,r}}^s f_0 \leq  \frac{1}{\delta} \cdot c \cdot n \cdot (nr^{d+2})^s \cdot M^2
\end{equation}
with probability at least $1 - \delta$.

\paragraph{\eqref{eqn:fixed_graph_estimation_sobolev_rate_1} holds with high probability.}

\paragraph{Putting the pieces together.}

Let $n$ be sufficiently large so that $n^{-1/(2(s - 1)+ d)} \leq r(n) \leq M^{-2/(d(2s + d))} n^{-1/(2s + d)}$. Then by~\eqref{eqn:roughness_functional_probability_sobolev} and~\eqref{eqn:neighborhood_eigenvalue_1}, there exists a set $\mathcal{E}(f_0) \subseteq \Xset^n$ of probability at least $1 - \delta - Cr^{-d}\exp\{-cnr^d\}$ such that whenever ${\bf X} \in \mathcal{E}(f_0)$,
\begin{equation*}
f_0^T \Lap_{n,r}^s f_0 \leq C \cdot n \cdot C_n^s \cdot M^2
\end{equation*}
and
\begin{equation*}
\lambda_{\kappa}(G_{n,r}) \geq c \cdot nr^{d+2}\kappa^{2/d}
\end{equation*}
Theorem~\ref{thm:laplacian_eigenmaps_estimation} then follows by Corollary~\ref{cor:fixed_graph_estimation_sobolev_rate}.

\section{Concentration Inequalities}
\begin{lemma}
	\label{lem:chi_square_bound}
	Let $\xi_1,\ldots,\xi_N$ be independent $N(0,1)$ random variables, and let $U := \sum_{k = 1}^{N} a_k(\xi_k^2 - 1)$.  Then for any $t > 0$,
	\begin{equation*}
	\Pbb\Bigl[U \geq 2 \norm{a}_2 \sqrt{t} + 2 \norm{a}_{\infty}t\Bigr] \leq \exp(-t).
	\end{equation*}
	In particular if $a_k = 1$ for each $k = 1,\ldots,N$, then
	\begin{equation*}
	\Pbb\Bigl[U - N \geq 2\sqrt{N t} + 2t\Bigr] \leq \exp(-t).
	\end{equation*}
\end{lemma}

\bibliographystyle{plainnat}
\bibliography{../graph_testing_bibliography} 

\end{document}