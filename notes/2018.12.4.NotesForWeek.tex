\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left( #1 \right)}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\Perp}{\perp \! \! \! \perp}


\newcommand{\Linv}{L^{\dagger}}
\newcommand{\tr}{\text{tr}}
\newcommand{\h}{\textbf{h}}
% \newcommand{\l}{\ell}
\newcommand{\x}{\textbf{x}}
\newcommand{\y}{\textbf{y}}
\newcommand{\z}{\textbf{z}}
\newcommand{\Lx}{\mathcal{L}_X}
\newcommand{\Ly}{\mathcal{L}_Y}
\DeclareMathOperator*{\argmin}{argmin}


\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\R}{\Reals}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\begin{document}
	
\title{Notes for the week 12/4 - 12/10}
\author{Alden Green}
\date{\today}
\maketitle

\section{Setup}

\paragraph{Data model.}

We are given two distributions, $P$ and $Q$, with the ability to sample from either one. Our goal is to test the hypothesis $H_0: P = Q$ vs. the alternative $H_a: P \neq Q$. 

Under the \textbf{binomial data model}, our sampling procedure is to draw i.i.d Rademacher labels $L_i \in \set{1, -1}$ for $i \in \set{1, \ldots, N}$, and then sample 
$Z_i \sim P$ if $L_i = 1$ and $Z_i \sim Q$ otherwise. Define $1_X$ to be the length-$N$ indicator vector for $L_i = 1$
\begin{equation*}
1_X[i] = 
\begin{cases}
1, L_i = 1\\
0 \text{ otherwise } 
\end{cases}
\end{equation*}
and similarly for $1_Y$
\begin{equation*}
1_Y[j] = 
\begin{cases}
1, L_i= -1 \\
0 \text{ otherwise } 
\end{cases}
\end{equation*}
and define $a = \frac{1_X}{N / 2} - \frac{1_Y}{N / 2}$. 

Under the \textbf{fixed label data model} we use the same data generating process as above, except fix $\mathcal{L}_X = \set{1, \ldots, N/2}$ and $\mathcal{L}_Y = \set{N/2, \ldots, N}$. Say that $L_i = 1$ for $i \in \mathcal{L}_X$ and $L_i = -1$ for $i \in \mathcal{L}_Y$, and call $\set{ X_1, \ldots, X_{\abs{\Lx}} } = \set{Z_i: i \in \mathcal{L}_X}$ and likewise for $Y$.

\paragraph{Graph.}

Form an $N \times N$ Gram matrix $A$, where $A_{ij} = K(Z_i, Z_j)$ for \textbf{kernel function} $K: \mathcal{X} \times \mathcal{X} \to [0, \infty)$. Let $G = (V,E)$ with $V = \set{Z_1, \ldots, Z_n}$ and $E = \set{A_{ij}: 1 \leq i < j \leq n}$. Take $L = D - A$ to be the (unnormalized) \textbf{Laplacian matrix} of $A$ (where $D$ is the diagonal degree matrix with $D_{ii} = \sum_{j \in [n + m]} A_{ij}$). Denote by $B$ the $N \times N^2$ \textbf{incidence matrix} of $A$, where the $i$th column of $B = B_i$ has entry $A_{ij}$ in position $i$, $-A_{ij}$ in position $j$, and $0$ everywhere else. 

\paragraph{Resistance distances.}

There are many distances one can define over nodes in a graph. The \textbf{resistance distance between nodes $u$ and $v$}, $R_{uv}$, is defined as
\begin{equation*}
R_{uv} = (e_u - e_v)^T \Linv (e_u - e_v).
\end{equation*}

\paragraph{Holder condition}

We say a function $f: \Rd \to \R$ is \textbf{$\alpha$-Holder continuous} when
\begin{equation*}
\abs{f(x) - f(y)} \leq \norm{x - y}^{\alpha}.
\end{equation*}

We will require this condition so that degrees in geometric graphs are well-behaved in the limit.

\section{Desiderata}

\begin{itemize}
	\item Let $K$ be a \textbf{uniform kernel of radius $\epsilon$}, meaning 
	\begin{equation*}
	K(x,y) = I(\norm{x - y} \leq \epsilon).
	\end{equation*}
	
	Assume $P$ and $Q$ have densities $p$ and $q$ with respect to Lebesgue measure. Say that for some $\alpha > 0$, $p$ and $q$ are $\alpha$-holder continuous. For the graph $G$ corresponding to the matrix $A$, with accompanying resistance distances, we wish to upper bound
	\begin{equation*}
	\abs{N \epsilon^d \Expect{R_{XY}} - \Expect{\frac{2}{p(X) + q(X)} + \frac{2}{p(Y) + q(Y)}}}
	\end{equation*}
\end{itemize}

\section{Supplemental Results}

Lemma \ref{lem: vonluxburg_poincare} follows from an application of a discrete version of Poincare's inequality. See (von Luxburg 12) for proof and details.
\begin{lemma}
	\label{lem: vonluxburg_poincare}
	For some $\widetilde{N}_{\max}, \widetilde{N}_{\min}, d_{\max}, d_{\min}$, for all $i \neq j$
	\begin{equation*}
	\abs{R_{ij} - \left(\frac{1}{d_i} + \frac{1}{d_j} \right)} \leq 2 a_1 \frac{1}{N \epsilon^{d+2}} \left( \frac{d_{\max}^2}{d_{\min}^3} \cdot(1 + 2 \frac{\widetilde{N}_{\max}^2}{\widetilde{N}_{\min}^2}) \right)
	\end{equation*}
	where $a_1 = \left( \frac{d \sqrt{d + 3}}{L_{\min}} \right)^{d + 1}$. 
\end{lemma}

Lemmas \ref{}

\begin{lemma}
Denote
\begin{alignat*}{2}
\mu_{\max} & := N \epsilon^d \nu_{d} (p_{\max} + q_{\max})/2,  ~~~ \mu_{\min} && := N \epsilon^d \nu_{d} (p_{\min} + q_{\min})/2 \beta
\end{alignat*}

and let $a_2 = \left(\frac{L_{\min}}{L_{\max}}\right)^d \frac{\nu_d}{2^d (d + 3)^{d/2}}$, $a_3 = \frac{\sqrt{d + 1}}{L_{\min}^d}$. 

For $\widetilde{N}_{\max}, \widetilde{N}_{\min}, d_{\max}, d_{\min}$ as in Lemma \ref{lem: vonluxburg_poincare}, the following bounds hold
\begin{align*}
\Prob{\widetilde{N}_{\max} \geq (1 + z) \mu_{\max}} & \leq \frac{a_3}{\epsilon^d} \cdot \exp(- z^2 \mu_{\max} / 3) \\
\Prob{\widetilde{N}_{\min} \leq a_2 (1 - z) \mu_{\min}} & \leq \frac{a_3}{\epsilon^d} \cdot \exp(- z^2 a_2 \mu_{\min} / 3) \\
\Prob{d_{\max} \geq (1 + z) \mu_{\max}} & \leq n \cdot \exp(- z^2 \mu_{\max} / 3) \\
\Prob{d_{\min} \leq (1 - z) \mu_{\min}} & \leq n \cdot \exp(- z^2 \mu_{\min} / 3)
\end{align*}
\end{lemma}

\begin{lemma}
	\label{lem: prob_to_expectation}
	For random variable $X$ satisfying
	\begin{equation*}
	\Prob{X \leq (1 - z)\mu_n} \leq \exp(-z^2 \mu_n/3 + \log n)
	\end{equation*}
	the inverse moment $\Expect{\frac{1}{(1 + X)^k}}$, $k > 0$, satisfies for any $z < 1$ 
	\begin{equation*}
	\Expect{\frac{1}{(1 + X)^k}} \leq \exp(-z^2 \mu_n/3 + \log n) + \frac{1}{(1 + \mu_n(1 - z))^k}
	\end{equation*}
	
	Similarly, for random variable $Y$ satisfying
	\begin{equation*}
	\Prob{Y \geq (1 + z)\mu_n} \leq \exp(-z^2 \mu_n/3 + c_n)
	\end{equation*}
	the moment $\Expect{(1 + Y)^k}$, $k > 0$, satisfies for any $z > 0$ 
	\begin{equation*}
	\Expect{(1 + Y)^k} \leq \frac{2 n}{} 
	\end{equation*}
\end{lemma}


\section{Proofs}

Begin by expanding
\begin{align}
\label{eqn: expansion}
 \bigg| N \epsilon^d & \Expect{R_{XY}} - \Expect{\frac{2}{p(X) + q(X)} + \frac{2}{p(Y) + q(Y)}} \bigg| \nonumber \\
& = N \epsilon^d \abs{\Expect{R_{XY}} - \Expect{\frac{1}{d(X)} + \frac{1}{d(Y)}}} \nonumber \\
& +  N \epsilon^d \abs{\Expect{\frac{1}{d(X)} - \frac{1}{N \Prob{B(X,\epsilon)} } + \frac{1}{d(Y)} - \frac{1}{N \Prob{B(Y,\epsilon)}}}} \nonumber \\
& + \abs{ \Expect{\frac{\epsilon^d }{\Prob{B(X,\epsilon)} }  - \frac{2}{p(X) + q(X)} } + \Expect{ \frac{\epsilon^d }{\Prob{B(Y,\epsilon)} }  - \frac{2}{p(Y) + q(Y)} } } 
\end{align}

We will bound the summands on the right side of (\ref{eqn: expansion}) from last to first. 

\paragraph{Third term.}

For the last term, we begin by rewriting
\begin{equation*}
\abs{ \frac{\epsilon^d }{\Prob{B(X,\epsilon)} }  - \frac{2}{p(X) + q(X)} } \leq \abs{ \frac{\epsilon^d(p(X) + q(X)) - 2 \Prob{B(X,\epsilon)}}{\Prob{B(X,\epsilon)}[p(X) + q(X)]} }
\end{equation*}

Then, we can bound the numerator using the fact we have required the densities $p$ and $q$ be Holder continuous, so
\begin{align*}
[p(X) + q(X)] \epsilon^d - 2 \Prob{B(X,\epsilon)} & = \int_{B(X,\epsilon)} [p(\x) - p(\z)] d\z + \int_{B(X,\epsilon)} [q(\x) - q(\z)] d\z \\
& \leq \int_{B(X,\epsilon)} 2 \norm{x - y}^{\alpha} d\z\\
& \leq 2 \epsilon^{\alpha + d}.
\end{align*}
We can lower bound the denominator using the lower bound on our densities
\begin{align*}
\Prob{B(X,\epsilon)}[p(X) + q(X)] \geq \epsilon^d(p_{\min} + q_{\min})^2 / 2
\end{align*}
and therefore
\begin{equation*}
\frac{\epsilon^d }{\Prob{B(X,\epsilon)} }  - \frac{2}{p(X) + q(X)} \leq \frac{4 \epsilon^{\alpha}}{(p_{\min} + q_{\min})^2}.
\end{equation*}

The same bound holds for the corresponding term with $Y$ instead of $X$.

\paragraph{Second term.}

To bound the second term, we will upper and lower bound $\Expect{ \frac{1}{d(X)} }$ by something close to $\Expect{ \frac{1}{N\Prob{B(X,\epsilon)}} }$.

The lower bound
\begin{align*}
\Expect{ \frac{1}{d(X)} } & = \Expect{ \Expect{\frac{1}{d(X)} \bigg| X} } \\
& \geq \Expect{ \frac{1}{1 + (N - 1)\Prob{B(X,\epsilon)}} } 
\end{align*}
follows from Jensen's inequality.

For the upper bound, note that the distribution of $d(X)$, conditional on $X$, is $1 + \text{Binomial}(N - 1,\Prob{B(X,\epsilon)})$. Then, letting $q = \Prob{B(X,\epsilon)}$
\begin{align*}
\Expect{\frac{1}{d(X)} \bigg| X} & = \sum_{k = 0}^{N - 1} \frac{1}{k + 1} {N - 1 \choose k} q^k (1 - q)^{N - 1 - k} \\
& = \frac{1}{Nq} \sum_{k = 0}^{N - 1}  {N - 1 \choose k + 1} q^{k + 1} (1 - q)^{N - 1 - k} \\
& \leq \frac{1}{Nq} \sum_{k = 0}^{N} {N \choose k } q^k (1 - q)^{N - k} \\
& = \frac{1}{Nq} \left(q + (1 - q)\right)^N = \frac{1}{Nq}.
\end{align*}

Combining this with the above, we have
\begin{align*}
N \epsilon^d \abs{\Expect{\frac{1}{d(X)} - \frac{1}{N \Prob{B(X,\epsilon)} } } } & \leq N \epsilon^d \abs{ \Expect{\frac{1}{1 + (N - 1)\Prob{B(X,\epsilon)}}}  - \Expect{\frac{1}{N\Prob(B(X, \epsilon))}}} \\
& \leq N \epsilon^d \abs{ \Expect{\frac{1}{N^2 \Prob{B(X, \epsilon)}^2}} }.
\end{align*}
with a corresponding bound holding for $Y$. 

\paragraph{First term.}
We begin by reducing the first term to a product of moments and inverse moments of maxima and minima of binomials.
\begin{align*}
N \epsilon^d \abs{\Expect{R_{XY}} - \Expect{\frac{1}{d(X)} + \frac{1}{d(Y)}}} & \overset{(i)}{\leq} \frac{2 a_1}{\epsilon^2} \Expect{\frac{d^2_{\max}}{d^3_{\min}} \cdot (1 + 2 \frac{\widetilde{N}_{\max}}{\widetilde{N}_{\min}})} \\
& \overset{(ii)}{\leq} \frac{2 a_1}{\epsilon^2} \left(2 \Expect{d_{\max}^8} \cdot \Expect{ d_{\min}^{12} } \cdot \Expect{ \widetilde{N}_{\max}^8} \cdot \Expect{\frac{1}{\widetilde{N}_{\min}^8}} \right)^{1/4}\\
& + \frac{2 a_1}{\epsilon^2} \left( \Expect{d_{\max}^4} \cdot \Expect{d_{\min}^6} \right)^{1/4}
\end{align*}

where $(i)$ follows from Lemma \ref{lem: vonluxburg_poincare} and $(ii)$ from repeated applications of Holder's inequality.



\end{document}