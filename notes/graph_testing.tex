\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Graph Testing}
\author{Alden Green}
\date{\today}
\maketitle

This document details the current status of the graph testing project. We divide by section based on the testing problem under consideration. At the end of each section, we list some areas we are interested in investigating. All proofs are left until the end.

We establish some notation that we will use throughout. Let $X = \{x_1,
\ldots, x_n\}$ be a sample drawn i.i.d. from a distribution $P$ on $\Rd$,
with density~$p$.  For a radius $r > 0$, we define $G_{n,r}=(V,E)$ to be the
\emph{$r$-neighborhood graph} of $X$, an unweighted, undirected graph with
vertices $V=X$, and an edge $(x_i,x_j) \in E$ if and only if $K_r(x_i,x_j) = \norm{x_i -x_j} \leq r$, where $\norm{\cdot}$ is the Euclidean norm. We denote by $A \in
\Reals^{n \times n}$ the adjacency matrix, with entries $A_{uv} = 1$ if
$(u,v) \in E$ and $0$ otherwise.  We also denote by $D$ the diagonal degree
matrix, with entries $D_{uu} := \sum_{v \in V} A_{uv}$. The graph Laplacian is $L = D - A$, and we write its spectral decomposition as $L = V S V^T$. 

\section{Regression goodness-of-fit testing with random design.}

Let $P$ be a distribution with density $p$ supported on $\mathcal{X} \subseteq \Reals^d$. Suppose we observe random design points $X = \set{x_1,\ldots,x_n} \sim P$, and additionally responses
\begin{equation}
\label{eqn:regression_known_variance}
y_i = f(x_i) + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation} 

We wish to distinguish
\begin{equation*}
\mathbf{H}_0: f = f_0 := 0 \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: f \neq f_0
\end{equation*}
We will evaluate our performance using worst-case risk: for a given function class $\mathcal{H}$ and test function $\phi: \Reals^n \to \set{0,1}$, let
\begin{equation*}
\mathcal{R}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, f \neq f_0} \Ebb_f(1 - \phi).
\end{equation*}
The worst-case risk may be quite close to $1$ unless we enforce some separation between null and alternative spaces. A more realistic measure of performance is therefore
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, \norm{f - f_0}_{\Leb^2} \geq \epsilon} \Ebb_f(1 - \phi).
\end{equation*}


\subsection{Test Statistics.}
We list the test statistics we use for the regression testing problem.

\paragraph{Eigenvector projection test statistic.}
Let $VSV^T$ be the spectral decomposition of the Laplacian matrix $L$ of the neighborhood graph $G_{n,r}$. To test whether $f = f_0$, we propose the following \emph{eigenvector projection} test statistic:
\begin{equation}
\label{eqn:graph_spectral_projections}
T_{\mathrm{spec}} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_{k,i} y_i\right)^2
\end{equation}

\subsection{Current Results.}

In Theorem~\ref{thm:sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the graph Laplacian eigenvector projection test $\phi_{\textrm{spec}} := \1\{T_{\mathrm{spec}} \geq \tau\}$ is a minimax optimal test over the Sobolev balls $W^{s,2}(\mathcal{X};R)$ whenever $4s > d$ and $\Xset = [0,1]^d$.

\begin{theorem}
	\label{thm:sobolev_testing_rate}
	Let $b \geq 1$ and $R > 0$ be fixed constants. Suppose that $\Pbb$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p \in C^{s-1}(\Xset;p_{\max})$ for some $p_{\max} < \infty$, and further that $p(x)$ is bounded away from zero, i.e. there exists $p_{\min} > 0$ such that 
	\begin{equation*}
	p_{\min} < p(x),~~ \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}$ is performed with parameter choices 
	\begin{equation*}
	\textrm{$K$ the uniform kernel,}~ n^{-1/(2(s-1) + d)} \leq r(n) \leq n^{-4/((4s + d)(2+d))}, ~\kappa = n^{2d/(4s + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists constants $c_1,c_2$ which do not depend on $n,b$ or $R$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate}
	\epsilon^2 \geq c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4s + d)}\} \cdot n^{-{4s}/(4s + d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; W^{2,s}(\mathcal{X};R)) \leq \frac{c_2}{b}.
	\end{equation}
\end{theorem}

The restriction that $p$ be supported on the unit cube is mostly for convenience. If instead $p$ is supported on any compact set with Lipschitz boundary, the Theorem statement should hold (up to constants), with only a few modifications to our proofs. However, we do not work through the details.

The restriction $4s > d$ is a fundamental consequence of the tightness of the Sobolev embedding theorem. When $4s \geq d$ the compact embedding
\begin{equation*}
W_d^{s,2}(\mathcal{X}) \subseteq \mathcal{L}_d^4(\mathcal{X}) 
\end{equation*}
does not hold (as it does when $d < 4s$). However, if we directly assume $f \in \mathcal{L}_d^4(\mathcal{X};R)$ (regardless of $d$) we obtain the following result.
\begin{proposition}
	\label{prop:L4_testing_rate}
	If $f \in \Leb^4(\mathcal{X};R)$, there exists a constant $c$ such that if
	\begin{equation}
	\label{eqn:L4_testing_rate}
	\epsilon^2 > b \cdot n^{-1/2}
	\end{equation}
	then the test
	\begin{equation*}
	\phi_{\mathrm{mean}} = \1\{\frac{1}{n}\sum_{i = 1}^{n} y_i^2 \geq 1\}
	\end{equation*}
	has worst-case risk
	\begin{equation*}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; W^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}. 
	\end{equation*}
\end{proposition}
Note that when $4s > d$ the critical radius~\eqref{eqn:sobolev_testing_rate} is smaller than \eqref{eqn:L4_testing_rate}. 

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $d \geq 4$, is it true that there does not exist a uniformly consistent
	test over the Sobolev ball $W_d^{1,2}(\mathcal{X};R)$?
	\item Assume the distribution $P$ is supported on a manifold $\mathcal{M}$ of intrinsic dimension $s < d$. Does $\mathrm{\phi_{\mathrm{spec}}}$ display adaptivity to the intrinsic dimension of $\mathcal{M}$?
	\item Assume that $f$ belongs to the Holder space $C_d^s(\mathcal{X})$. Moreover, suppose that instead of observing ${y_i}$ according to the regression testing model \eqref{eqn:regression_known_variance}, we observe
	\begin{equation*}
	y_i = f(x_i) + \sigma \varepsilon_i, ~\varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
	\end{equation*}
	where $\sigma > 0$ is unknown. When $d \geq 4$, what are the minimax regression testing rates over $C_d^1(\mathcal{X};R)$? Is the test $\phi_{\mathrm{spec}}$ minimax optimal, when the tuning parameters $r$ and $\kappa$ are appropriately chosen?
\end{enumerate}

\section{Two-sample density testing.}
In the two-sample density testing problem, we observe independent samples $Z = z_1,\ldots,z_N \sim P$ and $Y = y_1,\ldots,y_M \sim Q$, where $P$ and $Q$ are distributions over $\Reals^d$ with densities $p$ and $q$, respectively, and $N \sim \textrm{Bin}(n,1/2)$. Our goal is to distinguish the hypotheses
\begin{equation*}
\mathbf{H}_0: P = Q \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: P \neq Q
\end{equation*}
and we again evaluate our performance using worst-case risk; letting $\phi:\Reals^{N + M} \to \{0,1\}$, 
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \inf_{p \in \mathcal{H}}\Ebb_{p,p}(\phi) + \sup_{\substack{p,q \in \mathcal{H} \\ \norm{p - q}_{\Leb^2} \geq \epsilon}} \Ebb_{p,q}(1 - \phi).
\end{equation*}

\subsection{Test statistics.}

We suggest several two-sample test statistics. 

\paragraph{Eigenvector projection test statistic.}

It is straightforward to adapt the test statistic $T_{\mathrm{spec}}$ to the two-sample testing problem. Concatenate the samples $Z$ and $Y$ in $X = (z_1,\ldots,z_N,y_1,\ldots,y_M)$, and define $T_{\mathrm{spec}}^{(2)}$ to be
\begin{equation}
\label{eqn:graph_spectral_projections_2}
T_{\mathrm{spec}}^{(2)} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2, ~~\textrm{where}~~ a = (\underbrace{N^{-1},\ldots,{N^{-1}}}_{\textrm{length } N},\underbrace{-M^{-1},\ldots,-M^{-1}}_{\textrm{length } M})
\end{equation}

For convenience, we state our following two test statistics with respect to the empirical norm $\norm{\theta}_n = n^{-1/2}\norm{\theta}_2$ for $\theta \in \Reals^n$. They will each depend on a tuning parameter $\lambda > 0$.
\paragraph{Graph Sobolev IPM.}
Letting $C_n := nr^{(d + 2)/2}$ and
\begin{equation*}
\Theta_{1,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B\theta}_2 \leq 1\} 
\end{equation*}
we define the \emph{graph Sobolev IPM} to be
\begin{equation}
\label{eqn:sobolev_IPM}
T_{\textrm{sob}} := \sup_{\substack{\theta \in \Theta_{1,2} \\ \lambda \norm{\theta}_n \leq 1}} \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}. 
\end{equation}

\paragraph{Graph Total Variation IPM.}
Letting $C_n' := n^{2}r^{(d + 1)}$ and 
\begin{equation*}
\Theta_{1,1} := \{\theta \in \Reals^n:~ (C_n')^{-1} \norm{B\theta}_1 \leq 1\}
\end{equation*}
we define the \emph{graph Total Variation} IPM to be
\begin{equation}
\label{eqn:total_variation_IPM}
T_{\mathrm{TV}} := \sup_{\substack{\theta \in \Theta_{1,1}, \\ \lambda \norm{\theta}_n \leq 1} } \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}, \quad
\end{equation}

\subsection{Current results.}

In Theorem~\ref{thm:twosample_sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}}^{(2)} := \1\{T_{\mathrm{spec}}^{(2)} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $W^{1,2}(\mathcal{X};R)$ when $d = 1$.

\begin{theorem}
	\label{thm:twosample_sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d = 1$.  Suppose that $\mu = (P + Q)/2$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]$ with density functions $\rho$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < \rho_{\min} < \rho(x) < \rho_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}^{(2)}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right), ~\kappa = n^{2/5}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $R,p_{\max},q_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/5} (\log n)^{h(a,1)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}

We prove Theorem~\ref{thm:twosample_sobolev_testing_rate} by relating the density testing problem to a regression testing problem with a certain type of structured noise, and then proceeding along similar lines to the proof of Theorem~\ref{thm:sobolev_testing_rate}. To pursue this strategy, we require the eigenvectors to satisfy a certain type of incoherence condition; this is in constrast to the regression testing problem with known variance, where we did not require the eigenvectors to be smooth in any sense.

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $1 < d < 4$, is the test $\phi_{\spec}^{(2)}$ minimax optimal?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $C_d^1(\mathcal{X};R)$ for all values of $d$?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $W_d^{1,2}(\mathcal{X};R)$ when $d \leq 4$?
	\item Is the test statistic \eqref{eqn:graph_spectral_projections_2}, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $W^{s,2}$? 
	\item Modify the test statistic \eqref{eqn:sobolev_IPM} by replacing the function class $\Theta_{1,2}$ with
	\begin{equation}
	\Theta_{s,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B^{(s)}\theta}_2 \leq 1\} 
	\end{equation}
	Is the modified test statistic, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $W^{s,2}$?
	\item What is the minimax testing rate over $BV_d^{1}(\mathcal{X};R)$? Does it exhibit a phase transition analogous to the minimax estimation rate over bounded variation spaces?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over $BV_d^{1}(\mathcal{X};R)$?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over Sobolev and Holder function classes?
\end{enumerate}

\section{Definitions and Notation}

Here we collect definitions of some common function spaces and graph operators.

\subsection{Function Spaces}

\paragraph{Lebesgue spaces.}

We say a Borel measurable function $f: \mathcal{X} \to \Reals$ is in the space $\mathcal{L}^p(\mathcal{X})$ for $1 \leq p < \infty$ if 
$$\norm{f}_{\mathcal{L}^p(\mathcal{X})} := \int_{\mathcal{X}} \abs{f(x)}^p \,dx < \infty$$
and we let 
\begin{equation*}
\mathcal{L}^p(\mathcal{X};R) = \set{f \in \mathcal{L}^p(\mathcal{X}): \norm{f}_{\mathcal{L}^p(\mathcal{X})} < R}
\end{equation*}
be a ball in the Lebesgue space.


\paragraph{Holder spaces.}

For a given $s > 0$, let $\ell = \floor{s}$ be the largest integer strictly less than $s$. Then the $s$th Holder norm is given by
\begin{equation*}
\norm{f}_{C_d^{s}(\mathcal{X})} := \sum_{\abs{\alpha} < s} \norm{D^{\alpha}f}_{\infty} + \sum_{\abs{\alpha} = \ell} \sup_{x,y \in \mathcal{X}} \frac{\abs{D^{\alpha}f(y) - D^{\alpha}f(x)}}{\norm{x - y}_2^{s - \ell}}
\end{equation*}
and the $s$th Holder space $C_d^{s}(\mathcal{X})$ consists of all functions which are $\ell$ times continuously differentiable with finite Holder norm $\norm{\cdot}_{C_d^{s}(\mathcal{X})}$. Denote the Holder ball by $C_d^{s}(\mathcal{X},R) = \set{f \in C_d^{s}(\mathcal{X}): \norm{f}_{C_d^{s}(\mathcal{X})} \leq R}$.

\paragraph{Sobolev spaces.}

For a given $s > 0$, the Sobolev space $W_d^{s,2}(\mathcal{X})$ consists of all functions $f \in \mathcal{L}^2(\mathcal{X})$ such that for each $\alpha = (\alpha_1,\ldots,\alpha_d)$ with $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ belongs to $\mathcal{L}^2(\mathcal{X})$. The Sobolev $\{s,2\}$ norm is then 
\begin{equation*}
\norm{f}_{W_d^{s,2}(\mathcal{X})}^2 = \sum_{\abs{\alpha} \leq s} \int_{\mathcal{X}} \abs{D^{\alpha}f}^2 \,dx
\end{equation*}
and for a given $L > 0$, the corresponding ball is $W_d^{s,2}(\Xset; L) = \set{f: \norm{f}_{W^{s,2}(\Xset)} \leq L}$.

\paragraph{Bounded Variation spaces.}

For a function $f \in L^1(\mathcal{X})$ the \emph{total variation} semi-norm of $f$ is
\begin{equation*}
TV(f;\mathcal{X}) := \sup \left\{ \int_{\mathcal{X}} f \, \Xsetive \, \psi \,dx : \psi \in C_c^1(\mathcal{X}; \Reals^d), \abs{\psi} \leq 1 \right\};
\end{equation*}
and we write $BV_d(\mathcal{X})$ for the subset of functions $f \in L^1(\mathcal{X})$ which have bounded norm
\begin{equation*}
\norm{f}_{BV_d(\mathcal{X})} := \norm{f}_{\infty} + TV(f;\mathcal{X}).
\end{equation*}
For a given $R > 0$, the corresponding ball is $BV_d^{1}(\mathcal{X};R) = \set{f: \norm{f}_{BV_d(\mathcal{X})} \leq R}$. 

\subsection{Graph Operators}
Let $s \geq 1$ be an integer. The $s$th-order difference operator on $G_{n,r}$, denoted $B^{(s)}$, is defined by
\begin{equation*}
B^{(s)} :=
\begin{cases}
L^{s/2},& ~~ s \textrm{ even} \\
BL^{(s - 1)/2},& ~~ s \textrm{ odd.}
\end{cases}
\end{equation*}

\subsection{Kernels}
We say a kernel function $K(\cdot)$ is a $2$nd order kernel if $K$ is compactly supported on $B(0,1)$, uniformly upper bound $\abs{K(x)} \leq K_{\max} < \infty$ for all $x \in B(0,1)$, and
\begin{equation*}
\int K(x) \,dx = \nu_d,~~ \int x K(x) \,dx = 0.
\end{equation*}
Note that the uniform kernel, defined as $K(x) = \1\{\norm{x} \leq 1\}$, is a $2$nd order kernel. 

\subsection{Notation}

We will treat $f$ interchangeably as a function $f:\Rd \to \Reals$, and as a vector $f = (f(x_1),\ldots,f(x_n))$. It should always be clear from context how we are using $f$.
For sequences $(a_n), (b_n)$, we say $a_n = O(b_n)$ if there exists a constant $c$ such that $a_n \leq cb_n$ for all $c$. We use the notation $a \wedge b$ for the minimum of $a$ and $b$.

We use the notation $\Lambda(H)$ to denote the spectrum of a matrix $H$, and $\lambda_k(H)$ to denote the $k$th smallest eigenvalue of $H$.

\section{Proofs}

\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate}}

Our analysis will proceed according to the following steps.
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $Y$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over $n$ vertices $V$, and let $\beta = (\beta_1,\ldots,\beta_n) \in \Reals^n$ be a signal over the vertices. (Although one can think of $G$ as a realization of $G_{n,r}$ for a particular sample $X$, our theory in this section will deal with $G$ as if it were an arbitrary fixed graph.) We observe responses $Y = (y_1,\ldots,y_n)$ according to the model
\begin{equation*}
y_i = \beta_i + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} N(0,1)
\end{equation*}
Let $L$ denote the graph Laplacian of $G$, and let $0 = \lambda_1 \leq \lambda_2 \leq \ldots \leq \lambda_n$ be the ordered eigenvalues of $L$, with eigenvector $v_i$ corresponding to eigenvalue $\lambda_i$. Our graph spectral test statistic is 
\begin{equation*}
T_{\spec} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i y_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec} = \1\{T_{\spec} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

In Lemma~\ref{lem:fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}$. Our bound on the Type II error will be stated as a function of $\beta^T L^s \beta$--a measure of the smoothness the signal $\beta$ displays over the graph $G$--as well as the $\kappa$th eigenvalue $\lambda_{\kappa}$.

\begin{lemma}
	\label{lem:fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}$ is upper bounded
		\begin{equation}
		\label{eqn:graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b$ and $\beta$ such that
		\begin{equation}
		\label{eqn:fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{\beta^T L^s \beta}{n\lambda_{\kappa}^s}
		\end{equation}
		the Type II error of $\phi_{\spec}$ is upper bounded,
		\begin{equation}
		\label{eqn:graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

To prove Lemma~\ref{lem:fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2.
\end{align*}
When $\beta = 0$, this equals $\kappa/n$. Otherwise, we have the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{\lambda_{\kappa}^s}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \lambda_k^s \\
& \geq \norm{\beta}_2^2 - \frac{\beta^T L^s \beta}{\lambda_{\kappa}^s},
\end{align*}
and therefore $\Ebb(T_{\spec}) \geq \kappa/n + n^{-1}(\norm{\beta}_2^2 - \beta^T L^s \beta/\lambda_{\kappa}^s)$. 

\vspace{.2 in}

\textit{Variance of $T_{\mathrm{spec}}$:}
We write $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvector $v_k$ as its $k$th column.
\begin{align*}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa)
\end{align*}
where the last inequality follows from standard properties of the Gaussian distribution. We now move on to showing the desired inequalities \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{1}{b^2}.
\end{align*}

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:graph_spectral_type_II_error}:} For simplicity, we introduce the notation
\begin{equation*}
\Delta = \frac{\norm{\beta}_2^2}{n} - \frac{\beta^T L^s \beta}{n\lambda_{\kappa}^s}.
\end{equation*}
Assumption~\eqref{eqn:fixed_graph_testing_critical_radius} implies $\Delta \geq 2 t(b)$. Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & = \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\beta^T V_{\kappa} V_{\kappa}^T \beta /n^2}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term, noting that $\Delta = \beta^T V_{\kappa} V_{\kappa}^T \beta/n$, we have
\begin{align}
\frac{\beta^T V_{\kappa} V_{\kappa}^T \beta/n^2}{\Delta^2} & = \frac{1}{n\Delta} \nonumber \\
& \leq \frac{1}{2nt(b)} \nonumber \\
& = \frac{1}{2b\sqrt{2\kappa}}, \label{eqn:spectral_type_II_error_pf2}
\end{align}
and combining~\eqref{eqn:spectral_type_II_error_pf1} and~\eqref{eqn:spectral_type_II_error_pf2} yields~\eqref{eqn:graph_spectral_type_II_error}.

\subsubsection{Step 2: Bounding neighborhood graph functionals}

To make use of Lemma~\ref{lem:fixed_graph_testing} we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that the following statements each hold with probability at least $1 - c/b$ for sufficiently large $n$ (Here and in what follows, $c$ denotes a constant which is fixed in $n$ and $f$ and does not depend on $b$, but may depend on other fixed quantities such as $d$, $s$, etc.): 
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} For any $n^{-1/(2(s - 1) + d)}\leq r \leq 1$,
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm}
	f^T L^s f \leq c \cdot b \cdot \norm{f}_{W^{s,2}(\Xset)}^2 \cdot n^{s + 1} r^{s(d + 2)} 
	\end{equation}
	\item 
	\label{event:eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} For any $\kappa = 1,\ldots,n$ and $r \leq \kappa^{-2/(d(2 +d))}$, 
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound}
	\lambda_{\kappa} \geq c \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	In particular, the inequality~\eqref{eqn:eigenvalue_tail_bound} is satisfied when $\kappa = n^{2d/(4s + d)}$ for any $r \leq n^{-4/((2+d)(4s + d))}$. 
	\item 
	\label{event:l2_norm}
	\textbf{Empirical norm of $f$:} There exists a constant $c_1$ such that if $\norm{f}_{\Leb^2(\Xset)} \geq c_1 \cdot b \cdot n^{-ws/(4s + d)}$,
	\begin{equation}
	\label{eqn:l2_to_empirical_norm}
	\norm{f}_n^2 \geq \frac{1}{b} \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

\paragraph{Proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm}:}


The probabilistic bound~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows from the more general Lemma~\ref{lem:roughness_functional_expectation_sobolev} by Markov's inequality. 
\begin{lemma}
	\label{lem:roughness_functional_expectation_sobolev}
	Let $\Xset$ be a Lipschitz domain, and let $s \geq 1$ be an integer. Suppose that $f \in W^{s,2}(\Xset)$, and further that $p \in C^{s-1}(\Xset;p_{\max})$ for some constant $p_{\max}$. Then for any $2$nd-order kernel $K$ and any $n^{-1/(2(s - 1) + d)} \leq r(n) \leq 1$, the expected graph Sobolev seminorm is upper bounded
	\begin{equation}
	\label{eqn:roughness_functional_expectation_sobolev}
	\Ebb\bigl[f^T L^s f\bigr] \leq c \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2 \cdot n^{s + 1}r^{s(d + 2)}
	\end{equation}
\end{lemma}

The proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev} is lengthy, and we defer it to Section~\ref{sec:technical_lemma_proofs}.

\paragraph{Proof of~\eqref{eqn:eigenvalue_tail_bound}:}

We prove~\eqref{eqn:eigenvalue_tail_bound} by comparing $G_{n,r}$ to the tensor product of a $d$-dimensional lattice and a complete graph. The latter is a highly structured graph with known eigenvalues, which as we will see are sufficiently lower bounded for our purposes.

Let $\wt{r} = r/(3(\sqrt{d} + 1)), M = (1/\wt{r})^d, N = n\wt{r}^d$. Assume without loss of generality that $M$ and $N$ are integers. Additionally, for $t = n^{1/d}$ and $m = M^{1/d}$ let 
\begin{equation*}
\overline{X} = \set{\frac{1}{t}(k_1,\ldots,k_d): k \in [t]^d},~~ \overline{Z} = \set{\frac{1}{m}(j_1,\ldots,j_d): j \in [m]^d}.
\end{equation*}
For a given $\overline{z}_j \in \overline{Z}$, we write $Q(z_j) = m^{-1}[j_1 - 1,j_1] \times \cdots \times m^{-1}[j_d - 1,j_d]$ for the cube of side length $1/m$ with $z_j$ at one corner. 

Consider the graph $H = (\overline{X}, E_H)$, where $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$ if
\begin{equation*}
\textrm{there exists}~\ol{z}_i, \ol{z}_j \in \ol{Z}~\textrm{such that}~\ol{x}_k \in Q(\ol{z}_i),~ \ol{x}_\ell \in Q(\ol{z}_j),~\textrm{and}~\norm{i - j}_1 \leq 1.
\end{equation*}
On the one hand $H \cong \ol{G}^M_d \otimes K_N$ where $\ol{G}^M_d$ is the $d$-dimensional lattice on $M$ nodes, and $K_N$ is the complete graph on $N$ nodes. On the other hand, we now show that with high probability $G_{n,r} \succeq H$. If $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$, then there exist $\ol{z}_i, \ol{z}_j$ such that
\begin{equation*}
\norm{\ol{x}_k - \ol{x}_{\ell}}_2 \leq m^{-1} + \norm{\ol{x}_k - \ol{z}_{i}}_2 + \norm{\ol{x}_{\ell} - \ol{z}_{j}}_2 \leq \wt{r}(1 + \sqrt{d}) = r/3.
\end{equation*}

Assuming~\eqref{eqn:slepcev_transport_distance} holds, if $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$, then for sufficiently large $n$
\begin{equation*}
\norm{\pi(\ol{x}_k) - \pi(\ol{x}_\ell)}_2 \leq 2 c \left(\frac{\log n}{n}\right)^{1/d} + \frac{r}{3} \leq r,
\end{equation*}
implying that $(\pi(\ol{x}_k), \pi(\ol{x}_{\ell})) \in E$. Therefore, $G_{n,r} \succeq \ol{G}^M_d \otimes K_N$ whenever~\eqref{eqn:slepcev_transport_distance} holds.

The eigenvalues of lattices and complete graphs are known to satisfy, respectively
\begin{equation*}
\lambda_k(\ol{G}^{M}_d) \geq \frac{k^{2/d}}{M^{2/d}}~\textrm{for $k = 0,\ldots,M - 1$},~~ \textrm{and}~\lambda_{j}(K_N) \geq N\1\{j > 0\}~\textrm{for $j = 0,\ldots,N-1$.}
\end{equation*}
and by standard facts regarding the eigenvalues of tensor product graphs, we have that the spectrum $\Lambda(H)$ satisfies
\begin{equation*}
\Lambda(H) = \set{N\lambda_k(\ol{G}^{M}_d) + M\lambda_j(K_N): \textrm{for $k = 0,\ldots,M - 1$ and $j = 0,\ldots,N-1$}}
\end{equation*}
For all $j = 1,\ldots,N-1$, we have that $M\lambda_j(K_N) = MN = n$. Therefore,
\begin{align*}
\lambda_{\kappa}(H) & \geq \{n \wedge N\lambda_{\kappa}(\ol{G}^{M}_d)\} \\
& \geq \{n \wedge n\wt{r}^d\frac{\kappa^{2/d}}{M^{2/d}}\} \\
& \geq \{n \wedge (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d}\} \\
& \geq (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d},
\end{align*}
where the last inequality is satisfies since $r \leq \kappa^{-2/(d(d + 2))}$.

Finally, Theorem~\ref{thm:slepcev_transport_distance} establishes that \eqref{eqn:slepcev_transport_distance} occurs with probability at least $1 - c/n$. Since $\lambda_{\kappa}(G_{n,r}) \geq \lambda_{\kappa}(H)$ when~\eqref{eqn:slepcev_transport_distance} is satisfied, this completes the proof of~\eqref{eqn:eigenvalue_tail_bound}.

\paragraph{Proof of~\eqref{eqn:l2_to_empirical_norm}:}

\begin{lemma}
	\label{lem:empirical_norm_sobolev}
	Let $\Xset$ be a Lipschitz domain over which the density is upper and lower bounded 
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty ~~\textrm{for all $x \in \Xset$,}
	\end{equation*}
	and let $f \in W^{s,2}(\Xset)$.Then for any $b \geq 1$, there exists $c_1$ such that if 
	\begin{equation}
	\norm{f}_{\Leb^2(\Xset)} \geq 
	\begin{cases*}
	c_1 \cdot b \cdot \norm{f}_{W^{s,2}(\Xset)} \cdot \max\{n^{-1/2},n^{-s/d}\},~~\textrm{if}~2s \neq d \\
	c_1 \cdot b \cdot \norm{f}_{W^{s,2}(\Xset)} \cdot n^{-a/2},~~\textrm{if}~ 2s = d ~\textrm{for any}~ 0 < a < 1
	\end{cases*}
	\end{equation}
	then,
	\begin{equation}
	\label{eqn:l2_to_empirical_norm_sobolev}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}
	\end{equation}
	where $c_1$ and $c_2$ are constants which may depend only on $s$, $\Xset$, $d$, $p_{\min}$ and $p_{\max}$.
\end{lemma}
The lower bound~\eqref{eqn:l2_to_empirical_norm} results from the more general Lemma~\ref{lem:empirical_norm_sobolev}, which can be verified by checking the various orderings of $2s/(4s + d)$, $s/d$ and $1/2$ whenever $4s < d$. 
\begin{proof}[Proof of Lemma~\ref{lem:empirical_norm_sobolev}]
	To prove~\eqref{eqn:l2_to_empirical_norm_sobolev} we will show
	\begin{equation*}
	\mathbb{E}\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot \left(\mathbb{E}\bigl[\norm{f}_n^2\bigr]\right)^2
	\end{equation*}
	whence the claim follows from the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). Since $p \leq p_{\max}$ is uniformly bounded, we can relate $\mathbb{E}\bigl[\norm{f}_n^4\bigr]$ to the $\Leb^4$ norm,
	\begin{equation*}
	\mathbb{E}\bigl[\norm{f}_n^4\bigr] = \frac{(n-1)}{n}\left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + \frac{\mathbb{E}\Bigl[\bigl(f(x_1)\bigr)^4\Bigr]}{n} \leq \left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + p_{\max}^2\frac{\norm{f}_{\Leb^4}^4}{n}.
	\end{equation*}
	We will use a Sobolev inequality to relate $\norm{f}_{\Leb^4}$ to $\norm{f}_{W_d^{s,2}(\Xset)}$. The nature of this inequality depends on the relationship between $s$ and $d$ (see Theorem 6 in Section 5.6.3 of \textcolor{red}{Evans} for a formal statement), so from this point on we divide our analysis into three cases: (i) the case where $2s > d$, (ii) the case where $2s < d$, and (iii) the borderline case $2s = d$.
	
	\paragraph{Case 1: $2s > d$.}
	When $2s > d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in C^{\gamma}(\overline{\Xset})$ for some $\gamma > 0$ which depends on $s$ and $d$, with the accompanying estimate
	\begin{equation*}
	\sup_{x \in \Xset} \abs{f(x)} \leq \norm{f}_{C^{\gamma}(\Xset)} \leq c \norm{f}_{W^{s,2}(\Xset)}.
	\end{equation*}
	Therefore,
	\begin{align*}
	\norm{f}_{\Leb^4}^4 & = \int_{\Xset} [f(x)]^4 \,dx \\
	& \leq \left(\sup_{x \in \Xset} \abs{f(x)}\right)^2 \cdot \int_{\Xset} [f(x)]^2 \,dx \\
	& \leq c \norm{f}_{W^{s,2}(\Xset)}^2 \cdot \norm{f}_{\Leb^2(\Xset)}^2.
	\end{align*}
	Since by assumption
	\begin{equation*}
	\norm{f}_{\Leb^2(\Xset)}^2 \geq c_1^2 \cdot b^2 \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2 \cdot \frac{1}{n},
	\end{equation*}
	we have
	\begin{equation*}
	p_{\max}^2\frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c \norm{f}_{W^{s,2}(\Xset)}^2 \cdot \frac{\norm{f}_{\Leb^2(\Xset)}^4}{n \norm{f}_{\Leb^2(\Xset)}^2} \leq c \frac{\norm{f}_{\Leb^2(\Xset)}^4}{c_1^2 b^2} \leq \frac{\Ebb\bigl[\norm{f}_n^2\bigr]}{b^2},
	\end{equation*}
	where the last inequality follows by taking $c_1$ sufficiently large.
	
	\paragraph{Case 2: $2s < d$.}
	When $2s < d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in \Leb^q(\Xset)$ for $q = 2d/(d - 2s)$, and moreover that
	\begin{equation*}
	\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{W^{s,2}(\Xset)}.
	\end{equation*}
	Since $4 = 2\theta + (1 - \theta)q$ for $\theta = 2 - d/(2s)$, Lyapunov's inequality implies
	\begin{equation*}
	\norm{f}_{\Leb^4(\Xset)}^4 \leq \norm{f}_{\Leb^2}^{2\theta} \cdot \norm{f}_{\Leb^q(\Xset)}^{(1 - \theta)q} \leq c \norm{f}_{\Leb^2(\Xset)}^{4} \cdot \left(\frac{\norm{f}_{W^{s,2}(\Xset)}}{\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s}.
	\end{equation*}
	By assumption, $\norm{f}_{\Leb^2(\Xset)} \geq c_1 b \norm{f}_{W^{s,2}(\Xset)} n^{-s/d}$, and therefore
	\begin{equation*}
	p_{\max}^2 \frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c\norm{f}_{\Leb^2(\Xset)}^4 \left(\frac{\norm{f}_{W^{s,2}(\Xset)}}{n^{s/d}\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s} \leq \frac{c\norm{f}_{\Leb^2(\Xset)}^4}{c_1b^{d/s}} \leq \frac{\norm{f}_{\Leb^2(\Xset)}^4}{b^2}.
	\end{equation*}
	where the last inequality follows when $c_1$ is sufficiently large, and keeping in mind that $d/s > 2$ and $b \geq 1$. 
	
	\paragraph{Case 3: $2s = d$.}
	Assume $f$ satisfies~\eqref{eqn:paley_zygmund_1} for a given $0 < a < 1$. When $2s = d$, since $\Xset$ is a Lipschitz domain we have that $f \in L^q(\Xset)$ for any $q < \infty$, with the accompanying estimate
	\begin{equation*}
	\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{W^{s,2}(\Xset)}.
	\end{equation*}
	In particular the above holds for $q = 2/(1 - a)$ when $1/2 < a < 1$, and for any $q > 4$ when $0 < a < 1/2$. Using Lyapunov's inequality as in the previous case then implies the desired result.
\end{proof}

\subsubsection{Step 3: Putting the pieces together}

We note that for all possible values of $X \in \Xset^n$, under the null hypothesis $f = f_0 = 0$ and therefore $\beta = (f(x_1),\ldots,f(x_n)) = 0$ as well. Therefore by~\eqref{eqn:graph_spectral_type_I_error}, we have the following bound on Type I error:
\begin{equation}
\Ebb_{f_0}(\phi_{\mathrm{spec}}) = \mathbb{E}(\mathbb{E}_{\beta = 0}(\phi_{\spec}) | X) \leq \frac{1}{b^2}.
\end{equation}

Now, we bound Type II error under the assumption $f \in W^{s,2}(\mathcal{X};R), p \in C^{s - 1}(\Xset;p_{\max})$ uniformly bounded away from $0$ and $\infty$ over $\Xset$, and 
\begin{equation}
\label{eqn:critical_radius_1}
\norm{f}_{\Leb^2}^2 \geq \epsilon^2 = c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4s + d)}\} \cdot n^{-4s/(4s + d)}.
\end{equation}
Choosing $n^{-1/(2(s - 1) + d)}\leq r(n) \leq n^{-4/((2+d)(4s + d))}$, we may therefore apply our conclusions in Step 2; namely, that for every possible choice of $f \in W^{s,2}(\Xset;R)$ there exists a good set $\mathcal{E}_f \subseteq \Xset^n$ with $\Pbb(\mathcal{E}_f) \geq 1 - c/b$ such that each of \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, and \eqref{eqn:l2_to_empirical_norm_sobolev} hold for all $X \subseteq \mathcal{E}_f$. The choice $\kappa = (nR^2)^{2d/(4s + d)}$ balances the squared bias and variance terms on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius}, and we have that for all $X \subseteq \mathcal{E}_f$
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{f^T L^s f}{n\lambda_{\kappa}^s} & \leq 2bR^{2d/(4s + d)}n^{-4s/(4s+d)} + cbR^2\frac{n^{s} r^{s(d + 2)}}{\lambda_{\kappa}^s} \tag{by \eqref{eqn:continuous_to_discrete_sobolev_norm}} \\
& \leq cbR^{2d/(4s + d)}n^{-4s/(4s+d)} \tag{by \eqref{eqn:eigenvalue_tail_bound}} \\
& \leq \frac{1}{b}\norm{f}_{\Leb^2(\Xset)}^2 \\
& \leq \frac{1}{n}\sum_{i = 1} \bigl[f(x_i)\bigr]^2. \tag{by \eqref{eqn:l2_to_empirical_norm_sobolev}}
\end{align*}
where the last two inequalities follow for a suitably large choice of $c_1$ in~\eqref{eqn:critical_radius_1}.
We conclude that for all $X \subseteq \mathcal{E}_f$, the inequality \eqref{eqn:fixed_graph_testing_critical_radius} is satisfied with respect to $\beta = (f(x_1),\ldots,f(x_n))$ and $G = G_{n,r}$. As a result the worst-case Type II error is bounded
\begin{equation*}
\sup_{\substack{f \in W^{s,2}(\Xset;R), \\ \norm{f}_{\Leb^2(\Xset)} \geq \epsilon}}\mathbb{E}_{f}(1 - \phi_{\spec}) \leq \sup_{\substack{f \in W^{s,2}(\Xset;R), \\ \norm{f}_{\Leb^2(\Xset)} \geq \epsilon}} \mathbb{E}\bigl[\mathbb{E}_{\beta}(1 - \phi_{\spec}|X \in \mathcal{E}_f)\bigr] + \frac{c}{b} \leq \frac{3 + c}{b},
\end{equation*}
completing the proof of Theorem~\ref{thm:sobolev_testing_rate} upon proper choice of constants $c_1,c_2$ in that Theorem.









\clearpage

\subsection{Proof of Proposition~\ref{prop:L4_testing_rate}}

Let $T_{\mathrm{mean}} = \frac{1}{n}\sum_{i = 1}^{n} y_i^2$. The expectation of $T_{\mathrm{mean}}$ is
\begin{equation*}
\Ebb(T_{\mathrm{mean}}) = \mathbb{E}(f^2(x_1)) + 1,
\end{equation*}
and the variance can be upper bounded
\begin{equation*}
\Var(T_{\mathrm{mean}}) \leq \frac{1}{n}(3 + p_{\max} \norm{f}_{\Leb^4}^4 + p_{\max}\norm{f}_{\Leb^2}^2) = \frac{c}{n}.
\end{equation*}
Since $\mathbb{E}(f^2(x_1)) \geq p_{\min} \epsilon^2$, when $\epsilon^2 \gtrsim b\cdot n^{-1/2}$ we can apply Chebyshev's inequality to obtain the claimed result.

\subsection{Proof of Theorem~\ref{thm:twosample_sobolev_testing_rate}}

As mentioned previously, to prove Theorem~\ref{thm:twosample_sobolev_testing_rate}, we relate the two sample model to the following regression model: we observe
$X = \{x_1,\ldots,x_n\} \sim \mu$ (where we recall $\mu = (P + Q)/2$), and associated labels
\begin{equation*}
a_i = 
\begin{cases}
1, & \textrm{with probability}~ \frac{p(x_i)}{p(x_i) + q(x_i)} \\
-1, & \textrm{with probability}~ \frac{q(x_i)}{p(x_i) + q(x_i)}
\end{cases}
\end{equation*}
where $a_i$ is conditionally independent of $x_j,a_j$ given $x_i$. 

\textcolor{red}{TODO:} Complete the above argument.

Our analysis will trace a similar path to the proof of Theorem~\ref{thm:sobolev_testing_rate}, proceeding according to the following steps:
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $a$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded. One of these functionals will be a measure of eigenvector incoherence.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta^{(p)} = (\beta_1^{(p)},\ldots,\beta_n^{(p)}) \in \Reals^n$, $\beta^{(q)} = (\beta_1^{(q)},\ldots,\beta_n^{(q)}) \in \Reals^n$ be non-negative signals over the vertices $V$. We observe labels $a = (a_1,\ldots,a_n)$ according to the model
\begin{equation*}
a_i = 
\begin{cases*}
1, & \textrm{with probability $\frac{\betap_i}{\betap_i + \betaq_i}$} \\
-1, & \textrm{with probability $\frac{\betaq_i}{\betap_i + \betaq_i}$}
\end{cases*}
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec}^{(2)} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec}^{(2)} = \1\{T_{\spec}^{(2)} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $\Pi_{\max}(\kappa;G)$ be a measure of the incoherence of the eigenvectors $v_1,\ldots,v_\kappa$, given by
\begin{equation*}
\Pi_{\max}(\kappa;G) := \max_{i = 1,\ldots,n} \left\{\sum_{k = 1}^{\kappa} v_{k,i}^2\right\}
\end{equation*}
In Lemma~\ref{lem:twosample_fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}^{(2)}$. Our bound on the Type II error will be stated as a function of $\Pi_{\max}(\kappa;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$ and the smoothness functional $S_2(\beta;G)$. We use the notation $\varDelta^{(p,q)} = (\betap - \betaq)/(\betap + \betaq)$.

\begin{lemma}
	\label{lem:twosample_fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\betap = \betaq$, the Type I error of $\phi_{\spec}^{(2)}$ is upper bounded
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_I_error}
		\mathbb{E}_{\betap,\betap}(\phi_{\spec}^{(2)}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} Suppose $\Pi_{\max}(\kappa;G) \leq 1$. Then for any $b$, $\betap$ and $\betaq$ such that
		\begin{equation}
		\label{eqn:twosample_fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} (\varDelta_i^{(p,q)})^2 \geq \frac{1}{1 - \Pi_{\max}(\kappa;G)}\left(2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\varDelta;G)}{ns_{\kappa}}\right)
		\end{equation}
		the Type II error of $\phi_{\spec}^{(2)}$ is upper bounded,
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_II_error}
		\mathbb{E}_{\betap,\betaq}(1 - \phi_{\spec}^{(2)}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:twosample_fixed_graph_testing}.}

To prove Lemma~\ref{lem:twosample_fixed_graph_testing} we will compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}^{(2)}$. The inequalities \eqref{eqn:twosample_graph_spectral_type_I_error} and \eqref{eqn:twosample_graph_spectral_type_II_error} can then be computed using Chebyshev's inequality in a manner very similar to the proof of Lemma~\ref{lem:fixed_graph_testing}, and we omit the details. Let $w_i := a_i - \vardeltapq_{i}$, and note that $\Ebb(w_i) = 0$ and
\begin{equation*}
\Ebb(w_i^2) = \frac{\betap_i \betaq_i}{(\betap_i + \betaq_i)^2} = 1 - (\vardeltapq_{i})^2,~~ \Ebb(w_i^3) = \frac{1}{2}(1 - (\vardeltapq_{i})^2)\vardeltapq_{i}
\end{equation*}

\vspace{.2 in}

\textit{Mean of $T_{\spec}^{(2)}$}:

We have
\begin{align*}
\Ebb(T_{\spec}^{(2)}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\vardeltapq}{v_k}^2 + \Ebb\bigl(\dotp{w}{v_k}^2 + 2 \dotp{w}{v_k} \dotp{\vardeltapq}{v_k}\bigr)\right) \\
& = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{1 - (\vardeltapq)^2}{v_k^2} + \dotp{\vardeltapq}{v_k}\right) \\
& \geq \frac{\kappa}{n} + \left(\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} - \dotp{(\vardeltapq)^2}{v_k^2}\right)
\end{align*}
When $\vardeltapq = 0$, this equals $\kappa/n$. Otherwise, we have the following upper bound:
\begin{align*}
\sum_{k = 1}^{\kappa} \dotp{(\vardeltapq)^2}{v_k^2}  & = \sum_{i = 1}^{n} (\vardeltapq_{i})^2 \left(\sum_{k = 1}^{\kappa} v_{k,i}^2 \right)\\
& \leq \Pi_{\max}(\kappa,G) \cdot \sum_{i = 1}^{n} (\vardeltapq_{i})^2 = \Pi_{\max}(\kappa,G) \cdot \norm{\vardeltapq}_2^2.
\end{align*}
Additionally, $\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} \geq \norm{\vardeltapq}_2^2 - S_2(\beta;G)/s_{\kappa}$ by reasoning given in the proof of Lemma~\ref{lem:fixed_graph_testing}, and as a result 
\begin{equation*}
\Ebb(T_{\spec}^{(2)}) \geq \frac{\kappa}{n} -  \frac{\bigl(1 - \Pi_{\max}(\kappa;G)\bigr)}{n} \norm{\vardeltapq}_2^2 - \frac{S_2(\beta;G)}{ns_{\kappa}}
\end{equation*}

\vspace{.2 in}
\textit{Variance of $T_{\spec}^{(2)}$:}

We have
\begin{align}
\Var(T_{\spec}^{(2)}) & = \frac{1}{n^2}\Var(2\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w} + \dotp{V_{\kappa}V_{\kappa}^Tw}{w}) \nonumber \\
& = \frac{1}{n^2}\Bigl\{4\underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w})}_{=: V_1} + 2\underbrace{\Cov(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w},\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=: K_1} + \underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=:V_2}\Bigr\}, \label{eqn:var}
\end{align}
and we now upper bound each of the three terms on the right hand side of the previous display.

\textbf{Upper bound on $V_1$:}
Let $\Sigma := \Cov(w)$ be the covariance matrix of $w$. Noting that $\Sigma \preceq I$, we have
\begin{equation}
\label{eqn:var1}
\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w}) =(\vardeltapq)^T V_{\kappa}V_{\kappa}^T \Sigma V_{\kappa}V_{\kappa}^T \vardeltapq \leq \norm{V_{\kappa}V_{\kappa}^T\vardeltapq}^2.
\end{equation}

\textbf{Upper bound on $K_1$:}
Noting that $\Ebb(\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w}) = 0$, we have that
\begin{align}
K_1 & = \Ebb\left[\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w} \dotp{V_{\kappa}V_{\kappa}^T w}{w}\right] \nonumber \\
& = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{i' = 1}^{n} \Ebb\left[w_i w_j (V_{\kappa}V_{\kappa}^T)_{ij} w_{i'} (V_{\kappa}V_{\kappa}^T\vardeltapq)_i\right] \nonumber \\
& = \sum_{i = 1}^{n} \Ebb\left[w_i^3\right] (V_{\kappa}V_{\kappa}^T)_{ii} (V_{\kappa}V_{\kappa}^T \vardeltapq)_i \nonumber \\
& = \frac{1}{2}\sum_{i = 1}^{n} (1 - (\vardeltapq_{i})^2)(\vardeltapq_{i}) (V_{\kappa} V_{\kappa}^T)_{ii}  (V_{\kappa}V_{\kappa}^T\vardeltapq)_i \nonumber \\
& \leq \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \left(\sum_{i = 1}^{n} (\vardeltapq_{i})^2 (V_{\kappa} V_{\kappa}^T)_{ii}^2\right)^{1/2} \nonumber \\
& \leq \Pi(\kappa,G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq}  \label{eqn:var2} 
\end{align}

\textbf{Upper bound on $V_2$:}
$V_2$ is a variance of a sum, which we re-express as the sum of covariances:
\begin{align*}
V_2 & = \Var(\dotp{V_{\kappa}V_{\kappa}^T w}{w}) \\
& = \sum_{i,j,i',j' = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij} (V_{\kappa}V_{\kappa}^T)_{i'j'} \Cov(w_i w_j, w_{i'} w_{j'}).
\end{align*}
This covariance will be non-zero only when $i = i' \neq j = j'$, $i = j' \neq j = i'$, or $i = i' = j = j'$, and therefore
\begin{align}
V_2 & = 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 \Var(w_iw_j) + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \Var(w_i^2) \nonumber\\
& \leq 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \leq 3 \mathrm{tr}\bigl((V_{\kappa}V_{\kappa}^T)^2\bigr) = 3\kappa. \label{eqn:var3}
\end{align}

Now, plugging \eqref{eqn:var1}, \eqref{eqn:var2}, and \eqref{eqn:var3} back into \eqref{eqn:var}, we obtain
\begin{equation}
\label{eqn:var_5}
\Var(T_{\spec}^{(2)}) \leq \frac{1}{n^2}\left\{4\norm{V_{\kappa}V_{\kappa}^T \vardeltapq}^2 + \Pi(\kappa;G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq} + 3\kappa \right\}.
\end{equation}


\subsubsection{Step 2: Bounding neighborhood graph functionals}

Note that since $p,q \in W^{1,2}(\mathcal{X};R)$, we have $f := p - q \in W^{1,2}(\mathcal{X};2R)$. Therefore the bounds~\eqref{eqn:continuous_to_discrete_sobolev_norm} and \eqref{eqn:l2_to_empirical_norm} apply to $S_2(\varDelta,G_{n,r})$ and $(1/n)\sum_{i = 1}^{n}\varDelta_i^2$, respectively. The bound~\eqref{eqn:eigenvalue_tail_bound} continues to apply to $s_{\kappa}$. What remains is to upper bound the incoherence parameter
\begin{equation}
\label{eqn:pi_max}
\Pi_{\max}(\kappa,G_{n,r}) \leq \frac{1}{2}~~\textrm{when}~ \kappa = n^{2/5}~\textrm{and}~ r = \log^a n \cdot \left(\frac{\log n}{n}\right)
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$.

Our proof of~\eqref{eqn:pi_max} will proceed by relating $G_{n,r}$ to a graph which satsifies a strict notion of incoherence, the chain graph. When $G = \overline{G}$, the incoherence parameter $\Pi_{\max}$ is upper bounded
\begin{equation}
\Pi_{\max}(\kappa,\overline{G}) \leq \frac{\kappa}{n}.
\end{equation}
To prove that the weaker bound \eqref{eqn:pi_max} holds with respect to $G_{n,r}$, we will show that the two graphs $\overline{G}$ and $G_{n,r}$ are $\delta$-spectrally similar, meaning
\begin{equation}
\label{eqn:spectral_similarity}
(1 - \delta_n) x^T L x \leq x^T \overline{L} x \leq (1 + \delta_n) x^T L x~~\textrm{for all}~x \in \Reals^n, \delta_n = c \cdot (\log n)^{h(a,1) + da}\cdot \frac{\log n}{n}
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$. The proof of~\eqref{eqn:spectral_similarity} relies heavily on bounding the transport distance between $X$ and $\overline{X}$, i.e. \eqref{eqn:slepcev_transport_distance}. Then the following technical Lemma along with \eqref{eqn:spectral_similarity} will imply \eqref{eqn:pi_max}.
\begin{lemma}
	\label{lem:pi_max_pf_1}
	Let $G$ satisfy~\eqref{eqn:spectral_similarity} for a given $0 \leq \delta < 1$. Then for any $R \gtrsim n \delta^{1/2} (s_{n}^{1/2} \vee 1)$ and $k \leq n/8$,
	\begin{equation*}
	v_{k,i}^2 \lesssim \frac{1}{n}\left(R + \frac{n^4 \delta^2 s_{n}^2}{R^3}\right) \quad \textrm{for every $i = 1,\ldots,n$.}
	\end{equation*} 
\end{lemma}

\textcolor{red}{TODO}: Explain how Lemma~\ref{lem:pi_max_pf_1} implies~\eqref{eqn:pi_max}.


At a high level, Lemma~\ref{lem:pi_max_pf_1} is proved through repeated applications of the Davis-Kahan Theorem. The full proof is long and we delay presenting it until Section~\ref{sec:technical_lemma_proofs}.

\subsubsection{Step 3: Conclusion}

With Lemma~\ref{lem:twosample_fixed_graph_testing} as well as \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, \eqref{eqn:l2_to_empirical_norm} and \eqref{eqn:pi_max} in hand, the proof of Theorem~\ref{thm:twosample_sobolev_testing_rate} follows by similar reasoning to the conclusion of Theorem~\ref{thm:sobolev_testing_rate}.

\section{Proofs of Technical Results}
\label{sec:technical_lemma_proofs}

Here we give in full detail the proofs of Lemma~\ref{lem:roughness_functional_expectation_sobolev}, \eqref{eqn:spectral_similarity}, and Lemma~\ref{lem:pi_max_pf_1}.

\subsection{Proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev}}
To simplify exposition, we introduce the iterated difference operator, defined recursively as
\begin{equation*}
D_{jk}f(x) = (D_{k}f(x_j) - D_{k}f(x))\frac{K_r(x_j,x)}{r^d},~~ D_jf(x) = (f(x_j) - f(x))\frac{K_r(x_j,x)}{r^d}~~ \textrm{for $j \in [n], k \in [n]^q$}
\end{equation*}
We will also use the notation $d_jf(x) := (f(x_j) - f(x))$. We split our analysis into cases based on whether $s$ is even or odd. 

\subsubsection{Case 1: $s$ is even.}
When $s$ is even, letting $q = s/2$ we have the decomposition
\begin{equation}
\label{eqn:continuous_to_discrete_sobolev_norm_pf1}
f^T L^s f =  r^{ds} \cdot \sum_{i = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} D_kf(x_i) D_{\ell}f(x_i). 
\end{equation}

For given index vectors $k,\ell \in [n]^q$ and indices $i,j$, let $I = \abs{k \cup \ell \cup i}$ be the total number of unique indices. We separate our analysis into cases based on the magnitude of $I$, specifically whether $I = s + 1$ (the leading terms where all indices are distinct) $I < s + 1$ (the terms where at least one index is repeated) and show that
\begin{equation}
\label{eqn:expected_difference_operators_1}
\Ebb(D_kf(x_i) D_\ell f(x_i)) =
\begin{cases*}
O(r^{2s}) \cdot \norm{f}_{W^{s,2}(\Xset)}^2, & ~~\textrm{if $I = s + 1$} \\
O(r^{2} r^{d(I - (s + 1))}) \cdot [f]_{W^{1,2}(\Xset)}^2, & ~~\textrm{if $I < s + 1$}~ 
\end{cases*}
\end{equation}
We will prove~\eqref{eqn:expected_difference_operators_1} in Section~\ref{subsec:expected_difference_operators_pf}. First, we verify that~\eqref{eqn:continuous_to_discrete_sobolev_norm_pf1} and \eqref{eqn:expected_difference_operators_1} are together enough to show Lemma~\ref{lem:roughness_functional_expectation_sobolev} when $s$ is even. In the sum on the right hand side of~\eqref{eqn:continuous_to_discrete_sobolev_norm_pf1}, there are $O(n^{I})$ terms with exactly $I$ distinct indices. When $I < s + 1$, by~\eqref{eqn:expected_difference_operators_1} the total contribution of such terms to the sum is $O(n^{I}r^{d(I - 1) + 2}) \cdot [f]_{W^{1,2}(\Xset)}^2$. Since by assumption $r \geq n^{-1/d}$, this increases with $I$. Taking $I = s$ to be the largest integer less than $s + 1$, the contribution of these terms to the sum is therefore $O(n^sr^{d(s - 1) + 2}) \cdot [f]_{W^{1,2}(\Xset)}^2$ which in light of the restriction $r \geq n^{-1/(2(s - 1) + d)}$ is $O(n^{s+1}r^{s(d +2)}) \cdot [f]_{W^{1,2}(\Xset)}^2$. On the other hand when $I = s + 1$, by~\eqref{eqn:expected_difference_operators_1} we immediately have that the total contribution of these terms is $O(n^{s + 1}r^{2(s + d)}) \cdot \norm{f}_{W^{s,2}(\Xset)}$. Therefore,
\begin{equation*}
\Ebb(f^T L^s f) = O(n^{s+1}r^{s(d+2)}) \cdot \norm{f}_{W^{s,2}(\Xset)}.
\end{equation*}

\subsubsection{Case 2: $s$ is odd.}
When $s$ is odd, letting $q = (s - 1)/2$ we have
\begin{equation}
\label{eqn:roughness_functional_representation_odd}
f^T L^s f =  r^{ds} \cdot \sum_{i,j = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} \bigl(d_jD_kf(x_i)\bigr) \cdot  \bigl(d_jD_{\ell}f(x_i)\bigr) \cdot K_r(x_i,x_j).
\end{equation}
For given index vectors $k,\ell \in [n]^q$ and indices $i,j \in [n]$, let $I = \abs{k \cup \ell \cup i \cup j}$ be the total number of unique indices. Similar to the case when $s$ is even, we show that 
\begin{equation}
\label{eqn:expected_difference_operators_2}
\Ebb(d_iD_kf(x_j) d_iD_\ell f(x_j)) =
\begin{cases*}
O(r^{2s}) \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2, & ~~\textrm{if $I = s + 1$} \\
O(r^{2} \cdot r^{d(I - (s + 1))}) \cdot [f]_{W_d^{1,2}(\Xset)}^2, & ~~\textrm{if $I < s + 1$}~ 
\end{cases*}
\end{equation}
Then Lemma~\ref{lem:roughness_functional_expectation_sobolev} follows from similar reasoning to the case where $s$ was even.

\subsubsection{Proof of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2}}
\label{subsec:expected_difference_operators_pf}

Note that if $f$ is constant almost everywhere, the claim is immediate as $D_kf(x_i) = 0$ with probability one. Otherwise $[f]_{W^{1,2}(\Xset)} > 0$, which we shall assume in what follows.

Let $\delta = \min\{r^{2s},1\}\cdot[f]_{W^{1,2}(\Xset)} > 0$. Our analysis will make heavy use of Taylor expansions, and we therefore would like to show that there exists some smooth $g \in C^{\infty}(\Rd)$ such that 
\begin{equation*}
\Bigl|\Ebb[D_kf(x_i) D_{\ell} f(x_i)] - \Ebb[D_kg(x_i) D_\ell g(x_i)]\Bigr| <  \delta,
\end{equation*}
and additionally $[g]_{W^{\ell,2}(\Rd)} \leq c [f]_{W^{\ell,2}(\Rd)}$ for each $\ell = 0,\ldots,s$.  Then if \eqref{eqn:expected_difference_operators_1} and \eqref{eqn:expected_difference_operators_2} hold with respect to $g$, they hold (up to constants) with respect to $f$ as well. 

To construct such a $g$, we first take an extension of $f$ be defined on all of $\Rd$, and then mollify. Since $\Xset$ is a Lipschitz domain, there exists an extension \textcolor{red}{citation} $\wt{f}$ of $f$ compactly supported on $\Rd$ such that $\wt{f} = f$ a.e. on $\Xset$, and $[\wt{f}]_{W^{\ell,2}(\Rd)} \leq c [f]_{W^{\ell,2}(\Xset)}$ for each $\ell = 0,\ldots,s$. Since $\wt{f} = f$ a.e. on $\Xset$, the expected difference operators satisfy $\Ebb[D_k\wt{f}(x_i) D_\ell \wt{f}(x_i)] = \Ebb[D_kf(x_i) D_\ell f(x_i)]$. 

Now, since $\wt{f} \in W^{s,2}(\Rd)$, there exists a sequence $(g_m) \subset C^{\infty}(\Rd)$ such that $\norm{g_m - \wt{f}}_{W^{s,2}(\Rd)} \to 0$. On the one hand, by the Cauchy-Schwarz inequality
\begin{equation*}
\abs{\Ebb\bigl[D_k\wt{f}(x_i) D_{\ell} \wt{f}(x_i)\bigr] - \Ebb\bigl[D_kg_m(x_i) D_\ell g_m(x_i)\bigr]} \leq \frac{c}{r^{sd}} \cdot \norm{f - g_m}_{\Leb^2(\Rd)}
\end{equation*}
and taking $m$ to be sufficiently large, we can make the right hand side less than $\delta$. On the other hand, since $\norm{g_m - \wt{f}}_{W^{s,2}(\Rd)} \to 0$, there exists $m$ sufficiently large such that $[g_m]_{W^{s,2}(\Rd)}$ is at most say $2 [\wt{f}]_{W^{s,2}(\Rd)}$. We therefore take $g = g_m$ for $m$ large enough to satisfy both conditions. 

Our task is now to prove that \eqref{eqn:expected_difference_operators_1} and \eqref{eqn:expected_difference_operators_2} hold with respect to $g$. We first prove the desired bounds in the case when some indices are repeated, and then the desired bounds in the case when all indices are distinct.

\paragraph{Repeated indices.}

Since the proofs of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2} are essentially the same for the case where some index is repeated, we will assume without loss of generality that $s$ is even. Let $k,\ell \in [n]^q$ be index vectors for $q = s/2$. 

When at least one index is repeated, we obtain a sufficient upper bound by reducing the problem of upper bounding the iterated difference operator to that of upper bounding a single difference operator. Letting $k = (k_1,\ldots,k_q)$, we can show by induction that the absolute value of the iterated difference operator $\abs{D_kg(x_i)}$ is upper bounded by
\begin{equation*}
\abs{D_kg(x_i)} \leq \left(\frac{2K_{\max}}{r^d}\right)^{q-1} \sum_{h \in k \cup i} \abs{D_{k_q}g(x_h)} \cdot \1\{G_{n,r}[X_{k \cup i}]~\textrm{is a connected graph} \}.
\end{equation*}
Therefore,
\begin{align}
\abs{D_kg(x_i)} \cdot \abs{D_{\ell}g(x_i)} & \leq \left(\frac{2K_{\max}}{r^d}\right)^{2(q - 1)} \sum_{h,j \in k \cup \ell \cup i} \abs{D_{k_q}g(x_h)} \cdot \abs{D_{\ell_q}g(x_j)} \cdot \1\{G_{n,r}[X_{k \cup i}], G_{n,r}[X_{\ell \cup i}]~\textrm{are connected graphs.} \} \nonumber \\
& =  \left(\frac{2K_{\max}}{r^d}\right)^{2(q - 1)} \sum_{h,j \in k \cup \ell \cup i} \abs{D_{k_q}g(x_h)} \cdot \abs{D_{\ell_q}g(x_j)} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is a connected graph.} \} \label{eqn:expected_difference_operators_sobolev_pf0}
\end{align}

We now break our analysis into three cases, based on the number of distinct indices in $k_q,\ell_q,h,j$. In each case we will obtain the same rate
\begin{equation*}
\Ebb\Bigl[\abs{D_{k_q}g(x_h)} \cdot \abs{D_{\ell_q}g(x_j)}\Bigr] = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [g]_{W^{1,2}(\Rd)}^2,
\end{equation*}
and plugging this back in to~\eqref{eqn:expected_difference_operators_sobolev_pf0} we have that for any $k, \ell \in [n]^q$
\begin{equation*}
\Ebb\Bigl[\bigl|D_{k}g(x_i)\bigr| \cdot \bigl|D_{\ell}g(x_i)\bigr|\Bigr] = O(r^{(\abs{k \cup \ell \cup i} - (2q + 1))d + 2}) \cdot [g]_{W^{1,2}(\Rd)}^2.
\end{equation*}

\textit{Case 1: Two distinct indices.}
Let $k_q = \ell_q = i$, and $h = j$. Using the law of iterated expectation, we obtain
\begin{align*}
\Ebb \left[\bigl(D_{i}g(x_j)\bigr)^2 \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\right] &= \Ebb \left[\bigl(D_{i}g(x_j)\bigr)^2 \cdot \Pbb\bigl[\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \} |x_i,x_j\bigr]\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 2)d}) \cdot \Ebb\left[\bigl(D_{i}g(x_j)\bigr)^2\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d}) \cdot \Ebb\left[\bigl(d_{i}g(x_j)\bigr)^2K_r(x_i,x_j)\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [g]_{W^{1,2}(\Rd)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm}.

\textit{Case 2: Three distinct indices.}
Let $k_q = \ell_q = i$, for some $i \neq j \neq h$. Using the law of iterated expectation, we obtain
\begin{align*}
\Ebb \Bigl[ & \abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\Bigr] = \nonumber \\
& \Ebb\Bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \cdot \Pbb\bigl[\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \} |x_i,x_j,x_h\bigr]\Bigr] \nonumber \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d}) \cdot \Ebb\Bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \Bigr] \nonumber \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2})\cdot[g]_{W^{1,2}(\Rd)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm_2}.


\textit{Case 3: Four distinct indices.}
Using the law of iterated expectation, we find that
\begin{align*}
\Ebb\Bigl[ &\abs{D_{k_q}g(x_i)}\cdot{\abs{D_{\ell_q}g(x_j)}} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\Bigr] \\
& = \Ebb\Bigl[\abs{D_{k_q}g(x_i)}\cdot{\abs{D_{\ell_q}g(x_j)}} \cdot\Pbb\bigl[G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected}|x_i,x_j,x_{k_q},x_{\ell_q}\bigr]\Bigr] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 4)d}) \cdot \Ebb\Bigl[\abs{D_{k_q}g(x_i)}\cdot{\abs{D_{\ell_q}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [g]_{W^{1,2}(\Rd)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm_3}.

\paragraph{All indices distinct.}

We first show the desired result when $s$ is even, and then when $s$ is odd. 

\textit{Case 1: $s$ is even.}

By Lemma~\ref{lem:leading_term_sobolev} there exists some $f_s \in \Leb^2(\Rd)$ which satisfies $\norm{f_s}_{\Leb^2(\Rd)} \leq c \norm{g}_{W^{s,2}(\Rd)}$ such that
\begin{equation*}
\Ebb\Bigl[\bigl(D_kg(x)\bigr)^2\Bigr] = r^{2s} \cdot \norm{f_s}_{\Leb^2(\Rd)}.
\end{equation*} Therefore, by the law of iterated expectation along with this Lemma, 
\begin{align*}
\Ebb\bigl[D_kg(x_i)D_kg(x_j)\bigr] = \Ebb\Bigl[\bigl(\Ebb[D_kg(x_i)|x_i]\bigr)^2\Bigr] = r^{2s} \cdot \Ebb\Bigl[\bigl(f_s(x_i)\bigr)^2\Bigr] = r^{2s} \cdot \norm{g}_{W^{s,2}(\Rd)},
\end{align*}
proving the claimed result.

\textit{Case 2: $s$ is odd.}

By the law of iterated expectation, we have
\begin{align*}
\Ebb[d_iD_kg_m(x_j)d_iD_{\ell}g_m(x_j)K_r(x_i,x_j)] & = \Ebb\biggl[\Bigl(d_i\bigl(\Ebb(D_kf)\bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \\
& = \Ebb\biggl[\Bigl(d_i\bigl(r^{s - 1}\cdot f_{s - 1} + r^sf_s(x_j)\bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr].
\end{align*}
where the latter equality follows from Lemma~\ref{lem:leading_term_sobolev}. Here $f_{s -1}$ and $f_s$ satisfy the conclusions of that Lemma, namely that $f_{s - 1}$ and $f_s \in C^{\infty}(\Rd)$, and
\begin{equation*}
\norm{f_{s - 1}}_{W^{1,2}(\Rd)}, \norm{f_s}_{\Leb^2(\Rd)} \leq c \norm{g}_{W^{s,2}(\Rd)}.
\end{equation*}
By the linearity of the difference operator $d_i$, we have
\begin{align*}
\Ebb\biggl[\Bigl(d_i\bigl(r^{s - 1}\cdot f_{s - 1} + r^s f_s(x_j)\bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] & = \Ebb\biggl[\Bigl(r^{s - 1} \cdot d_if_{s - 1}(x_j) + r^s d_if_s(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \\
& \leq 2r^{2(s - 1)} \Ebb\Bigl[\bigl(d_if_{s-1}(x_j)\bigr)^2K_r(x_i,x_j)\Bigr] + 2r^{2s}\Ebb\Bigl[\bigl(d_if_s(x_j)\bigr)^2\Bigr] \\
& \leq 2r^{2(s - 1)} \Ebb\Bigl[\bigl(d_if_{s-1}(x_j)\bigr)^2K_r(x_i,x_j)\Bigr] + 4r^{2s} \norm{g}_{W^{s,2}(\Rd)}^2 \\
& \leq r^{2s} \cdot c\norm{g}_{W^{s,2}(\Rd)}^2
\end{align*}
where the last inequality follows from Lemma~\ref{lem:expected_first_order_seminorm}. This concludes the proof of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2}.



\subsection{Proof of~\eqref{eqn:spectral_similarity}}

Assuming~\eqref{eqn:slepcev_transport_distance} holds and $r$ is chosen according to~\eqref{eqn:pi_max}, we have already shown that the upper bound in~\eqref{eqn:spectral_similarity} holds for $\delta = 0$ (see the proof of ~\eqref{eqn:eigenvalue_tail_bound}). Now we need only to show the lower bound, which we will do following these steps:
\begin{enumerate}[(i)]
	\item First, we consider the case of two fixed graphs $G$ and $\wt{G}$, and establish a Poincare inequality. In other words, we show that if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$, then the Laplacian matrix $L$ cannot be much larger $\wt{L}$, in the usual sense of partial ordering of matrices.
	\item Then, we treat the case of $G = G_{n,r}$ and $\wt{G} = \overline{G}$. We assume that~\eqref{eqn:slepcev_transport_distance} holds, and exhibit such a mapping between edges in $G_{n,r}$ and paths in $\overline{G}$. The resulting Poincare inequality will imply~\eqref{eqn:spectral_similarity}.
\end{enumerate}
In step (ii) we assume that~\eqref{eqn:slepcev_transport_distance} holds. Since \eqref{eqn:slepcev_transport_distance} holds with probability at least $1 - c/n$, this will imply that~\eqref{eqn:spectral_similarity} holds with at least the same probability.

\subsubsection{Step 1: Poincare inequality}

Let $G = (V,E)$ and $\wt{G} = (V,\wt{E})$ be two graphs over a common vertex set $V$. The set $\mathcal{P}_{\wt{G}}$ consists of all paths $P$ over $\wt{G}$, meaning all tuples
\begin{equation*}
P = (\wt{e}_1,\wt{e}_2,\ldots,\wt{e}_m),~~m \in \mathbb{N}~, \wt{e}_j \in \wt{E}~ \forall~ j \in [m], (\wt{e}_j)_2 = (\wt{e}_{j + 1})_1 \forall~j \in [m - 1].
\end{equation*}
Let $\gamma:E \to \mathcal{P}_{\wt{G}}$ be a mapping from edges in $G$ to paths in $\wt{G}$. The maximum path length $M(\gamma)$ is defined
\begin{equation*}
M(\gamma) = \max_{e \in E} \abs{\gamma(e)}
\end{equation*} 
and the bottleneck $b(\gamma)$ is defined
\begin{equation*}
b(\gamma) = \max_{\wt{e} \in \wt{E}} \abs{\{e \in E:  \wt{e} \in \gamma(e)\}}.
\end{equation*}
As mentioned previously, the Laplacian matrix $L$ cannot be much larger than $\wt{L}$ if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$. Formally, we will show the following: for any $\gamma: E \to \mathcal{P}_{\wt{E}}$,
\begin{equation}
\label{eqn:poincare_inequality}
x^T L x \leq M(\gamma)\cdot b(\gamma) \cdot (x^T \wt{L} x)~\forall{x \in \Reals^n}
\end{equation} 

\paragraph{Proof of~\eqref{eqn:poincare_inequality}:}

Letting $c \in \Reals$, we will use the notation $G \preceq c \cdot  \wt{G}$ as shorthand for
\begin{equation*}
x^T L x \leq c \cdot  x^T \wt{L} x,~~\textrm{for all $x \in \Reals^n$}.
\end{equation*}


Let $G_e = (V, \set{e})$ and $P_e = (V, \set{\widetilde{e}: \widetilde{e} \in \gamma(e)})$ be the graphs associated with $e$ and $\gamma(e)$, respectively. By Lemma \ref{lem: path_poincare}, we have
\begin{equation*}
G_{e} \preceq \abs{P_e} P_e
\end{equation*}
Summing over all $e \in E_G$, we obtain
\begin{align*}
G & \preceq \sum_{e \in E_G} \abs{P_e} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} \sum_{e \in E_G} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} b_{\gamma}\cdot \widetilde{G}
\end{align*}

\subsubsection{Step 2: Mapping $E_{G_{n,r}}$ to $\mathcal{P}_{\overline{G}}$}
We assume there exists a mapping $\pi:\overline{X} \to X$ which satisfies \eqref{eqn:slepcev_transport_distance}. We will apply the Poincare inequality to $\pi^{-1}(G_{n,r})$, the isomorphism of $G_{n,r}$ induced by the mapping of vertices $\pi^{-1}: X \to \overline{X}$. We will map a given $(\overline{x}_{k},\overline{x}_{\ell}) \in \pi^{-1}(E)$ to a shortest path $P \in \mathcal{P}_{\overline{G}}$ (measured by Manhattan distance) between $\overline{x}_k$ and $\overline{x}_{\ell}$. Recalling the notation $t = n^{1/d}$ and letting
\begin{equation*}
s_i = \frac{1}{t}(\textrm{sign}(k_i - \ell_i),0,\ldots,0),~~\textrm{for $i = 1,\ldots,d$,}
\end{equation*}
our mapping $\gamma: \pi^{-1}(E) \to \mathcal{P}_{\overline{G}}$ is given by
\begin{align*}
\gamma((\overline{x}_{k},\overline{x}_{\ell})) = \bigl(& (\overline{x}_k, \overline{x}_{k} + s_1), (\overline{x}_{k} + s_1,\overline{x}_{k} + 2s_1),\ldots,(\overline{x}_{k} + (\abs{k_1 - \ell_1} - 1)s_1, \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1)) \\
& (\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1), \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + s_2),\ldots,(\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + (\abs{k_2 - \ell_2} - 1)s_2, \overline{x}_{k}  + \frac{1}{t}((k_1 - \ell_1) + (k_2 - \ell_2)) \\
& \vdots \\
& (\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + s_d,\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + 2s_d), \ldots, (\overline{x}_k + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + (\abs{k_d - \ell_d} - 1)s_d, \overline{x}_{\ell})\bigr)
\end{align*}
As a result of~\eqref{eqn:transport_distance} we obtain the following bounds on $M(\gamma)$ and $b(\gamma)$:
\begin{align}
M(\gamma) & \leq c n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right) \label{eqn:maximum_path_length}\\
b(\gamma) & \leq c \left(n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right)\right)^{2d}, \label{eqn:bottleneck}
\end{align}
which we now prove. 

\paragraph{Proof of~\eqref{eqn:maximum_path_length}:}

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \pi^{-1}(E)$. Then
\begin{align*}
\gamma(\overline{x}_k, \overline{x}_{\ell})) = \norm{\overline{x}_k - \overline{x}_{\ell}}_1 & \leq \sqrt{d} \norm{\overline{x}_k - \overline{x}_{\ell}}_2 \\
& \leq \sqrt{d}\bigl(\norm{\overline{x}_k - \pi(\ol{x}_{k})}_2 + \norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + \norm{\overline{x}_\ell - \pi(\ol{x}_{\ell})}_2\bigr) \\
& \leq \sqrt{d}\left(\norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{by \eqref{eqn:transport_distance}} \\
& \leq \sqrt{d}\left(r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{since $(\pi(\ol{x}_k),\pi(\ol{x}_{\ell})) \in E$}
\end{align*} 

\paragraph{Proof of~\eqref{eqn:bottleneck}:} 

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)$ for some $e = (\overline{x}_i, \overline{x}_j) \in \pi^{-1}(E)$.  Then, by the construction of $\gamma$,
\begin{equation}
\label{eqn:bottleneck_pf_1}
\norm{\overline{x}_k - \overline{x}_i}_2 \wedge \norm{\overline{x}_\ell - \overline{x}_j}_2  \leq \norm{\overline{x}_j - \overline{x}_i}_2
\end{equation}
As shown in the preceding paragraph, assuming~\eqref{eqn:transport_distance} we have
\begin{equation*}
\norm{\overline{x}_j - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}
\end{equation*}
and therefore by~\eqref{eqn:bottleneck_pf_1},
\begin{align*}
\abs{\set{e \in E: (\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)}} & \leq \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \cdot \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \\
& \leq \left(n^{1/d}\left( r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right)\right)^2.
\end{align*}

Putting the pieces together, we have that when $d = 1$ and $r = (\log n)^a (\log n/n)$, then
\begin{equation*}
G_{n,r} \preceq c \cdot (\log n)^{(1/d + a) + 2 + 2da}\cdot \overline{G}
\end{equation*}
with probability at least $1 - o(1)$.

\subsection{Proof of Lemma~\ref{lem:pi_max_pf_1}}

Write $\overline{L} = \overline{V}\overline{S} \overline{V}^T$ for the spectral decomposition of $\overline{L}$. The proof of Lemma~\ref{lem:pi_max_pf_1} will proceed according to the following steps.
\begin{enumerate}
	\item We show that the incoherence $\max_{i} v_{k,i}^2$ can be upper bounded
	\begin{equation}
	\label{eqn:incoherence_proof_1}
	\max_{i} v_{k,i}^2 \leq \frac{1}{n} \left(\sum_{j = 1}^{n} \abs{\dotp{v_k}{\overline{v}_j}}\right)^2
	\end{equation}
	replacing the norm $\norm{v_k^2}_{\infty}$ by the inner products $\dotp{v_k}{\overline{v}_j}$.
	\item Let $I(k,r) = \set{j \geq 0: \abs{j - k} \leq r}$. Using Davis-Kahan, we establish that the following upper bounds
	\begin{equation}
	\label{eqn:incoherence_proof_2}
	\sum_{j \not\in I(k,r)}^{n} (\dotp{v_k}{\overline{v}_j})^2 \leq 400 \frac{n^4 \delta^2 s_{\max}^2}{R^4}
	\end{equation}
	hold for each $k \leq n/8$ and any $r \geq 8k\sqrt{\delta}$. 
	\item We carefully upper bound the $L_1$ norm present in \eqref{eqn:incoherence_proof_1} given the various bounds on $L_2$ norm we've established in \eqref{eqn:incoherence_proof_2}.
\end{enumerate}

\paragraph{Step 1: Proof of~\eqref{eqn:incoherence_proof_1}.}

We can re-express $v_k$ in the basis $(\overline{v}_j)$ as
\begin{equation*}
v_k = \sum_{j = 1}^{n} \dotp{v_k}{\overline{v}_j} \overline{v}_j,
\end{equation*}
whereupon the bound~\eqref{eqn:incoherence_proof_1} follows from the incoherence property of the chain
\begin{equation*}
\max_{i,k = 1,\ldots,n} \overline{v}_{k,i}^2 \leq \frac{1}{n}.
\end{equation*}
\paragraph{Step 2: Proof of~\eqref{eqn:incoherence_proof_2}.}

Let $\widetilde{L} = L + H$. By Davis Kahan, we have that
\begin{equation}
\label{eqn:davis_kahan}
\sum_{j \not\in I(k,r)} (\dotp{v_k}{\overline{v}_j})^2 \leq \left(\frac{\norm{H}_{\textrm{op}}}{\min{\abs{\overline{s}_j - s_k}:j \in I(k,r)}}\right)^2
\end{equation}
To upper bound the numerator, we use~\eqref{eqn:spectral_similarity} to get
\begin{equation*}
\norm{H}_{op} \leq \delta s_{n}.
\end{equation*}

To lower bound the denominator, we note
\begin{align}
\abs{\overline{s}_j - s_k} & \geq \abs{\overline{s}_j - \overline{s}_k} - \abs{\overline{s}_k - s_k} \nonumber \\
& \geq \abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k. \label{eqn:incoherence_proof_8}
\end{align}
The eigenvalues of the chain graph are well known to be
\begin{equation*}
\overline{s}_j = 2\left(1 - \cos\left(\frac{j\pi}{n}\right)\right) = 4\sin^2\left(\frac{k\pi}{n}\right) ~~\textrm{for $k = 0,\ldots,n - 1$.}
\end{equation*}
By Taylor expansion, for $k \leq j \leq n/(2\pi)$ we have
\begin{equation}
\label{eqn:incoherence_proof_5}
\overline{s}_j - \overline{s}_k = 2\left(\cos\left(\frac{k\pi}{n}\right) - \cos\left(\frac{j\pi}{n}\right)\right) \geq \frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
and when $j \geq n/(2\pi)$ we have $\overline{s}_j - \overline{s}_k \geq 2(\cos\left(\frac{\pi}{8}\right) - \cos\left(\frac{1}{2}\right) > .09$. Additionally since $\sin(x) \leq x$ for all $x \geq 0$, we have
\begin{equation}
\label{eqn:incoherence_proof_6}
\overline{s}_k \leq 4\frac{k^2\pi^2}{n^2}.
\end{equation}
Combining~\eqref{eqn:incoherence_proof_5},\eqref{eqn:incoherence_proof_6}, and the lower bound $\abs{k - j} \geq R \geq 8k\sqrt{\delta}$, we have that
\begin{equation}
\label{eqn:incoherence_proof_7}
\abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k \geq .04\frac{(k - j)^2\pi^2}{n^2} - 4 \delta\frac{k^2\pi^2}{n^2} \geq .02\frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
The result follows from \eqref{eqn:incoherence_proof_7}, \eqref{eqn:incoherence_proof_8} and \eqref{eqn:davis_kahan}.

\paragraph{Step 3: $L_1$ norm to $L_2$ norm.}

Let $u \in \Reals^n$, and suppose we know that the $L_2$ norm of $u$ is bounded,
\begin{equation}
\label{eqn:incoherence_proof_3}
\norm{u}_2^2 \leq 1.
\end{equation}
Under no other conditions on $u$, the upper bound $\norm{u}_1 \leq \sqrt{n}$ is the best that can be hoped for (achieved when $u = (n^{-1/2},\ldots,n^{-1/2})$). However, suppose we also know that for some $B_2^2 \geq B_3^2 \geq \ldots \geq B_n^2$, we have that there exists some $R \in [n]$ such that
\begin{equation}
\label{eqn:incoherence_proof_4}
\sum_{j = r}^{n} v_j^2 \leq B_r^2 ~~\textrm{for each $r = R,\ldots,n$.}
\end{equation}
Clearly, if $B_n^2 \leq \frac{1}{n}$ then $u$ cannot be equal to $(n^{-1/2},\ldots,n^{-1/2})$. We might hope for more general improvements on the bound $\norm{u}_1 \leq \sqrt{n}$ if $B_R^2$ is quite small once $R$ gets sufficiently large. The following Lemma gives such a result.
\begin{lemma}
	\label{lem:pi_max_pf_1_util_1}
	Suppose $u \in \Reals^n$ satisfies \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} for some $1 = B_1^2 = B_2^2 \geq \ldots \geq B_n^2 \geq B_{n+1}^2 = 0$. Assume additionally that there exists an $R \in [n]$ such that
	\begin{equation}
	\label{eqn:incoherence_util_1}
	B_r^2 - B_{r + 1}^2 \leq \frac{1}{r}~\textrm{for each $r = R, R+1,\ldots, n - 1$.}
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:incoherence_util_2}
	\norm{u}_1 \leq \sqrt{R}\sqrt{1 - B_R^2} + \sum_{j = R}^{n} \sqrt{B_j^2 - B_{j+1}^2}.
	\end{equation}
\end{lemma}
\begin{proof}
	Set
	\begin{equation*}
	(u_\star)_j^2 = 
	\begin{cases*}
	\frac{1}{R}(1 - B_R^2), ~ j \leq R \\
	B_j^2 - B_{j + 1}^2, j > R
	\end{cases*}
	\end{equation*}
	so that $\norm{u_\star}_1$ is equal to the right hand side of~\eqref{eqn:incoherence_util_2}. We now prove by contradiction that $u_\star$ maximizes $\norm{\cdot}_1$ under the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4}. Suppose this were not true, and that the maximum is achieved for some $u \neq u_{\star}$.
	\begin{itemize}
		\item since $\norm{u}_1 > \norm{u_{\star}}$ then there exists some index $j$ such that
		\begin{equation}
		\label{eqn:incoherence_util_3}
		u_j > (u_{\star})_j
		\end{equation} 
		\item by~\eqref{eqn:incoherence_proof_3} $\norm{u}_2 \leq 1$. Since $\norm{u_{\star}}_2 = 1$, by \eqref{eqn:incoherence_util_3} there must exist some index $k$ such that
		\begin{equation}
		\label{eqn:incoherence_util_4}
		(u_{\star})_k > u_k.
		\end{equation} 
		Choose $k$ to be the largest of all such indices.
		\item suppose $k < j$ and $j > R$. Since $k$ was chosen to be the largest such index which satisfied~\eqref{eqn:incoherence_util_4}, clearly $v_i \geq (u_{\star})_i$ for all $i > j$, and by \eqref{eqn:incoherence_util_3} $u_j > (u_{\star})_j$. But then 
		\begin{equation*}
		B_j^2 = \sum_{i = j}^{N} (u_{\star})_i^2 < \sum_{i = j}^{N} u_i^2 
		\end{equation*}
		and so $v$ does not satisfy~\eqref{eqn:incoherence_proof_4}. Therefore either $k > j$ or $j \leq R$.
		\item In either case, we have
		\begin{equation*}
		u_j > (u_{\star})_j \geq (u_{\star})_k > u_k.
		\end{equation*}
		Let $j \leq l < k$ be the largest index for which $v_j > (a_{\star})_j$. 
		
		We now construct a vector $\wt{u}$ which satisfies the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} such that $\norm{\wt{u}}_1 > \norm{u}_1$. Once we have shown this, we will have established a contradiction, and the proof of Lemma~\ref{lem:pi_max_pf_1_util_1} will be complete. Let $\wt{u} = (\wt{u}_i)$ be given as follows:
		\begin{equation*}
		\wt{u}_i = 
		\begin{cases*}
		u_i, ~~\textrm{if $i \neq k,l$}, \\
		u_{\star,k}, ~~\textrm{if $i = k$}, \\
		\sqrt{u_l^2 - (u_{\star,k}^2 - a_k^2)}, ~~\textrm{if $i = l$.}
		\end{cases*}
		\end{equation*}
		Clearly $\norm{\wt{u}}_2 = \norm{u}_2 = 1$ and so $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_3}. To see that $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4} we separate the analysis into cases. When $r \geq l + 1$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		When $l < r \leq k$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq \sum_{i = r}^{k} u_{\star,i}^2 + \sum_{i = k +1}^{n} u_{i}^2 \leq B_r^2 - B_{k + 1}^2 + B_{k + 1}^2 = B_r^2
		\end{equation*}
		Finally, when $R \leq r \leq l$, we have
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq u_l^2 - (u_{\star,k}^2 - u_k^2) + (u_{\star,k}^2) + \sum_{i \neq k,l} u_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		Therefore $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4}. Finally, we have that
		\begin{equation*}
		\norm{\wt{u}}_1 = \norm{u}_1 + (u_{\star,k}  - u_k) - (\sqrt{u_l^2 - (u_{\star,k}^2 - u_k^2)} - u_l) > \norm{u}_1 + (u_{\star,k}  - u_k) - (u_{\star,k}  - u_k) = \norm{u}_1,
		\end{equation*}
		so we have established the desired contradiction.
	\end{itemize}
\end{proof}

\paragraph{Putting the pieces together.}
We apply Lemma~\ref{lem:pi_max_pf_1_util_1} to the vector $u = (u_i)_{i = 1}^{n}$, where
\begin{equation*}
u_i = \frac{1}{\sqrt{2}}\left(\abs{\dotp{v_k}{\overline{v}_{k - i}}} + \abs{\dotp{v_k}{\overline{v}_{k + i}}}\right).
\end{equation*}
Note the following:
\begin{itemize}
	\item $\norm{u}_2 \leq 1$.
	\item By \eqref{eqn:incoherence_proof_2}, the vector $u$ satisfies the constraint~\eqref{eqn:incoherence_proof_4} with
	\begin{equation*}
	B_r^2 = 400\frac{n^4\delta^2s_{n}^2}{r^4} ~~\textrm{when $r \geq 8 k \sqrt{\delta}$.}
	\end{equation*}
	\item Taylor expanding $f(r + 1) = (r + 1)^{-4}$ around $r$, we have that
	\begin{equation*}
	B_r^2 - B_{r + 1}^2 \leq 3200 \frac{n^4 \delta^2 s_{n}^2}{r^5} ~~\textrm{when $r \geq 2$.}
	\end{equation*}
	Therefore, the sequence $(B_r)$ satisfies \eqref{eqn:incoherence_util_1} for all $r$ large enough such that 
	\begin{equation*}
	r \geq 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}
	\end{equation*}
	\item Since $u$ and $(B_r)$ satisfy the constraints \eqref{eqn:incoherence_proof_3}, \eqref{eqn:incoherence_proof_4} and \eqref{eqn:incoherence_proof_5}, we may apply Lemma~\ref{lem:pi_max_pf_1_util_1} to $u$, obtaining that
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{v_j}{\overline{v}_k}} = \sqrt{2}\norm{u}_1 \leq \sqrt{2R} + 60\sqrt{2} n^2 \delta s_{n} \sum_{r = R}^{n} r^{-5/2}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}$. Bounding sum by integral, we have that if $R \geq 2$ then
	\begin{equation*}
	\sum_{r = R}^{n} r^{-5/2} \leq \int_{R - 1}^{n - 1} x^{-5/2} \,dx \leq \frac{2^{3/2}2}{3} R^{-3/2}.
	\end{equation*}
	Plugging this into the previous expression, we arrive at
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{\overline{v}_j}{v_k}} \leq \sqrt{R} + 60 \sqrt{2} n^2 \delta s_{\max} \frac{2^{3/2}2}{3R^{3/2}}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{\max}}$. Lemma~\ref{lem:pi_max_pf_1} then follows from~\eqref{eqn:incoherence_proof_1}.
\end{itemize}

\section{Auxiliary Results}

\subsection{Results of Others}

Let $\overline{X}$ denote the $n$ evenly spaced grid points on $[0,1]^d$; formally, letting $t = n^{1/d}$
\begin{equation*}
\overline{X} = \biggl\{\frac{1}{t}(k_1,\ldots,k_d): k \in [t]^d\biggr\}
\end{equation*}

\begin{theorem}[Theorem 1 of \textcolor{red}{Garcia-Trillos and Slepcev}]
	\label{thm:slepcev_transport_distance}
	With probability at least $1 - c/n$ there exists a bijection $\pi: \overline{X} \to X$ such that
	\begin{equation}
	\label{eqn:slepcev_transport_distance}
	\max_{k \in [t]^d} \abs{\overline{x}_k - \pi(\overline{x}_k)} \leq c \left(\frac{\log n}{n}\right)^{1/d}
	\end{equation}
\end{theorem}

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.4, Theorem 1]
	\label{thm:evans_extension}
	Assume $U$ is bounded and $\partial U$ is $C^1$. Select a bounded open set $V$ such that $U \subset \subset V$ ($U$ is compactly contained in $V$). Then there exists a bounded linear operator $E: W^{1,2}(U) \to W^{1,2}(\Rd)$ such that for each $u \in W^{1,2}(U)$:
	\begin{enumerate}
		\item $Eu = u$ a.e. in $U$,
		\item $Eu$ has support within $V$, and 
		\item 
		\begin{equation*}
		\norm{Eu}_{W^{1,2}(\Rd)} \leq C \norm{u}_{W^{1,2}(\Rd)}
		\end{equation*}
		the constant $C$ depending only on $U$ and $V$.
	\end{enumerate}
\end{theorem}

\begin{lemma}[Poincare inequality for path graphs.]
	\label{lem: path_poincare}
	Fix $m \geq 0$. For vertices $V = \set{1, \ldots,m}$ define the path $P(1 \to m) = ((1,2),(2,3),\ldots, (m-1,m))$ and $G_{(1,m)}$ to be the graph consisting only of an edge between $1$ and $m$. Then,
	\begin{equation*}
	(m - 1) \cdot P(1 \to m) \succeq G_{(1,m)}
	\end{equation*}
\end{lemma}

\subsection{Integrals}

\begin{lemma}
	\label{lem:expected_first_order_seminorm}
	Suppose $g \in C^{\infty}(\Rd)$, that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Rd$, and that $K$ is a $2$nd order kernel. Then
	\begin{equation*}
	\Ebb[(g(x_j) - g(x_i))^2K_r(x_i,x_j)] \leq c K_{\max} p_{\max}^2 r^2 [g]_{W^{1,2}(\Rd)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	By the fundamental theorem of calculus we have for any $y,x \in \Rd$,
	\begin{equation*}
	g(y) - g(x) = \int_{0}^{1} \frac{d}{dt}\bigl[g(x + t(y - x))\bigr] \,dt = \int_{0}^{1} \dotp{\nabla(g(x + t(y - x)))}{y - x} \,dt
	\end{equation*}
	By the upper bound on $p$, we obtain
	\begin{align*}
	\Ebb[(g(x_j) - g(x_i))^2K_r(x_i,x_j)] & \leq p_{\max}^2 \int_{\Rd} \int_{\Rd} (g(y) - g(x))^2 K_r(y,x) \,dy \,dx\\
	& = p_{\max}^2 \int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \dotp{\nabla(g(x + t(y - x)))}{y - x} \,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(i)}{\leq} p_{\max}^2 \int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \norm{\nabla(g(x + t(y - x)))}\norm{y - x} \,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(ii)}{\leq} p_{\max}^2 r^2\int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \norm{\nabla(g(x + t(y - x)))}\,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(iii)}{\leq} p_{\max}^2 r^2\int_{\Rd} \int_{\Rd} \int_{0}^{1} \norm{\nabla(g(x + t(y - x)))}^2 \,dt K_r(y,x) \,dy \,dx \\
	& \overset{(iv)}{\leq} p_{\max}^2 K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(x,r)} \norm{\nabla(g(x + t(y - x)))}^2 \,dy \,dt \,dx \\
	& \overset{(v)}{\leq}  p_{\max}^2 K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(0,r)} \norm{\nabla(g(x + z))}^2  \,dz \,dt \,dx
	\end{align*}
	where $(i)$ follows by Cauchy-Schwarz, $(ii)$ follows since either $\norm{y - x} \leq r$ or $K_r(y,x) = 0$, $(iii)$ follows by Jensen's, $(iv)$ follows by the assumption $K \leq K_{\max}$ supported on $B(0,1)$, and $(v)$ follows from the change of variables $z = x + t(y - x)$. Finally, again using Fubini's Theorem, we have
	\begin{align*}
	K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(0,r)} \norm{\nabla(g(x + z))}^2  \,dz \,dt \,dx & = r^{2 - d}\int_{B(0,r)} \int_{0}^{1} \int_{\Rd} \norm{\nabla(g(x + z))}^2  \,dz \,dt \,dx \\
	& = K_{\max} r^2 [g]_{W_d^{1,2}(\Rd)}.
	\end{align*}
\end{proof}

\begin{lemma}
	\label{lem:expected_first_order_seminorm_2}
	Suppose $g \in C^{\infty}(\Rd)$, that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Rd$, and that $K$ is a $2$nd order kernel. Then
	\begin{equation*}
	\Ebb\Bigl[\abs{D_ig(x_h)}\cdot\abs{D_ig(x_j)} \Bigr] \leq c K_{\max} p_{\max}^2 r^2 [g]_{W^{1,2}(\Rd)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	We rewrite $\Ebb\bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \bigr]$ as follows,
	\begin{align*}
	\Ebb\Bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \Bigr] & = \int \int \int \abs{g(z) - g(x)} \cdot \abs{g(z) - g(y)} K_r(z,y) K_r(z,x) \,dP(x) \,dP(y) \,dP(x) \\
	& = \int \left[\int \abs{g(z) - g(x)} K_r(z,x) \,dP(x)\right]^2 \,dP(z) \\
	& \leq p_{\max}^3 \int_{\Xset} \left[\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx\right]^2 \,dz
	\end{align*}
	Then we obtain
	\begin{align*}
	\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx & \leq \int_{\Rd} \abs{g(z) - g(x)} K_r(z,x) \,dx \\
	& = \int_{\Rd} \abs{\int_{0}^{1} \dotp{\nabla g(x + t(z - x))}{z - x} \,dt} K_r(z,x) \,dx \\
	& \leq \int_{\Rd} \int_{0}^{1} \norm{\nabla g(x + t(z - x))}\cdot\norm{z - x} \,dt K_r(z,x) \,dx \\
	& \leq r \int_{\Rd} \int_{0}^{1} \norm{\nabla g(x + t(z - x))} \,dt K_r(z,x) \,dx \\
	& \leq r \frac{K_{\max}}{r^d} \int_{B(z,r)} \int_{0}^{1} \norm{\nabla g(x + t(z - x))} \,dt  \,dx \\
	& \leq r K_{\max} \int_{B(0,1)} \int_{0}^{1} \norm{\nabla g(x - try)} \,dt  \,dy,
	\end{align*}
	and as a result, 
	\begin{equation*}
	p_{\max}^3 \int_{\Xset} \left[\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx\right]^2 \,dz \leq c\cdot p_{\max}^3 r^2 K_{\max}^3 [f]_{W_d^{1,2}(\Rd)}^2 = O(r^2)\cdot [f]_{W_d^{1,2}(\Xset)}^2.
	\end{equation*}
\end{proof}

\begin{lemma}
	\label{lem:expected_first_order_seminorm_3}
	Suppose $g \in C^{\infty}(\Rd)$, that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Rd$, and that $K$ is a $2$nd order kernel. Then
	\begin{equation*}
	\Ebb\Bigl[\abs{D_{k_q}g(x_i)}\cdot{\abs{D_{\ell_q}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \leq c K_{\max} p_{\max}^2 r^{2 + d} [g]_{W^{1,2}(\Rd)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	We rewrite the expectation as an integral,
	\begin{align*}
	\Ebb\Bigl[& \abs{D_{k_q}g(x_i)}\cdot{\abs{D_{\ell_q}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \\
	& \leq p_{\max}^4 \int_{\Xset^4} \abs{g(x) - g(y)} \cdot \abs{g(u) - g(v)} \cdot  K_r(x,y) K_r(u,v) \1\{\norm{y - v} \leq (2q + 1)r\} \,dy \,dx \,du \,dv
	\end{align*}
	By substituting $z_1 = (y - v)/r$, $z_2 = (u - v)/r$, and $z_3 = (x - y)/r = (x - v)/r + z_1$, we can simplify the integral in the previous display,
	\begin{align*}
	\int_{\Xset^4} & \abs{g(x) - g(y)} \cdot \abs{g(u) - g(v)} \cdot K_r(x,y) K_r(u,v) \1\{\norm{y - v} \leq (2q + 1)r\} \,dy \,dx \,du \,dv \\
	& \leq K_{\max}^2 r^d \int_{\Xset} \int_{[B(0,1)]^3} \abs{g\bigl((z_3 + z_1)r + v\bigr) - g(z_1r + v)} \cdot \bigl|g(z_2r + v) - g(v)\bigr| \,dz_1 \,dz_2 \,dz_3 \,dv \\
	& \leq  K_{\max}^2 r^{d + 2} \int_{[B(0,1)]^3} \int_{[0,1]^2} \int_{\Xset} \norm{\nabla g(t z_3 r + z_1r + v)} \cdot \norm{\nabla g(t z_2 r + v)} \,dv \,dt_1 \,dt_2 \,dz_1 \,dz_2 \,dz_3 \\
	& \leq c \nu_d^3 K_{\max}^2 r^{d + 2} [f]_{W_{d}^{1,2}(\Xset)}^2.
	\end{align*}
\end{proof}


\begin{lemma}
	\label{lem:remainder_term}
	Suppose $f \in \Leb^2(\Rd)$, and $h:\Rd \times \Rd \times [0,1] \to \Reals$ is uniformly bounded. Then, the function $g(x) = \int_{0}^{1} \int_{B(0,1)} f(x + aty) h(y,x, t) \,dy \,dt$ also belongs to $\Leb^2(\Rd)$, with norm
	\begin{equation*}
	\norm{g}_{\Leb^2(\Rd)} \leq \nu_d \cdot \norm{f}_{\Leb^2(\Rd)} \cdot \norm{h}_{\infty}
	\end{equation*}
\end{lemma}
\begin{proof}
	We compute the squared norm of $g$,
	\begin{align*}
	\norm{g}_{\Leb^2(\Rd)}^2 & = \int_{\Rd} \left(\int_{0}^{1} \int_{B(0,1)} f(x + aty) h(y,x,t) \,dt \,dy \right)^2 \,dx \\
	& \leq \norm{h}_{\infty}^2 \int_{\Rd} \left(\int_{0}^{1} \int_{B(0,1)} f(x + aty) \,dt \,dy \right)^2 \,dx \\
	& \leq \nu_d^2 \norm{h}_{\infty}^2 \int_{\Rd} \int_{0}^{1} \frac{1}{\nu_d}\int_{B(0,1)} f^2(x + aty) \,dt \,dy \,dx \tag{Jensen's inequality} \\
	& = \nu_d^2 \norm{h}_{\infty}^2 \int_{0}^{1} \int_{B(0,1)} \frac{1}{\nu_d}\int_{\Rd}f^2(x + aty) \,dt \,dy \,dx \tag{Fubini's theorem} \\
	& = \nu_d^2 \norm{h}_{\infty}^2 \norm{f}_{\Leb^2(\Rd)}^2.
	\end{align*}
\end{proof}

\begin{lemma}
	\label{lem:leading_term_sobolev}
	Let $k \in [n]^q$ for some $q \geq 1$, and let $K_r$ be a second order kernel. For $s \geq 0$ an integer, let $p$ be a density satisfying $p \in C^{s - 1}(\Rd;p_{\max})$ for some $p_{\max} > 0$ if $s \geq 1$, and otherwise $p \in C^{0}(\Rd;p_{\max})$. Then for any $f \in C^{\infty}(\Rd)$, there exist functions $f_{\ell}, \ell = 2q,\ldots,s$ satisfying $f_{\ell} \in C^{\infty}(\Rd)$ and
	\begin{equation*}
	\norm{f_{\ell}}_{W^{s - \ell,2}(\Rd)} \leq c \norm{f}_{W^{s,2}(\Rd)}
	\end{equation*}
	for some constant $c$ which depends only on $d$, $\Xset$ and $p_{\max}$ such that
	\begin{equation}
	\label{eqn:leading_term_sobolev}
	\Ebb(D_kf(x)) =
	\begin{cases*}
	\sum_{\ell = 2q}^{s} f_{\ell}(x) r^{\ell} ,~~& \textrm{if $2q < s$} \\
	r^s \cdot f_{s}(x),~~& \textrm{if $2q \geq s$}
	\end{cases*}
	\end{equation}
\end{lemma}
\begin{proof}	
	We proceed by induction on $q$. Note that without loss of generality, we will assume $p \in C^{\infty}(\Rd)$, i.e. $p$ is smooth. Otherwise we can simply mollify $p$, with neglible change to the Holder norm $\norm{p}_{C^{s-1}(\Xset)}$ or the expectation $\Ebb[D_kf(x)]$.
	
	\paragraph{Base case.}
	We begin with the base case of $q = 1$. When $s = 0$, we need to show that $\Ebb[D_kf(x)] = f_s(x)$ for some $\norm{f_s}_{\Leb^2(\Rd)} \leq c\norm{f}_{W^{s,2}(\Rd)}$. But
	\begin{align*}
	\Ebb\Bigl[D_kf(x)\Bigr] & = \int f(z) K_r(x,z) p(z) \,dz  - f(x) \\
	& = \int f(yr + x) K(\norm{y}) p(yr + z) \,dy - f(x).
	\end{align*}
	and since
	\begin{equation*}
	\biggl\|\int f(yr + x) K(\norm{y}) p(yr + x) \,dy\biggr\|_{\Leb^2(\Rd)} \leq p_{\max} K_{\max} \norm{f}_{\Leb^2(\Rd)}
	\end{equation*}
	the claim follows.
	
	Now, when $s \geq 1$ since $f$ is smooth it admits a Taylor expansion of the following form for all $x,z \in \Rd$:
	\begin{equation*}
	f(z) = \sum_{\abs{\alpha} < s} \frac{f^{(\alpha)}(x)}{\alpha!} (x - z)^{\alpha} + \frac{1}{(s - 1)!}\sum_{\abs{\alpha} = s} (x - z)^{\alpha} \int_{0}^{1}(1 - t)^{s - 1} f^{(\alpha)}(x + t(z - x)) \,dt
	\end{equation*}
	where $f^{(\alpha)} \in C^{\infty}(\Rd)$ additionally satisfies
	\begin{equation*}
	\norm{f^{(\alpha)}}_{W^{s - \abs{\alpha},2}(\Rd)} \leq \norm{f}_{W^{s,2}(\Rd)}
	\end{equation*}
	Replacing $f$ by its Taylor expansion inside the expected first order difference operator $\Ebb(D_kf(x))$ we have
	\begin{equation}
	\label{eqn:leading_term_sobolev_pf1}
	\Ebb(D_kf(x)) = \sum_{1 < \abs{\alpha} < s} \frac{f^{(\alpha)}(x)}{\alpha!} E_{\alpha,x} + \frac{1}{(s - 1)!}\sum_{\abs{\alpha} = s} \int_{0}^{1}(1 - t)^{s - 1} E_{\alpha,x,t}(f)  \,dt 
	\end{equation}
	where we use the notation $E_{\alpha,x} := \Ebb\left[(x - x_k)^{\alpha}K_r(x_k,x)\right]$ and  $E_{\alpha,x,t}(f) := \Ebb\bigl[f^{(\alpha)}(x + t(x_k - x)) (x_k - x)^{\alpha} K_r(x_k,x)\bigr]$.
	
	 By a change of variables, we have
	\begin{align*}
	E_{\alpha,x,t}(f) & = \frac{1}{r^d}\int_{\Rd} f^{(\alpha)}\bigl(x + t(z - x)\bigr) (z - x)^{\alpha}  K\Bigl(\frac{\norm{z - x}}{r}\Bigr) p(z) \,dz \\
	& = r^s \int_{\Rd} y^{\alpha} f^{(\alpha)}\bigl(x + tyr) K(\norm{y}) p(yr + x) \,dy,
	\end{align*}
	and as a result the remainder term in \eqref{eqn:leading_term_sobolev_pf1} reduces to
	\begin{align*}
	\frac{1}{(s - 1)!}\sum_{\abs{\alpha} = s} \int_{0}^{1}(1 - t)^{s - 1} E_{\alpha,x,t}(f)  \,dt & = \frac{r^s}{(s - 1)!}\sum_{\abs{\alpha} = s} \int_{0}^{1} \int_{\Rd} (1 - t)^{s - 1}  y^{\alpha} f^{(\alpha)}\bigl(x + tyr) K(\norm{y}) p(yr + x) \,dy \,dt \\
	& =: r^s g_s(x) ~~\textrm{for $g_s \in C^{\infty}(\Rd)$} 
	\end{align*}
	where $\norm{g_s}_{\Leb^2(\Rd)} \leq c \norm{f}_{W^{s,2}(\Rd)}$ follows from Lemma~\ref{lem:remainder_term}. When $s = 1$, there are no $1 < \abs{\alpha} < s$, and so only the remainder term in \eqref{eqn:leading_term_sobolev_pf1} is non-zero; thus we have shown~\eqref{eqn:leading_term_sobolev} when $q = 1$ and $s = 1$.
	
	
	When $s \geq 2$, we can analyze $E_{\alpha,x}$ using a Taylor expansion of $p$. For any $x,z \in \Rd$, we have
	\begin{equation*}
	p(z) = \sum_{\abs{\beta} < s  - 1} \frac{p^{(\beta)}(x)}{\beta!} (x - z)^{\beta} + \frac{1}{(s - 2)!}\sum_{\abs{\beta} = s - 1} (x - z)^{\beta} \int_{0}^{1}(1 - t)^{s - 2} p^{(\beta)}(x + t(z - x)) \,dt
	\end{equation*}
	and $p^{(\beta)} \in C^{\infty}(\Rd)$ additionally satisfies
	\begin{equation*}
	\norm{p^{(\beta)}}_{C^{s - \abs{\beta} - 1}(\Rd)} \leq p_{\max}.
	\end{equation*}
	Replacing $p$ by its Taylor expansion, the term $E_{\alpha,x}$ can be rewritten as
	\begin{align*}
	E_{\alpha,x} & = \int_{\Rd} (x - z)^{\alpha} K_r(x,z) p(z) \,dz \\
	& =  \sum_{\abs{\beta} = 0}^{s - 2} \frac{p^{(\beta)}(x)}{\beta!} \int_{\Rd} (x - z)^{\alpha + \beta}K_r(x,z)\,dz ~~+ \\
	& ~~~\frac{1}{(s - 2)!}\sum_{\abs{\beta} = s - 1} \int_{\Rd} \int_{0}^{1} (1 - t)^{s - 2} (x - z)^{\alpha + \beta - 1}  p^{(\beta)}(x + t(z - x)) K_r(x,z) \,dz \,dt \\
	& = \sum_{\abs{\beta} = 0}^{s - 2} r^{\abs{\alpha} + \abs{\beta}} \frac{p^{(\beta)}(x)}{\beta!} \int_{\Rd} y^{\alpha + \beta}K(y)\,dy +~ \frac{r^{\abs{\alpha} + s - 1}}{(s - 2)!} \sum_{\abs{\beta} = s - 1} \int_{\Rd} \int_{0}^{1} (1 - t)^{s - 2} y^{\alpha + \beta - 1} p^{(\beta)}(x + ytr) K(\norm{y}) \,dy \,dt \\
	& =: \sum_{\abs{\beta} = 0}^{s - 2} \1\{\abs{\alpha} + \abs{\beta} > 1\}r^{\abs{\alpha} + \abs{\beta}} p_{\abs{\beta}}(x) + r^s p_s(x) ~~\textrm{for $p_{\abs{\beta}} \in C^{\infty}(\Rd), \norm{p_{\abs{\beta}}}_{C^{s - \abs{\beta} - ,1}(\Rd)} \leq c \norm{p}_{C^{s - 1}(\Rd)}$},
	\end{align*}
	where the last line follows since $K$ is a 2nd order kernel. 
	Substituting our expressions for $E_{\alpha,x}$ and $E_{\alpha,x,t}(f)$ back into~\eqref{eqn:leading_term_sobolev_pf1}, we obtain
	\begin{align*}
	\Ebb\bigl[D_kf(x)\big] & = \sum_{\abs{\alpha} = 1}^{s - 1} \sum_{\abs{\beta} = 0}^{s-2} \1\{\abs{\alpha} + \abs{\beta} > 1\} r^{\abs{\alpha} + \abs{\beta}} \frac{f^{(\alpha)}(x)p_{\abs{\beta}}(x)}{\alpha!} + r^s \biggl(\sum_{\abs{\alpha} = 1}^{s - 1} \frac{f^{(\alpha)}(x)p_s(x)}{\alpha!} + g_s(x)\biggr) \\
	& = \sum_{\ell = 2}^{s - 1} r^{\ell} \sum_{ \substack{\abs{\alpha} + \abs{\beta} = \ell, \\ \abs{\beta} > 0} }   \frac{f^{(\alpha)}(x)p_{\abs{\beta}}(x)}{\alpha!} + r^s \biggl(\sum_{\abs{\alpha} = 1}^{s - 1} \frac{f^{(\alpha)}(x)p_s(x)}{\alpha!} + g_s(x)\biggr) \\
	& =: \sum_{\ell = 2}^{s - 1} r^{\ell} f_{\ell}(x) + r^s f_s(x).
	\end{align*}
	It remains to show that $\norm{f_{\ell}}_{W^{s - \ell,2}(\Rd)} \leq c \norm{f}_{W^{s,2}(\Rd)}$ for each $\ell = 2,\ldots,s$. To see this, note that for $\abs{\alpha} + \abs{\beta} = \ell$ and $\min\{s - \abs{\alpha}, s - 1 - \abs{\beta}\} = m$, the inequality $s - \ell \leq m$ holds. Therefore,
	\begin{equation*}
	\norm{f^{(\alpha)}p_{\abs{\beta}}}_{W^{s - \ell,2}(\Rd)} \leq \norm{f^{(\alpha)}p_{\abs{\beta}}}_{W^{m,2}(\Rd)} \leq p_{\max} \norm{f^{(\alpha)}}_{W^{s - \abs{\alpha},2}(\Xset)} \leq p_{\max} \norm{f}_{W^{s,2}(\Rd)}
	\end{equation*} 
	which finishes the proof of Lemma~\ref{lem:leading_term_sobolev} when $q = 1$.
	
	\paragraph{Induction Step.}

	
	Now, we assume \eqref{eqn:leading_term_sobolev} holds for all $k \in [n]^q$, and prove the desired estimate on $\Ebb[D_{k}D_jf(x)]$ for each $j \in [n]$. We will build a proof piece-by-piece, depending on the relative size of $s$ and $q$.
	
	If $s \leq 2$, by hypothesis there exists $f_s$ satisfying
	\begin{equation*}
	\norm{f_s}_{\Leb^2(\Rd)} \leq c \norm{f}_{W^{s,2}(\Rd)}
	\end{equation*}
	such that for any $z \in \Rd$,
	\begin{equation*}
	\Ebb\Bigl[D_jf(z)\Bigr] = r^s f_s(z).
	\end{equation*}
	Therefore by the law of iterated expectation,
	\begin{equation*}
	\Ebb\Bigl[D_kD_jf(x)\Bigr] = \Ebb\Bigl[D_k(\Ebb[D_jf])(x)\Bigr] = \Ebb\Bigl[D_k\bigl(r^s f_s\bigr)(x)\Bigr] = r^s \Ebb\Bigl[D_kf_s(x)\Bigr]
	\end{equation*}
	and again using the inductive hypothesis, we have that
	\begin{equation}
	\label{eqn:leading_term_sobolev_pf3}
	\biggl\|\Ebb\Bigl[D_kf_s(x)\Bigr]\biggr\|_{\Leb^2(\Rd)} \leq c \norm{f_s}_{\Leb^2(\Rd)},
	\end{equation}
	establishing~\eqref{eqn:leading_term_sobolev} for all $q$ in the case when $s \leq 2$. 
	
	Otherwise, when $s \geq 3$ by hypothesis there exist functions $f_{\ell}$ for $\ell = 2,\ldots,s$ satisfying
	\begin{equation*}
	\norm{f_{\ell}}_{W^{s - \ell,2}(\Rd)} \leq c \norm{f}_{W^{s,2}(\Rd)}
	\end{equation*}
	such that for any $z \in \Rd$,
	\begin{equation*}
	\Ebb(D_jf(z)) = \sum_{\ell = 2}^{s} r^{\ell} f_{\ell}(z).
	\end{equation*}
	As a result, by the law of iterated expectation
	\begin{align*}
	\Ebb\Bigl[D_{k}D_jf(x)\Bigr] & = \Ebb\Bigl[D_k\bigl(\Ebb[D_jf]\bigr)(x)\Bigr] \\
	& = \Ebb\biggl[D_k\Bigl(\sum_{\ell = 2}^{s - 1} r^{\ell} f_{\ell} + r^s f_s\Bigr)(x)\biggr] \\
	& = \sum_{\ell = 2}^{s - 1} r^{\ell} \cdot \Ebb\Bigl[D_kf_{\ell}(x)\Bigr] + r^s \Ebb\Bigl[D_kf_s(x)\Bigr].
	\end{align*}
	Recalling that $\Ebb\Bigl[D_kf_s(x)\Bigr]$ satisfies~\eqref{eqn:leading_term_sobolev_pf3}, we now apply the inductive hypothesis to $\Ebb(D_kf_{\ell}(x))$ for each $\ell = 2,\ldots,s-1$, to prove~\eqref{eqn:leading_term_sobolev}.
	
	First we consider the case when $2(q + 1) \geq s$. Note that $\ell \geq 2$ implies $2q \geq s - \ell$. Therefore by hypothesis, for each $\ell = 2,\ldots,s - 1$ the expectation $\Ebb[D_kf_{\ell}(x)] = r^{s - \ell}f_{\ell,s}(x)$ for some $f_{\ell,s} \in \Leb^2(\Rd)$ satisfying
	\begin{equation}
	\label{eqn:leading_term_sobolev_pf4}
	\norm{f_{\ell,s}}_{\Leb^2(\Rd)} \leq c\norm{f_{\ell}}_{W^{s - \ell,2}(\Rd)} \leq c\norm{f}_{W^{s,2}(\Rd)}
	\end{equation}
	and as a result
	\begin{equation*}
	\sum_{\ell = 2}^{s - 1} r^{\ell} \cdot \Ebb(D_kf_{\ell}(x)) = r^s \sum_{\ell = 2}^{s - 1} f_{\ell,s}(x)
	\end{equation*}
	establishing that the second part of~\eqref{eqn:leading_term_sobolev} holds for all $q$ and all $s \leq 2(q + 1)$. 
	
	Otherwise $2(q + 1) < s - 1$. For each $\ell = 2,\ldots, s - 1$, if additionally  $2q \leq s - \ell - 1$, then by hypothesis $\Ebb(D_kf_{\ell}(x)) = \sum_{m = 2q}^{s - \ell - 1} r^{m} \cdot f_{\ell, \ell + m}(x) + r^{s - {\ell}} f_{\ell,s}$, and otherwise $\Ebb(D_kf_{\ell}(x)) = r^{s - \ell}f_{\ell,s}(x)$, where for each $\ell$ and $m$,
	\begin{equation*}
	\norm{f_{\ell,\ell + m}}_{W^{s - (\ell + m),2}(\Rd)} \leq c\norm{f_{\ell}}_{W^{s - \ell}} \leq c\norm{f}_{W^{s,2}(\Rd)}
	\end{equation*}
	and $f_{\ell,s}$ satisfies \eqref{eqn:leading_term_sobolev_pf4}. Therefore,
	\begin{align*}
	\sum_{\ell = 2}^{s - 1}  r^{\ell} \cdot \Ebb(D_kf_\ell(x)) & = \sum_{\ell = 2}^{s - 1 - 2q} r^{\ell} \cdot \left\{\sum_{m = 2q}^{s - \ell - 1} r^m \cdot f_{\ell,\ell + m}(x) + r^{s - \ell} \cdot f_{\ell,s}(x) \right\} + \sum_{\ell = s - 1 - 2q}^{s - 1} r^s \cdot f_{\ell,s}(x) \\
	& = \sum_{\ell = 2}^{s - 1 - 2q} r^{\ell} \cdot \left\{\sum_{m = 2q}^{s - \ell - 1} r^m \cdot f_{\ell, \ell + m}(x)\right\}  + r^{s}\sum_{\ell = 2}^{s - 1}f_{\ell,s}(x) \\
	& = \sum_{m = 2q}^{s - 3} \sum_{\ell = 2}^{s - m - 1} r^{m + \ell} \cdot f_{\ell, \ell + m}(x) + r^{s}\sum_{\ell = 2}^{s - 1}f_{\ell,s}(x).
	\end{align*}
	Rewriting the first sum in the final equation as a sum over $\ell + m = 2(q + 1),\ldots, s - 1$ establishes~\eqref{eqn:leading_term_sobolev}.
\end{proof}
	
The proof of Lemma~\ref{lem:empirical_norm_sobolev} relies on (a variant of) the Paley-Zygmund Inequality.
\begin{lemma}
	\label{lem:paley_zygmund}
	Let $f$ satisfy the following moment inequality for some $b \geq 1$:
	\begin{equation}
	\label{eqn:paley_zygmund_1}
	\Ebb\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot\Bigl(\Ebb\bigl[\norm{f}_n^2\bigr]\Bigr)^2.
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:paley_zygmund_2}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}.
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. The Paley-Zygmund inequality says that for all $0 \leq \lambda \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund_pf1}
	\mathbb{P}(Z > \lambda \mathbb{E}(Z^p)) \geq \left[(1 - \lambda^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
	Applying~\eqref{eqn:paley_zygmund_pf1} with $Z = \norm{f}_n^2$, $p = 1$, $q = 2$ and $\lambda = \frac{1}{b}$, by assumption~\eqref{eqn:paley_zygmund_1} we have
	\begin{equation*}
	\mathbb{P}\Bigl(\norm{f}_n^2 > \frac{1}{b} \mathbb{E}[\norm{f}_n^2]\Bigr) \geq \Bigl(1 - \frac{1}{b}\Bigr)^2 \cdot  \frac{\bigl(\mathbb{E}[\norm{f}_n^2]\bigr)^2}{\mathbb{E}[\norm{f}_n^4]} \geq \frac{\Bigl(1 - \frac{2}{b}\Bigr)}{\Bigl(1 + \frac{1}{b^2}\Bigr)} \geq 1 - \frac{5}{b}.
	\end{equation*}
\end{proof}

\end{document}