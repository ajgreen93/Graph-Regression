\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Graph Testing}
\author{Alden Green}
\date{\today}
\maketitle

This document details the current status of the graph testing project. We divide by section based on the testing problem under consideration. At the end of each section, we list some areas we are interested in investigating. All proofs are left until the end.

We establish some notation that we will use throughout. A graph $G = (V,W)$ consists of a collection of vertices $V = \{v_1,\ldots,v_n\}$, and a weight matrix $W \in \Reals^{\abs{V} \times \abs{V}}$ with $ij$th entry $W_{ij}$ encoding affinity between $v_i$ and $v_j$. We will sometimes write $W_G$ to make it clear which graph a weight matrix is associated with. When the weight matrix $W_G$ consists only of $0$s and $1$s, we will use the equivalent representation $G = (V,E(G))$ when convenient; here $E(G) = V \times V \cap \{(v_i,v_j): W_{ij} = 1\}$. 

We will focus our attention on neighborhood graphs, and operators associated with them. For a kernel $K:\Reals \to \Reals$ and radius $r > 0$, we define $G_{n,K}=(V,W)$ to be the neighborhood graph with vertices $V = X$, and weights $W_{ij} = K(\norm{x_i - x_j}/r)$. Of particular interest to us will be the \emph{random geometric graph} $G_{n,r}$, the neighborhood graph constructed using the uniform kernel $K(z) := \1\{z \leq 1\}$. The weight matrix $W(G_{n,r})$ consists only of $0$s and $1$s, and we will thus use the equivalent representation $G_{n,r} = (X,E(G_{n,r}))$.

The primary operator we are interested in (at this point) is the graph Laplacian. Let $D_G$ be the diagonal degree
matrix of a graph $G$, with entries $D_{uu} := \sum_{v \in V} W_{uv}$. The graph Laplacian is then $L_G = D_G - W_G$. Let $\Lambda(G)$ consist of the eigenvalues $0 = \lambda_1(G) \leq \lambda_2(G) \leq \cdots \leq \lambda_n(G)$ of $L_G$, with $v_k(G) = (v_{k,1}(G),\ldots,v_{k,n}(G)) \in \Reals^n$ denoting the eigenvector corresponding to the $k$th eigenvalue $\lambda_k(G)$.  

\section{Regression goodness-of-fit testing.}

Suppose we observe samples $(y_i,x_i)$ for $i = 1,\ldots,n$, where conditional on $X$ the responses $Y = \{y_1,\ldots,y_n\}$ are assumed to follow the model
\begin{equation}
\label{eqn:regression_known_variance}
y_i = f(x_i) + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation} 
and our task is to distinguish
\begin{equation*}
\mathbf{H}_0: f = f_0 := 0 \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: f \neq f_0
\end{equation*}

We will evaluate our performance using worst-case risk: for a given function class $\mathcal{H}$ and test function $\phi: \Reals^n \to \set{0,1}$, let
\begin{equation*}
\mathcal{R}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, f \neq f_0} \Ebb_f(1 - \phi).
\end{equation*}
The worst-case risk may be quite close to $1$ unless we enforce some separation between null and alternative spaces. A more realistic measure of performance is therefore
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, \norm{f - f_0}_{\Leb^2} \geq \epsilon} \Ebb_f(1 - \phi).
\end{equation*}

\subsection{Test Statistics.}
We list the test statistics we use for the regression testing problem.

\paragraph{Eigenvector projection test statistic.}
The graph Laplacian eigenvector projection test is a truncated-series test. Suppose we are given a graph $G = (X,W)$ defined on the sample points $X$.  Letting $\kappa$ be some integer between $1$ and $n$, our test projects $Y$ onto the eigenvectors $v_1(G),\ldots,v_{\kappa}(G)$, and then takes the empirical norm of this projection to be the test statistic. Formally,
\begin{equation*}
T_{\mathrm{spec}}(G) := \frac{1}{n} \sum_{k = 1}^{\kappa} \Biggl(\sum_{i = 1}^{n} y_i v_{k,i}(G)\Biggr)^2
\end{equation*}
Then the test $\phi_{\spec}(G) := \1\{T_{\spec}(G) \geq \tau\}$ rejects the null hypothesis when the test statistic $T_{\mathrm{spec}}(G)$ is greater than a pre-specified cutoff $\tau$.

\subsection{Random design.}
In the random design case, we assume that each design point $x_i$ is sampled independently from some distribution $P$. Formally, our model for the samples is the following:
\begin{equation}
\label{eqn:regression_random_design_known_variance}
x_1,\ldots,x_n \overset{\textrm{i.i.d}}{\sim} P,~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)~\textrm{and}~ \varepsilon \perp X,~~ y_i = f(x_i) + \varepsilon_i.
\end{equation}

In Theorem~\ref{thm:sobolev_testing_rate_order1} we show that under some typical regularity conditions on $P$, the neighborhood graph Laplacian eigenvector projection test $\phi_{\textrm{spec}}(G_{n,r})$ is a minimax optimal test over the Sobolev ball $H^1(\mathcal{X};R)$ for $d = 1,2,3$ and $\Xset = [0,1]^d$. Let $p_d$ equal $3/4$ when $d = 2$ and equal $1/d$ for $d \geq 3$.

\begin{theorem}
	\label{thm:sobolev_testing_rate_order1}
	Suppose we observe samples $(x_i,y_i)_{i = 1}^{n}$ according to the model~\eqref{eqn:regression_random_design_known_variance}. Let $R > 0$ and $b \geq 1$ be fixed constants, and $d = 1,2$ or $3$. Suppose that $P$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p(x)$ is bounded away from zero and infinity, 
	\begin{equation*}
	0 < p_{\min} < p(x) < p_{\max} < \infty,~~ \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	and the test $\phi_{\spec}(G_{n,r})$ is performed with parameter choices
	\begin{equation*}
	c \frac{(\log n)^{p_d}}{n^{1/d}} \leq r(n) \leq n^{-4/((4 + d)(2+d))}, ~\kappa = (nR)^{2d/(4 + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	for $c$ a constant that depends only on $\Xset, p_{\min}$ and $p_{\max}$. Then the following statements holds for every $n$ sufficiently large: there exists constants $c_1,c_2$ which do not depend on $n,b$ or $R$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate_order1}
	\epsilon^2 \geq c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4 + d)}\} \cdot n^{-4/(4 + d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_order1_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}(G_{n,r}); H^1(\mathcal{X};R)) \leq \frac{c_2}{b}.
	\end{equation}
\end{theorem}

The testing problem in the random design case with known variance fundamentally changes in the low-smoothness regime, where the number of bounded derivatives is small compared to the dimension $d$. In the case where $f \in H^1(\Xset;R)$ is assumed to have one derivative which is bounded in $\Leb^2(\Xset)$ norm, this transition occurs at $d = 4$. We shall discuss testing in the low-smoothness regime shortly. First, let's consider the situation when we assume more derivatives worth of smoothness.

Theorem~\ref{thm:sobolev_testing_rate_order1} establishes that a truncated series test using graph eigenvectors performs just as well, in a minimax sense, as a truncated series test using a classic Fourier series; for analysis of the latter test as well as derivation of the minimax rate of nonparametric goodness-of-fit testing over Sobolev classes see \textcolor{red}{(Ingster)}. Intuitively, we can understand this by thinking of the graph eigenvectors as estimates of eigenfunctions of a density-weighted continuum Laplacian operator. When the density is uniform, and the domain $\Xset$ is the unit cube, under appropriate boundary conditions these eigenfunctions are themselves elements of the Fourier basis. When the density is merely close to uniform, the eigenfunctions no longer belong to the Fourier basis; nevertheless, they share enough properties with the latter that the resulting minimax rate is unchanged. Theorem~\ref{thm:sobolev_testing_rate_order1} shows that the additional error due to using \textit{estimates} of these eigenfunctions does not fundamentally change the minimax rate. 

When we additionally believe $H^s(\Xset)$, we know (again from Ingster) that the minimax testing rate is $n^{-4s/(4s + d)}$. Now, however, our estimates of the eigenvectors no longer track the derivatives finely enough at the boundaries of $\Xset$ to establish the same minimax rate over all $H^s(\mathcal{X})$. However, if we assume $f$ and its derivatives are compactly supported on $\Xset$, we recover the usual rate. Theorem~\ref{thm:sobolev_testing_rate} presents our formal result, that $\phi_{\textrm{spec}}(G_{n,r})$ is a minimax optimal test over the Sobolev balls $H_0^s(\mathcal{X};R)$ whenever $4s > d$ and $\Xset = [0,1]^d$.

\begin{theorem}
	\label{thm:sobolev_testing_rate}
	Suppose we observe samples $(x_i,y_i)_{i = 1}^{n}$ according to the model~\eqref{eqn:regression_random_design_known_variance}. Let $R > 0$, $b,s,d \geq 1$ be fixed constants, with $s$ and $d$ integers. Suppose that $P$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p \in C^{s-1}(\Xset;p_{\max})$ for some $p_{\max} < \infty$, and further that $p(x)$ is bounded away from zero, i.e. there exists $p_{\min} > 0$ such that 
	\begin{equation*}
	p_{\min} < p(x),~~ \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds for all $n$ sufficiently large: if the test $\phi_{\spec}(G_{n,r})$ is performed with parameter choice
	\begin{equation*}
	n^{-1/(2(s-1) + d)} \leq r(n) \leq n^{-4/((4s + d)(2+d))}, ~\kappa = n^{2d/(4s + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists constants $c_1,c_2$ which do not depend on $n,b$ or $R$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate}
	\epsilon^2 \geq c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4s + d)}\} \cdot n^{-{4s}/(4s + d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; H_0^{s}(\mathcal{X};R)) \leq \frac{c_2}{b}.
	\end{equation}
\end{theorem}

The restriction that $p$ be supported on the unit cube is mostly for convenience. If instead $p$ is supported on any compact set with Lipschitz boundary, the Theorem statement should hold (up to constants), with only a few modifications to our proofs. However, we do not work through the details.

The restriction $4s > d$ is a fundamental consequence of the tightness of the Sobolev embedding theorem. When $4s \geq d$ the compact embedding
\begin{equation*}
H^{s}(\mathcal{X}) \subseteq \Leb^4(\mathcal{X}) 
\end{equation*}
does not hold (as it does when $d < 4s$). However, if we directly assume $f \in \mathcal{L}_d^4(\mathcal{X};R)$ (regardless of $d$) we obtain the following result.
\begin{proposition}
	\label{prop:L4_testing_rate}
	If $f \in \Leb^4(\mathcal{X};R)$, there exists a constant $c$ such that if
	\begin{equation}
	\label{eqn:L4_testing_rate}
	\epsilon^2 > b \cdot n^{-1/2}
	\end{equation}
	then the test
	\begin{equation*}
	\phi_{\mathrm{mean}} = \1\{\frac{1}{n}\sum_{i = 1}^{n} y_i^2 \geq 1\}
	\end{equation*}
	has worst-case risk
	\begin{equation*}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; H^1(\mathcal{X};R)) \lesssim \frac{1}{b}. 
	\end{equation*}
\end{proposition}
Note that when $4s > d$ the critical radius~\eqref{eqn:sobolev_testing_rate} is smaller than \eqref{eqn:L4_testing_rate}. 

\subsection{Analysis}

To prove Theorem~\ref{thm:sobolev_testing_rate}, we show that there exists a high-probability set $E \subseteq \Xset^n$ such that conditional on $X \in E$, the test $\phi_{\spec}(G_{n,r})$ has small risk. Since $G_{n,r}$ is a function only of $X$ and not of $Y$, this amounts to reasoning about the behavior of the test $\phi_{\spec}$ over a fixed graph $G = (X,E)$, where we observe
\begin{equation}
y_i = \beta_i + \varepsilon_i,~~\varepsilon_i \sim \mathcal{N}(0,1)
\end{equation}
for some fixed $\beta \in \Reals^n$.  In Lemma~\ref{lem:fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}(G)$. Our bound on the Type II error will be stated as a function of $\beta^T L^s \beta$--a measure of the smoothness the signal $\beta$ displays over the graph $G$--as well as the $\kappa$th eigenvalue $\lambda_{\kappa}$.

\begin{lemma}
	\label{lem:fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}$ is upper bounded
		\begin{equation}
		\label{eqn:graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b \geq 1$ and $\beta$ such that
		\begin{equation}
		\label{eqn:fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{\beta^T L^s \beta}{n\lambda_{\kappa}^s}
		\end{equation}
		the Type II error of $\phi_{\spec}$ is upper bounded,
		\begin{equation}
		\label{eqn:graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $d \geq 4$, is it true that there does not exist a uniformly consistent
	test over the Sobolev ball $W_d^{1,2}(\mathcal{X};R)$?
	\item Assume the distribution $P$ is supported on a manifold $\mathcal{M}$ of intrinsic dimension $s < d$. Does $\mathrm{\phi_{\mathrm{spec}}}$ display adaptivity to the intrinsic dimension of $\mathcal{M}$?
	\item Assume that $f$ belongs to the Holder space $C_d^s(\mathcal{X})$. Moreover, suppose that instead of observing ${y_i}$ according to the regression testing model \eqref{eqn:regression_known_variance}, we observe
	\begin{equation*}
	y_i = f(x_i) + \sigma \varepsilon_i, ~\varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
	\end{equation*}
	where $\sigma > 0$ is unknown. When $d \geq 4$, what are the minimax regression testing rates over $C_d^1(\mathcal{X};R)$? Is the test $\phi_{\mathrm{spec}}$ minimax optimal, when the tuning parameters $r$ and $\kappa$ are appropriately chosen?
\end{enumerate}

\section{Two-sample density testing.}
In the two-sample density testing problem, we observe independent samples $Z = z_1,\ldots,z_N \sim P$ and $Y = y_1,\ldots,y_M \sim Q$, where $P$ and $Q$ are distributions over $\Reals^d$ with densities $p$ and $q$, respectively, and $N \sim \textrm{Bin}(n,1/2)$. Our goal is to distinguish the hypotheses
\begin{equation*}
\mathbf{H}_0: P = Q \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: P \neq Q
\end{equation*}
and we again evaluate our performance using worst-case risk; letting $\phi:\Reals^{N + M} \to \{0,1\}$, 
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \inf_{p \in \mathcal{H}}\Ebb_{p,p}(\phi) + \sup_{\substack{p,q \in \mathcal{H} \\ \norm{p - q}_{\Leb^2} \geq \epsilon}} \Ebb_{p,q}(1 - \phi).
\end{equation*}

\subsection{Test statistics.}

We suggest several two-sample test statistics. 

\paragraph{Eigenvector projection test statistic.}

It is straightforward to adapt the test statistic $T_{\mathrm{spec}}$ to the two-sample testing problem. Concatenate the samples $Z$ and $Y$ in $X = (z_1,\ldots,z_N,y_1,\ldots,y_M)$, and define $T_{\mathrm{spec}}^{(2)}$ to be
\begin{equation}
\label{eqn:graph_spectral_projections_2}
T_{\mathrm{spec}}^{(2)} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2, ~~\textrm{where}~~ a = (\underbrace{N^{-1},\ldots,{N^{-1}}}_{\textrm{length } N},\underbrace{-M^{-1},\ldots,-M^{-1}}_{\textrm{length } M})
\end{equation}

For convenience, we state our following two test statistics with respect to the empirical norm $\norm{\theta}_n = n^{-1/2}\norm{\theta}_2$ for $\theta \in \Reals^n$. They will each depend on a tuning parameter $\lambda > 0$.
\paragraph{Graph Sobolev IPM.}
Letting $C_n := nr^{(d + 2)/2}$ and
\begin{equation*}
\Theta_{1,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B\theta}_2 \leq 1\} 
\end{equation*}
we define the \emph{graph Sobolev IPM} to be
\begin{equation}
\label{eqn:sobolev_IPM}
T_{\textrm{sob}} := \sup_{\substack{\theta \in \Theta_{1,2} \\ \lambda \norm{\theta}_n \leq 1}} \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}. 
\end{equation}

\paragraph{Graph Total Variation IPM.}
Letting $C_n' := n^{2}r^{(d + 1)}$ and 
\begin{equation*}
\Theta_{1,1} := \{\theta \in \Reals^n:~ (C_n')^{-1} \norm{B\theta}_1 \leq 1\}
\end{equation*}
we define the \emph{graph Total Variation} IPM to be
\begin{equation}
\label{eqn:total_variation_IPM}
T_{\mathrm{TV}} := \sup_{\substack{\theta \in \Theta_{1,1}, \\ \lambda \norm{\theta}_n \leq 1} } \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}, \quad
\end{equation}

\subsection{Current results.}

In Theorem~\ref{thm:twosample_sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}}^{(2)} := \1\{T_{\mathrm{spec}}^{(2)} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $H^1(\mathcal{X};R)$ when $d = 1$.

\begin{theorem}
	\label{thm:twosample_sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d = 1$.  Suppose that $\mu = (P + Q)/2$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]$ with density functions $\rho$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < \rho_{\min} < \rho(x) < \rho_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}^{(2)}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right), ~\kappa = n^{2/5}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $R,p_{\max},q_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/5} (\log n)^{h(a,1)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}

We prove Theorem~\ref{thm:twosample_sobolev_testing_rate} by relating the density testing problem to a regression testing problem with a certain type of structured noise, and then proceeding along similar lines to the proof of Theorem~\ref{thm:sobolev_testing_rate}. To pursue this strategy, we require the eigenvectors to satisfy a certain type of incoherence condition; this is in constrast to the regression testing problem with known variance, where we did not require the eigenvectors to be smooth in any sense.

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $1 < d < 4$, is the test $\phi_{\spec}^{(2)}$ minimax optimal?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $C_d^1(\mathcal{X};R)$ for all values of $d$?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $W_d^{1,2}(\mathcal{X};R)$ when $d \leq 4$?
	\item Is the test statistic \eqref{eqn:graph_spectral_projections_2}, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $H^s$? 
	\item Modify the test statistic \eqref{eqn:sobolev_IPM} by replacing the function class $\Theta_{1,2}$ with
	\begin{equation}
	\Theta_{s,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B^{(s)}\theta}_2 \leq 1\} 
	\end{equation}
	Is the modified test statistic, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $H^s$?
	\item What is the minimax testing rate over $BV_d^{1}(\mathcal{X};R)$? Does it exhibit a phase transition analogous to the minimax estimation rate over bounded variation spaces?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over $BV_d^{1}(\mathcal{X};R)$?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over Sobolev and Holder function classes?
\end{enumerate}

\section{Definitions and Notation}

Here we collect definitions of some common function spaces and graph operators.

\subsection{Function Spaces}

\paragraph{Lebesgue spaces.}

We say a Borel measurable function $f: \mathcal{X} \to \Reals$ is in the space $\mathcal{L}^p(\mathcal{X})$ for $1 \leq p < \infty$ if 
$$\norm{f}_{\mathcal{L}^p(\mathcal{X})} := \int_{\mathcal{X}} \abs{f(x)}^p \,dx < \infty$$
and we let 
\begin{equation*}
\mathcal{L}^p(\mathcal{X};R) = \set{f \in \mathcal{L}^p(\mathcal{X}): \norm{f}_{\mathcal{L}^p(\mathcal{X})} < R}
\end{equation*}
be a ball in the Lebesgue space.


\paragraph{Holder spaces.}

For a given $s > 0$, let $\ell = \floor{s}$ be the largest integer strictly less than $s$. Then the $s$th Holder norm is given by
\begin{equation*}
\norm{f}_{C^{s}(\mathcal{X})} := \sum_{\abs{\alpha} < s} \norm{D^{\alpha}f}_{\infty} + \sum_{\abs{\alpha} = \ell} \sup_{x,y \in \mathcal{X}} \frac{\abs{D^{\alpha}f(y) - D^{\alpha}f(x)}}{\norm{x - y}_2^{s - \ell}}
\end{equation*}
and the $s$th Holder space $C^{s}(\mathcal{X})$ consists of all functions which are $\ell$ times continuously differentiable with finite Holder norm $\norm{\cdot}_{C^{s}(\mathcal{X})}$. Denote the Holder ball by $C_d^{s}(\mathcal{X},R) = \set{f \in C^{s}(\mathcal{X}): \norm{f}_{C_d^{s}(\mathcal{X})} \leq R}$. Let $C_c^{s}(\Xset)$ consist of those functions in $C^s(\Xset)$ which are compactly supported within $\Xset$; formally $f \in C_c^{s}(\Xset)$ if and only if $f \in C^s(\Xset)$ and there exists an open set $V \subset \ol{V} \subset \Xset$ such that $\mathrm{supp}(f) \subset V$. 

\paragraph{Sobolev spaces.}

For a given $s$ and $p > 0$, the Sobolev space $W^{s,p}(\mathcal{X})$ consists of all functions $f \in \Leb^2(\mathcal{X})$ such that for each $\alpha = (\alpha_1,\ldots,\alpha_d)$ with $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ belongs to $\mathcal{L}^2(\mathcal{X})$. The Sobolev $\{s,p\}$ norm is then 
\begin{equation*}
\norm{f}_{W^{s,p}(\mathcal{X})}^2 = \sum_{\abs{\alpha} \leq s} \int_{\mathcal{X}} \abs{D^{\alpha}f}^p \,dx
\end{equation*}
and for a given $R > 0$, the corresponding ball is $W^{s,p}(\Xset; R) = \set{f: \norm{f}_{H^s(\Xset)} \leq R}$. In the special case when $p = 2$, the Sobolev space $W$ is a Hilbert space; we adopt the usual convention of writing $H^s(\Xset) = H^s(\Xset)$. 

We write $W_0^{s,p}(\Xset)$ for the completion of $C_c^{\infty}(\Xset)$, and let $H_0(\Xset) = W_0^{s,2}(\Xset)$.  

\paragraph{Bounded Variation spaces.}

For a function $f \in L^1(\mathcal{X})$ the \emph{total variation} semi-norm of $f$ is
\begin{equation*}
TV(f;\mathcal{X}) := \sup \left\{ \int_{\mathcal{X}} f \, \Xsetive \, \psi \,dx : \psi \in C_c^1(\mathcal{X}; \Reals^d), \abs{\psi} \leq 1 \right\};
\end{equation*}
and we write $BV_d(\mathcal{X})$ for the subset of functions $f \in L^1(\mathcal{X})$ which have bounded norm
\begin{equation*}
\norm{f}_{BV_d(\mathcal{X})} := \norm{f}_{\infty} + TV(f;\mathcal{X}).
\end{equation*}
For a given $R > 0$, the corresponding ball is $BV_d^{1}(\mathcal{X};R) = \set{f: \norm{f}_{BV_d(\mathcal{X})} \leq R}$. 

\subsection{Graph Operators}
Let $s \geq 1$ be an integer. The $s$th-order difference operator on $G_{n,r}$, denoted $B^{(s)}$, is defined by
\begin{equation*}
B^{(s)} :=
\begin{cases}
L^{s/2},& ~~ s \textrm{ even} \\
BL^{(s - 1)/2},& ~~ s \textrm{ odd.}
\end{cases}
\end{equation*}

\subsection{Kernels}
We say a kernel function $K(\cdot)$ is a $2$nd order kernel if $K$ is compactly supported on $B(0,1)$, uniformly upper bound $\abs{K(x)} \leq K_{\max} < \infty$ for all $x \in B(0,1)$, and
\begin{equation*}
\int K(x) \,dx = \nu_d,~~ \int x K(x) \,dx = 0.
\end{equation*}
Note that the uniform kernel, defined as $K(x) = \1\{\norm{x} \leq 1\}$, is a $2$nd order kernel. 

\subsection{Notation}

We will treat $f$ interchangeably as a function $f:\Rd \to \Reals$, and as a vector $f = (f(x_1),\ldots,f(x_n))$. More generally, for $U \subset V$ and $f:V \to \Reals$, the action of $f$ on $U$ is naturally defined via the restriction operator $R_U$; we will forego additional notation and simply treat $f$ as a function on $U$, defined as $f(x) = R_Uf(x)$ for $x \in U$. It should always be clear from context how we are using $f$. 

For sequences $(a_n), (b_n)$, we say $a_n = O(b_n)$ if there exists a constant $c$ such that $a_n \leq cb_n$ for all $c$. We use the notation $a \wedge b$ for the minimum of $a$ and $b$.

We use the notation $\Lambda(H)$ to denote the spectrum of a matrix $H$, and $\lambda_k(H)$ to denote the $k$th smallest eigenvalue of $H$.


\section{Proofs}

\subsection{Proof of Lemma~\ref{lem:fixed_graph_testing}}

In this proof, we will drop all notational dependence on the graph $G$ for ease of reading. 

To prove Lemma~\ref{lem:fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\paragraph{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2.
\end{align*}
When $\beta = 0$, this equals $\kappa/n$. Otherwise, we have the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{\lambda_{\kappa}^s}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \lambda_k^s \\
& \geq \norm{\beta}_2^2 - \frac{\beta^T L^s \beta}{\lambda_{\kappa}^s},
\end{align*}
and therefore $\Ebb(T_{\spec}) \geq \kappa/n + n^{-1}(\norm{\beta}_2^2 - \beta^T L^s \beta/\lambda_{\kappa}^s)$. 

\paragraph{Variance of $T_{\mathrm{spec}}$:}
We write $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvector $v_k$ as its $k$th column.
\begin{align*}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa)
\end{align*}
where the last inequality follows from standard properties of the Gaussian distribution. We now move on to showing the desired inequalities \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\paragraph{Proof of~\eqref{eqn:graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{1}{b^2}.
\end{align*}

\paragraph{Proof of~\eqref{eqn:graph_spectral_type_II_error}:} For simplicity, we introduce the notation
\begin{equation*}
\Delta = \frac{\norm{\beta}_2^2}{n} - \frac{\beta^T L^s \beta}{n\lambda_{\kappa}^s}.
\end{equation*}
Assumption~\eqref{eqn:fixed_graph_testing_critical_radius} implies $\Delta \geq 2 t(b)$. Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & = \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\beta^T V_{\kappa} V_{\kappa}^T \beta /n^2}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term, noting that $\Delta = \beta^T V_{\kappa} V_{\kappa}^T \beta/n$, we have
\begin{align}
\frac{\beta^T V_{\kappa} V_{\kappa}^T \beta/n^2}{\Delta^2} & = \frac{1}{n\Delta} \nonumber \\
& \leq \frac{1}{2nt(b)} \nonumber \\
& = \frac{1}{2b\sqrt{2\kappa}}, \label{eqn:spectral_type_II_error_pf2}
\end{align}
and combining~\eqref{eqn:spectral_type_II_error_pf1} and~\eqref{eqn:spectral_type_II_error_pf2} yields~\eqref{eqn:graph_spectral_type_II_error}.

\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate_order1}}

We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = G_{n,r}$. In order to do this, we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that the following statements each hold with probability at least $1 - c/b$ for sufficiently large $n$ (Here and in what follows, $c$ denotes a constant which is fixed in $n$ and $f$ and does not depend on $b$, but may depend on other fixed quantities such as $d$, $s$, etc.): 
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm_order1}
	\textbf{Graph Sobolev norm:} For any $n^{-1/(2 + d)}\leq r \leq 1$,
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm_order1}
	f^T L f \leq c \cdot b \cdot \norm{f}_{H^1(\Xset)}^2 \cdot n^{2} r^{d + 2} 
	\end{equation}
	\item 
	\label{event:eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} Let $t = n^{1/d}$, and let
	\begin{equation*}
	\overline{X} := \Bigl\{\frac{1}{t}(j_1,\ldots,j_d): k \in [t]^d\Bigr\}
	\end{equation*} 
	consist of $n$ evenly spaced grid points on $[0,1]^d$. The transportation distance between $X$ and $\ol{X}$ is
	\begin{equation*}
	d_{\infty}(X,\overline{X}) := \inf_{\pi} \sup\Bigl\{\norm{\pi(\ol{x}_i) - \ol{x}_i}: \ol{x}_i \in \ol{X}\Bigr\}
	\end{equation*}
	where the infimum is taken over all bijections $\pi: \ol{X} \to X$. 
	There exists a constant $c$ such that for any $\kappa = 1,\ldots,n$ if $\max\{ 3 d_{\infty}(X,\ol{X}),n^{-1/d}\} \leq r \leq \kappa^{-2/(d(2 +d))}$, 
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound}
	\lambda_{\kappa} \geq c \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	Theorem~\ref{thm:slepcev_transport_distance} (Theorem 1.1 of \textcolor{red}{(Garcia Trillos and Slepcev)}) establishes that there exists a constant $c$ such that 
	\begin{equation*}
	d_{\infty}(X,\ol{X}) \leq c\biggl(\frac{\log n}{n}\biggr)^{1/d}
	\end{equation*}
	with probability $1  - O(1/n)$. Therefore in particular, the inequality~\eqref{eqn:eigenvalue_tail_bound} is satisfied for sufficiently large $n$ when $\kappa = n^{2d/(4 + d)}$ and $c(\log n/n)^{1/d} \leq r \leq n^{-4/((2+d)(4s + d))}$. 
	\item 
	\label{event:l2_norm}
	\textbf{Empirical norm of $f$:} There exists a constant $c_1$ such that if $\norm{f}_{\Leb^2(\Xset)} \geq c_1 \cdot b \cdot n^{-2/(4 + d)}$, and $d = 1,2$ or $3$,
	\begin{equation}
	\label{eqn:l2_to_empirical_norm}
	\norm{f}_n^2 \geq \frac{1}{b} \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

\subsubsection{Proof of \eqref{eqn:continuous_to_discrete_sobolev_norm}}

The proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows from Lemma~\ref{lem:expected_first_order_seminorm}--which upper bounds the expected first order graph Sobolev seminorm-- and then an application of Markov's inequality.

\subsubsection{Proof of \eqref{eqn:eigenvalue_tail_bound}}
\label{subsubsec:eigenvalue_tail_bound_pf}

We prove~\eqref{eqn:eigenvalue_tail_bound} by comparing $G_{n,r}$ to the tensor product of a $d$-dimensional lattice and a complete graph. The latter is a highly structured graph with known eigenvalues, which as we will see are sufficiently lower bounded for our purposes.

Let $\wt{r} = r/(3(\sqrt{d} + 1)), M = (1/\wt{r})^d$ and $N = n\wt{r}^d$; assume without loss of generality that $M$ and $N$ are integers. Additionally for $m = M^{1/d}$ define
\begin{equation*}
\overline{Z} = \set{\frac{1}{m}(j_1,\ldots,j_d): j \in [m]^d}
\end{equation*}
to be the $M$ evenly spaced grid points over $[0,1]^d$.
For a given $\overline{z}_j \in \overline{Z}$, we write $Q(z_j) = m^{-1}[j_1 - 1,j_1] \times \cdots \times m^{-1}[j_d - 1,j_d]$ for the cube of side length $1/m$ with $z_j$ at one corner. 

Consider the graph $H = (\overline{X}, E_H)$, where $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$ if
\begin{equation*}
\textrm{there exists}~\ol{z}_i, \ol{z}_j \in \ol{Z}~\textrm{such that}~\ol{x}_k \in Q(\ol{z}_i),~ \ol{x}_\ell \in Q(\ol{z}_j),~\textrm{and}~\norm{i - j}_1 \leq 1.
\end{equation*}
On the one hand $H \cong \ol{G}^M_d \otimes K_N$ where $\ol{G}^M_d$ is the $d$-dimensional lattice on $M$ nodes, and $K_N$ is the complete graph on $N$ nodes. On the other hand, we now show that when $\max\{3d_{\infty}(X,\ol{X}), n^{-1/d}\} \leq r$ then $G_{n,r} \succeq H$ as a result of the triangle inequality. If $(\ol{x}_k, \ol{x}_{\ell}) \in E(H)$, by definition there exist $\ol{z}_i, \ol{z}_j$ connected in $\ol{G}_d^M$ such that $\ol{x}_k \in Q(\ol{z}_i)$ and $\ol{x}_{\ell} \in Q(\ol{z}_j)$. This implies that $\ol{x}_k$ and $\ol{x}_{\ell}$ must themselves be close together, since
\begin{equation*}
\norm{\ol{x}_k - \ol{x}_{\ell}}_2 \leq \norm{\ol{x}_k - \ol{z}_{i}}_2 + \norm{\ol{z}_i - \ol{z}_j}_2 +  \norm{\ol{z}_{j} - \ol{x}_{\ell} }_2 \leq \wt{r}(1 + 2\sqrt{d}) = r/3.
\end{equation*}
Since we also assume $r/3 \geq d_{\infty}(X,\ol{X})$, another application of the triangle inequality gives
\begin{equation*}
\norm{\pi(\ol{x}_k) - \pi(\ol{x}_\ell)}_2 \leq \norm{\pi(\ol{x}_k) - \ol{x}_\ell}_2 + \norm{\ol{x}_k - \ol{x}_\ell}_2 \leq \norm{\ol{x}_{\ell} - \pi(\ol{x}_\ell)}_2 \leq r,
\end{equation*}
implying that $(\pi(\ol{x}_k), \pi(\ol{x}_{\ell})) \in E$ and consequently that $G_{n,r} \succeq \ol{G}^M_d \otimes K_N$.

The eigenvalues of lattices and complete graphs are known to satisfy, respectively
\begin{equation*}
\lambda_k(\ol{G}^{M}_d) \geq \frac{k^{2/d}}{M^{2/d}}~\textrm{for $k = 0,\ldots,M - 1$},~~ \textrm{and}~\lambda_{j}(K_N) \geq N\1\{j > 0\}~\textrm{for $j = 0,\ldots,N-1$.}
\end{equation*}
and by standard facts regarding the eigenvalues of tensor product graphs, we have that the spectrum $\Lambda(H)$ satisfies
\begin{equation*}
\Lambda(H) = \set{N\lambda_k(\ol{G}^{M}_d) + M\lambda_j(K_N): \textrm{for $k = 0,\ldots,M - 1$ and $j = 0,\ldots,N-1$}}
\end{equation*}
For all $j = 1,\ldots,N-1$, we have that $M\lambda_j(K_N) = MN = n$. Therefore,
\begin{align*}
\lambda_{\kappa}(H) & \geq \{n \wedge N\lambda_{\kappa}(\ol{G}^{M}_d)\} \\
& \geq \{n \wedge n\wt{r}^d\frac{\kappa^{2/d}}{M^{2/d}}\} \\
& \geq \{n \wedge (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d}\} \\
& \geq (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d},
\end{align*}
where the last inequality is satisfied since $r \leq \kappa^{-2/(d(d + 2))}$, completing the proof of~\eqref{eqn:eigenvalue_tail_bound}.

\subsubsection{Proof of \eqref{eqn:l2_to_empirical_norm_order1}}
The inequality~\eqref{eqn:l2_to_empirical_norm_order1} is implied by the more general Lemma~\ref{lem:empirical_norm_sobolev}, which we state in Section~\ref{subsec:sobolev_testing_rate_pf} and use to prove a similar inequality when $f \in H^s(\Xset)$ for general $s$. 

\subsubsection{Putting the pieces together}

The final proof of Theorem~\ref{thm:sobolev_testing_rate_order1} proceeds by taking $s = 1$ and proceeding as in Section~\ref{subsubsec:sobolev_testing_rate_pf_conclusion}.


\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate}}
\label{subsec:sobolev_testing_rate_pf}

We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = G_{n,r}$. In order to do this, we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that the following statements each hold with probability at least $1 - c/b$ for sufficiently large $n$ (Here and in what follows, $c$ denotes a constant which is fixed in $n$ and $f$ and does not depend on $b$, but may depend on other fixed quantities such as $d$, $s$, etc.): 
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} There exists an $r^{\star}$ (which does not depend on $f$) such that for any $n^{-1/(2(s - 1) + d)}\leq r \leq r^{\star}$,
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm}
	f^T L^s f \leq c \cdot b \cdot \norm{f}_{H^s(\Xset)}^2 \cdot n^{s + 1} r^{s(d + 2)}.
	\end{equation}
	\item 
	\label{event:eigenvalue_tail_decay_2}
	\textbf{Eigenvalue tail bound:} There exists a constant $c$ such that for any $\kappa = 1,\ldots,n$, if $\max\{ 3 d_{\infty}(X,\ol{X}),n^{-1/d}\} \leq r \leq \kappa^{-2/(d(2 +d))}$ then,
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound_2}
	\lambda_{\kappa} \geq c \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	In particular, the inequality~\eqref{eqn:eigenvalue_tail_bound_2} is satisfied for sufficiently large $n$ when $\kappa = n^{2d/(4s + d)}$ and $c(\log n/n)^{1/d} \leq r \leq n^{-4/((2+d)(4s + d))}$. 
	\item 
	\label{event:l2_norm_order1}
	\textbf{Empirical norm of $f$:} There exists a constant $c_1$ such that if $\norm{f}_{\Leb^2(\Xset)} \geq c_1 \cdot b \cdot n^{-2s/(4s + d)}$ and $4s > d$, then
	\begin{equation}
	\label{eqn:l2_to_empirical_norm_order1}
	\norm{f}_n^2 \geq \frac{1}{b} \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

\subsubsection{Proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm}}

The probabilistic bound~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows from the more general Lemma~\ref{lem:roughness_functional_expectation_sobolev} by Markov's inequality. 
\begin{lemma}
	\label{lem:roughness_functional_expectation_sobolev}
	Let $\Xset$ be a Lipschitz domain and let $s \geq 1$ be an integer. Suppose that $f \in H_0^{s}(\Xset)$, and further that $p \in C^{s-1}(\Xset;p_{\max})$ for some constant $p_{\max} < \infty$. Then for any $2$nd-order kernel $K$ and any $n^{-1/(2(s - 1) + d)} \leq r(n)$ such that $r \to 0$ as $n \to \infty$, the expected graph Sobolev seminorm is upper bounded
	\begin{equation}
	\label{eqn:roughness_functional_expectation_sobolev}
	\Ebb\bigl[f^T L^s f\bigr] \leq c \cdot \norm{f}_{H^s(\Xset)}^2 \cdot n^{s + 1}r^{s(d + 2)}
	\end{equation}
	for all $n$ sufficiently large.
\end{lemma}

We note that the proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev} is where we rely on the fact that $f$ and its derivatives are compactly supported on $\Xset$. The proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev} is lengthy, and we defer it to Section~\ref{sec:technical_lemma_proofs}.

\subsubsection{Proof of~\eqref{eqn:eigenvalue_tail_bound}}
See Section~\ref{subsubsec:eigenvalue_tail_bound_pf}.

\subsubsection{Proof of~\eqref{eqn:l2_to_empirical_norm}}

\begin{lemma}
	\label{lem:empirical_norm_sobolev}
	Let $\Xset$ be a Lipschitz domain over which the density is upper and lower bounded 
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty ~~\textrm{for all $x \in \Xset$,}
	\end{equation*}
	and let $f \in H^s(\Xset)$.Then for any $b \geq 1$, there exists $c_1$ such that if 
	\begin{equation}
	\norm{f}_{\Leb^2(\Xset)} \geq 
	\begin{cases*}
	c_1 \cdot b \cdot \norm{f}_{H^s(\Xset)} \cdot \max\{n^{-1/2},n^{-s/d}\},~~\textrm{if}~2s \neq d \\
	c_1 \cdot b \cdot \norm{f}_{H^s(\Xset)} \cdot n^{-a/2},~~\textrm{if}~ 2s = d ~\textrm{for any}~ 0 < a < 1
	\end{cases*}
	\end{equation}
	then,
	\begin{equation}
	\label{eqn:l2_to_empirical_norm_sobolev}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}
	\end{equation}
	where $c_1$ and $c_2$ are constants which may depend only on $s$, $\Xset$, $d$, $p_{\min}$ and $p_{\max}$.
\end{lemma}
The lower bound~\eqref{eqn:l2_to_empirical_norm} results from the more general Lemma~\ref{lem:empirical_norm_sobolev}, which can be verified by checking the various orderings of $2s/(4s + d)$, $s/d$ and $1/2$ whenever $4s < d$. 

\paragraph{Proof of Lemma~\ref{lem:empirical_norm_sobolev}:}

To prove~\eqref{eqn:l2_to_empirical_norm_sobolev} we will show
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot \left(\mathbb{E}\bigl[\norm{f}_n^2\bigr]\right)^2
\end{equation*}
whence the claim follows from the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). Since $p \leq p_{\max}$ is uniformly bounded, we can relate $\mathbb{E}\bigl[\norm{f}_n^4\bigr]$ to the $\Leb^4$ norm,
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] = \frac{(n-1)}{n}\left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + \frac{\mathbb{E}\Bigl[\bigl(f(x_1)\bigr)^4\Bigr]}{n} \leq \left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + p_{\max}^2\frac{\norm{f}_{\Leb^4}^4}{n}.
\end{equation*}
We will use a Sobolev inequality to relate $\norm{f}_{\Leb^4}$ to $\norm{f}_{W_d^{s,2}(\Xset)}$. The nature of this inequality depends on the relationship between $s$ and $d$ (see Theorem 6 in Section 5.6.3 of \textcolor{red}{Evans} for a formal statement), so from this point on we divide our analysis into three cases: (i) the case where $2s > d$, (ii) the case where $2s < d$, and (iii) the borderline case $2s = d$.

\textit{Case 1: $2s > d$.}
When $2s > d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in C^{\gamma}(\overline{\Xset})$ for some $\gamma > 0$ which depends on $s$ and $d$, with the accompanying estimate
\begin{equation*}
\sup_{x \in \Xset} \abs{f(x)} \leq \norm{f}_{C^{\gamma}(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Therefore,
\begin{align*}
\norm{f}_{\Leb^4}^4 & = \int_{\Xset} [f(x)]^4 \,dx \\
& \leq \left(\sup_{x \in \Xset} \abs{f(x)}\right)^2 \cdot \int_{\Xset} [f(x)]^2 \,dx \\
& \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \norm{f}_{\Leb^2(\Xset)}^2.
\end{align*}
Since by assumption
\begin{equation*}
\norm{f}_{\Leb^2(\Xset)}^2 \geq c_1^2 \cdot b^2 \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2 \cdot \frac{1}{n},
\end{equation*}
we have
\begin{equation*}
p_{\max}^2\frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \frac{\norm{f}_{\Leb^2(\Xset)}^4}{n \norm{f}_{\Leb^2(\Xset)}^2} \leq c \frac{\norm{f}_{\Leb^2(\Xset)}^4}{c_1^2 b^2} \leq \frac{\Ebb\bigl[\norm{f}_n^2\bigr]}{b^2},
\end{equation*}
where the last inequality follows by taking $c_1$ sufficiently large.

\textit{Case 2: $2s < d$.}
When $2s < d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in \Leb^q(\Xset)$ for $q = 2d/(d - 2s)$, and moreover that
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Since $4 = 2\theta + (1 - \theta)q$ for $\theta = 2 - d/(2s)$, Lyapunov's inequality implies
\begin{equation*}
\norm{f}_{\Leb^4(\Xset)}^4 \leq \norm{f}_{\Leb^2}^{2\theta} \cdot \norm{f}_{\Leb^q(\Xset)}^{(1 - \theta)q} \leq c \norm{f}_{\Leb^2(\Xset)}^{4} \cdot \left(\frac{\norm{f}_{H^s(\Xset)}}{\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s}.
\end{equation*}
By assumption, $\norm{f}_{\Leb^2(\Xset)} \geq c_1 b \norm{f}_{H^s(\Xset)} n^{-s/d}$, and therefore
\begin{equation*}
p_{\max}^2 \frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c\norm{f}_{\Leb^2(\Xset)}^4 \left(\frac{\norm{f}_{H^s(\Xset)}}{n^{s/d}\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s} \leq \frac{c\norm{f}_{\Leb^2(\Xset)}^4}{c_1b^{d/s}} \leq \frac{\norm{f}_{\Leb^2(\Xset)}^4}{b^2}.
\end{equation*}
where the last inequality follows when $c_1$ is sufficiently large, and keeping in mind that $d/s > 2$ and $b \geq 1$. 

\textit{Case 3: $2s = d$.}
Assume $f$ satisfies~\eqref{eqn:paley_zygmund_1} for a given $0 < a < 1$. When $2s = d$, since $\Xset$ is a Lipschitz domain we have that $f \in L^q(\Xset)$ for any $q < \infty$, with the accompanying estimate
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
In particular the above holds for $q = 2/(1 - a)$ when $1/2 < a < 1$, and for any $q > 4$ when $0 < a < 1/2$. Using Lyapunov's inequality as in the previous case then implies the desired result.

\subsubsection{Putting the pieces together}
\label{subsubsec:sobolev_testing_rate_pf_conclusion}

We note that for all possible values of $X \in \Xset^n$, under the null hypothesis $f = f_0 = 0$ and therefore $\beta = (f(x_1),\ldots,f(x_n)) = 0$ as well. Therefore by~\eqref{eqn:graph_spectral_type_I_error}, we have the following bound on Type I error:
\begin{equation}
\Ebb_{f_0}(\phi_{\mathrm{spec}}) = \mathbb{E}(\mathbb{E}_{\beta = 0}(\phi_{\spec}) | X) \leq \frac{1}{b^2}.
\end{equation}

Now, we bound Type II error under the assumption $f \in H_0^{s}(\mathcal{X};R), p \in C^{s - 1}(\Xset;p_{\max})$ uniformly bounded away from $0$ and $\infty$ over $\Xset$, and 
\begin{equation}
\label{eqn:critical_radius_1}
\norm{f}_{\Leb^2(\Xset)}^2 \geq \epsilon^2 = c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4s + d)}\} \cdot n^{-4s/(4s + d)}.
\end{equation}
Choosing $n^{-1/(2(s - 1) + d)}\leq r(n) \leq n^{-4/((2+d)(4s + d))}$, we may therefore apply our conclusions in Step 2; namely, that for every possible choice of $f \in H^s(\Xset;R)$ there exists a good set $\mathcal{E}_f \subseteq \Xset^n$ with $\Pbb(\mathcal{E}_f) \geq 1 - c/b$ such that each of \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, and \eqref{eqn:l2_to_empirical_norm_sobolev} hold for all $X \subseteq \mathcal{E}_f$. The choice $\kappa = (nR^2)^{2d/(4s + d)}$ balances the squared bias and variance terms on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius}, and we have that for all $X \subseteq \mathcal{E}_f$
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{f^T L^s f}{n\lambda_{\kappa}^s} & \leq 2bR^{2d/(4s + d)}n^{-4s/(4s+d)} + cbR^2\frac{n^{s} r^{s(d + 2)}}{\lambda_{\kappa}^s} \tag{by \eqref{eqn:continuous_to_discrete_sobolev_norm}} \\
& \leq cbR^{2d/(4s + d)}n^{-4s/(4s+d)} \tag{by \eqref{eqn:eigenvalue_tail_bound}} \\
& \leq \frac{1}{b}\norm{f}_{\Leb^2(\Xset)}^2 \\
& \leq \frac{1}{n}\sum_{i = 1} \bigl[f(x_i)\bigr]^2. \tag{by \eqref{eqn:l2_to_empirical_norm_sobolev}}
\end{align*}
where the last two inequalities follow for a suitably large choice of $c_1$ in~\eqref{eqn:critical_radius_1}.
We conclude that for all $X \subseteq \mathcal{E}_f$, the inequality \eqref{eqn:fixed_graph_testing_critical_radius} is satisfied with respect to $\beta = (f(x_1),\ldots,f(x_n))$ and $G = G_{n,r}$. As a result the worst-case Type II error is bounded
\begin{equation*}
\sup_{\substack{f \in H^s(\Xset;R), \\ \norm{f}_{\Leb^2(\Xset)} \geq \epsilon}}\mathbb{E}_{f}(1 - \phi_{\spec}) \leq \sup_{\substack{f \in H^s(\Xset;R), \\ \norm{f}_{\Leb^2(\Xset)} \geq \epsilon}} \mathbb{E}\bigl[\mathbb{E}_{\beta}(1 - \phi_{\spec}|X \in \mathcal{E}_f)\bigr] + \frac{c}{b} \leq \frac{3 + c}{b},
\end{equation*}
completing the proof of Theorem~\ref{thm:sobolev_testing_rate} upon proper choice of constants $c_1,c_2$ in that Theorem.

\subsection{Proof of Proposition~\ref{prop:L4_testing_rate}}

Let $T_{\mathrm{mean}} = \frac{1}{n}\sum_{i = 1}^{n} y_i^2$. The expectation of $T_{\mathrm{mean}}$ is
\begin{equation*}
\Ebb(T_{\mathrm{mean}}) = \mathbb{E}(f^2(x_1)) + 1,
\end{equation*}
and the variance can be upper bounded
\begin{equation*}
\Var(T_{\mathrm{mean}}) \leq \frac{1}{n}(3 + p_{\max} \norm{f}_{\Leb^4}^4 + p_{\max}\norm{f}_{\Leb^2}^2) = \frac{c}{n}.
\end{equation*}
Since $\mathbb{E}(f^2(x_1)) \geq p_{\min} \epsilon^2$, when $\epsilon^2 \gtrsim b\cdot n^{-1/2}$ we can apply Chebyshev's inequality to obtain the claimed result.

\subsection{Proof of Theorem~\ref{thm:twosample_sobolev_testing_rate}}

As mentioned previously, to prove Theorem~\ref{thm:twosample_sobolev_testing_rate}, we relate the two sample model to the following regression model: we observe
$X = \{x_1,\ldots,x_n\} \sim \mu$ (where we recall $\mu = (P + Q)/2$), and associated labels
\begin{equation*}
a_i = 
\begin{cases}
1, & \textrm{with probability}~ \frac{p(x_i)}{p(x_i) + q(x_i)} \\
-1, & \textrm{with probability}~ \frac{q(x_i)}{p(x_i) + q(x_i)}
\end{cases}
\end{equation*}
where $a_i$ is conditionally independent of $x_j,a_j$ given $x_i$. 

\textcolor{red}{TODO:} Complete the above argument.

Our analysis will trace a similar path to the proof of Theorem~\ref{thm:sobolev_testing_rate}, proceeding according to the following steps:
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $a$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded. One of these functionals will be a measure of eigenvector incoherence.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta^{(p)} = (\beta_1^{(p)},\ldots,\beta_n^{(p)}) \in \Reals^n$, $\beta^{(q)} = (\beta_1^{(q)},\ldots,\beta_n^{(q)}) \in \Reals^n$ be non-negative signals over the vertices $V$. We observe labels $a = (a_1,\ldots,a_n)$ according to the model
\begin{equation*}
a_i = 
\begin{cases*}
1, & \textrm{with probability $\frac{\betap_i}{\betap_i + \betaq_i}$} \\
-1, & \textrm{with probability $\frac{\betaq_i}{\betap_i + \betaq_i}$}
\end{cases*}
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec}^{(2)} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec}^{(2)} = \1\{T_{\spec}^{(2)} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $\Pi_{\max}(\kappa;G)$ be a measure of the incoherence of the eigenvectors $v_1,\ldots,v_\kappa$, given by
\begin{equation*}
\Pi_{\max}(\kappa;G) := \max_{i = 1,\ldots,n} \left\{\sum_{k = 1}^{\kappa} v_{k,i}^2\right\}
\end{equation*}
In Lemma~\ref{lem:twosample_fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}^{(2)}$. Our bound on the Type II error will be stated as a function of $\Pi_{\max}(\kappa;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$ and the smoothness functional $S_2(\beta;G)$. We use the notation $\varDelta^{(p,q)} = (\betap - \betaq)/(\betap + \betaq)$.

\begin{lemma}
	\label{lem:twosample_fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\betap = \betaq$, the Type I error of $\phi_{\spec}^{(2)}$ is upper bounded
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_I_error}
		\mathbb{E}_{\betap,\betap}(\phi_{\spec}^{(2)}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} Suppose $\Pi_{\max}(\kappa;G) \leq 1$. Then for any $b$, $\betap$ and $\betaq$ such that
		\begin{equation}
		\label{eqn:twosample_fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} (\varDelta_i^{(p,q)})^2 \geq \frac{1}{1 - \Pi_{\max}(\kappa;G)}\left(2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\varDelta;G)}{ns_{\kappa}}\right)
		\end{equation}
		the Type II error of $\phi_{\spec}^{(2)}$ is upper bounded,
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_II_error}
		\mathbb{E}_{\betap,\betaq}(1 - \phi_{\spec}^{(2)}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:twosample_fixed_graph_testing}.}

To prove Lemma~\ref{lem:twosample_fixed_graph_testing} we will compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}^{(2)}$. The inequalities \eqref{eqn:twosample_graph_spectral_type_I_error} and \eqref{eqn:twosample_graph_spectral_type_II_error} can then be computed using Chebyshev's inequality in a manner very similar to the proof of Lemma~\ref{lem:fixed_graph_testing}, and we omit the details. Let $w_i := a_i - \vardeltapq_{i}$, and note that $\Ebb(w_i) = 0$ and
\begin{equation*}
\Ebb(w_i^2) = \frac{\betap_i \betaq_i}{(\betap_i + \betaq_i)^2} = 1 - (\vardeltapq_{i})^2,~~ \Ebb(w_i^3) = \frac{1}{2}(1 - (\vardeltapq_{i})^2)\vardeltapq_{i}
\end{equation*}

\vspace{.2 in}

\textit{Mean of $T_{\spec}^{(2)}$}:

We have
\begin{align*}
\Ebb(T_{\spec}^{(2)}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\vardeltapq}{v_k}^2 + \Ebb\bigl(\dotp{w}{v_k}^2 + 2 \dotp{w}{v_k} \dotp{\vardeltapq}{v_k}\bigr)\right) \\
& = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{1 - (\vardeltapq)^2}{v_k^2} + \dotp{\vardeltapq}{v_k}\right) \\
& \geq \frac{\kappa}{n} + \left(\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} - \dotp{(\vardeltapq)^2}{v_k^2}\right)
\end{align*}
When $\vardeltapq = 0$, this equals $\kappa/n$. Otherwise, we have the following upper bound:
\begin{align*}
\sum_{k = 1}^{\kappa} \dotp{(\vardeltapq)^2}{v_k^2}  & = \sum_{i = 1}^{n} (\vardeltapq_{i})^2 \left(\sum_{k = 1}^{\kappa} v_{k,i}^2 \right)\\
& \leq \Pi_{\max}(\kappa,G) \cdot \sum_{i = 1}^{n} (\vardeltapq_{i})^2 = \Pi_{\max}(\kappa,G) \cdot \norm{\vardeltapq}_2^2.
\end{align*}
Additionally, $\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} \geq \norm{\vardeltapq}_2^2 - S_2(\beta;G)/s_{\kappa}$ by reasoning given in the proof of Lemma~\ref{lem:fixed_graph_testing}, and as a result 
\begin{equation*}
\Ebb(T_{\spec}^{(2)}) \geq \frac{\kappa}{n} -  \frac{\bigl(1 - \Pi_{\max}(\kappa;G)\bigr)}{n} \norm{\vardeltapq}_2^2 - \frac{S_2(\beta;G)}{ns_{\kappa}}
\end{equation*}

\vspace{.2 in}
\textit{Variance of $T_{\spec}^{(2)}$:}

We have
\begin{align}
\Var(T_{\spec}^{(2)}) & = \frac{1}{n^2}\Var(2\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w} + \dotp{V_{\kappa}V_{\kappa}^Tw}{w}) \nonumber \\
& = \frac{1}{n^2}\Bigl\{4\underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w})}_{=: V_1} + 2\underbrace{\Cov(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w},\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=: K_1} + \underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=:V_2}\Bigr\}, \label{eqn:var}
\end{align}
and we now upper bound each of the three terms on the right hand side of the previous display.

\textbf{Upper bound on $V_1$:}
Let $\Sigma := \Cov(w)$ be the covariance matrix of $w$. Noting that $\Sigma \preceq I$, we have
\begin{equation}
\label{eqn:var1}
\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w}) =(\vardeltapq)^T V_{\kappa}V_{\kappa}^T \Sigma V_{\kappa}V_{\kappa}^T \vardeltapq \leq \norm{V_{\kappa}V_{\kappa}^T\vardeltapq}^2.
\end{equation}

\textbf{Upper bound on $K_1$:}
Noting that $\Ebb(\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w}) = 0$, we have that
\begin{align}
K_1 & = \Ebb\left[\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w} \dotp{V_{\kappa}V_{\kappa}^T w}{w}\right] \nonumber \\
& = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{i' = 1}^{n} \Ebb\left[w_i w_j (V_{\kappa}V_{\kappa}^T)_{ij} w_{i'} (V_{\kappa}V_{\kappa}^T\vardeltapq)_i\right] \nonumber \\
& = \sum_{i = 1}^{n} \Ebb\left[w_i^3\right] (V_{\kappa}V_{\kappa}^T)_{ii} (V_{\kappa}V_{\kappa}^T \vardeltapq)_i \nonumber \\
& = \frac{1}{2}\sum_{i = 1}^{n} (1 - (\vardeltapq_{i})^2)(\vardeltapq_{i}) (V_{\kappa} V_{\kappa}^T)_{ii}  (V_{\kappa}V_{\kappa}^T\vardeltapq)_i \nonumber \\
& \leq \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \left(\sum_{i = 1}^{n} (\vardeltapq_{i})^2 (V_{\kappa} V_{\kappa}^T)_{ii}^2\right)^{1/2} \nonumber \\
& \leq \Pi(\kappa,G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq}  \label{eqn:var2} 
\end{align}

\textbf{Upper bound on $V_2$:}
$V_2$ is a variance of a sum, which we re-express as the sum of covariances:
\begin{align*}
V_2 & = \Var(\dotp{V_{\kappa}V_{\kappa}^T w}{w}) \\
& = \sum_{i,j,i',j' = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij} (V_{\kappa}V_{\kappa}^T)_{i'j'} \Cov(w_i w_j, w_{i'} w_{j'}).
\end{align*}
This covariance will be non-zero only when $i = i' \neq j = j'$, $i = j' \neq j = i'$, or $i = i' = j = j'$, and therefore
\begin{align}
V_2 & = 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 \Var(w_iw_j) + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \Var(w_i^2) \nonumber\\
& \leq 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \leq 3 \mathrm{tr}\bigl((V_{\kappa}V_{\kappa}^T)^2\bigr) = 3\kappa. \label{eqn:var3}
\end{align}

Now, plugging \eqref{eqn:var1}, \eqref{eqn:var2}, and \eqref{eqn:var3} back into \eqref{eqn:var}, we obtain
\begin{equation}
\label{eqn:var_5}
\Var(T_{\spec}^{(2)}) \leq \frac{1}{n^2}\left\{4\norm{V_{\kappa}V_{\kappa}^T \vardeltapq}^2 + \Pi(\kappa;G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq} + 3\kappa \right\}.
\end{equation}


\subsubsection{Step 2: Bounding neighborhood graph functionals}

Note that since $p,q \in H^1(\mathcal{X};R)$, we have $f := p - q \in H^1(\mathcal{X};2R)$. Therefore the bounds~\eqref{eqn:continuous_to_discrete_sobolev_norm} and \eqref{eqn:l2_to_empirical_norm} apply to $S_2(\varDelta,G_{n,r})$ and $(1/n)\sum_{i = 1}^{n}\varDelta_i^2$, respectively. The bound~\eqref{eqn:eigenvalue_tail_bound} continues to apply to $s_{\kappa}$. What remains is to upper bound the incoherence parameter
\begin{equation}
\label{eqn:pi_max}
\Pi_{\max}(\kappa,G_{n,r}) \leq \frac{1}{2}~~\textrm{when}~ \kappa = n^{2/5}~\textrm{and}~ r = \log^a n \cdot \left(\frac{\log n}{n}\right)
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$.

Our proof of~\eqref{eqn:pi_max} will proceed by relating $G_{n,r}$ to a graph which satsifies a strict notion of incoherence, the chain graph. When $G = \overline{G}$, the incoherence parameter $\Pi_{\max}$ is upper bounded
\begin{equation}
\Pi_{\max}(\kappa,\overline{G}) \leq \frac{\kappa}{n}.
\end{equation}
To prove that the weaker bound \eqref{eqn:pi_max} holds with respect to $G_{n,r}$, we will show that the two graphs $\overline{G}$ and $G_{n,r}$ are $\delta$-spectrally similar, meaning
\begin{equation}
\label{eqn:spectral_similarity}
(1 - \delta_n) x^T L x \leq x^T \overline{L} x \leq (1 + \delta_n) x^T L x~~\textrm{for all}~x \in \Reals^n, \delta_n = c \cdot (\log n)^{h(a,1) + da}\cdot \frac{\log n}{n}
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$. The proof of~\eqref{eqn:spectral_similarity} relies heavily on bounding the transport distance between $X$ and $\overline{X}$, i.e. \eqref{eqn:slepcev_transport_distance}. Then the following technical Lemma along with \eqref{eqn:spectral_similarity} will imply \eqref{eqn:pi_max}.
\begin{lemma}
	\label{lem:pi_max_pf_1}
	Let $G$ satisfy~\eqref{eqn:spectral_similarity} for a given $0 \leq \delta < 1$. Then for any $R \gtrsim n \delta^{1/2} (s_{n}^{1/2} \vee 1)$ and $k \leq n/8$,
	\begin{equation*}
	v_{k,i}^2 \lesssim \frac{1}{n}\left(R + \frac{n^4 \delta^2 s_{n}^2}{R^3}\right) \quad \textrm{for every $i = 1,\ldots,n$.}
	\end{equation*} 
\end{lemma}

\textcolor{red}{TODO}: Explain how Lemma~\ref{lem:pi_max_pf_1} implies~\eqref{eqn:pi_max}.


At a high level, Lemma~\ref{lem:pi_max_pf_1} is proved through repeated applications of the Davis-Kahan Theorem. The full proof is long and we delay presenting it until Section~\ref{sec:technical_lemma_proofs}.

\subsubsection{Step 3: Conclusion}

With Lemma~\ref{lem:twosample_fixed_graph_testing} as well as \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, \eqref{eqn:l2_to_empirical_norm} and \eqref{eqn:pi_max} in hand, the proof of Theorem~\ref{thm:twosample_sobolev_testing_rate} follows by similar reasoning to the conclusion of Theorem~\ref{thm:sobolev_testing_rate}.

\section{Proofs of Technical Results}
\label{sec:technical_lemma_proofs}

Here we give in full detail the proofs of Lemma~\ref{lem:roughness_functional_expectation_sobolev}, \eqref{eqn:spectral_similarity}, and Lemma~\ref{lem:pi_max_pf_1}.

\subsection{Proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev}}
To simplify exposition, we introduce the iterated difference operator, defined recursively as
\begin{equation*}
D_{jk}f(x) = (D_{k}f(x_j) - D_{k}f(x))\frac{K_r(x_j,x)}{r^d},~~ D_jf(x) = (f(x_j) - f(x))\frac{K_r(x_j,x)}{r^d}~~ \textrm{for $j \in [n], k \in [n]^q$}
\end{equation*}
We will also use the notation $d_jf(x) := (f(x_j) - f(x))$. We split our analysis into cases based on whether $s$ is even or odd. 

\subsubsection{Case 1: $s$ is even.}
When $s$ is even, letting $q = s/2$ we have the decomposition
\begin{equation}
\label{eqn:continuous_to_discrete_sobolev_norm_pf1}
f^T L^s f =  r^{ds} \cdot \sum_{i = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} D_kf(x_i) D_{\ell}f(x_i). 
\end{equation}

For given index vectors $k,\ell \in [n]^q$ and indices $i,j$, let $I = \abs{k \cup \ell \cup i}$ be the total number of unique indices. We separate our analysis into cases based on the magnitude of $I$, specifically whether $I = s + 1$ (the leading terms where all indices are distinct) $I < s + 1$ (the terms where at least one index is repeated) and show that
\begin{equation}
\label{eqn:expected_difference_operators_1}
\Ebb(D_kf(x_i) D_\ell f(x_i)) =
\begin{cases*}
O(r^{2s}) \cdot \norm{f}_{H^s(\Xset)}^2, & ~~\textrm{if $I = s + 1$} \\
O(r^{2} r^{d(I - (s + 1))}) \cdot [f]_{H^1(\Xset)}^2, & ~~\textrm{if $I < s + 1$}~ 
\end{cases*}
\end{equation}
We will prove~\eqref{eqn:expected_difference_operators_1} in Section~\ref{subsec:expected_difference_operators_pf}. First, we verify that~\eqref{eqn:continuous_to_discrete_sobolev_norm_pf1} and \eqref{eqn:expected_difference_operators_1} are together enough to show Lemma~\ref{lem:roughness_functional_expectation_sobolev} when $s$ is even. In the sum on the right hand side of~\eqref{eqn:continuous_to_discrete_sobolev_norm_pf1}, there are $O(n^{I})$ terms with exactly $I$ distinct indices. When $I < s + 1$, by~\eqref{eqn:expected_difference_operators_1} the total contribution of such terms to the sum is $O(n^{I}r^{d(I - 1) + 2}) \cdot [f]_{H^1(\Xset)}^2$. Since by assumption $r \geq n^{-1/d}$, this increases with $I$. Taking $I = s$ to be the largest integer less than $s + 1$, the contribution of these terms to the sum is therefore $O(n^sr^{d(s - 1) + 2}) \cdot [f]_{H^1(\Xset)}^2$ which in light of the restriction $r \geq n^{-1/(2(s - 1) + d)}$ is $O(n^{s+1}r^{s(d +2)}) \cdot [f]_{H^1(\Xset)}^2$. On the other hand when $I = s + 1$, by~\eqref{eqn:expected_difference_operators_1} we immediately have that the total contribution of these terms is $O(n^{s + 1}r^{2(s + d)}) \cdot \norm{f}_{H^s(\Xset)}$. Therefore,
\begin{equation*}
\Ebb(f^T L^s f) = O(n^{s+1}r^{s(d+2)}) \cdot \norm{f}_{H^s(\Xset)}^2.
\end{equation*}

\subsubsection{Case 2: $s$ is odd.}
When $s$ is odd, letting $q = (s - 1)/2$ we have
\begin{equation}
\label{eqn:roughness_functional_representation_odd}
f^T L^s f =  r^{ds} \cdot \sum_{i,j = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} \bigl(d_jD_kf(x_i)\bigr) \cdot  \bigl(d_jD_{\ell}f(x_i)\bigr) \cdot K_r(x_i,x_j).
\end{equation}
For given index vectors $k,\ell \in [n]^q$ and indices $i,j \in [n]$, let $I = \abs{k \cup \ell \cup i \cup j}$ be the total number of unique indices. Similar to the case when $s$ is even, we show that 
\begin{equation}
\label{eqn:expected_difference_operators_2}
\Ebb(d_iD_kf(x_j) d_iD_\ell f(x_j)) =
\begin{cases*}
O(r^{2s}) \cdot \norm{f}_{H^s(\Xset)}^2, & ~~\textrm{if $I = s + 1$} \\
O(r^{2} \cdot r^{d(I - (s + 1))}) \cdot [f]_{H^1(\Xset)}^2, & ~~\textrm{if $I < s + 1$}~ 
\end{cases*}
\end{equation}
Then Lemma~\ref{lem:roughness_functional_expectation_sobolev} follows from similar reasoning to the case where $s$ was even.

\subsubsection{Proof of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2}}
\label{subsec:expected_difference_operators_pf}

We first prove the desired bounds in the case when some indices are repeated, and then the desired bounds in the case when all indices are distinct. 

\paragraph{Repeated indices.}

Since the proofs of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2} are essentially the same for the case where some index is repeated, we will assume without loss of generality that $s$ is even. Let $k,\ell \in [n]^q$ be index vectors for $q = s/2$. 

When at least one index is repeated, we obtain a sufficient upper bound by reducing the problem of upper bounding the iterated difference operator to that of upper bounding a single difference operator. Letting $k = (k_1,\ldots,k_q)$, we can show by induction that the absolute value of the iterated difference operator $\abs{D_kf(x_i)}$ is upper bounded by
\begin{equation*}
\abs{D_kf(x_i)} \leq \left(\frac{2K_{\max}}{r^d}\right)^{q-1} \sum_{h \in k \cup i} \abs{D_{k_q}f(x_h)} \cdot \1\{G_{n,r}[X_{k \cup i}]~\textrm{is a connected graph} \}.
\end{equation*}
Therefore,
\begin{align}
\abs{D_kf(x_i)} \cdot \abs{D_{\ell}f(x_i)} & \leq \left(\frac{2K_{\max}}{r^d}\right)^{2(q - 1)} \sum_{h,j \in k \cup \ell \cup i} \abs{D_{k_q}f(x_h)} \cdot \abs{D_{\ell_q}f(x_j)} \cdot \1\{G_{n,r}[X_{k \cup i}], G_{n,r}[X_{\ell \cup i}]~\textrm{are connected graphs.} \} \nonumber \\
& =  \left(\frac{2K_{\max}}{r^d}\right)^{2(q - 1)} \sum_{h,j \in k \cup \ell \cup i} \abs{D_{k_q}f(x_h)} \cdot \abs{D_{\ell_q}f(x_j)} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is a connected graph.} \} \label{eqn:expected_difference_operators_sobolev_pf0}
\end{align}

We now break our analysis into three cases, based on the number of distinct indices in $k_q,\ell_q,h,j$. In each case we will obtain the same rate
\begin{equation*}
\Ebb\Bigl[\abs{D_{k_q}f(x_h)} \cdot \abs{D_{\ell_q}f(x_j)}\Bigr] = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [f]_{H^1(\Xset)}^2,
\end{equation*}
and plugging this back in to~\eqref{eqn:expected_difference_operators_sobolev_pf0} we have that for any $k, \ell \in [n]^q$
\begin{equation*}
\Ebb\Bigl[\bigl|D_{k}f(x_i)\bigr| \cdot \bigl|D_{\ell}f(x_i)\bigr|\Bigr] = O(r^{(\abs{k \cup \ell \cup i} - (2q + 1))d + 2}) \cdot [f]_{H^1(\Rd)}^2.
\end{equation*}

\textit{Case 1: Two distinct indices.}
Let $k_q = \ell_q = i$, and $h = j$. Using the law of iterated expectation, we obtain
\begin{align*}
\Ebb \left[\bigl(D_{i}f(x_j)\bigr)^2 \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\right] &= \Ebb \left[\bigl(D_{i}f(x_j)\bigr)^2 \cdot \Pbb\bigl[\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \} |x_i,x_j\bigr]\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 2)d}) \cdot \Ebb\left[\bigl(D_{i}f(x_j)\bigr)^2\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d}) \cdot \Ebb\left[\bigl(d_{i}f(x_j)\bigr)^2K_r(x_i,x_j)\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [f]_{H^1(\Xset)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm}.

\textit{Case 2: Three distinct indices.}
Let $k_q = \ell_q = i$, for some $i \neq j \neq h$. Using the law of iterated expectation, we obtain
\begin{align*}
\Ebb \Bigl[ & \abs{D_{i}f(x_j)} \cdot \abs{D_if(x_h)} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\Bigr] = \nonumber \\
& \Ebb\Bigl[\abs{D_{i}f(x_j)} \cdot \abs{D_if(x_h)} \cdot \Pbb\bigl[\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \} |x_i,x_j,x_h\bigr]\Bigr] \nonumber \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d}) \cdot \Ebb\Bigl[\abs{D_{i}f(x_j)} \cdot \abs{D_if(x_h)} \Bigr] \nonumber \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2})\cdot[f]_{H^1(\Xset)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm_2}.


\textit{Case 3: Four distinct indices.}
Using the law of iterated expectation, we find that
\begin{align*}
\Ebb\Bigl[ &\abs{D_{k_q}f(x_i)}\cdot{\abs{D_{\ell_q}f(x_j)}} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\Bigr] \\
& = \Ebb\Bigl[\abs{D_{k_q}f(x_i)}\cdot{\abs{D_{\ell_q}f(x_j)}} \cdot\Pbb\bigl[G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected}|x_i,x_j,x_{k_q},x_{\ell_q}\bigr]\Bigr] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 4)d}) \cdot \Ebb\Bigl[\abs{D_{k_q}f(x_i)}\cdot{\abs{D_{\ell_q}f(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [f]_{H^1(\Xset)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm_3}.

\paragraph{All indices distinct.}

We first show the desired result when $s$ is even, and then when $s$ is odd. 

\textit{Case 1: $s$ is even.}

The result follows from the law of iterated expectation, the condition that the density be upper bounded, and equations~\eqref{eqn:leading_term_sobolev_compact_1} and \eqref{eqn:leading_term_sobolev_compact_2} in Lemma~\ref{lem:leading_term_sobolev_compact}
\begin{equation*}
\Ebb\bigl[D_kf(x_i)D_kf(x_j)\bigr] = \Ebb\Bigl[\bigl(\Ebb[D_kf(x_i)|x_i]\bigr)^2\Bigr] \leq p_{\max} \norm{\Ebb\bigl[D_kf\bigr]}_{\Leb^2(\Xset)}^2 \leq c r^{2s} \norm{f}_{H^s(\Xset)}^2.
\end{equation*}

\textit{Case 2: $s$ is odd.}

By Lemma~\ref{lem:leading_term_sobolev_compact}, we have that for $k \in (n)^{(s - 1)/2}$, there exists a function $f_{s - 1,q} \in H_0^1(\Xset)$ such that
\begin{equation*}
\Bigl\|\Ebb\bigl[D_kf\bigr] - r^{2{s - 1}}f_{s - 1,q}\Bigr\|_{\Leb^2(\Xset)} \leq c r^{2s} \norm{f}_{H^s(\Rd)}^2
\end{equation*}
and $\norm{f_{s - 1,q}}_{H^1(\Xset)} \leq c \norm{f}_{H^s(\Xset)}$. By the law of iterated expectation, and writing $\varDelta := \Ebb\bigl[D_kf\bigr] - r^{2{s - 1}}f_{s - 1,q}$, we have
\begin{align*}
\Ebb\biggl[\Bigl(d_iD_kf(x_j)\Bigr) \Bigl(d_iD_{\ell}f(x_j)\Bigr) K_r(x_i,x_j)\biggr] & =  \Ebb\biggl[\Bigl(d_i\bigl(\Ebb[D_kf|x_i,x_j]\bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \nonumber \\
& = \Ebb\biggl[\Bigl(d_i\bigl(r^{s - 1}\cdot f_{s - 1,q} + \varDelta \bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \\
& \leq 2r^{2(s - 1)}\Ebb\biggl[\Bigl(d_i\bigl(f_{s - 1,q}\bigr)\Bigr)^2K_r(x_i,x_j)\biggr]  + 2 \Ebb \biggl[\Bigl(d_i\varDelta(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \\
& \leq 2r^{2(s - 1)}\Ebb\biggl[\Bigl(d_i\bigl(f_{s - 1,q}\bigr)\Bigr)^2K_r(x_i,x_j)\biggr] + 2p_{\max}^2\norm{\varDelta}_{\Leb^2(\Xset)}^2 \\
& \leq c r^{2s} \norm{f}_{H^s(\Xset)}^2
\label{eqn:expected_difference_operators_sobolev_pf1}
\end{align*}
where the last inequality follows by Lemma~\ref{lem:expected_first_order_seminorm}.

\subsection{Proof of~\eqref{eqn:spectral_similarity}}

Assuming~\eqref{eqn:slepcev_transport_distance} holds and $r$ is chosen according to~\eqref{eqn:pi_max}, we have already shown that the upper bound in~\eqref{eqn:spectral_similarity} holds for $\delta = 0$ (see the proof of ~\eqref{eqn:eigenvalue_tail_bound}). Now we need only to show the lower bound, which we will do following these steps:
\begin{enumerate}[(i)]
	\item First, we consider the case of two fixed graphs $G$ and $\wt{G}$, and establish a Poincare inequality. In other words, we show that if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$, then the Laplacian matrix $L$ cannot be much larger $\wt{L}$, in the usual sense of partial ordering of matrices.
	\item Then, we treat the case of $G = G_{n,r}$ and $\wt{G} = \overline{G}$. We assume that~\eqref{eqn:slepcev_transport_distance} holds, and exhibit such a mapping between edges in $G_{n,r}$ and paths in $\overline{G}$. The resulting Poincare inequality will imply~\eqref{eqn:spectral_similarity}.
\end{enumerate}
In step (ii) we assume that~\eqref{eqn:slepcev_transport_distance} holds. Since \eqref{eqn:slepcev_transport_distance} holds with probability at least $1 - c/n$, this will imply that~\eqref{eqn:spectral_similarity} holds with at least the same probability.

\subsubsection{Step 1: Poincare inequality}

Let $G = (V,E)$ and $\wt{G} = (V,\wt{E})$ be two graphs over a common vertex set $V$. The set $\mathcal{P}_{\wt{G}}$ consists of all paths $P$ over $\wt{G}$, meaning all tuples
\begin{equation*}
P = (\wt{e}_1,\wt{e}_2,\ldots,\wt{e}_m),~~m \in \mathbb{N}~, \wt{e}_j \in \wt{E}~ \forall~ j \in [m], (\wt{e}_j)_2 = (\wt{e}_{j + 1})_1 \forall~j \in [m - 1].
\end{equation*}
Let $\gamma:E \to \mathcal{P}_{\wt{G}}$ be a mapping from edges in $G$ to paths in $\wt{G}$. The maximum path length $M(\gamma)$ is defined
\begin{equation*}
M(\gamma) = \max_{e \in E} \abs{\gamma(e)}
\end{equation*} 
and the bottleneck $b(\gamma)$ is defined
\begin{equation*}
b(\gamma) = \max_{\wt{e} \in \wt{E}} \abs{\{e \in E:  \wt{e} \in \gamma(e)\}}.
\end{equation*}
As mentioned previously, the Laplacian matrix $L$ cannot be much larger than $\wt{L}$ if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$. Formally, we will show the following: for any $\gamma: E \to \mathcal{P}_{\wt{E}}$,
\begin{equation}
\label{eqn:poincare_inequality}
x^T L x \leq M(\gamma)\cdot b(\gamma) \cdot (x^T \wt{L} x)~\forall{x \in \Reals^n}
\end{equation} 

\paragraph{Proof of~\eqref{eqn:poincare_inequality}:}

Letting $c \in \Reals$, we will use the notation $G \preceq c \cdot  \wt{G}$ as shorthand for
\begin{equation*}
x^T L x \leq c \cdot  x^T \wt{L} x,~~\textrm{for all $x \in \Reals^n$}.
\end{equation*}


Let $G_e = (V, \set{e})$ and $P_e = (V, \set{\widetilde{e}: \widetilde{e} \in \gamma(e)})$ be the graphs associated with $e$ and $\gamma(e)$, respectively. By Lemma \ref{lem: path_poincare}, we have
\begin{equation*}
G_{e} \preceq \abs{P_e} P_e
\end{equation*}
Summing over all $e \in E_G$, we obtain
\begin{align*}
G & \preceq \sum_{e \in E_G} \abs{P_e} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} \sum_{e \in E_G} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} b_{\gamma}\cdot \widetilde{G}
\end{align*}

\subsubsection{Step 2: Mapping $E_{G_{n,r}}$ to $\mathcal{P}_{\overline{G}}$}
We assume there exists a mapping $\pi:\overline{X} \to X$ which satisfies \eqref{eqn:slepcev_transport_distance}. We will apply the Poincare inequality to $\pi^{-1}(G_{n,r})$, the isomorphism of $G_{n,r}$ induced by the mapping of vertices $\pi^{-1}: X \to \overline{X}$. We will map a given $(\overline{x}_{k},\overline{x}_{\ell}) \in \pi^{-1}(E)$ to a shortest path $P \in \mathcal{P}_{\overline{G}}$ (measured by Manhattan distance) between $\overline{x}_k$ and $\overline{x}_{\ell}$. Recalling the notation $t = n^{1/d}$ and letting
\begin{equation*}
s_i = \frac{1}{t}(\textrm{sign}(k_i - \ell_i),0,\ldots,0),~~\textrm{for $i = 1,\ldots,d$,}
\end{equation*}
our mapping $\gamma: \pi^{-1}(E) \to \mathcal{P}_{\overline{G}}$ is given by
\begin{align*}
\gamma((\overline{x}_{k},\overline{x}_{\ell})) = \bigl(& (\overline{x}_k, \overline{x}_{k} + s_1), (\overline{x}_{k} + s_1,\overline{x}_{k} + 2s_1),\ldots,(\overline{x}_{k} + (\abs{k_1 - \ell_1} - 1)s_1, \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1)) \\
& (\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1), \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + s_2),\ldots,(\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + (\abs{k_2 - \ell_2} - 1)s_2, \overline{x}_{k}  + \frac{1}{t}((k_1 - \ell_1) + (k_2 - \ell_2)) \\
& \vdots \\
& (\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + s_d,\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + 2s_d), \ldots, (\overline{x}_k + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + (\abs{k_d - \ell_d} - 1)s_d, \overline{x}_{\ell})\bigr)
\end{align*}
As a result of~\eqref{eqn:slepcev_transport_distance} we obtain the following bounds on $M(\gamma)$ and $b(\gamma)$:
\begin{align}
M(\gamma) & \leq c n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right) \label{eqn:maximum_path_length}\\
b(\gamma) & \leq c \left(n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right)\right)^{2d}, \label{eqn:bottleneck}
\end{align}
which we now prove. 

\paragraph{Proof of~\eqref{eqn:maximum_path_length}:}

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \pi^{-1}(E)$. Then
\begin{align*}
\gamma(\overline{x}_k, \overline{x}_{\ell})) = \norm{\overline{x}_k - \overline{x}_{\ell}}_1 & \leq \sqrt{d} \norm{\overline{x}_k - \overline{x}_{\ell}}_2 \\
& \leq \sqrt{d}\bigl(\norm{\overline{x}_k - \pi(\ol{x}_{k})}_2 + \norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + \norm{\overline{x}_\ell - \pi(\ol{x}_{\ell})}_2\bigr) \\
& \leq \sqrt{d}\left(\norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{by \eqref{eqn:transport_distance}} \\
& \leq \sqrt{d}\left(r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{since $(\pi(\ol{x}_k),\pi(\ol{x}_{\ell})) \in E$}
\end{align*} 

\paragraph{Proof of~\eqref{eqn:bottleneck}:} 

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)$ for some $e = (\overline{x}_i, \overline{x}_j) \in \pi^{-1}(E)$.  Then, by the construction of $\gamma$,
\begin{equation}
\label{eqn:bottleneck_pf_1}
\norm{\overline{x}_k - \overline{x}_i}_2 \wedge \norm{\overline{x}_\ell - \overline{x}_j}_2  \leq \norm{\overline{x}_j - \overline{x}_i}_2
\end{equation}
As shown in the preceding paragraph, assuming~\eqref{eqn:transport_distance} we have
\begin{equation*}
\norm{\overline{x}_j - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}
\end{equation*}
and therefore by~\eqref{eqn:bottleneck_pf_1},
\begin{align*}
\abs{\set{e \in E: (\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)}} & \leq \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \cdot \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \\
& \leq \left(n^{1/d}\left( r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right)\right)^2.
\end{align*}

Putting the pieces together, we have that when $d = 1$ and $r = (\log n)^a (\log n/n)$, then
\begin{equation*}
G_{n,r} \preceq c \cdot (\log n)^{(1/d + a) + 2 + 2da}\cdot \overline{G}
\end{equation*}
with probability at least $1 - o(1)$.

\subsection{Proof of Lemma~\ref{lem:pi_max_pf_1}}

Write $\overline{L} = \overline{V}\overline{S} \overline{V}^T$ for the spectral decomposition of $\overline{L}$. The proof of Lemma~\ref{lem:pi_max_pf_1} will proceed according to the following steps.
\begin{enumerate}
	\item We show that the incoherence $\max_{i} v_{k,i}^2$ can be upper bounded
	\begin{equation}
	\label{eqn:incoherence_proof_1}
	\max_{i} v_{k,i}^2 \leq \frac{1}{n} \left(\sum_{j = 1}^{n} \abs{\dotp{v_k}{\overline{v}_j}}\right)^2
	\end{equation}
	replacing the norm $\norm{v_k^2}_{\infty}$ by the inner products $\dotp{v_k}{\overline{v}_j}$.
	\item Let $I(k,r) = \set{j \geq 0: \abs{j - k} \leq r}$. Using Davis-Kahan, we establish that the following upper bounds
	\begin{equation}
	\label{eqn:incoherence_proof_2}
	\sum_{j \not\in I(k,r)}^{n} (\dotp{v_k}{\overline{v}_j})^2 \leq 400 \frac{n^4 \delta^2 s_{\max}^2}{R^4}
	\end{equation}
	hold for each $k \leq n/8$ and any $r \geq 8k\sqrt{\delta}$. 
	\item We carefully upper bound the $L_1$ norm present in \eqref{eqn:incoherence_proof_1} given the various bounds on $L_2$ norm we've established in \eqref{eqn:incoherence_proof_2}.
\end{enumerate}

\paragraph{Step 1: Proof of~\eqref{eqn:incoherence_proof_1}.}

We can re-express $v_k$ in the basis $(\overline{v}_j)$ as
\begin{equation*}
v_k = \sum_{j = 1}^{n} \dotp{v_k}{\overline{v}_j} \overline{v}_j,
\end{equation*}
whereupon the bound~\eqref{eqn:incoherence_proof_1} follows from the incoherence property of the chain
\begin{equation*}
\max_{i,k = 1,\ldots,n} \overline{v}_{k,i}^2 \leq \frac{1}{n}.
\end{equation*}
\paragraph{Step 2: Proof of~\eqref{eqn:incoherence_proof_2}.}

Let $\widetilde{L} = L + H$. By Davis Kahan, we have that
\begin{equation}
\label{eqn:davis_kahan}
\sum_{j \not\in I(k,r)} (\dotp{v_k}{\overline{v}_j})^2 \leq \left(\frac{\norm{H}_{\textrm{op}}}{\min{\abs{\overline{s}_j - s_k}:j \in I(k,r)}}\right)^2
\end{equation}
To upper bound the numerator, we use~\eqref{eqn:spectral_similarity} to get
\begin{equation*}
\norm{H}_{op} \leq \delta s_{n}.
\end{equation*}

To lower bound the denominator, we note
\begin{align}
\abs{\overline{s}_j - s_k} & \geq \abs{\overline{s}_j - \overline{s}_k} - \abs{\overline{s}_k - s_k} \nonumber \\
& \geq \abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k. \label{eqn:incoherence_proof_8}
\end{align}
The eigenvalues of the chain graph are well known to be
\begin{equation*}
\overline{s}_j = 2\left(1 - \cos\left(\frac{j\pi}{n}\right)\right) = 4\sin^2\left(\frac{k\pi}{n}\right) ~~\textrm{for $k = 0,\ldots,n - 1$.}
\end{equation*}
By Taylor expansion, for $k \leq j \leq n/(2\pi)$ we have
\begin{equation}
\label{eqn:incoherence_proof_5}
\overline{s}_j - \overline{s}_k = 2\left(\cos\left(\frac{k\pi}{n}\right) - \cos\left(\frac{j\pi}{n}\right)\right) \geq \frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
and when $j \geq n/(2\pi)$ we have $\overline{s}_j - \overline{s}_k \geq 2(\cos\left(\frac{\pi}{8}\right) - \cos\left(\frac{1}{2}\right) > .09$. Additionally since $\sin(x) \leq x$ for all $x \geq 0$, we have
\begin{equation}
\label{eqn:incoherence_proof_6}
\overline{s}_k \leq 4\frac{k^2\pi^2}{n^2}.
\end{equation}
Combining~\eqref{eqn:incoherence_proof_5},\eqref{eqn:incoherence_proof_6}, and the lower bound $\abs{k - j} \geq R \geq 8k\sqrt{\delta}$, we have that
\begin{equation}
\label{eqn:incoherence_proof_7}
\abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k \geq .04\frac{(k - j)^2\pi^2}{n^2} - 4 \delta\frac{k^2\pi^2}{n^2} \geq .02\frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
The result follows from \eqref{eqn:incoherence_proof_7}, \eqref{eqn:incoherence_proof_8} and \eqref{eqn:davis_kahan}.

\paragraph{Step 3: $L_1$ norm to $L_2$ norm.}

Let $u \in \Reals^n$, and suppose we know that the $L_2$ norm of $u$ is bounded,
\begin{equation}
\label{eqn:incoherence_proof_3}
\norm{u}_2^2 \leq 1.
\end{equation}
Under no other conditions on $u$, the upper bound $\norm{u}_1 \leq \sqrt{n}$ is the best that can be hoped for (achieved when $u = (n^{-1/2},\ldots,n^{-1/2})$). However, suppose we also know that for some $B_2^2 \geq B_3^2 \geq \ldots \geq B_n^2$, we have that there exists some $R \in [n]$ such that
\begin{equation}
\label{eqn:incoherence_proof_4}
\sum_{j = r}^{n} v_j^2 \leq B_r^2 ~~\textrm{for each $r = R,\ldots,n$.}
\end{equation}
Clearly, if $B_n^2 \leq \frac{1}{n}$ then $u$ cannot be equal to $(n^{-1/2},\ldots,n^{-1/2})$. We might hope for more general improvements on the bound $\norm{u}_1 \leq \sqrt{n}$ if $B_R^2$ is quite small once $R$ gets sufficiently large. The following Lemma gives such a result.
\begin{lemma}
	\label{lem:pi_max_pf_1_util_1}
	Suppose $u \in \Reals^n$ satisfies \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} for some $1 = B_1^2 = B_2^2 \geq \ldots \geq B_n^2 \geq B_{n+1}^2 = 0$. Assume additionally that there exists an $R \in [n]$ such that
	\begin{equation}
	\label{eqn:incoherence_util_1}
	B_r^2 - B_{r + 1}^2 \leq \frac{1}{r}~\textrm{for each $r = R, R+1,\ldots, n - 1$.}
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:incoherence_util_2}
	\norm{u}_1 \leq \sqrt{R}\sqrt{1 - B_R^2} + \sum_{j = R}^{n} \sqrt{B_j^2 - B_{j+1}^2}.
	\end{equation}
\end{lemma}
\begin{proof}
	Set
	\begin{equation*}
	(u_\star)_j^2 = 
	\begin{cases*}
	\frac{1}{R}(1 - B_R^2), ~ j \leq R \\
	B_j^2 - B_{j + 1}^2, j > R
	\end{cases*}
	\end{equation*}
	so that $\norm{u_\star}_1$ is equal to the right hand side of~\eqref{eqn:incoherence_util_2}. We now prove by contradiction that $u_\star$ maximizes $\norm{\cdot}_1$ under the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4}. Suppose this were not true, and that the maximum is achieved for some $u \neq u_{\star}$.
	\begin{itemize}
		\item since $\norm{u}_1 > \norm{u_{\star}}$ then there exists some index $j$ such that
		\begin{equation}
		\label{eqn:incoherence_util_3}
		u_j > (u_{\star})_j
		\end{equation} 
		\item by~\eqref{eqn:incoherence_proof_3} $\norm{u}_2 \leq 1$. Since $\norm{u_{\star}}_2 = 1$, by \eqref{eqn:incoherence_util_3} there must exist some index $k$ such that
		\begin{equation}
		\label{eqn:incoherence_util_4}
		(u_{\star})_k > u_k.
		\end{equation} 
		Choose $k$ to be the largest of all such indices.
		\item suppose $k < j$ and $j > R$. Since $k$ was chosen to be the largest such index which satisfied~\eqref{eqn:incoherence_util_4}, clearly $v_i \geq (u_{\star})_i$ for all $i > j$, and by \eqref{eqn:incoherence_util_3} $u_j > (u_{\star})_j$. But then 
		\begin{equation*}
		B_j^2 = \sum_{i = j}^{N} (u_{\star})_i^2 < \sum_{i = j}^{N} u_i^2 
		\end{equation*}
		and so $v$ does not satisfy~\eqref{eqn:incoherence_proof_4}. Therefore either $k > j$ or $j \leq R$.
		\item In either case, we have
		\begin{equation*}
		u_j > (u_{\star})_j \geq (u_{\star})_k > u_k.
		\end{equation*}
		Let $j \leq l < k$ be the largest index for which $v_j > (a_{\star})_j$. 
		
		We now construct a vector $\wt{u}$ which satisfies the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} such that $\norm{\wt{u}}_1 > \norm{u}_1$. Once we have shown this, we will have established a contradiction, and the proof of Lemma~\ref{lem:pi_max_pf_1_util_1} will be complete. Let $\wt{u} = (\wt{u}_i)$ be given as follows:
		\begin{equation*}
		\wt{u}_i = 
		\begin{cases*}
		u_i, ~~\textrm{if $i \neq k,l$}, \\
		u_{\star,k}, ~~\textrm{if $i = k$}, \\
		\sqrt{u_l^2 - (u_{\star,k}^2 - a_k^2)}, ~~\textrm{if $i = l$.}
		\end{cases*}
		\end{equation*}
		Clearly $\norm{\wt{u}}_2 = \norm{u}_2 = 1$ and so $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_3}. To see that $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4} we separate the analysis into cases. When $r \geq l + 1$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		When $l < r \leq k$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq \sum_{i = r}^{k} u_{\star,i}^2 + \sum_{i = k +1}^{n} u_{i}^2 \leq B_r^2 - B_{k + 1}^2 + B_{k + 1}^2 = B_r^2
		\end{equation*}
		Finally, when $R \leq r \leq l$, we have
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq u_l^2 - (u_{\star,k}^2 - u_k^2) + (u_{\star,k}^2) + \sum_{i \neq k,l} u_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		Therefore $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4}. Finally, we have that
		\begin{equation*}
		\norm{\wt{u}}_1 = \norm{u}_1 + (u_{\star,k}  - u_k) - (\sqrt{u_l^2 - (u_{\star,k}^2 - u_k^2)} - u_l) > \norm{u}_1 + (u_{\star,k}  - u_k) - (u_{\star,k}  - u_k) = \norm{u}_1,
		\end{equation*}
		so we have established the desired contradiction.
	\end{itemize}
\end{proof}

\paragraph{Putting the pieces together.}
We apply Lemma~\ref{lem:pi_max_pf_1_util_1} to the vector $u = (u_i)_{i = 1}^{n}$, where
\begin{equation*}
u_i = \frac{1}{\sqrt{2}}\left(\abs{\dotp{v_k}{\overline{v}_{k - i}}} + \abs{\dotp{v_k}{\overline{v}_{k + i}}}\right).
\end{equation*}
Note the following:
\begin{itemize}
	\item $\norm{u}_2 \leq 1$.
	\item By \eqref{eqn:incoherence_proof_2}, the vector $u$ satisfies the constraint~\eqref{eqn:incoherence_proof_4} with
	\begin{equation*}
	B_r^2 = 400\frac{n^4\delta^2s_{n}^2}{r^4} ~~\textrm{when $r \geq 8 k \sqrt{\delta}$.}
	\end{equation*}
	\item Taylor expanding $f(r + 1) = (r + 1)^{-4}$ around $r$, we have that
	\begin{equation*}
	B_r^2 - B_{r + 1}^2 \leq 3200 \frac{n^4 \delta^2 s_{n}^2}{r^5} ~~\textrm{when $r \geq 2$.}
	\end{equation*}
	Therefore, the sequence $(B_r)$ satisfies \eqref{eqn:incoherence_util_1} for all $r$ large enough such that 
	\begin{equation*}
	r \geq 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}
	\end{equation*}
	\item Since $u$ and $(B_r)$ satisfy the constraints \eqref{eqn:incoherence_proof_3}, \eqref{eqn:incoherence_proof_4} and \eqref{eqn:incoherence_proof_5}, we may apply Lemma~\ref{lem:pi_max_pf_1_util_1} to $u$, obtaining that
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{v_j}{\overline{v}_k}} = \sqrt{2}\norm{u}_1 \leq \sqrt{2R} + 60\sqrt{2} n^2 \delta s_{n} \sum_{r = R}^{n} r^{-5/2}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}$. Bounding sum by integral, we have that if $R \geq 2$ then
	\begin{equation*}
	\sum_{r = R}^{n} r^{-5/2} \leq \int_{R - 1}^{n - 1} x^{-5/2} \,dx \leq \frac{2^{3/2}2}{3} R^{-3/2}.
	\end{equation*}
	Plugging this into the previous expression, we arrive at
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{\overline{v}_j}{v_k}} \leq \sqrt{R} + 60 \sqrt{2} n^2 \delta s_{\max} \frac{2^{3/2}2}{3R^{3/2}}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{\max}}$. Lemma~\ref{lem:pi_max_pf_1} then follows from~\eqref{eqn:incoherence_proof_1}.
\end{itemize}

\section{Auxiliary Results}

\subsection{Results of Others}

Let $\overline{X}$ denote the $n$ evenly spaced grid points on $[0,1]^d$; formally, letting $t = n^{1/d}$
\begin{equation*}
\overline{X} = \biggl\{\frac{1}{t}(k_1,\ldots,k_d): k \in [t]^d\biggr\}
\end{equation*}

\begin{theorem}[Theorem 1 of \textcolor{red}{Garcia-Trillos and Slepcev}]
	\label{thm:slepcev_transport_distance}
	With probability at least $1 - c/n$ there exists a bijection $\pi: \overline{X} \to X$ such that
	\begin{equation}
	\label{eqn:slepcev_transport_distance}
	\max_{k \in [t]^d} \abs{\overline{x}_k - \pi(\overline{x}_k)} \leq c \left(\frac{\log n}{n}\right)^{1/d}
	\end{equation}
\end{theorem}

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.4, Theorem 1]
	\label{thm:evans_extension}
	Assume $U$ is bounded and $\partial U$ is $C^1$. Select a bounded open set $V$ such that $U \subset \subset V$ ($U$ is compactly contained in $V$). Then there exists a bounded linear operator $E: H^1(U) \to H^1(\Rd)$ such that for each $u \in H^1(U)$:
	\begin{enumerate}
		\item $Eu = u$ a.e. in $U$,
		\item $Eu$ has support within $V$, and 
		\item 
		\begin{equation*}
		\norm{Eu}_{H^1(\Rd)} \leq C \norm{u}_{H^1(\Rd)}
		\end{equation*}
		the constant $C$ depending only on $U$ and $V$.
	\end{enumerate}
\end{theorem}

\begin{lemma}[Poincare inequality for path graphs.]
	\label{lem: path_poincare}
	Fix $m \geq 0$. For vertices $V = \set{1, \ldots,m}$ define the path $P(1 \to m) = ((1,2),(2,3),\ldots, (m-1,m))$ and $G_{(1,m)}$ to be the graph consisting only of an edge between $1$ and $m$. Then,
	\begin{equation*}
	(m - 1) \cdot P(1 \to m) \succeq G_{(1,m)}
	\end{equation*}
\end{lemma}

\subsection{Integrals}

For Lemmas~\ref{lem:expected_first_order_seminorm} - \ref{lem:expected_first_order_seminorm_3}, we will assume that $K$ is a kernel function compactly supported on $B(0,1)$ and upper bound $K(x) \leq K_{\max}$. Additionally, we note that although each Lemma assumes $g \in C^1(\Xset)$, in each case if we assume merely $g \in H^1(\Xset)$ the results still hold due to the density of $C^{\infty}$ in $H^1(\Xset)$.

\begin{lemma}
	\label{lem:expected_first_order_seminorm}
	Suppose $g \in C^{1}(\Xset)$ for $\Xset$ a Lipschitz domain and that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Xset$. Then
	\begin{equation*}
	\Ebb\Bigl[(g(x_j) - g(x_i))^2K_r(x_i,x_j)\Bigr] \leq c K_{\max} p_{\max}^2 r^2 [g]_{H^1(\Xset)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	Since $\Xset$ is a Lipschitz domain, we may take $h \in C^1(\Rd)$ to be an extension of $g$ such that $h = g$ a.e. on $\Xset$, and additionally
	\begin{equation*}
	[h]_{H^1(\Rd)} \leq c[g]_{H^1(\Xset)}
	\end{equation*}
	Since $h = g$ a.e on $\Xset$, the expectations satisfy
	\begin{equation*}
	\Ebb\Bigl[(g(x_j) - g(x_i))^2K_r(x_i,x_j)\Bigr] = \Ebb\Bigl[(h(x_j) - h(x_i))^2K_r(x_i,x_j)\Bigr].
	\end{equation*}
	By the fundamental theorem of calculus we have for any $y,x \in \Rd$,
	\begin{equation}
	\label{eqn:expected_first_order_seminorm_pf1}
	h(y) - h(x) = \int_{0}^{1} \frac{d}{dt}\bigl[h(x + t(y - x))\bigr] \,dt = \int_{0}^{1} \dotp{\nabla(h(x + t(y - x)))}{y - x} \,dt
	\end{equation}
	where the integral is well-defined as $\nabla h(z)$ exists almost everywhere since $h \in C^1(\Rd)$. We now perform some standard calculus:
	\begin{align*}
	\Ebb[(h(x_j) - h(x_i))^2K_r(x_i,x_j)] & \leq p_{\max}^2 \int_{\Rd} \int_{\Rd} (h(y) - h(x))^2 K_r(y,x) \,dy \,dx\\
	& = p_{\max}^2 \int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \dotp{\nabla(h(x + t(y - x)))}{y - x} \,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(i)}{\leq} p_{\max}^2 \int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \norm{\nabla(h(x + t(y - x)))}\norm{y - x} \,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(ii)}{\leq} p_{\max}^2 r^2\int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \norm{\nabla(h(x + t(y - x)))}\,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(iii)}{\leq} p_{\max}^2 r^2\int_{\Rd} \int_{\Rd} \int_{0}^{1} \norm{\nabla(h(x + t(y - x)))}^2 \,dt K_r(y,x) \,dy \,dx \\
	& \overset{(iv)}{\leq} p_{\max}^2 K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(x,r)} \norm{\nabla(h(x + t(y - x)))}^2 \,dy \,dt \,dx \\
	& \overset{(v)}{\leq}  p_{\max}^2 K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(0,r)} \norm{\nabla(h(x + z))}^2  \,dz \,dt \,dx
	\end{align*}
	where $(i)$ follows by Cauchy-Schwarz, $(ii)$ follows since either $\norm{y - x} \leq r$ or $K_r(y,x) = 0$, $(iii)$ follows by Jensen's, $(iv)$ follows by the assumption $K \leq K_{\max}$ supported on $B(0,1)$, and $(v)$ follows from the change of variables $z = x + t(y - x)$. Finally, again using Fubini's Theorem, we have
	\begin{align*}
	K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(0,r)} \norm{\nabla(h(x + z))}^2  \,dz \,dt \,dx & = r^{2 - d}\int_{B(0,r)} \int_{0}^{1} \int_{\Rd} \norm{\nabla(h(x + z))}^2  \,dz \,dt \,dx \\
	& = K_{\max} r^2 [h]_{W_d^{1,2}(\Rd)}.
	\end{align*}
\end{proof}

\begin{lemma}
	\label{lem:expected_first_order_seminorm_2}
	Suppose $f \in C^{1}(\Xset)$ for $\Xset$ a Lipschitz domain and that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Xset$. Then
	\begin{equation*}
	\Ebb\Bigl[\abs{D_if(x_h)}\cdot\abs{D_if(x_j)} \Bigr] \leq c K_{\max}^3 p_{\max}^3 r^2 [f]_{H^1(\Xset)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	Since $\Xset$ is a Lipschitz domain, we may take $g \in C^1(\Rd)$ to be an extension of $f$ such that $g = f$ a.e. on $\Xset$, and additionally
	\begin{equation*}
	[g]_{H^1(\Rd)} \leq c[f]_{H^1(\Xset)}
	\end{equation*}
	Since $g = f$ a.e on $\Xset$, the expectations satisfy
	\begin{equation*}
	\Ebb\Bigl[\abs{D_if(x_h)}\cdot\abs{D_if(x_j)} \Bigr] =\Ebb\Bigl[\abs{D_ig(x_h)}\cdot\abs{D_ig(x_j)} \Bigr]
	\end{equation*}
	
	We rewrite $\Ebb\bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \bigr]$ as follows,
	\begin{align*}
	\Ebb\Bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \Bigr] & = \int \int \int \abs{g(z) - g(x)} \cdot \abs{g(z) - g(y)} K_r(z,y) K_r(z,x) \,dP(x) \,dP(y) \,dP(x) \\
	& = \int \left[\int \abs{g(z) - g(x)} K_r(z,x) \,dP(x)\right]^2 \,dP(z) \\
	& \leq p_{\max}^3 \int_{\Xset} \left[\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx\right]^2 \,dz
	\end{align*}
	Applying~\eqref{eqn:expected_first_order_seminorm_pf1} inside the integral gives
	\begin{align*}
	\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx & \leq \int_{\Rd} \abs{g(z) - g(x)} K_r(z,x) \,dx \\
	& = \int_{\Rd} \abs{\int_{0}^{1} \dotp{\nabla g(x + t(z - x))}{z - x} \,dt} K_r(z,x) \,dx \\
	& \leq \int_{\Rd} \int_{0}^{1} \norm{\nabla g(x + t(z - x))}\cdot\norm{z - x} \,dt K_r(z,x) \,dx \\
	& \leq r \int_{\Rd} \int_{0}^{1} \norm{\nabla g(x + t(z - x))} \,dt K_r(z,x) \,dx \\
	& \leq r \frac{K_{\max}}{r^d} \int_{B(z,r)} \int_{0}^{1} \norm{\nabla g(x + t(z - x))} \,dt  \,dx \\
	& \leq r K_{\max} \int_{B(0,1)} \int_{0}^{1} \norm{\nabla g(x - try)} \,dt  \,dy,
	\end{align*}
	and as a result, 
	\begin{equation*}
	p_{\max}^3 \int_{\Xset} \left[\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx\right]^2 \,dz \leq c\cdot p_{\max}^3 r^2 K_{\max}^3 [f]_{W_d^{1,2}(\Rd)}^2.
	\end{equation*}
\end{proof}

\begin{lemma}
	\label{lem:expected_first_order_seminorm_3}
	Suppose $f \in C^{1}(\Xset)$ and that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Xset$. Then for any distinct $i,j,k,\ell$ each in $[n]$,
	\begin{equation*}
	\Ebb\Bigl[\abs{D_{k}f(x_i)}\cdot{\abs{D_{\ell}f(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \leq c K_{\max} p_{\max}^2 r^{2 + d} [f]_{H^1(\Rd)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	Since $\Xset$ is a Lipschitz domain, we may take $g \in C^1(\Rd)$ to be an extension of $f$ such that $g = f$ a.e. on $\Xset$, and additionally
	\begin{equation*}
	[g]_{H^1(\Rd)} \leq c[f]_{H^1(\Xset)}
	\end{equation*}
	Since $g = f$ a.e on $\Xset$, the expectations satisfy
	\begin{equation*}
	\Ebb\Bigl[\abs{D_{k}f(x_i)}\cdot{\abs{D_{\ell}f(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] = 	\Ebb\Bigl[\abs{D_{k}g(x_i)}\cdot{\abs{D_{\ell}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr].
	\end{equation*}
	We rewrite the expectation as an integral,
	\begin{align*}
	\Ebb\Bigl[& \abs{D_{k}g(x_i)}\cdot{\abs{D_{\ell}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \\
	& \leq p_{\max}^4 \int_{\Xset^4} \abs{g(x) - g(y)} \cdot \abs{g(u) - g(v)} \cdot  K_r(x,y) K_r(u,v) \1\{\norm{y - v} \leq (2q + 1)r\} \,dy \,dx \,du \,dv
	\end{align*}
	By substituting $z_1 = (y - v)/r$, $z_2 = (u - v)/r$, and $z_3 = (x - y)/r = (x - v)/r + z_1$, we can simplify the integral in the previous display,
	\begin{align*}
	\int_{\Xset^4} & \abs{g(x) - g(y)} \cdot \abs{g(u) - g(v)} \cdot K_r(x,y) K_r(u,v) \1\{\norm{y - v} \leq (2q + 1)r\} \,dy \,dx \,du \,dv \\
	& \leq K_{\max}^2 r^d \int_{\Xset} \int_{[B(0,1)]^3} \abs{g\bigl((z_3 + z_1)r + v\bigr) - g(z_1r + v)} \cdot \bigl|g(z_2r + v) - g(v)\bigr| \,dz_1 \,dz_2 \,dz_3 \,dv \\
	& \leq  K_{\max}^2 r^{d + 2} \int_{[B(0,1)]^3} \int_{[0,1]^2} \int_{\Xset} \norm{\nabla g(t z_3 r + z_1r + v)} \cdot \norm{\nabla g(t z_2 r + v)} \,dv \,dt_1 \,dt_2 \,dz_1 \,dz_2 \,dz_3 \\
	& \leq c \nu_d^3 K_{\max}^2 r^{d + 2} [f]_{W_{d}^{1,2}(\Xset)}^2.
	\end{align*}
\end{proof}


\begin{lemma}
	\label{lem:remainder_term}
	Suppose that $f \in \Leb^2(U)$ for some open domain $U$, and that $h:U \times U \times [0,1] \to \Reals$ is uniformly bounded. Then, the function $g(x) = \int_{0}^{1} \int_{B(0,1)} f(x + aty) h(y,x, t) \,dy \,dt$ also belongs to $\Leb^2(U)$, with norm
	\begin{equation*}
	\norm{g}_{\Leb^2(U)} \leq \nu_d \cdot \norm{f}_{\Leb^2(U)} \cdot \norm{h}_{\infty}
	\end{equation*}
\end{lemma}
\begin{proof}
	We compute the squared norm of $g$,
	\begin{align*}
	\norm{g}_{\Leb^2(\Rd)}^2 & = \int_{U} \left(\int_{0}^{1} \int_{B(0,1)} f(x + aty) h(y,x,t) \,dt \,dy \right)^2 \,dx \\
	& \leq \norm{h}_{\infty}^2 \int_{U} \left(\int_{0}^{1} \int_{B(0,1)} f(x + aty) \,dt \,dy \right)^2 \,dx \\
	& \leq \nu_d^2 \norm{h}_{\infty}^2 \int_{U} \int_{0}^{1} \frac{1}{\nu_d}\int_{B(0,1)} f^2(x + aty) \,dt \,dy \,dx \tag{Jensen's inequality} \\
	& = \nu_d^2 \norm{h}_{\infty}^2 \int_{0}^{1} \int_{B(0,1)} \frac{1}{\nu_d}\int_{U}f^2(x + aty) \,dt \,dy \,dx \tag{Fubini's theorem} \\
	& = \nu_d^2 \norm{h}_{\infty}^2 \norm{f}_{\Leb^2(U)}^2.
	\end{align*}
\end{proof}

Lemma~\ref{lem:leading_term_sobolev_compact} supplies an equivalent result when $f \in H_0^{s}(\Xset)$. In this Lemma, we write $\Ebb[D_kf]:\Xset \to \Reals$ for the mapping $x \mapsto \Ebb[D_kf(x)]$. We will also denote $U_r = \Xset \setminus \Xset_{r}$.
\begin{lemma}
	\label{lem:leading_term_sobolev_compact}
	Fix integers $s \geq 0$ and $q \geq 1$, and an index vector $k \in (n)^q$. Suppose that $p \in C^0(\Xset;p_{\max})$, and additionally that $p \in C^{s-1}(\Xset;p_{\max})$ if $s \geq 2$. Then there exists an $r' > 0$ such that for all $0 < r < r'$, the following statements hold for all $f \in H_0^{s}(\Xset)$:
	\begin{itemize}
		\item The expected difference operator $\Ebb[D_kf]$ belongs to $\Leb^2(U_{qr})$, with norm
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_1}
		\Bigl\|\Ebb\bigl[D_kf\bigr]\Bigr\|_{\Leb^2(U_{qr})} \leq c r^s \norm{f}_{H^s(\Xset)}
		\end{equation}
		\item If $2q \geq s$, then additionally $\Ebb[D_kf]$ belongs to $\Leb^2(\Xset_{qr})$, with norm
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_2}
		\Bigl\|\Ebb\bigl[D_kf\bigr]\Bigr\|_{\Leb^2(\Xset_{qr})} \leq c r^s \norm{f}_{H^s(\Xset)}.
		\end{equation}
		Otherwise there exist functions $f_{\ell} \in H_0^{s - \ell}(\Xset)$, which additionally satisfy
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_3}
		\norm{f_{\ell}}_{H^{s - \ell}(\Xset)} \leq c \norm{f}_{H^s(\Xset)}
		\end{equation}
		for each $\ell = 2q,\ldots,s-1$, such that
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_4}
		\Bigl\|\Ebb\bigl[D_kf\bigr] - \sum_{\ell = 2q}^{s - 1} r^{\ell} f_{\ell}\Bigr\|_{\Leb^2(\Xset_{qr})} \leq c r^s \norm{f}_{H^s(\Xset)}
		\end{equation}
	\end{itemize}
\end{lemma}
\begin{proof}
	One can interpret the conclusions of Lemma~\ref{lem:leading_term_sobolev_compact} as demonstrating that expected difference operators behave similarly to derivatives over $H_0^{s}(\Xset)$. The proof of Lemma~\ref{lem:leading_term_sobolev_compact} is therefore naturally centered on taking Taylor expansions, but in order to do this, we must relate $f$ to a function $g$ which has classical derivatives. 
	
	Since $f \in H_0^s(\Xset)$, there exists a sequence $(f_m) \subset C_c^s(\Xset)$ such that $\norm{f_m - f}_{H^s(\Xset)} \to 0$ as $m \to \infty$ (Indeed $f_m$ will be smooth for each $m$, but we will not need that fact.) Picking $m$ large enough so that
	\begin{equation*}
	\norm{f_m - f}_{H^s(\Xset)} \leq r^s \norm{f}_{H^s(\Xset)}
	\end{equation*}
	we have that for any $\wt{f} \in \Leb^2(\Xset)$,
	\begin{align*}
	\norm{\Ebb\Bigl[D_kf\Bigr] - \wt{f}}_{\Leb^2(\Xset)} & \leq \norm{\Ebb\Bigl[D_kf_m\Bigr] - \wt{f}}_{\Leb^2(\Xset)} + \norm{\Ebb\Bigl[D_k(f - f_m)\Bigr]}_{\Leb^2(\Xset)} \\
	& \leq \norm{\Ebb\Bigl[D_kf_m\Bigr] - \wt{f}}_{\Leb^2(\Xset)} + c \norm{f - f_m}_{\Leb^2(\Xset)} \\
	& \leq \norm{\Ebb\Bigl[D_kf_m\Bigr] - \wt{f}}_{\Leb^2(\Xset)} + c r^s \norm{f}_{H^s(\Xset)}
	\end{align*}
	Since $f_m \in C_c^s(\Xset)$, it can be continuously extended to $g: \Rd \to \Reals,~ g \in C_c^s(\Xset)$ by taking $g(x) = 0$ for all $x \in \Rd \setminus \Xset$, such that $\norm{g}_{H^s(\Rd)} = \norm{f_m}_{H^s(\Xset)}$ and $g = f$ everywhere on $\Xset$. Therefore $\Ebb[D_kg] =\Ebb[D_kf_m]$, and it suffices to prove the estimates~\eqref{eqn:leading_term_sobolev_compact_1}-\eqref{eqn:leading_term_sobolev_compact_4} hold with respect to $g$. 
	
	Before we do so, let us establish some notation. When $s \geq 1$, since $g \in C_c^s(\Rd)$ it admits a Taylor expansion of the form
	\begin{equation*}
	g(y) =  \sum_{\abs{\alpha} = 0}^{s - 1} g^{(\alpha)}(x) (z - x)^{\alpha} + \sum_{\abs{\alpha} = s} (y - x)^{\alpha} G_{\alpha}(x,y),
	\end{equation*}
	for any $y,x \in \Rd$. When $s \geq 2$, since $p \in C_c^{s - 1}(\Xset)$ it also admits a Taylor expansion,
	\begin{equation*}
	p(y) = \sum_{\abs{\beta} = 0}^{s - 2} p^{\beta}(x) (y - x)^{\beta} + \sum_{\abs{\beta} = s - 1} (x - y)^s P_{\beta}(x,y).
	\end{equation*}
	for any $y,x \in \Xset$.
	In both cases we use the integral form of the remainders:
	\begin{align*}
	G_{\alpha}(x,y) & = \int_{0}^{1} g^{(\alpha)}\bigl(x + t(y - x)\bigr)(1 - t)^{\abs{\alpha}} \,dt \\
	P_{\beta}(x,y) & = \int_{0}^{1} p^{(\beta)}\bigl(x + t(y - x)\big) (1 - t)^{\abs{\beta}}  \,dt 
	\end{align*}
	where by Rademacher's Theorem $g^{(\alpha)}$ and $p^{(\beta)}$ exist almost everywhere, and the preceding integrals are therefore well defined.
	
	It will also be helpful to introduce some notation. For $G: \Xset \times \Xset \to \Reals$, let
	\begin{align*}
	\Bigl(\Ebb_{\alpha}[G]\Bigr)(x) & := \int_{\Xset} (y - x)^{\alpha} G(x,y) K_r(y,x) p(y) \,dy,~~ && \Ebb_{\alpha}(x) := \Bigl(\Ebb_{\alpha}[1]\Bigr)(x) \\
	\Bigl(\Ibb_{\alpha}[G]\Bigr)(x) & := \int_{\Xset} (y - x)^{\alpha} G(x,y) K_r(y,x) \,dy,~~ && \Ibb_{\alpha}(x)  := \Bigl(\Ibb_{\alpha}[1]\Bigr)(x)
	\end{align*}
	Additionally we define
	\begin{equation*}
	I_{\alpha} := \int z^{\alpha} K\bigl(\norm{z}\bigr) \,dz.
	\end{equation*}
	and note that when $B(x,r) \subset \Xset$, the following two facts are true: first, that $\Ibb_{\alpha,x} = r^{\abs{\alpha}} I_{\alpha}$, and second that $I_{\alpha} = 0$ when $\abs{\alpha} = 1$.
	
	We begin with $s = 0$. When $q = 1$, we have
	\begin{align*}
	\norm{\Ebb\bigl[D_kg\bigr]}_{\Leb^2(\Xset)} & = \int_{\Xset} \biggl[\int_{\Xset} \Bigl(g(y) - g(x)\Bigr)K_r(y,x) p(y) \,dy \biggr]^2 \,dx \\
	& \leq p_{\max}^2 \int_{\Rd} \biggl[\int_{\Rd} \Bigl(\abs{g(y)} + \abs{g(x)}\Bigr)K_r(y,x) \,dy \biggr]^2 \,dx \\
	& \leq K_{\max}^2 p_{\max}^2 \int_{\Rd} \biggl[\int_{B(0,1)} \abs{g(zr + x)} + \abs{g(x)} \,dz \biggr]^2 \,dx
	\end{align*}
	and the statement follows by Lemma~\ref{lem:remainder_term}. The same result holds (up to different constants) for $s = 0$ and general $q$ by induction.
	
	For $s \geq 1$, we first show the desired estimate over $U_{qr}$.
	
	\subsubsection{Boundary region}
	
	Take $q = 1$, and $k \in [n]$. We begin by relating the $\Leb^2$ norm of $\Ebb[D_kg]$ over $U_r$ to the $\Leb^2$ norm of $g$ over $U_{2r}$, as follows:
	\begin{align*}
	\Bigl\|\Ebb\bigl[D_kg\bigr]\Bigr\|_{\Leb^2(U_r)}^2 & = \int_{U_r} \biggl[\int_{\Xset} \bigl(g(y) - g(x)\bigr)K_r(x,y) p(y) \,dy \biggr]^2 \,dx \\
	& \leq p_{\max}^2 \int_{U_r} \biggl[\int_{\Xset} \bigl(\abs{g(y)} +  \abs{g(x)}\bigr)K_r(x,y) \,dy \biggr]^2 \,dx \\
	& \overset{(i)}{\leq}  p_{\max}^2 K_{\max}^2 \int_{U_r} \biggl[\int_{B(0,1) \cap (\Xset - x)/r} \bigl(\abs{g(zr + x)} +  \abs{g(x)}\bigr)\,dz \biggr]^2 \,dx \\
	& \overset{(ii)}{\leq} 2 p_{\max}^2 K_{\max}^2 \int_{U_r} \nu_d^2 \bigl(g(x)\bigr)^2 \,dx + 2 p_{\max}^2 K_{\max}^2 \nu_d \int_{U_r}\biggl[\int_{B(0,1) \cap (\Xset - x)/r} \bigl(g(zr + x)\bigr)^2\,dz\biggr] \,dx \\
	& \leq 2 p_{\max}^2 K_{\max}^2 \int_{U_r} \nu_d^2 \bigl(g(x)\bigr)^2 \,dx + 2 p_{\max}^2 K_{\max}^2 \nu_d \int_{B(0,1)} \int_{U_r} \bigl(g(zr + x)\bigr)^2\,dz \,dx \\
	& \leq 4 p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{g}_{\Leb^2(U_{2r})}^2
	\end{align*}
	where $(i)$ follows from change of variables and $(ii)$ from Young's and Jensen's inequality. Reasoning by induction, we see that it suffices to show that
	\begin{equation*}
	\norm{g}_{\Leb^2(U_{(q + 1)r})}^2 \leq c r^{2s} \norm{g}_{H^s(\Xset)}^2
	\end{equation*}
	to establish~\eqref{eqn:leading_term_sobolev_compact_1}. The previous inequality is established in Lemma~\ref{lem:boundary_term_sobolev} for all $r > 0$ sufficiently small, and we have therefore proved the desired estimate over the boundary. 
	
	\subsubsection{Interior region}
	
	To show the desired bounds on $\Xset_{qr}$ when $s \geq 1$, we reason by induction on $q$.
	
	\paragraph{Base case.}
	In the base case $q = 1$, meaning $D_kg$ is only a single-difference operator.
	Since $s \geq 1$, replacing $g$ by its Taylor expansion inside the first order expected difference operator $\Ebb[D_kg(x)]$ yields
	\begin{equation}
	\label{eqn:leading_term_sobolev_compact_pf1}
	\Ebb\Bigl[D_kg(x)\Bigr] = \sum_{1 \leq \abs{\alpha} < s} \Ebb_{\alpha}(x) \cdot g^{(\alpha)}(x)  + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x)
	\end{equation}
	When $s = 1$ only the second term in the previous expression is non-zero, and we therefore begin by analyzing this term, obtaining that for each $\abs{\alpha} = s$,
	\begin{align}
	\biggl\|\Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)\biggr\|_{\Leb^2(\Xset_{r})}^2 & = \biggl\|\int (y - \cdot)^{\alpha} G_{\alpha}(\cdot,y) K_r(y,\cdot) p(y) \,dy\biggr\|_{\Leb^2(\Xset_{r})}^2 \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2  \biggl\|\int_{B(0,1)} G_{\alpha}(\cdot,zr + \cdot) \,dy\biggr\|_{\Leb^2(\Xset_{r})}^2 \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2 \nu_d \int_{B(0,1)} \Bigl\|G_{\alpha}(\cdot,zr + \cdot)\Bigr\|_{\Leb^2(\Xset_{r})}^2 \,dz \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{g^{(\alpha)}}_{\Leb^2(\Xset)}^2 \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{g}_{H^s(\Xset)}^2; \nonumber
	\end{align}
	hence~\eqref{eqn:leading_term_sobolev_compact_2} follows when $q = 1, s = 1$.
	
	When $s \geq 2$ we must analyze $\Ebb_{\alpha}(x) \cdot g^{(\alpha)}(x)$, which we do by using the Taylor expansion of $p$. Since $B(x,r) \subset \Xset$, we recall that $\Ibb_{\alpha + \beta,x} = r^{\abs{\alpha} + \abs{\beta}}I_{\alpha,\beta}$; thus
	\begin{align*}
	\Ebb_{\alpha}(x) & = \int(y - x)^{\alpha} K_r(y,x) p(y) \,dy \\
	& = \sum_{\abs{\beta} = 0}^{s - 2} p^{(\beta)}(x) \Ibb_{\alpha + \beta,x} + \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x) \\
	& = \sum_{\abs{\beta} = 0}^{s - 2} p^{(\beta)}(x) r^{\abs{\alpha} + \abs{\beta}} I_{\alpha + \beta} + \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x).
	\end{align*}
	Replacing $\Ebb_{\alpha}(x)$ by this expansion in~\eqref{eqn:leading_term_sobolev_compact_pf1} gives
	\begin{equation*}
	\Ebb\Bigl[D_kg(x)\Bigr] = \sum_{\abs{\alpha} = 1}^{s - 1} \sum_{\abs{\beta} = 0}^{s - 2} r^{\abs{\alpha} + \abs{\beta}} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x)  + \sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = s - 1} g^{(\alpha)}(x) \Bigl( \Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x)  + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x)
	\end{equation*}
	We now divide the sum in the first term based on the size of $\abs{\alpha} + \abs{\beta}$. The critical fact is that $I_{\alpha + \beta} = 0$ when $\abs{\alpha} + \abs{\beta} = 1$. When $s = 2$ this leaves
	\begin{equation*}
	\Ebb\Bigl[D_kg(x)\Bigr] = \sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x) g^{(\alpha)}(x) + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x).
	\end{equation*}
	We have already shown that the second term belongs to $\Leb^2(\Xset)$, and provided an appropriate upper bound on its norm. The first term is similarly upper bounded, since for each term inside the sum
	\begin{equation*}
	\norm{\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]g^{(\alpha)}}_{\Leb^2(\Xset)}^2 \leq r^{2(\abs{\alpha} + \abs{\beta})} p_{\max}^2 \norm{g^{(\alpha)}}_{\Leb^2(\Xset)}^2 \leq r^{2(\abs{\alpha} + \abs{\beta})} p_{\max}^2 \norm{g}_{H^s(\Xset)}^2 \nonumber
	\end{equation*}
	thus establishing~\eqref{eqn:leading_term_sobolev_compact_2} when $s = 2$. Otherwise when $s > 2$, we rearrange
	\begin{equation*}
	\sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = 0}^{s - 2} r^{\abs{\alpha} + \abs{\beta}} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x) = \sum_{\ell = 2}^{s - 1} r^{\ell} \Biggl\{\underbrace{\sum_{\abs{\alpha} + \abs{\beta} = \ell} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x)}_{:=g_{\ell,1}(x)}\Biggr\} + \sum_{\ell = s + 1}^{2s - 2} r^{\ell} \sum_{\abs{\alpha} + \abs{\beta} = \ell} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x).
	\end{equation*}
	and therefore
	\begin{align*}
	& \Ebb\Bigl[D_kg(x)\Bigr] - \sum_{\ell = 2}^{s - 1} r^{\ell} g_{\ell,1}(x) = \\ & ~~~~ \sum_{\ell = s + 1}^{2s - 2} r^{\ell} \sum_{\abs{\alpha} + \abs{\beta} = \ell} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x) +  \sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x) g^{(\alpha)}(x) + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x)
	\end{align*}
	On the left hand side, taking $\ell = \abs{\alpha} + \abs{\beta}$, note that for each $\ell < s$ the function $g_{\ell,1} \in C_c^{s - \ell}(\Xset) \subset H_0^{s - \ell}(\Xset)$ and further
	\begin{equation}
	\label{eqn:leading_term_sobolev_compact_pf3}
	\norm{g_{\ell,1}}_{H^{s - \ell}(\Xset)} \leq c p_{\max} \norm{g}_{H^s(\Xset)}.
	\end{equation}
	
	The right hand side consists of three terms, and we have already obtained sufficient estimates on the second and third term, so it remains to deal with the first term. We have that $g^{(\alpha)}\cdot p^{(\beta)} \in C_c^{0}(\Xset) \subset \Leb_0^2(\Xset)$ and
	\begin{equation}
	\label{eqn:leading_term_sobolev_compact_pf4}
	\norm{g^{(\alpha)}p^{(\beta)}}_{\Leb^2(\Xset)} \leq p_{\max} \norm{g}_{H^s(\Xset)},
	\end{equation}
	establishing~\eqref{eqn:leading_term_sobolev_compact_1} when $s > 2$.
	
	\paragraph{Induction step.}
	We now assume that~\eqref{eqn:leading_term_sobolev_compact_2}-\eqref{eqn:leading_term_sobolev_compact_4} hold with respect to $g$ for all $k \in (n)^q$, and show the desired estimates on $\Ebb[D_jD_kg]$ for all $(kj) \in (n)^{q + 1}$.
	
	We first consider the case where $s \leq 2q$. Then,
	\begin{align*}
	\norm{\Ebb\bigl[D_jD_kg\bigr]}_{\Leb^2(X_{(q + 1)r})}^2 \leq 2 p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{D_kg}_{\Leb^2(X_{qr})}^2 \leq c r^{2s} \norm{g}_{H^s(\Xset)}
	\end{align*}
	where the final inequality follows by hypothesis, and gives the desired estimate.
	
	Otherwise $s \geq 2q + 1$. We make use of the inductive hypothesis through the following three facts:
	\begin{enumerate}
		\item There exist functions $g_{2q,q}, \ldots, g_{s - 1,q}$ satisfying~\eqref{eqn:leading_term_sobolev_compact_4} such that
		\begin{equation*}
		\norm{\Ebb\bigl[D_kg\bigr] - \sum_{\ell = 2q}^{s - 1}r^{\ell}g_{\ell,q}}_{\Leb^2(X_{qr})} \leq c r^s \norm{g}_{H^s(\Xset)}
		\end{equation*}
		\item The functions $f_{\ell,q}$ belong to $H_0^{s - \ell}(\Xset)$. Thus by hypothesis, when $\ell = s - 1$ or $\ell = s - 2$,
		\begin{equation*}
		\norm{\Ebb\bigl[D_jg_{\ell,q}\bigr]}_{\Leb^2(\Xset_r)} \leq c r^s \norm{f}_{H^s(\Xset)}.
		\end{equation*}
		\item Otherwise if $s - \ell > 2$, there exist further functions $g_{\ell,m,q}$ for $m = 2,\ldots,s - \ell - 1$ such that
		\begin{equation*}
		\norm{\Ebb\bigl[D_jf\bigr] - \sum_{m = 2}^{s - \ell - 1}r^{m}g_{\ell,m,q}}_{\Leb^2(\Xset_r)} \leq  c r^s \norm{g}_{H^s(\Xset)}
		\end{equation*}
		The functions $g_{\ell,m,q} \in H_0^{s - (\ell + m)}(\Xset)$ additionally satisfy
		\begin{equation*}
		\norm{g_{\ell,m,q}}_{H^{s - (\ell + m)}(\Xset)} \leq c \norm{g_{\ell}}_{H^{s - \ell}(\Xset)} \leq c \norm{g}_{H^s(\Xset)}.
		\end{equation*}
	\end{enumerate}
	Making use of the law of iterated expectation and the Fact 1, we have
	\begin{align}
	\Ebb\Bigl[D_jD_kg(x)\Bigr] & = \Ebb\biggl[\Bigl(\Ebb\bigl[D_kg(x_j)|x_j\bigr] - \Ebb\bigl[D_kg(x)\bigr]\Bigr)K_r(x_j,x)\biggr] \nonumber \\
	& = \sum_{\ell = 2q}^{s - 1} r^{\ell} \Ebb\bigl[D_jg_{\ell}(x)\bigr] + \Ebb\biggl[\Bigl(\Ebb\bigl[D_kg\bigr](x_j) - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}(x_j)\Bigr)K_r(x_j,x)\biggr] + \Ebb\bigl[D_kg\bigr](x) - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}(x). \label{eqn:leading_term_sobolev_compact_pf6}
	\end{align}
	The above expansion consists of three terms. By Fact 1, the third term has bounded norm
	\begin{equation*}
	\norm{\Ebb\bigl[D_kg\bigr] - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}}_{\Leb^2(\Xset_{(q + 1)r})} \leq \norm{\Ebb\bigl[D_kg\bigr] - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}}_{\Leb^2(\Xset_{(q)r})} \leq c r^{s} \norm{g}_{H^s(\Xset)}
	\end{equation*}
	By Fact 1 and Lemma~\ref{lem:remainder_term_sobolev}, the same estimate holds with respect to the second term (up to constants). 
	
	When $s = (2q + 1)$ or $s = (2q + 2)$, by Fact 2
	\begin{equation*}
	\norm{ \sum_{\ell = 2q}^{s - 1} r^{\ell} \Ebb\bigl[D_jg_{\ell}\bigr]}_{\Leb^2(\Xset_r)} \leq c r^s \norm{g}_{H^s(\Xset)}
	\end{equation*}
	and the desired result~\eqref{eqn:leading_term_sobolev_compact_2} follows from the triangle inequality. Finally when $s > (2q + 2)$, by using Facts 2 and 3 we obtain
	\begin{align*}
	\norm{\sum_{\ell = 2q}^{s - 1} r^{\ell} \Ebb\bigl[D_jg_{\ell}\bigr] - \sum_{\ell = 2q}^{s - 3} \sum_{m = 2}^{s - \ell - 1} r^{\ell + m} g_{\ell,m,q}}_{\Leb^2(X_r)} \leq cr^s \norm{g}_{H^s(X)}
	\end{align*}
	Rewriting the double sum as a single sum over $\ell + m = 2q,\ldots,s - 1$ and plugging back in to~\eqref{eqn:leading_term_sobolev_compact_pf6} gives the desired result~\eqref{eqn:leading_term_sobolev_compact_4}.
\end{proof}

Here are a couple of Lemmas which helped us with the proof of Lemma~\ref{lem:leading_term_sobolev_compact}.

\begin{lemma}
	\label{lem:boundary_term_sobolev}
	Let $\Xset \subset \Rd$ be a bounded open set with Lipschitz boundary. For any $g \in C_c^{\infty}(\Xset)$, we have that 
	\begin{equation*}
	\norm{g}_{\Leb^2(U_r)}^2 \leq c r^s \norm{g}_{H^s(\Xset)}
	\end{equation*}
	for all $r > 0$ sufficiently small.
\end{lemma}
\begin{proof}
	Fix $x_0 \in \partial \Xset$, and let $Q_d(x_0,r)$ be the $d$-dimensional cube centered at $x_0$ of side length $r$. We will show that for all sufficiently small $r > 0$,
	\begin{equation}
	\norm{g}_{\Xset \cap \Leb^2(Q_d(x_0,r))} \leq c r^s \norm{g}_{H^s(Q_d(x_0,r))}
	\end{equation}
	The Lemma then follows by taking a finite covering of $\partial X$ -- possible since $X$ is assumed to be bounded -- in a similar manner to e.g. Theorem 18.1 of \textcolor{red}{(Leoni)} or Theorem 1 in 5.5 of \textcolor{red}{(Evans)}.
	
	We begin by straightening the boundary. Since $\Xset$ has a Lipschitz boundary, for any $x_0 \in \partial X$ there exists a rigid motion $\Phi: \Rd \to \Rd$ with $T(x_0) = 0$, a Lipschitz continuous function $\gamma: \Reals^{d-1} \to \Reals^d$, and a radius $r_0' > 0$ such that, setting $y = \Phi(x)$ and writing $y = (y',y_d)$, we have that for all $r < r_0$,
	\begin{equation*}
	\Phi(\Xset \cap Q(x_0,r)) = \bigl\{y \in Q(0,r): y_d > \gamma(y')\bigr\}.
	\end{equation*}
	Fixing $0 < r < r_0/[\mathrm{Lip}(\gamma)\sqrt{d}]$, we have that $\gamma(y') > -r$ for all $y' \in Q_{d - 1}(x_0,r)$; writing $\wt{g}(y) = g(T^{-1}(y))$, taking a Taylor expansion of $\wt{g}(y)$ around $\wt{g}((y',\gamma(y')))$ thus yields
	\begin{align*}
	\wt{g}(y) & = \sum_{\ell = 1}^{s - 1} g^{(\ell e_d)}\bigl((y',\gamma(y'))\bigr) (y_d - \gamma(y'))^{\ell} + \int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr) h^{s - 1}\,dh \\
	& = \int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr) h^{s - 1}\,dh
	\end{align*}
	where the second equality follows from the assumption $g \in C_c^{\infty}(\Xset)$. We now analyze the $\Leb^2$ norm over $Q_d(x_0,r)$,
	\begin{align*}
	\norm{g}_{\Xset \cap \Leb^2(Q_d(x_0,r))}^2 & = \norm{\wt{g}}_{\Leb^2(\Phi(\Xset \cap Q(0,r)))}^2 \\
	& = \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \biggl[\int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr)h^{s - 1} \,dh\biggr]^2 \,dy_d \,dy' \\
	& \overset{(i)}{\leq} (2r)^{2(s - 1)} \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \bigl(y_d - \gamma(y')\bigr)^2 \biggl[\frac{1}{y_d - \gamma(y')}\int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr) \,dh\biggr]^2 \,dy_d \,dy' \\
	& \overset{(ii)}{\leq}  (2r)^{2(s - 1)} \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \bigl(y_d - \gamma(y')\bigr) \int_{\gamma(y')}^{y_d} \biggl[\wt{g}^{(se_d)}\bigl((y',he_d)\bigr)\biggr]^2 \,dh \,dy_d \,dy' \\
	& \overset{(iii)}{\leq}  (2r)^{2s - 1} \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \int_{\gamma(y')}^{r} \biggl[\wt{g}^{(se_d)}\bigl((y',he_d)\bigr)\biggr]^2 \,dh \,dy_d \,dy' \\
	& \leq  (2r)^{2s} \int_{Q_{d - 1}(0,r)}  \int_{\gamma(y')}^{r} \biggl[\wt{g}^{(se_d)}\bigl((y',he_d)\bigr)\biggr]^2 \,dh  \,dy' \\
	& \overset{(iv)}{\leq} (2r)^{2s} \int_{Q(x_0,r)} \bigl[g^{(se_d)}(x)\bigr]^2 \,dx \leq (2r)^{2s} \norm{g}_{H^s(B(x_0,r))}^2
	\end{align*} 
	where $(i)$ follows since $0 < y_d - \gamma(y') < r - \gamma(y') < 2r$, $(ii)$ follows by Jensen's inequality, $(iii)$ follows since $y_d < r$, and $(iv)$ follows from a change of variables. This completes the proof of Lemma~\ref{lem:boundary_term_sobolev}.
\end{proof}

The following Lemma helps us deal with remainder terms in the Sobolev case.
\begin{lemma}
	\label{lem:remainder_term_sobolev}
	Let $U \subset \Rd$ be an open set, and $U_r = \set{x \in \Rd: \mathrm{dist}(x,U) < x}$. Suppose $f \in \Leb^2(U_r)$ and $k \in (n)$. Then,
	\begin{equation*}
	\norm{\Ebb\bigl[D_kf\bigr]}_{\Leb^2(U)}, \norm{\Ebb\bigl[fK_r(x_k,\cdot)\bigr]}_{\Leb^2(U)} \leq c \norm{f}_{\Leb^2(U_r)}
	\end{equation*}
\end{lemma}

\subsection{One-Sided Concentration}
	
The proof of Lemma~\ref{lem:empirical_norm_sobolev} relies on (a variant of) the Paley-Zygmund Inequality.
\begin{lemma}
	\label{lem:paley_zygmund}
	Let $f$ satisfy the following moment inequality for some $b \geq 1$:
	\begin{equation}
	\label{eqn:paley_zygmund_1}
	\Ebb\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot\Bigl(\Ebb\bigl[\norm{f}_n^2\bigr]\Bigr)^2.
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:paley_zygmund_2}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}.
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. The Paley-Zygmund inequality says that for all $0 \leq \lambda \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund_pf1}
	\mathbb{P}(Z > \lambda \mathbb{E}(Z^p)) \geq \left[(1 - \lambda^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
	Applying~\eqref{eqn:paley_zygmund_pf1} with $Z = \norm{f}_n^2$, $p = 1$, $q = 2$ and $\lambda = \frac{1}{b}$, by assumption~\eqref{eqn:paley_zygmund_1} we have
	\begin{equation*}
	\mathbb{P}\Bigl(\norm{f}_n^2 > \frac{1}{b} \mathbb{E}[\norm{f}_n^2]\Bigr) \geq \Bigl(1 - \frac{1}{b}\Bigr)^2 \cdot  \frac{\bigl(\mathbb{E}[\norm{f}_n^2]\bigr)^2}{\mathbb{E}[\norm{f}_n^4]} \geq \frac{\Bigl(1 - \frac{2}{b}\Bigr)}{\Bigl(1 + \frac{1}{b^2}\Bigr)} \geq 1 - \frac{5}{b}.
	\end{equation*}
\end{proof}

\end{document}