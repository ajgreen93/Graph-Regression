\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Graph Testing}
\author{Alden Green}
\date{\today}
\maketitle

This document details the current status of the graph testing project. We divide by section based on the testing problem under consideration. At the end of each section, we list some areas we are interested in investigating. All proofs are left until the end.

We establish some notation that we will use throughout. Let $X = \{x_1,
\ldots, x_n\}$ be a sample drawn i.i.d. from a distribution $P$ on $\Rd$,
with density~$p$.  For a radius $r > 0$, we define $G_{n,r}=(V,E)$ to be the
\emph{$r$-neighborhood graph} of $X$, an unweighted, undirected graph with
vertices $V=X$, and an edge $(x_i,x_j) \in E$ if and only if $K_r(x_i,x_j) = \norm{x_i -x_j} \leq r$, where $\norm{\cdot}$ is the Euclidean norm. We denote by $A \in
\Reals^{n \times n}$ the adjacency matrix, with entries $A_{uv} = 1$ if
$(u,v) \in E$ and $0$ otherwise.  We also denote by $D$ the diagonal degree
matrix, with entries $D_{uu} := \sum_{v \in V} A_{uv}$. The graph Laplacian is $L = D - A$, and we write its spectral decomposition as $L = V S V^T$. 

\section{Regression goodness-of-fit testing with random design.}

Let $P$ be a distribution with density $p$ supported on $\mathcal{X} \subseteq \Reals^d$. Suppose we observe random design points $X = \set{x_1,\ldots,x_n} \sim P$, and additionally responses
\begin{equation}
\label{eqn:regression_known_variance}
y_i = f(x_i) + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation} 

We wish to distinguish
\begin{equation*}
\mathbf{H}_0: f = f_0 := 0 \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: f \neq f_0
\end{equation*}
We will evaluate our performance using worst-case risk: for a given function class $\mathcal{H}$ and test function $\phi: \Reals^n \to \set{0,1}$, let
\begin{equation*}
\mathcal{R}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, f \neq f_0} \Ebb_f(1 - \phi).
\end{equation*}
The worst-case risk may be quite close to $1$ unless we enforce some separation between null and alternative spaces. A more realistic measure of performance is therefore
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, \norm{f - f_0}_2 \geq \epsilon} \Ebb_f(1 - \phi).
\end{equation*}


\subsection{Test Statistics.}
We list the test statistics we use for the regression testing problem.

\paragraph{Eigenvector projection test statistic.}
Let $VSV^T$ be the spectral decomposition of the Laplacian matrix $L$ of the neighborhood graph $G_{n,r}$. To test whether $f = f_0$, we propose the following \emph{eigenvector projection} test statistic:
\begin{equation}
\label{eqn:graph_spectral_projections}
T_{\mathrm{spec}} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_{k,i} y_i\right)^2
\end{equation}

\subsection{Current Results.}

In Theorem~\ref{thm:sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}} := \1\{T_{\mathrm{spec}} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $W^{1,2}(\mathcal{X};R)$. To conveniently state our results we introduce the notation
\begin{equation*}
h(a,d) = a(d+2) + (1 + 2/d),~~\textrm{for $a > 0$}
\end{equation*}

\begin{theorem}
	\label{thm:sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d < 4$. Suppose that $\Pbb$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < p_{\min} < p(x) < p_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right)^{1/d}, ~\kappa = n^{2d/(4 + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $d,R,p_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/(4 + d)} (\log n)^{h(a,d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}
The testing rate~\eqref{eqn:sobolev_testing_rate} matches the minimax critical radius up to a factor of $\log^{h(a,d)}n$.

When $d \geq 4$ the compact embedding
\begin{equation*}
W_d^{1,2}(\mathcal{X}) \subseteq \mathcal{L}_d^4(\mathcal{X}) 
\end{equation*}
does not hold (as it does when $d < 4$). However, if we directly assume $f \in \mathcal{L}_d^4(\mathcal{X};R)$ (regardless of $d$) we obtain the following result.
\begin{proposition}
	\label{prop:L4_testing_rate}
	If $f \in \mathcal{L}_d^4(\mathcal{X};R)$, there exists a constant $c$ such that if
	\begin{equation}
	\label{eqn:L4_testing_rate}
	\epsilon^2 > b \cdot n^{-1/2}
	\end{equation}
	then the test
	\begin{equation*}
	\phi_{\mathrm{mean}} = \1\{\frac{1}{n}\sum_{i = 1}^{n} y_i^2 \geq 1\}
	\end{equation*}
	has worst-case risk
	\begin{equation*}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; W^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}. 
	\end{equation*}
\end{proposition}

Note that when $d < 4$ the rate~\eqref{eqn:sobolev_testing_rate} is sharper than \eqref{eqn:L4_testing_rate}. 

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $d \geq 4$, is it true that there does not exist a uniformly consistent
	test over the Sobolev ball $W_d^{1,2}(\mathcal{X};R)$?
	\item Does the testing rate of $\phi_{\mathrm{spec}}$ over Sobolev spaces $W_d^{s,2}(\mathcal{X};R)$ match the minimax rate, for an appropriate choice of kernel $K$?
	\item Assume the distribution $P$ is supported on a manifold $\mathcal{M}$ of intrinsic dimension $s < d$. Does $\mathrm{\phi_{\mathrm{spec}}}$ display adaptivity to the intrinsic dimension of $\mathcal{M}$?
	\item Assume that $f$ belongs to the Holder space $C_d^s(\mathcal{X})$. Moreover, suppose that instead of observing ${y_i}$ according to the regression testing model \eqref{eqn:regression_known_variance}, we observe
	\begin{equation*}
	y_i = f(x_i) + \sigma \varepsilon_i, ~\varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
	\end{equation*}
	where $\sigma > 0$ is unknown. When $d \geq 4$, what are the minimax regression testing rates over $C_d^1(\mathcal{X};R)$? Is the test $\phi_{\mathrm{spec}}$ minimax optimal, when the tuning parameters $r$ and $\kappa$ are appropriately chosen?
\end{enumerate}

\section{Two-sample density testing.}
In the two-sample density testing problem, we observe independent samples $Z = z_1,\ldots,z_N \sim P$ and $Y = y_1,\ldots,y_M \sim Q$, where $P$ and $Q$ are distributions over $\Reals^d$ with densities $p$ and $q$, respectively, and $N \sim \textrm{Bin}(n,1/2)$. Our goal is to distinguish the hypotheses
\begin{equation*}
\mathbf{H}_0: P = Q \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: P \neq Q
\end{equation*}
and we again evaluate our performance using worst-case risk; letting $\phi:\Reals^{N + M} \to \{0,1\}$, 
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \inf_{p \in \mathcal{H}}\Ebb_{p,p}(\phi) + \sup_{\substack{p,q \in \mathcal{H} \\ \norm{p - q}_{\Leb^2} \geq \epsilon}} \Ebb_{p,q}(1 - \phi).
\end{equation*}

\subsection{Test statistics.}

We suggest several two-sample test statistics. 

\paragraph{Eigenvector projection test statistic.}

It is straightforward to adapt the test statistic $T_{\mathrm{spec}}$ to the two-sample testing problem. Concatenate the samples $Z$ and $Y$ in $X = (z_1,\ldots,z_N,y_1,\ldots,y_M)$, and define $T_{\mathrm{spec}}^{(2)}$ to be
\begin{equation}
\label{eqn:graph_spectral_projections_2}
T_{\mathrm{spec}}^{(2)} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2, ~~\textrm{where}~~ a = (\underbrace{N^{-1},\ldots,{N^{-1}}}_{\textrm{length } N},\underbrace{-M^{-1},\ldots,-M^{-1}}_{\textrm{length } M})
\end{equation}

For convenience, we state our following two test statistics with respect to the empirical norm $\norm{\theta}_n = n^{-1/2}\norm{\theta}_2$ for $\theta \in \Reals^n$. They will each depend on a tuning parameter $\lambda > 0$.
\paragraph{Graph Sobolev IPM.}
Letting $C_n := nr^{(d + 2)/2}$ and
\begin{equation*}
\Theta_{1,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B\theta}_2 \leq 1\} 
\end{equation*}
we define the \emph{graph Sobolev IPM} to be
\begin{equation}
\label{eqn:sobolev_IPM}
T_{\textrm{sob}} := \sup_{\substack{\theta \in \Theta_{1,2} \\ \lambda \norm{\theta}_n \leq 1}} \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}. 
\end{equation}

\paragraph{Graph Total Variation IPM.}
Letting $C_n' := n^{2}r^{(d + 1)}$ and 
\begin{equation*}
\Theta_{1,1} := \{\theta \in \Reals^n:~ (C_n')^{-1} \norm{B\theta}_1 \leq 1\}
\end{equation*}
we define the \emph{graph Total Variation} IPM to be
\begin{equation}
\label{eqn:total_variation_IPM}
T_{\mathrm{TV}} := \sup_{\substack{\theta \in \Theta_{1,1}, \\ \lambda \norm{\theta}_n \leq 1} } \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}, \quad
\end{equation}

\subsection{Current results.}

In Theorem~\ref{thm:twosample_sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}}^{(2)} := \1\{T_{\mathrm{spec}}^{(2)} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $W^{1,2}(\mathcal{X};R)$ when $d = 1$.

\begin{theorem}
	\label{thm:twosample_sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d = 1$.  Suppose that $\mu = (P + Q)/2$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]$ with density functions $\rho$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < \rho_{\min} < \rho(x) < \rho_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}^{(2)}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right), ~\kappa = n^{2/5}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $R,p_{\max},q_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/5} (\log n)^{h(a,1)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}

We prove Theorem~\ref{thm:twosample_sobolev_testing_rate} by relating the density testing problem to a regression testing problem with a certain type of structured noise, and then proceeding along similar lines to the proof of Theorem~\ref{thm:sobolev_testing_rate}. To pursue this strategy, we require the eigenvectors to satisfy a certain type of incoherence condition; this is in constrast to the regression testing problem with known variance, where we did not require the eigenvectors to be smooth in any sense.

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $1 < d < 4$, is the test $\phi_{\spec}^{(2)}$ minimax optimal?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $C_d^1(\mathcal{X};R)$ for all values of $d$?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $W_d^{1,2}(\mathcal{X};R)$ when $d \leq 4$?
	\item Is the test statistic \eqref{eqn:graph_spectral_projections_2}, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $W^{s,2}$? 
	\item Modify the test statistic \eqref{eqn:sobolev_IPM} by replacing the function class $\Theta_{1,2}$ with
	\begin{equation}
	\Theta_{s,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B^{(s)}\theta}_2 \leq 1\} 
	\end{equation}
	Is the modified test statistic, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $W^{s,2}$?
	\item What is the minimax testing rate over $BV_d^{1}(\mathcal{X};R)$? Does it exhibit a phase transition analogous to the minimax estimation rate over bounded variation spaces?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over $BV_d^{1}(\mathcal{X};R)$?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over Sobolev and Holder function classes?
\end{enumerate}

\section{Definitions}

Here we collect definitions of some common function spaces and graph operators.

\subsection{Function Spaces}

\paragraph{Lebesgue spaces.}

We say a Borel measurable function $f: \mathcal{X} \to \Reals$ is in the space $\mathcal{L}^p(\mathcal{X})$ for $1 \leq p < \infty$ if 
$$\norm{f}_{\mathcal{L}^p(\mathcal{X})} := \int_{\mathcal{X}} \abs{f(x)}^p \,dx < \infty$$
and we let 
\begin{equation*}
\mathcal{L}^p(\mathcal{X};R) = \set{f \in \mathcal{L}^p(\mathcal{X}): \norm{f}_{\mathcal{L}^p(\mathcal{X})} < R}
\end{equation*}
be a ball in the Lebesgue space.


\paragraph{Holder spaces.}

For a given $s > 0$, let $\ell = \floor{s}$ be the largest integer strictly less than $s$. Then the $s$th Holder norm is given by
\begin{equation*}
\norm{f}_{C_d^{s}(\mathcal{X})} := \sum_{\abs{\alpha} < s} \norm{D^{\alpha}f}_{\infty} + \sum_{\abs{\alpha} = \ell} \sup_{x,y \in \mathcal{X}} \frac{\abs{D^{\alpha}f(y) - D^{\alpha}f(x)}}{\norm{x - y}_2^{s - \ell}}
\end{equation*}
and the $s$th Holder space $C_d^{s}(\mathcal{X})$ consists of all functions which are $\ell$ times continuously differentiable with finite Holder norm $\norm{\cdot}_{C_d^{s}(\mathcal{X})}$. Denote the Holder ball by $C_d^{s}(\mathcal{X},R) = \set{f \in C_d^{s}(\mathcal{X}): \norm{f}_{C_d^{s}(\mathcal{X})} \leq R}$.

\paragraph{Sobolev spaces.}

For a given $s > 0$, the Sobolev space $W_d^{s,2}(\mathcal{X})$ consists of all functions $f \in \mathcal{L}^2(\mathcal{X})$ such that for each $\alpha = (\alpha_1,\ldots,\alpha_d)$ with $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ belongs to $\mathcal{L}^2(\mathcal{X})$. The Sobolev $\{s,2\}$ norm is then 
\begin{equation*}
\norm{f}_{W_d^{s,2}(\mathcal{X})}^2 = \sum_{\abs{\alpha} \leq s} \int_{\mathcal{X}} \abs{D^{\alpha}f}^2 \,dx
\end{equation*}
and for a given $L > 0$, the corresponding ball is $W_d^{s,2}(\Xset; L) = \set{f: \norm{f}_{W^{s,2}(\Xset)} \leq L}$.

\paragraph{Bounded Variation spaces.}

For a function $f \in L^1(\mathcal{X})$ the \emph{total variation} semi-norm of $f$ is
\begin{equation*}
TV(f;\mathcal{X}) := \sup \left\{ \int_{\mathcal{X}} f \, \Xsetive \, \psi \,dx : \psi \in C_c^1(\mathcal{X}; \Reals^d), \abs{\psi} \leq 1 \right\};
\end{equation*}
and we write $BV_d(\mathcal{X})$ for the subset of functions $f \in L^1(\mathcal{X})$ which have bounded norm
\begin{equation*}
\norm{f}_{BV_d(\mathcal{X})} := \norm{f}_{\infty} + TV(f;\mathcal{X}).
\end{equation*}
For a given $R > 0$, the corresponding ball is $BV_d^{1}(\mathcal{X};R) = \set{f: \norm{f}_{BV_d(\mathcal{X})} \leq R}$. 

\subsection{Graph Operators.}
Let $s \geq 1$ be an integer. The $s$th-order difference operator on $G_{n,r}$, denoted $B^{(s)}$, is defined by
\begin{equation*}
B^{(s)} :=
\begin{cases}
L^{s/2},& ~~ s \textrm{ even} \\
BL^{(s - 1)/2},& ~~ s \textrm{ odd.}
\end{cases}
\end{equation*}

\section{\textcolor{red}{In Progress}}

We say $K_{r}$ is an order-$s$ kernel when 
\begin{equation*}
\int x^j K_{r}(x) \,dx = 0, \textrm{for $j = 1,\dots,s$}
\end{equation*}

Our goal is to prove the following theorem.

\begin{theorem}
	\label{thm:higher_order_sobolev_testing_rate}
	Let $b,s,d \geq 1$ and $a > 0$ be fixed constants such that $d < 4s$. Suppose that $\Pbb$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < p_{\min} < p(x) < p_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}$ is performed with parameter choices 
	\begin{equation*}
	\textrm{$K_r$ an order-$s$ kernel with}~ r = \log^a n \cdot \left(\frac{\log n}{n}\right)^{1/d}, ~\kappa = n^{2d/(4s + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $d,R,p_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:higher_order_sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/(4s + d)} (\log n)^{h(a,d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:higher_order_sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{s,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}

\section{\textcolor{red}{In Progress Proofs}}

\subsection{Proof of Theorem~\ref{thm:higher_order_sobolev_testing_rate}}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta = (\beta_1,\ldots,\beta_n) \in \Reals^n$ be a signal over the vertices $V$. We observe responses $Y = (y_1,\ldots,y_n)$ according to the model
\begin{equation*}
y_i = \beta_i + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i y_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec} = \1\{T_{\spec} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $S_s(\beta;G)$ be a measure of smoothness the signal $\beta$ displays over the graph $G$, given by
\begin{equation*}
S_s(\beta;G) := \beta^T L^s \beta
\end{equation*}
In Lemma~\ref{lem:higher_order_fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}$. Our bound on the Type II error will be stated as a function of $S_2(\beta;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$.

\begin{lemma}
	\label{lem:higher_order_fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}$ is upper bounded
		\begin{equation}
		\label{eqn:higher_order_graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b$ and $\beta$ such that
		\begin{equation}
		\label{eqn:higher_order_fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_s(\beta;G)}{ns_{\kappa}^s}
		\end{equation}
		the Type II error of $\phi_{\spec}$ is upper bounded,
		\begin{equation}
		\label{eqn:higher_order_graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

To prove Lemma~\ref{lem:higher_order_fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:higher_order_graph_spectral_type_I_error} and \eqref{eqn:higher_order_graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2.
\end{align*}
When $\beta = 0$, this equals $\kappa/n$. Otherwise, we have the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{s_{\kappa}^s}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 s_k^s \\
& \geq \norm{\beta}_2^2 - \frac{S_s(\beta;G)}{s_{\kappa}^s},
\end{align*}
and therefore $\Ebb(T_{\spec}) \geq \kappa/n + n^{-1}(\norm{\beta}_2^2 - S_s(\beta;G)/s_{\kappa}^s)$. 

\vspace{.2 in}

\textit{Variance of $T_{\mathrm{spec}}$:}
We write $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvectors $v_1,\ldots,v_{\kappa}$ as columns. Consequently,
\begin{align}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa)
\end{align}
where the last inequality follows from standard properties of the Gaussian distribution. We now move on to showing the desired inequalities \eqref{eqn:higher_order_graph_spectral_type_I_error} and \eqref{eqn:higher_order_graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:higher_order_graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{1}{b^2}.
\end{align*}

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:higher_order_graph_spectral_type_II_error}:} For simplicity, we introduce the notation
\begin{equation*}
\Delta = \frac{\norm{\beta}_2^2}{n} - \frac{S_s(\beta;G)}{ns_{\kappa}^s}.
\end{equation*}
Assumption~\eqref{eqn:higher_order_fixed_graph_testing_critical_radius} implies $\Delta \geq 2 t(b)$. Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & = \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\beta^T V_{\kappa} V_{\kappa}^T \beta /n^2}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:higher_order_spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term, noting that $\Delta = \beta^T V_{\kappa} V_{\kappa}^T \beta/n$, we have
\begin{align}
\frac{\beta^T V_{\kappa} V_{\kappa}^T \beta/n^2}{\Delta^2} & = \frac{1}{n\Delta} \nonumber \\
& \leq \frac{1}{2nt(b)} \nonumber \\
& = \frac{1}{2b\sqrt{2\kappa}}, \label{eqn:higher_order_spectral_type_II_error_pf2}
\end{align}
and combining~\eqref{eqn:higher_order_spectral_type_II_error_pf1} and~\eqref{eqn:higher_order_spectral_type_II_error_pf2} yields~\eqref{eqn:higher_order_graph_spectral_type_II_error}.


\subsubsection{Step 2: Bounding neighborhood graph functionals}

To make use of Lemma~\ref{lem:fixed_graph_testing} we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that for some constants $c_1,c_2,c_3$, the following statements:
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:higher_order_discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} When $\beta = (f(x_1),\ldots,f(x_n))$ for $f \in W^{s,2}(\mathcal{X};R)$, and $r = n^{-1/(2 + d)}$,
	\begin{equation}
	\label{eqn:higher_order_continuous_to_discrete_sobolev_norm}
	S_s(\beta;G_{n,r}) \leq c_1 \cdot n^{2 + s} r^{s(d + 2)} 
	\end{equation}
	\item 
	\label{event:higher_order_eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} For any $r \geq n^{-1/(2 + d)}$ and for all $\kappa \in [n]$:
	\begin{equation}
	\label{eqn:higher_ordereigenvalue_tail_bound}
	s_{\kappa} \geq c_2 \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	\item 
	\label{event:higher_order_l2_norm}
	\textbf{Empirical norm of $f$:} When $f \in W^{s,2}(\mathcal{X};R)$ and \textcolor{red}{some other condition}
	\begin{equation}
	\label{eqn:higher_order_l2_to_empirical_norm}
	\frac{1}{n}\sum_{i = 1}^{n}\beta_i^2 = \norm{f}_n^2 \geq c_3 \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

each hold with probability $1 - o(1)$ as $n \to \infty$.



\section{Proofs}

\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate}}

To prove Theorem~\ref{thm:sobolev_testing_rate}, we will show that when
\begin{equation}
\label{eqn:critical_radius}
\epsilon^2 \geq c \cdot b \cdot (\log n)^{h(a,d)} \cdot n^{-4/(4 + d)}
\end{equation}
and we choose tuning parameters as specified in Theorem~\ref{thm:sobolev_testing_rate}, the test $\phi_{\mathrm{spec}}$ satisfies
\begin{equation}
\mathcal{R}_{\epsilon}(\phi_{\spec}; W^{1,2}(\mathcal{X};R)) = \Ebb_{f_0}(\phi_{\spec}) + \sup_{\substack{f \in W^{1,2}(\mathcal{X};R) \\ \norm{f}_{\Leb^2(\mathcal{X})} > \epsilon}} \Ebb_{f}(1 - \phi_{\spec}) \leq \left(\frac{1}{b^2} + \frac{3}{b}\right) + o(1).
\end{equation}

Our analysis will proceed according to the following steps.
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $Y$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta = (\beta_1,\ldots,\beta_n) \in \Reals^n$ be a signal over the vertices $V$. We observe responses $Y = (y_1,\ldots,y_n)$ according to the model
\begin{equation*}
y_i = \beta_i + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i y_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec} = \1\{T_{\spec} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $S_2(\beta;G)$ be a measure of smoothness the signal $\beta$ displays over the graph $G$, given by
\begin{equation*}
S_2(\beta;G) = \sum_{i = 1}^{n} \sum_{j = 1}^{n} (\beta_i - \beta_j)^2 \1((v_i,v_j) \in E).
\end{equation*}
In Lemma~, we upper bound the Type I and Type II error of the test $\phi_{\spec}$. Our bound on the Type II error will be stated as a function of $S_2(\beta;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$.

\begin{lemma}
	\label{lem:fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}$ is upper bounded
		\begin{equation}
		\label{eqn:graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b$ and $\beta$ such that
		\begin{equation}
		\label{eqn:fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\beta;G)}{ns_{\kappa}}
		\end{equation}
		the Type II error of $\phi_{\spec}$ is upper bounded,
		\begin{equation}
		\label{eqn:graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

The first term on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius} grows with $\kappa$, while the second term shrinks. These can be thought of as the variance and squared bias terms, respectively, of the error.

\paragraph{Proof of Lemma~\ref{lem:fixed_graph_testing}.}

To prove Lemma~\ref{lem:fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2.
\end{align*}
When $\beta = 0$, this equals $\kappa/n$. Otherwise, we have the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{s_{\kappa}}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 s_k \\
& \geq \norm{\beta}_2^2 - \frac{S_2(\beta;G)}{s_{\kappa}},
\end{align*}
and therefore $\Ebb(T_{\spec}) \geq \kappa/n + n^{-1}(\norm{\beta}_2^2 - S_2(\beta;G)/s_{\kappa})$. 

\vspace{.2 in}

\textit{Variance of $T_{\mathrm{spec}}$:}
We write $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvectors $v_1,\ldots,v_{\kappa}$ as columns. Consequently,
\begin{align}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa)
\end{align}
where the last inequality follows from standard properties of the Gaussian distribution. We now move on to showing the desired inequalities \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{1}{b^2}.
\end{align*}

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:graph_spectral_type_II_error}:} For simplicity, we introduce the notation
\begin{equation*}
\Delta = \frac{\norm{\beta}_2^2}{n} - \frac{S_2(\beta;G)}{ns_{\kappa}}.
\end{equation*}
Assumption~\eqref{eqn:fixed_graph_testing_critical_radius} implies $\Delta \geq 2 t(b)$. Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & = \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\beta^T V_{\kappa} V_{\kappa}^T \beta /n^2}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term, noting that $\Delta = \beta^T V_{\kappa} V_{\kappa}^T \beta/n$, we have
\begin{align}
\frac{\beta^T V_{\kappa} V_{\kappa}^T \beta/n^2}{\Delta^2} & = \frac{1}{n\Delta} \nonumber \\
& \leq \frac{1}{2nt(b)} \nonumber \\
& = \frac{1}{2b\sqrt{2\kappa}}, \label{eqn:spectral_type_II_error_pf2}
\end{align}
and combining~\eqref{eqn:spectral_type_II_error_pf1} and~\eqref{eqn:spectral_type_II_error_pf2} yields~\eqref{eqn:graph_spectral_type_II_error}.


\subsubsection{Step 2: Bounding neighborhood graph functionals}

To make use of Lemma~\ref{lem:fixed_graph_testing} we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that for some constants $c_1,c_2,c_3$, the following statements:
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} When $\beta = (f(x_1),\ldots,f(x_n))$ for $f \in W^{1,2}(\mathcal{X};R)$:
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm}
	S_2(\beta;G_{n,r}) \leq c_1 \cdot n^2 r^{d + 2} 
	\end{equation}
	\item 
	\label{event:eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} If $r = (\log^{a}n)\cdot(\log n/n)^{1/d}$ for any $a > 0$, then for all $\kappa \in [n]$:
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound}
	s_{\kappa} \geq c_2 \cdot \frac{\kappa^{2/d}}{n^{2/d}}
	\end{equation}
	\item 
	\label{event:l2_norm}
	\textbf{Empirical norm of $f$:} When $f \in W^{1,2}(\mathcal{X};R)$ and one of (a) $d = 1,2$, or (b) $d = 3$ and $\norm{f}_{\Leb^2}^2 \geq n^{-2/(d - 2)}$, then
	\begin{equation}
	\label{eqn:l2_to_empirical_norm}
	\frac{1}{n}\sum_{i = 1}^{n}\beta_i^2 = \norm{f}_n^2 \geq c_3 \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

each hold with probability $1 - o(1)$ as $n \to \infty$.

\paragraph{Proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm}:}

Observe that
\begin{align*}
\frac{1}{n^2}\Ebb(\beta^T L \beta) = \Ebb\left(\frac{1}{n^2} \sum_{i,j = 1}^{n} (\beta_i - \beta_j)^2 A_{ij}\right) & = \frac{(n - 1)}{n} \int_{\Xset} \int_{\Xset} (f(x) - f(y))^2K_r(x,y) \,dP(y) \,dP(x) \\
& \leq p_{\max}^2 \frac{(n - 1)}{n} \int_{\Xset} \int_{\X} (f(x) - f(y))^2K_r(x,y) \,dy \,dx.
\end{align*}
We will show that for any $f \in \mathcal{W}^{1,2}(\X;R)$
\begin{equation}
\label{eqn:nonlocal_continuous_sobolev_norm}
\int_{\Xset} \int_{\Xset} (f(x) - f(y))^2K_r(x,y) \,dy \,dx \leq c r^{d + 2} \norm{f}_{W^{1,2}(\mathcal{X})}
\end{equation}
whence the desired result~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows by Markov's inequality and the assumption $f \in W^{1,2}(\mathcal{X};R)$.

We begin by dealing with complications due to smoothing at the boundary of $\mathcal{X} = [0,1]^d$. Let $U$ be any bounded open set such that $\Xset \subset U$ is compactly contained in $U$. Note that as $\partial \Xset$ is $C^1$, by Theorem \ref{thm:evans_extension} there exists $g \in W^{1,2}(\Reals^d)$ such that
\begin{enumerate}[(i)]
	\item
	\label{eqn:evans_extension_1}
	$g = f$, $P$-almost-everywhere in $\Xset$
	\item 
	$g$ has support within $V$, and  
	\item 
	\label{eqn:sobolev_1_bound_2}
	$\norm{g}_{W^{1,2}(\Rd)} \leq c \norm{f}_{W^{1,2}(\Xset)}$ for a constant $c$ which depends only on $\Xset$.
\end{enumerate}
As a result of \ref{eqn:evans_extension_1},
\begin{equation}
\label{eqn:sobolev_1_bound_1}
\int_{\Xset} \int_{\Xset} (f(x) - f(y))^2K_r(x,y) \,dy \,dx \leq \int_{\Rd} \int_{\Rd} (g(x) - g(y))^2K_r(x,y) \,dy \,dx.
\end{equation}

Next we smooth $g$, so that we may work with ordinary partial derivatives.
We let $\eta \in C^{\infty}(\Rd)$ be given by
\begin{equation*}
\eta(x) :=
\begin{cases}
C \exp \left\{\frac{1}{\norm{x}^2 - 1}\right\}& \quad \textrm{if $\norm{x}_2 \leq 1$} \\
0 & \quad \textrm{if $\norm{x}_2 \geq 1$}
\end{cases}
\end{equation*}
where the normalizing constant $C > 0$ is chosen so that $\int_{\Rd} \eta dx = 1$. Let $\eta_r(x) := (1/r^d) \eta(x/r)$. Then, the mollification of $g$ by $\eta_r$ is given by
\begin{align*}
g^r & := g \ast \eta_r \\
& = \int_{\Rd} \eta_r(x - y) g(y) dy
\end{align*}
(Refer to \textcolor{red}{Evans}, Appendix C, Theorem 7 for a proof that $g^r \in C^{\infty}(\Rd)$.)
Adding and subtracting within \eqref{eqn:sobolev_1_bound_1}, we have
\begin{align}
\int_{\Rd} & \int_{\Rd} (g(x) - g(y))^2K_r(x,y) \,dy \,dx \\
& \leq 3 \int_{\Rd} \int_{\Rd} \bigl((g(x) - g^r(x))^2 + (g^r(x) - g^r(y))^2 + (g^r(y) - g(y))^2\bigr)K_r(x,y) \,dy \,dx \nonumber \\
& = 6 \int_{\Rd} \int_{\Rd} \bigl((g(y) - g^r(y))^2 K_r(x,y) \,dy \,dx + 2 \int_{\Rd} \int_{\Rd} \bigl((g^r(x) - g^r(y))^2 K_r(x,y) \,dy \,dx \label{eqn:sobolev_1_bound_5}
\end{align}
We deal with each summand individually, beginning with the first one. Let $\nu_d = \pi^{d/2}/\Gamma(d/2 + 1)$ denote the volume of a ball with unit radius in $\Rd$. Then, we have
\begin{align}
\int_{\Rd} \int_{\Rd} \bigl((g(y) - g^r(y))^2 K_r(x,y) \,dy \,dx & = \int_{\Rd} \int_{B(x,r)} \bigl((g(y) - g^r(y))^2\,dy \,dx \nonumber \\
& = \int_{\Rd} \int_{B(y,r)} \bigl((g(y) - g^r(y))^2\,dx \,dy \tag{by Tonelli's Theorem} \nonumber \\
& = \nu_d r^d \int_{\Rd}\bigl((g(y) - g^r(y))^2 \,dy \\
& \leq \nu_d r^{d + 2} \int_{\Rd} \norm{\nabla g(y)}^2 \,dy \label{eqn:sobolev_1_bound_4}
\end{align}
where the last line follows from Lemma \ref{lem:poincare_mollify}.

We now turn our attention to the second summand. Note that as $g^r \in C^{\infty}(\Rd)$, we may apply Theorem \ref{thm:taylor_expansion} and obtain
\begin{align*}
(g^r(x) - g^r(y))^2 & = \left(\int_{0}^{1} \nabla g^r(x + t(y - x)) \cdot (y - x) \,dt\right)^2 \\
& \leq \int_{0}^{1} \bigl(\nabla g^r(x + t(y - x)) \cdot (y - x)\bigr)^2 \,dt  \tag{Jensen's inequality} \\
& \leq \norm{y - x}^2 \int_{0}^{1} \norm{\nabla g^r(x + t(y - x))}^2 \,dt. \tag{Cauchy-Schwarz inequality}
\end{align*}
As a result, we have
\begin{align*}
\int_{\Rd} \int_{\Rd} \bigl((g^r(x) - g^r(y))^2 K_r(x,y) \,dy) \,dx & \leq \int_{\Rd} \int_{\Rd} \norm{y - x}^2  \int_{0}^{1} \norm{\nabla g^r(x + t(y - x))}^2 K_r(x,y) \,dt \,dy \,dx \\
& \leq r^{2} \int_{\Rd} \int_{\Rd} \int_{0}^{1} \norm{\nabla g^r(x + t(y - x))}^2 K_r(x,y) \,dt \,dy \,dx \\
& = r^{2} \int_{\Rd} \int_{0}^{1} \int_{\Rd} \norm{\nabla g^r(x + t(y - x))}^2 K_r(x,y) \,dx \,dt \,dy \\
& = r^{2} \int_{\Rd} \int_{0}^{1} \int_{\Rd} \norm{\nabla g^r(x + tz)}^2 K_r(z) \,dx \,dt \,dz \tag{$z = y - x$}
\end{align*}

where we write $K_r(z) = \mathbf{1}(\norm{z} \leq r)$ in an abuse of notation. Next, we note that
\begin{equation*}
\int_{\Rd} \norm{\nabla g^r(x + tz)}^2 \,dx = \int_{\Rd} \norm{\nabla g^r(x)}^2 \,dx \leq \int_{\Rd} \norm{\nabla g(x)}^2 \,dx
\end{equation*}
with the inequality following from Lemma \ref{lem:gradient_mollify_commute}. Therefore,
\begin{align}
r^{2} \int_{\Rd} \int_{0}^{1} \int_{\Rd} \norm{\nabla g^r(x + tz)}^2 K_r(z) \,dx \,dt \,dz & \leq r^{2} \int_{\Rd} K_r(z) \int_{0}^{1} \int_{\Rd} \norm{\nabla g(x)}^2 \,dx  \,dt \,dz \nonumber \\
& = \nu_d r^{2 + d} \int_{\Rd} \norm{\nabla g(x)}^2 \,dx. \label{eqn:sobolev_1_bound_3}
\end{align}
By \eqref{eqn:sobolev_1_bound_5}, \eqref{eqn:sobolev_1_bound_4} and \eqref{eqn:sobolev_1_bound_3}, we have that 
\begin{equation*}
\int_{\Rd} \int_{\Rd} (g(x) - g(y))^2 K_r(x,y) \,dx \,dy \leq 9 \nu_d r^{d + 2} \int_{\Rd} \norm{\nabla g(x)}^2 \,dx
\end{equation*}
Then by \ref{eqn:sobolev_1_bound_2}, $\int_{\Rd} \norm{\nabla g(x)}^2 \,dx \leq c \int_{\Xset} \norm{\nabla f(x)}^2 \,dx$, and the desired result of \eqref{eqn:sobolev_1_bound_1} follows.

\paragraph{Proof of~\eqref{eqn:eigenvalue_tail_bound}:}

For ease of notation we write $t = n^{1/d}$, and assume without loss of generality that $t$ is whole number. Let $\overline{G} = (\overline{X},\overline{E})$ be the lattice over $[0,1]^d$, formally
\begin{align*}
\overline{X} & = \bigl\{\overline{x}_k := \frac{1}{t}(k_1,\ldots,k_d): k \in [t]^d\bigr\} \\
\overline{E} & = \bigl\{(\overline{x}_k, \overline{x}_{\ell}): \norm{k - \ell}_1 = 1\bigr\}.
\end{align*}

By Theorem 1.1 of \textcolor{red}{Garcia Trillos and Slepcev}, with probability at least $1 - n^{-1}$ there exists a bijection $\pi: \overline{V} \to X$ such that
\begin{equation}
\label{eqn:transport_distance}
\max_{k \in [t]^d} \abs{\overline{x}_k - \pi(\overline{x}_k)} \leq c \left(\frac{\log n}{n}\right)^{1/d}
\end{equation}
Since our choice $r = \log^a(n) \cdot (\log n/n)^{1/d}$ implies $r = \omega(\left(\frac{\log n}{n}\right)^{1/d})$, for $n$ sufficiently large with probability $1 - n^{-1} = 1 - o(1)$ we have
\begin{equation}
\label{eqn:eigenvalue_tail_bound_pf1}
(\overline{x}_k,\overline{x}_{\ell}) \in \overline{E} \Longrightarrow (\pi(\overline{x}_k),\pi(\overline{x}_{\ell})) \in E.
\end{equation}
Let $\overline{L}$ be the Laplacian of $\overline{G}$, with eigenvalues $\overline{s}_1 < \cdots \leq \overline{s}_n$. If \eqref{eqn:eigenvalue_tail_bound_pf1} holds, then for all $x \in \Reals^n$,
\begin{equation*}
x^T \overline{L} x \leq x^T L x \Longrightarrow \overline{s}_j \leq s_j~~ \textrm{for all}~ j \in [n].
\end{equation*}
The eigenvalues of the $d$-dimensional lattice are explicitly known. In particular when $j = h^d$ for some whole number $h$, we have $s_j \geq \lambda_j = 4 d \sin^2(\pi h/2t) \geq \pi^2 h^2/ (4 t^2)$, with a similar inequality (for possible different constants) holding when $j^{1/d}$ is not integral; this shows~\eqref{eqn:eigenvalue_tail_bound}.


\paragraph{Proof of~\eqref{eqn:l2_to_empirical_norm}:}

To prove~\eqref{eqn:l2_to_empirical_norm} we will upper bound
\begin{equation*}
\mathbb{E}\biggl[\Bigl(\frac{1}{n}\sum_{i = 1}^{n}f^2(x_i)\Bigr)^2\biggr],
\end{equation*}
and then apply the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). We have
\begin{align*}
\mathbb{E}\biggl[\Bigl(\frac{1}{n}\sum_{i = 1}^{n}f^2(x_i)\Bigr)^2\biggr] & = \frac{(n-1)}{n}\mathbb{E}(f^2(x_1)) + \frac{\mathbb{E}[f^4(x_1)]}{n} \\
& \leq p_{\max}^2 \left(\norm{f}_{\Leb^2}^2 + \frac{\norm{f}_{\Leb^4}^4}{n}\right) \\
& \overset{(i)}{\leq} p_{\max}^2 \left(\norm{f}_{\Leb^2}^2 + c\frac{\norm{f}_{\Leb^2}^{(4 - d)}}{n}\right) \\
& \overset{(ii)}{\leq}  p_{\max}^2 \left(\norm{f}_{\Leb^2}^2 + c\norm{f}_{\Leb^2}^2\right).
\end{align*}
Here $(i)$ follows from two facts, the first being the embedding $W^{1,2}(\mathcal{X};R) \subseteq \Leb^q(\mathcal{X};R')$ for $q = 2d/(d - 2)$ when $d > 2$ and $q = \infty$ when $d \leq 2$ and some constant $R'$, and the second being a standard result on interpolation of $\Leb^p$ spaces, namely that since $f \in \Leb^2(\mathcal{X}) \cap \Leb^q(\mathcal{X})$, then 
\begin{equation*}
\norm{f}_{\Leb^4} \leq \norm{f}_{\Leb^2}^{\lambda} \cdot \norm{f}_{\Leb^q}^{1 - \lambda},~~\textrm{where}~ \frac{1}{4} = \frac{\lambda}{2} + \frac{1 - \lambda}{q}.
\end{equation*} 
Then $(ii)$ follows from our assumptions on $\norm{f}_{\Leb^2}^2$: either 
\begin{enumerate}[(a)]
	\item $d \leq 2$ and $\norm{f}_{\Leb^2}^2 \geq 1/n$ so $\norm{f}_{\Leb^2}^{(4 - d)} \leq \max(R^{4 - d}/n, \norm{f}_{\Leb^2}^2) \leq c\norm{f}_{\Leb^2}^2$, or
	\item $d = 3$ and $\norm{f}_{\Leb^2}^2 \geq n^{-2/(d - 2)}$.
\end{enumerate} 
Applying Lemma~\ref{lem:paley_zygmund} to  $\norm{f}_n^2$ yields~\eqref{eqn:l2_to_empirical_norm}.

\subsubsection{Step 3: Conclusion}

We note that for all possible values of $X \in \Xset^n$, under the null hypothesis $f = f_0 = 0$ and therefore $\beta = (f(x_1),\ldots,f(x_n)) = 0$ as well. Therefore by~\eqref{eqn:graph_spectral_type_I_error}, we have the following bound on Type I error:
\begin{equation}
\Ebb_{f_0}(\phi_{\mathrm{spec}}) = \mathbb{E}(\mathbb{E}_{\beta = 0}(\phi_{\spec}) | X) \leq \frac{1}{b^2}.
\end{equation}

Now, we bound Type II error under the assumption $f \in W^{1,2}(\mathcal{X};R)$ and 
\begin{equation}
\label{eqn:critical_radius_1}
\norm{f}_{\Leb^2}^2 \geq \epsilon^2 = c \cdot b \cdot (\log n)^{h(\beta,d)} \cdot n^{-4/(4 + d)}.
\end{equation}
We verify that the conditions required to apply Step 2 are satisfied. In particular, we choose $r = (\log^a n) \cdot (\log n/n)^{1/d}$, and observe that~\eqref{eqn:critical_radius_1} implies $\norm{f}_{\Leb^2}^2 \geq n^{-1}$ when $d \leq 2$ and $\norm{f}_{\Leb^2}^2 \geq n^{-2/(d - 2)}$ when $2 < d < 4$. We may therefore apply our conclusions in Step 2; namely, that for every possible choice of $f$ there exists a good set $\mathcal{E}_f \subseteq \Xset^n$ with $\Pbb(\mathcal{E}_f) \geq 1 - o(1)$ such that each of \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, and \eqref{eqn:l2_to_empirical_norm} hold for all $X \subseteq \mathcal{E}_f$. Choosing $\kappa = n^{2d/(4 + d)}$ to balance the squared bias and variance terms on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius}, we have that for all $X \subseteq \mathcal{E}_f$
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\beta;G)}{ns_{\kappa}} & \leq 2bn^{-4/(4+d)} + \frac{n r^{d + 2}}{s_{\kappa}} \tag{by \eqref{eqn:continuous_to_discrete_sobolev_norm}} \\
& \leq 2bn^{-4/(4+d)} + c\cdot\frac{n r^{d + 2}}{n^{4/(4+d)}} \tag{by \eqref{eqn:eigenvalue_tail_bound}} \\
& \leq 2bn^{-4/(4+d)} + c\cdot(\log^{a(d + 2) + (1 + 2/d)}n)\cdot n^{-4/(4+d)} \\
& \leq \frac{1}{10}\norm{f}_{\Leb^2} \tag{for a suitably large choice of $c$ in \eqref{eqn:critical_radius_1}}\\
& \leq \frac{1}{n}\sum_{i = 1} \beta_i^2. \tag{by \eqref{eqn:l2_to_empirical_norm}}
\end{align*}
We conclude that for all $X \subseteq \mathcal{E}_f$, the inequality \eqref{eqn:fixed_graph_testing_critical_radius} is satisfied with respect to $\beta = (f(x_1),\ldots,f(x_n))$ and $G = G_{n,r}$. As a result the worst-case Type II error is bounded
\begin{equation*}
\sup_{\substack{f \in W^{1,2}(\mathcal{X;R}) \\ \norm{f}_{\Leb^2} \geq \epsilon}}\mathbb{E}_{f}(1 - \phi_{\spec}) \leq \sup_{\substack{f \in W^{1,2}(\mathcal{X;R}) \\ \norm{f}_{\Leb^2} \geq \epsilon}} \mathbb{E}\bigl[\mathbb{E}_{\beta}(1 - \phi_{\spec}|X \in \mathcal{E}_f)\bigr] + o(1) \leq \frac{3}{b} + o(1).
\end{equation*}

\subsection{Proof of Proposition~\ref{prop:L4_testing_rate}}

Let $T_{\mathrm{mean}} = \frac{1}{n}\sum_{i = 1}^{n} y_i^2$. The expectation of $T_{\mathrm{mean}}$ is
\begin{equation*}
\Ebb(T_{\mathrm{mean}}) = \mathbb{E}(f^2(x_1)) + 1,
\end{equation*}
and the variance can be upper bounded
\begin{equation*}
\Var(T_{\mathrm{mean}}) \leq \frac{1}{n}(3 + p_{\max} \norm{f}_{\Leb^4}^4 + p_{\max}\norm{f}_{\Leb^2}^2) = \frac{c}{n}.
\end{equation*}
Since $\mathbb{E}(f^2(x_1)) \geq p_{\min} \epsilon^2$, when $\epsilon^2 \gtrsim b\cdot n^{-1/2}$ we can apply Chebyshev's inequality to obtain the claimed result.

\subsection{Proof of Theorem~\ref{thm:twosample_sobolev_testing_rate}}

As mentioned previously, to prove Theorem~\ref{thm:twosample_sobolev_testing_rate}, we relate the two sample model to the following regression model: we observe
$X = \{x_1,\ldots,x_n\} \sim \mu$ (where we recall $\mu = (P + Q)/2$), and associated labels
\begin{equation*}
a_i = 
\begin{cases}
1, & \textrm{with probability}~ \frac{p(x_i)}{p(x_i) + q(x_i)} \\
-1, & \textrm{with probability}~ \frac{q(x_i)}{p(x_i) + q(x_i)}
\end{cases}
\end{equation*}
where $a_i$ is conditionally independent of $x_j,a_j$ given $x_i$. 

\textcolor{red}{TODO:} Complete the above argument.

Our analysis will trace a similar path to the proof of Theorem~\ref{thm:sobolev_testing_rate}, proceeding according to the following steps:
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $a$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded. One of these functionals will be a measure of eigenvector incoherence.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta^{(p)} = (\beta_1^{(p)},\ldots,\beta_n^{(p)}) \in \Reals^n$, $\beta^{(q)} = (\beta_1^{(q)},\ldots,\beta_n^{(q)}) \in \Reals^n$ be non-negative signals over the vertices $V$. We observe labels $a = (a_1,\ldots,a_n)$ according to the model
\begin{equation*}
a_i = 
\begin{cases*}
1, & \textrm{with probability $\frac{\betap_i}{\betap_i + \betaq_i}$} \\
-1, & \textrm{with probability $\frac{\betaq_i}{\betap_i + \betaq_i}$}
\end{cases*}
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec}^{(2)} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec}^{(2)} = \1\{T_{\spec}^{(2)} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $\Pi_{\max}(\kappa;G)$ be a measure of the incoherence of the eigenvectors $v_1,\ldots,v_\kappa$, given by
\begin{equation*}
\Pi_{\max}(\kappa;G) := \max_{i = 1,\ldots,n} \left\{\sum_{k = 1}^{\kappa} v_{k,i}^2\right\}
\end{equation*}
In Lemma~\ref{lem:twosample_fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}^{(2)}$. Our bound on the Type II error will be stated as a function of $\Pi_{\max}(\kappa;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$ and the smoothness functional $S_2(\beta;G)$. We use the notation $\varDelta^{(p,q)} = (\betap - \betaq)/(\betap + \betaq)$.

\begin{lemma}
	\label{lem:twosample_fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\betap = \betaq$, the Type I error of $\phi_{\spec}^{(2)}$ is upper bounded
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_I_error}
		\mathbb{E}_{\betap,\betap}(\phi_{\spec}^{(2)}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} Suppose $\Pi_{\max}(\kappa;G) \leq 1$. Then for any $b$, $\betap$ and $\betaq$ such that
		\begin{equation}
		\label{eqn:twosample_fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} (\varDelta_i^{(p,q)})^2 \geq \frac{1}{1 - \Pi_{\max}(\kappa;G)}\left(2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\varDelta;G)}{ns_{\kappa}}\right)
		\end{equation}
		the Type II error of $\phi_{\spec}^{(2)}$ is upper bounded,
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_II_error}
		\mathbb{E}_{\betap,\betaq}(1 - \phi_{\spec}^{(2)}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:twosample_fixed_graph_testing}.}

To prove Lemma~\ref{lem:twosample_fixed_graph_testing} we will compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}^{(2)}$. The inequalities \eqref{eqn:twosample_graph_spectral_type_I_error} and \eqref{eqn:twosample_graph_spectral_type_II_error} can then be computed using Chebyshev's inequality in a manner very similar to the proof of Lemma~\ref{lem:fixed_graph_testing}, and we omit the details. Let $w_i := a_i - \vardeltapq_{i}$, and note that $\Ebb(w_i) = 0$ and
\begin{equation*}
\Ebb(w_i^2) = \frac{\betap_i \betaq_i}{(\betap_i + \betaq_i)^2} = 1 - (\vardeltapq_{i})^2,~~ \Ebb(w_i^3) = \frac{1}{2}(1 - (\vardeltapq_{i})^2)\vardeltapq_{i}
\end{equation*}

\vspace{.2 in}

\textit{Mean of $T_{\spec}^{(2)}$}:

We have
\begin{align*}
\Ebb(T_{\spec}^{(2)}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\vardeltapq}{v_k}^2 + \Ebb\bigl(\dotp{w}{v_k}^2 + 2 \dotp{w}{v_k} \dotp{\vardeltapq}{v_k}\bigr)\right) \\
& = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{1 - (\vardeltapq)^2}{v_k^2} + \dotp{\vardeltapq}{v_k}\right) \\
& \geq \frac{\kappa}{n} + \left(\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} - \dotp{(\vardeltapq)^2}{v_k^2}\right)
\end{align*}
When $\vardeltapq = 0$, this equals $\kappa/n$. Otherwise, we have the following upper bound:
\begin{align*}
\sum_{k = 1}^{\kappa} \dotp{(\vardeltapq)^2}{v_k^2}  & = \sum_{i = 1}^{n} (\vardeltapq_{i})^2 \left(\sum_{k = 1}^{\kappa} v_{k,i}^2 \right)\\
& \leq \Pi_{\max}(\kappa,G) \cdot \sum_{i = 1}^{n} (\vardeltapq_{i})^2 = \Pi_{\max}(\kappa,G) \cdot \norm{\vardeltapq}_2^2.
\end{align*}
Additionally, $\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} \geq \norm{\vardeltapq}_2^2 - S_2(\beta;G)/s_{\kappa}$ by reasoning given in the proof of Lemma~\ref{lem:fixed_graph_testing}, and as a result 
\begin{equation*}
\Ebb(T_{\spec}^{(2)}) \geq \frac{\kappa}{n} -  \frac{\bigl(1 - \Pi_{\max}(\kappa;G)\bigr)}{n} \norm{\vardeltapq}_2^2 - \frac{S_2(\beta;G)}{ns_{\kappa}}
\end{equation*}

\vspace{.2 in}
\textit{Variance of $T_{\spec}^{(2)}$:}

We have
\begin{align}
\Var(T_{\spec}^{(2)}) & = \frac{1}{n^2}\Var(2\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w} + \dotp{V_{\kappa}V_{\kappa}^Tw}{w}) \nonumber \\
& = \frac{1}{n^2}\Bigl\{4\underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w})}_{=: V_1} + 2\underbrace{\Cov(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w},\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=: K_1} + \underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=:V_2}\Bigr\}, \label{eqn:var}
\end{align}
and we now upper bound each of the three terms on the right hand side of the previous display.

\textbf{Upper bound on $V_1$:}
Let $\Sigma := \Cov(w)$ be the covariance matrix of $w$. Noting that $\Sigma \preceq I$, we have
\begin{equation}
\label{eqn:var1}
\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w}) =(\vardeltapq)^T V_{\kappa}V_{\kappa}^T \Sigma V_{\kappa}V_{\kappa}^T \vardeltapq \leq \norm{V_{\kappa}V_{\kappa}^T\vardeltapq}^2.
\end{equation}

\textbf{Upper bound on $K_1$:}
Noting that $\Ebb(\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w}) = 0$, we have that
\begin{align}
K_1 & = \Ebb\left[\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w} \dotp{V_{\kappa}V_{\kappa}^T w}{w}\right] \nonumber \\
& = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{i' = 1}^{n} \Ebb\left[w_i w_j (V_{\kappa}V_{\kappa}^T)_{ij} w_{i'} (V_{\kappa}V_{\kappa}^T\vardeltapq)_i\right] \nonumber \\
& = \sum_{i = 1}^{n} \Ebb\left[w_i^3\right] (V_{\kappa}V_{\kappa}^T)_{ii} (V_{\kappa}V_{\kappa}^T \vardeltapq)_i \nonumber \\
& = \frac{1}{2}\sum_{i = 1}^{n} (1 - (\vardeltapq_{i})^2)(\vardeltapq_{i}) (V_{\kappa} V_{\kappa}^T)_{ii}  (V_{\kappa}V_{\kappa}^T\vardeltapq)_i \nonumber \\
& \leq \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \left(\sum_{i = 1}^{n} (\vardeltapq_{i})^2 (V_{\kappa} V_{\kappa}^T)_{ii}^2\right)^{1/2} \nonumber \\
& \leq \Pi(\kappa,G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq}  \label{eqn:var2} 
\end{align}

\textbf{Upper bound on $V_2$:}
$V_2$ is a variance of a sum, which we re-express as the sum of covariances:
\begin{align*}
V_2 & = \Var(\dotp{V_{\kappa}V_{\kappa}^T w}{w}) \\
& = \sum_{i,j,i',j' = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij} (V_{\kappa}V_{\kappa}^T)_{i'j'} \Cov(w_i w_j, w_{i'} w_{j'}).
\end{align*}
This covariance will be non-zero only when $i = i' \neq j = j'$, $i = j' \neq j = i'$, or $i = i' = j = j'$, and therefore
\begin{align}
V_2 & = 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 \Var(w_iw_j) + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \Var(w_i^2) \nonumber\\
& \leq 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \leq 3 \mathrm{tr}\bigl((V_{\kappa}V_{\kappa}^T)^2\bigr) = 3\kappa. \label{eqn:var3}
\end{align}

Now, plugging \eqref{eqn:var1}, \eqref{eqn:var2}, and \eqref{eqn:var3} back into \eqref{eqn:var}, we obtain
\begin{equation}
\label{eqn:var_5}
\Var(T_{\spec}^{(2)}) \leq \frac{1}{n^2}\left\{4\norm{V_{\kappa}V_{\kappa}^T \vardeltapq}^2 + \Pi(\kappa;G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq} + 3\kappa \right\}.
\end{equation}


\subsubsection{Step 2: Bounding neighborhood graph functionals}

Note that since $p,q \in W^{1,2}(\mathcal{X};R)$, we have $f := p - q \in W^{1,2}(\mathcal{X};2R)$. Therefore the bounds~\eqref{eqn:continuous_to_discrete_sobolev_norm} and \eqref{eqn:l2_to_empirical_norm} apply to $S_2(\varDelta,G_{n,r})$ and $(1/n)\sum_{i = 1}^{n}\varDelta_i^2$, respectively. The bound~\eqref{eqn:eigenvalue_tail_bound} continues to apply to $s_{\kappa}$. What remains is to upper bound the incoherence parameter
\begin{equation}
\label{eqn:pi_max}
\Pi_{\max}(\kappa,G_{n,r}) \leq \frac{1}{2}~~\textrm{when}~ \kappa = n^{2/5}~\textrm{and}~ r = \log^a n \cdot \left(\frac{\log n}{n}\right)
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$.

Our proof of~\eqref{eqn:pi_max} will proceed by relating $G_{n,r}$ to a graph which satsifies a strict notion of incoherence, the chain graph. When $G = \overline{G}$, the incoherence parameter $\Pi_{\max}$ is upper bounded
\begin{equation}
\Pi_{\max}(\kappa,\overline{G}) \leq \frac{\kappa}{n}.
\end{equation}
To prove that the weaker bound \eqref{eqn:pi_max} holds with respect to $G_{n,r}$, we will show that the two graphs $\overline{G}$ and $G_{n,r}$ are $\delta$-spectrally similar, meaning
\begin{equation}
\label{eqn:spectral_similarity}
(1 - \delta_n) x^T L x \leq x^T \overline{L} x \leq (1 + \delta_n) x^T L x~~\textrm{for all}~x \in \Reals^n, \delta_n = c \cdot (\log n)^{h(a,1) + da}\cdot \frac{\log n}{n}
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$. The proof of~\eqref{eqn:spectral_similarity} relies heavily on bounding the transport distance between $X$ and $\overline{X}$, i.e. \eqref{eqn:transport_distance}. Then the following technical Lemma along with \eqref{eqn:spectral_similarity} will imply \eqref{eqn:pi_max}.
\begin{lemma}
	\label{lem:pi_max_pf_1}
	Let $G$ satisfy~\eqref{eqn:spectral_similarity} for a given $0 \leq \delta < 1$. Then for any $R \gtrsim n \delta^{1/2} (s_{n}^{1/2} \vee 1)$ and $k \leq n/8$,
	\begin{equation*}
	v_{k,i}^2 \lesssim \frac{1}{n}\left(R + \frac{n^4 \delta^2 s_{n}^2}{R^3}\right) \quad \textrm{for every $i = 1,\ldots,n$.}
	\end{equation*} 
\end{lemma}

\textcolor{red}{TODO}: Explain how Lemma~\ref{lem:pi_max_pf_1} implies~\eqref{eqn:pi_max}.


At a high level, Lemma~\ref{lem:pi_max_pf_1} is proved through repeated applications of the Davis-Kahan Theorem. The full proof is long and we delay presenting it until Section~\ref{sec:technical_lemma_proofs}.

\subsubsection{Step 3: Conclusion}

With Lemma~\ref{lem:twosample_fixed_graph_testing} as well as \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, \eqref{eqn:l2_to_empirical_norm} and \eqref{eqn:pi_max} in hand, the proof of Theorem~\ref{thm:twosample_sobolev_testing_rate} follows by similar reasoning to the conclusion of Theorem~\ref{thm:sobolev_testing_rate}.

\section{Proofs of Technical Results}
\label{sec:technical_lemma_proofs}

Here we give in full detail the proof of \eqref{eqn:spectral_similarity} and Lemma~\ref{lem:pi_max_pf_1}.



\subsection{Proof of~\eqref{eqn:spectral_similarity}}

Assuming~\eqref{eqn:transport_distance} holds and $r$ is chosen according to~\eqref{eqn:pi_max}, we have already shown that the upper bound in~\eqref{eqn:spectral_similarity} holds for $\delta = 0$ (see~\eqref{eqn:eigenvalue_tail_bound_pf1}). Now we need only to show the lower bound, which we will do following these steps:
\begin{enumerate}[(i)]
	\item First, we consider the case of two fixed graphs $G$ and $\wt{G}$, and establish a Poincare inequality. In other words, we show that if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$, then the Laplacian matrix $L$ cannot be much larger $\wt{L}$, in the usual sense of partial ordering of matrices.
	\item Now, we treat the case of $G = G_{n,r}$ and $\wt{G} = \overline{G}$. We assume that~\eqref{eqn:transport_distance} holds, and exhibit such a mapping between edges in $G_{n,r}$ and paths in $\overline{G}$. The resulting Poincare inequality will imply~\eqref{eqn:spectral_similarity}.
\end{enumerate}
In step (ii) we assume that~\eqref{eqn:transport_distance} holds. Since \eqref{eqn:transport_distance} holds with probability at least $1 - c/n$, this will imply that~\eqref{eqn:spectral_similarity} holds with at least the same probability.

\subsubsection{Step 1: Poincare inequality}

Let $G = (V,E)$ and $\wt{G} = (V,\wt{E})$ be two graphs over a common vertex set $V$. The set $\mathcal{P}_{\wt{G}}$ consists of all paths $P$ over $\wt{G}$, meaning all tuples
\begin{equation*}
P = (\wt{e}_1,\wt{e}_2,\ldots,\wt{e}_m),~~m \in \mathbb{N}~, \wt{e}_j \in \wt{E}~ \forall~ j \in [m], (\wt{e}_j)_2 = (\wt{e}_{j + 1})_1 \forall~j \in [m - 1].
\end{equation*}
Let $\gamma:E \to \mathcal{P}_{\wt{G}}$ be a mapping from edges in $G$ to paths in $\wt{G}$. The maximum path length $M(\gamma)$ is defined
\begin{equation*}
M(\gamma) = \max_{e \in E} \abs{\gamma(e)}
\end{equation*} 
and the bottleneck $b(\gamma)$ is defined
\begin{equation*}
b(\gamma) = \max_{\wt{e} \in \wt{E}} \abs{\{e \in E:  \wt{e} \in \gamma(e)\}}.
\end{equation*}
As mentioned previously, the Laplacian matrix $L$ cannot be much larger than $\wt{L}$ if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$. Formally, we will show the following: for any $\gamma: E \to \mathcal{P}_{\wt{E}}$,
\begin{equation}
\label{eqn:poincare_inequality}
x^T L x \leq M(\gamma)\cdot b(\gamma) \cdot (x^T \wt{L} x)~\forall{x \in \Reals^n}
\end{equation} 

\paragraph{Proof of~\eqref{eqn:poincare_inequality}:}

Letting $c \in \Reals$, we will use the notation $G \preceq c \cdot  \wt{G}$ as shorthand for
\begin{equation*}
x^T L x \leq c \cdot  x^T \wt{L} x,~~\textrm{for all $x \in \Reals^n$}.
\end{equation*}


Let $G_e = (V, \set{e})$ and $P_e = (V, \set{\widetilde{e}: \widetilde{e} \in \gamma(e)})$ be the graphs associated with $e$ and $\gamma(e)$, respectively. By Lemma \ref{lem: path_poincare}, we have
\begin{equation*}
G_{e} \preceq \abs{P_e} P_e
\end{equation*}
Summing over all $e \in E_G$, we obtain
\begin{align*}
G & \preceq \sum_{e \in E_G} \abs{P_e} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} \sum_{e \in E_G} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} b_{\gamma}\cdot \widetilde{G}
\end{align*}

\subsubsection{Step 2: Mapping $E_{G_{n,r}}$ to $\mathcal{P}_{\overline{G}}$}
We assume there exists a mapping $\pi:\overline{X} \to X$ which satisfies \eqref{eqn:transport_distance}. We will apply the Poincare inequality to $\pi^{-1}(G_{n,r})$, the isomorphism of $G_{n,r}$ induced by the mapping of vertices $\pi^{-1}: X \to \overline{X}$. We will map a given $(\overline{x}_{k},\overline{x}_{\ell}) \in \pi^{-1}(E)$ to a shortest path $P \in \mathcal{P}_{\overline{G}}$ (measured by Manhattan distance) between $\overline{x}_k$ and $\overline{x}_{\ell}$. Recalling the notation $t = n^{1/d}$ and letting
\begin{equation*}
s_i = \frac{1}{t}(\textrm{sign}(k_i - \ell_i),0,\ldots,0),~~\textrm{for $i = 1,\ldots,d$,}
\end{equation*}
our mapping $\gamma: \pi^{-1}(E) \to \mathcal{P}_{\overline{G}}$ is given by
\begin{align*}
\gamma((\overline{x}_{k},\overline{x}_{\ell})) = \bigl(& (\overline{x}_k, \overline{x}_{k} + s_1), (\overline{x}_{k} + s_1,\overline{x}_{k} + 2s_1),\ldots,(\overline{x}_{k} + (\abs{k_1 - \ell_1} - 1)s_1, \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1)) \\
& (\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1), \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + s_2),\ldots,(\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + (\abs{k_2 - \ell_2} - 1)s_2, \overline{x}_{k}  + \frac{1}{t}((k_1 - \ell_1) + (k_2 - \ell_2)) \\
& \vdots \\
& (\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + s_d,\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + 2s_d), \ldots, (\overline{x}_k + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + (\abs{k_d - \ell_d} - 1)s_d, \overline{x}_{\ell})\bigr)
\end{align*}
As a result of~\eqref{eqn:transport_distance} we obtain the following bounds on $M(\gamma)$ and $b(\gamma)$:
\begin{align}
M(\gamma) & \leq c n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right) \label{eqn:maximum_path_length}\\
b(\gamma) & \leq c \left(n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right)\right)^{2d}, \label{eqn:bottleneck}
\end{align}
which we now prove. 

\paragraph{Proof of~\eqref{eqn:maximum_path_length}:}

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \pi^{-1}(E)$. Then
\begin{align*}
\gamma(\overline{x}_k, \overline{x}_{\ell})) = \norm{\overline{x}_k - \overline{x}_{\ell}}_1 & \leq \sqrt{d} \norm{\overline{x}_k - \overline{x}_{\ell}}_2 \\
& \leq \sqrt{d}\bigl(\norm{\overline{x}_k - \pi(\ol{x}_{k})}_2 + \norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + \norm{\overline{x}_\ell - \pi(\ol{x}_{\ell})}_2\bigr) \\
& \leq \sqrt{d}\left(\norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{by \eqref{eqn:transport_distance}} \\
& \leq \sqrt{d}\left(r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{since $(\pi(\ol{x}_k),\pi(\ol{x}_{\ell})) \in E$}
\end{align*} 

\paragraph{Proof of~\eqref{eqn:bottleneck}:} 

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)$ for some $e = (\overline{x}_i, \overline{x}_j) \in \pi^{-1}(E)$.  Then, by the construction of $\gamma$,
\begin{equation}
\label{eqn:bottleneck_pf_1}
\norm{\overline{x}_k - \overline{x}_i}_2 \wedge \norm{\overline{x}_\ell - \overline{x}_j}_2  \leq \norm{\overline{x}_j - \overline{x}_i}_2
\end{equation}
As shown in the preceding paragraph, assuming~\eqref{eqn:transport_distance} we have
\begin{equation*}
\norm{\overline{x}_j - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}
\end{equation*}
and therefore by~\eqref{eqn:bottleneck_pf_1},
\begin{align*}
\abs{\set{e \in E: (\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)}} & \leq \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \cdot \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \\
& \leq \left(n^{1/d}\left( r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right)\right)^2.
\end{align*}

Putting the pieces together, we have that when $d = 1$ and $r = (\log n)^a (\log n/n)$, then
\begin{equation*}
G_{n,r} \preceq c \cdot (\log n)^{(1/d + a) + 2 + 2da}\cdot \overline{G}
\end{equation*}
with probability at least $1 - o(1)$.

\subsection{Proof of Lemma~\ref{lem:pi_max_pf_1}}

Write $\overline{L} = \overline{V}\overline{S} \overline{V}^T$ for the spectral decomposition of $\overline{L}$. The proof of Lemma~\ref{lem:pi_max_pf_1} will proceed according to the following steps.
\begin{enumerate}
	\item We show that the incoherence $\max_{i} v_{k,i}^2$ can be upper bounded
	\begin{equation}
	\label{eqn:incoherence_proof_1}
	\max_{i} v_{k,i}^2 \leq \frac{1}{n} \left(\sum_{j = 1}^{n} \abs{\dotp{v_k}{\overline{v}_j}}\right)^2
	\end{equation}
	replacing the norm $\norm{v_k^2}_{\infty}$ by the inner products $\dotp{v_k}{\overline{v}_j}$.
	\item Let $I(k,r) = \set{j \geq 0: \abs{j - k} \leq r}$. Using Davis-Kahan, we establish that the following upper bounds
	\begin{equation}
	\label{eqn:incoherence_proof_2}
	\sum_{j \not\in I(k,r)}^{n} (\dotp{v_k}{\overline{v}_j})^2 \leq 400 \frac{n^4 \delta^2 s_{\max}^2}{R^4}
	\end{equation}
	hold for each $k \leq n/8$ and any $r \geq 8k\sqrt{\delta}$. 
	\item We carefully upper bound the $L_1$ norm present in \eqref{eqn:incoherence_proof_1} given the various bounds on $L_2$ norm we've established in \eqref{eqn:incoherence_proof_2}.
\end{enumerate}

\paragraph{Step 1: Proof of~\eqref{eqn:incoherence_proof_1}.}

We can re-express $v_k$ in the basis $(\overline{v}_j)$ as
\begin{equation*}
v_k = \sum_{j = 1}^{n} \dotp{v_k}{\overline{v}_j} \overline{v}_j,
\end{equation*}
whereupon the bound~\eqref{eqn:incoherence_proof_1} follows from the incoherence property of the chain
\begin{equation*}
\max_{i,k = 1,\ldots,n} \overline{v}_{k,i}^2 \leq \frac{1}{n}.
\end{equation*}
\paragraph{Step 2: Proof of~\eqref{eqn:incoherence_proof_2}.}

Let $\widetilde{L} = L + H$. By Davis Kahan, we have that
\begin{equation}
\label{eqn:davis_kahan}
\sum_{j \not\in I(k,r)} (\dotp{v_k}{\overline{v}_j})^2 \leq \left(\frac{\norm{H}_{\textrm{op}}}{\min{\abs{\overline{s}_j - s_k}:j \in I(k,r)}}\right)^2
\end{equation}
To upper bound the numerator, we use~\eqref{eqn:spectral_similarity} to get
\begin{equation*}
\norm{H}_{op} \leq \delta s_{n}.
\end{equation*}

To lower bound the denominator, we note
\begin{align}
\abs{\overline{s}_j - s_k} & \geq \abs{\overline{s}_j - \overline{s}_k} - \abs{\overline{s}_k - s_k} \nonumber \\
& \geq \abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k. \label{eqn:incoherence_proof_8}
\end{align}
The eigenvalues of the chain graph are well known to be
\begin{equation*}
\overline{s}_j = 2\left(1 - \cos\left(\frac{j\pi}{n}\right)\right) = 4\sin^2\left(\frac{k\pi}{n}\right) ~~\textrm{for $k = 0,\ldots,n - 1$.}
\end{equation*}
By Taylor expansion, for $k \leq j \leq n/(2\pi)$ we have
\begin{equation}
\label{eqn:incoherence_proof_5}
\overline{s}_j - \overline{s}_k = 2\left(\cos\left(\frac{k\pi}{n}\right) - \cos\left(\frac{j\pi}{n}\right)\right) \geq \frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
and when $j \geq n/(2\pi)$ we have $\overline{s}_j - \overline{s}_k \geq 2(\cos\left(\frac{\pi}{8}\right) - \cos\left(\frac{1}{2}\right) > .09$. Additionally since $\sin(x) \leq x$ for all $x \geq 0$, we have
\begin{equation}
\label{eqn:incoherence_proof_6}
\overline{s}_k \leq 4\frac{k^2\pi^2}{n^2}.
\end{equation}
Combining~\eqref{eqn:incoherence_proof_5},\eqref{eqn:incoherence_proof_6}, and the lower bound $\abs{k - j} \geq R \geq 8k\sqrt{\delta}$, we have that
\begin{equation}
\label{eqn:incoherence_proof_7}
\abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k \geq .04\frac{(k - j)^2\pi^2}{n^2} - 4 \delta\frac{k^2\pi^2}{n^2} \geq .02\frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
The result follows from \eqref{eqn:incoherence_proof_7}, \eqref{eqn:incoherence_proof_8} and \eqref{eqn:davis_kahan}.

\paragraph{Step 3: $L_1$ norm to $L_2$ norm.}

Let $u \in \Reals^n$, and suppose we know that the $L_2$ norm of $u$ is bounded,
\begin{equation}
\label{eqn:incoherence_proof_3}
\norm{u}_2^2 \leq 1.
\end{equation}
Under no other conditions on $u$, the upper bound $\norm{u}_1 \leq \sqrt{n}$ is the best that can be hoped for (achieved when $u = (n^{-1/2},\ldots,n^{-1/2})$). However, suppose we also know that for some $B_2^2 \geq B_3^2 \geq \ldots \geq B_n^2$, we have that there exists some $R \in [n]$ such that
\begin{equation}
\label{eqn:incoherence_proof_4}
\sum_{j = r}^{n} v_j^2 \leq B_r^2 ~~\textrm{for each $r = R,\ldots,n$.}
\end{equation}
Clearly, if $B_n^2 \leq \frac{1}{n}$ then $u$ cannot be equal to $(n^{-1/2},\ldots,n^{-1/2})$. We might hope for more general improvements on the bound $\norm{u}_1 \leq \sqrt{n}$ if $B_R^2$ is quite small once $R$ gets sufficiently large. The following Lemma gives such a result.
\begin{lemma}
	\label{lem:pi_max_pf_1_util_1}
	Suppose $u \in \Reals^n$ satisfies \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} for some $1 = B_1^2 = B_2^2 \geq \ldots \geq B_n^2 \geq B_{n+1}^2 = 0$. Assume additionally that there exists an $R \in [n]$ such that
	\begin{equation}
	\label{eqn:incoherence_util_1}
	B_r^2 - B_{r + 1}^2 \leq \frac{1}{r}~\textrm{for each $r = R, R+1,\ldots, n - 1$.}
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:incoherence_util_2}
	\norm{u}_1 \leq \sqrt{R}\sqrt{1 - B_R^2} + \sum_{j = R}^{n} \sqrt{B_j^2 - B_{j+1}^2}.
	\end{equation}
\end{lemma}
\begin{proof}
	Set
	\begin{equation*}
	(u_\star)_j^2 = 
	\begin{cases*}
	\frac{1}{R}(1 - B_R^2), ~ j \leq R \\
	B_j^2 - B_{j + 1}^2, j > R
	\end{cases*}
	\end{equation*}
	so that $\norm{u_\star}_1$ is equal to the right hand side of~\eqref{eqn:incoherence_util_2}. We now prove by contradiction that $u_\star$ maximizes $\norm{\cdot}_1$ under the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4}. Suppose this were not true, and that the maximum is achieved for some $u \neq u_{\star}$.
	\begin{itemize}
		\item since $\norm{u}_1 > \norm{u_{\star}}$ then there exists some index $j$ such that
		\begin{equation}
		\label{eqn:incoherence_util_3}
		u_j > (u_{\star})_j
		\end{equation} 
		\item by~\eqref{eqn:incoherence_proof_3} $\norm{u}_2 \leq 1$. Since $\norm{u_{\star}}_2 = 1$, by \eqref{eqn:incoherence_util_3} there must exist some index $k$ such that
		\begin{equation}
		\label{eqn:incoherence_util_4}
		(u_{\star})_k > u_k.
		\end{equation} 
		Choose $k$ to be the largest of all such indices.
		\item suppose $k < j$ and $j > R$. Since $k$ was chosen to be the largest such index which satisfied~\eqref{eqn:incoherence_util_4}, clearly $v_i \geq (u_{\star})_i$ for all $i > j$, and by \eqref{eqn:incoherence_util_3} $u_j > (u_{\star})_j$. But then 
		\begin{equation*}
		B_j^2 = \sum_{i = j}^{N} (u_{\star})_i^2 < \sum_{i = j}^{N} u_i^2 
		\end{equation*}
		and so $v$ does not satisfy~\eqref{eqn:incoherence_proof_4}. Therefore either $k > j$ or $j \leq R$.
		\item In either case, we have
		\begin{equation*}
		u_j > (u_{\star})_j \geq (u_{\star})_k > u_k.
		\end{equation*}
		Let $j \leq l < k$ be the largest index for which $v_j > (a_{\star})_j$. 
		
		We now construct a vector $\wt{u}$ which satisfies the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} such that $\norm{\wt{u}}_1 > \norm{u}_1$. Once we have shown this, we will have established a contradiction, and the proof of Lemma~\ref{lem:pi_max_pf_1_util_1} will be complete. Let $\wt{u} = (\wt{u}_i)$ be given as follows:
		\begin{equation*}
		\wt{u}_i = 
		\begin{cases*}
		u_i, ~~\textrm{if $i \neq k,l$}, \\
		u_{\star,k}, ~~\textrm{if $i = k$}, \\
		\sqrt{u_l^2 - (u_{\star,k}^2 - a_k^2)}, ~~\textrm{if $i = l$.}
		\end{cases*}
		\end{equation*}
		Clearly $\norm{\wt{u}}_2 = \norm{u}_2 = 1$ and so $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_3}. To see that $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4} we separate the analysis into cases. When $r \geq l + 1$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		When $l < r \leq k$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq \sum_{i = r}^{k} u_{\star,i}^2 + \sum_{i = k +1}^{n} u_{i}^2 \leq B_r^2 - B_{k + 1}^2 + B_{k + 1}^2 = B_r^2
		\end{equation*}
		Finally, when $R \leq r \leq l$, we have
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq u_l^2 - (u_{\star,k}^2 - u_k^2) + (u_{\star,k}^2) + \sum_{i \neq k,l} u_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		Therefore $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4}. Finally, we have that
		\begin{equation*}
		\norm{\wt{u}}_1 = \norm{u}_1 + (u_{\star,k}  - u_k) - (\sqrt{u_l^2 - (u_{\star,k}^2 - u_k^2)} - u_l) > \norm{u}_1 + (u_{\star,k}  - u_k) - (u_{\star,k}  - u_k) = \norm{u}_1,
		\end{equation*}
		so we have established the desired contradiction.
	\end{itemize}
\end{proof}

\paragraph{Putting the pieces together.}
We apply Lemma~\ref{lem:pi_max_pf_1_util_1} to the vector $u = (u_i)_{i = 1}^{n}$, where
\begin{equation*}
u_i = \frac{1}{\sqrt{2}}\left(\abs{\dotp{v_k}{\overline{v}_{k - i}}} + \abs{\dotp{v_k}{\overline{v}_{k + i}}}\right).
\end{equation*}
Note the following:
\begin{itemize}
	\item $\norm{u}_2 \leq 1$.
	\item By \eqref{eqn:incoherence_proof_2}, the vector $u$ satisfies the constraint~\eqref{eqn:incoherence_proof_4} with
	\begin{equation*}
	B_r^2 = 400\frac{n^4\delta^2s_{n}^2}{r^4} ~~\textrm{when $r \geq 8 k \sqrt{\delta}$.}
	\end{equation*}
	\item Taylor expanding $f(r + 1) = (r + 1)^{-4}$ around $r$, we have that
	\begin{equation*}
	B_r^2 - B_{r + 1}^2 \leq 3200 \frac{n^4 \delta^2 s_{n}^2}{r^5} ~~\textrm{when $r \geq 2$.}
	\end{equation*}
	Therefore, the sequence $(B_r)$ satisfies \eqref{eqn:incoherence_util_1} for all $r$ large enough such that 
	\begin{equation*}
	r \geq 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}
	\end{equation*}
	\item Since $u$ and $(B_r)$ satisfy the constraints \eqref{eqn:incoherence_proof_3}, \eqref{eqn:incoherence_proof_4} and \eqref{eqn:incoherence_proof_5}, we may apply Lemma~\ref{lem:pi_max_pf_1_util_1} to $u$, obtaining that
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{v_j}{\overline{v}_k}} = \sqrt{2}\norm{u}_1 \leq \sqrt{2R} + 60\sqrt{2} n^2 \delta s_{n} \sum_{r = R}^{n} r^{-5/2}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}$. Bounding sum by integral, we have that if $R \geq 2$ then
	\begin{equation*}
	\sum_{r = R}^{n} r^{-5/2} \leq \int_{R - 1}^{n - 1} x^{-5/2} \,dx \leq \frac{2^{3/2}2}{3} R^{-3/2}.
	\end{equation*}
	Plugging this into the previous expression, we arrive at
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{\overline{v}_j}{v_k}} \leq \sqrt{R} + 60 \sqrt{2} n^2 \delta s_{\max} \frac{2^{3/2}2}{3R^{3/2}}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{\max}}$. Lemma~\ref{lem:pi_max_pf_1} then follows from~\eqref{eqn:incoherence_proof_1}.
\end{itemize}

\section{Auxiliary Results}

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.4, Theorem 1]
	\label{thm:evans_extension}
	Assume $U$ is bounded and $\partial U$ is $C^1$. Select a bounded open set $V$ such that $U \subset \subset V$ ($U$ is compactly contained in $V$). Then there exists a bounded linear operator $E: W^{1,2}(U) \to W^{1,2}(\Rd)$ such that for each $u \in W^{1,2}(U)$:
	\begin{enumerate}
		\item $Eu = u$ a.e. in $U$,
		\item $Eu$ has support within $V$, and 
		\item 
		\begin{equation*}
		\norm{Eu}_{W^{1,2}(\Rd)} \leq C \norm{u}_{W^{1,2}(\Rd)}
		\end{equation*}
		the constant $C$ depending only on $U$ and $V$.
	\end{enumerate}
\end{theorem}

For $u \in W^{1,2}(\Rd)$ and $x \in \Rd$, write $\nabla u(x) = (D^{e_1}(x),\ldots,D^{e_d}(x)$, for the gradient of $u$.

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.8.1, Theorem 2]
	\label{thm:evans_poincare}
	There exists a constant $C$, depending only on $d$, such that
	\begin{equation*}
	\norm{u - u^r}_{L^2(B(x,r))} \leq Cr\norm{\nabla u}_{L^2(B(x,r))}
	\end{equation*}
\end{theorem}

\begin{theorem}[Taylor expansion.]
	\label{thm:taylor_expansion}
	For any function $u \in C^1$, and any $x,y \in \Rd$, 
	\begin{equation*}
	u(y) - u(x) = \int_{0}^{1} \nabla(u(x + t(y - x))) \cdot (y - x) \,dt
	\end{equation*}
\end{theorem}

\begin{lemma}
	\label{lem:poincare_mollify}
	For any function $g \in W^{1,2}(\Rd)$ compactly supported in a bounded open set $V \subset \Rd$, we have
	\begin{equation}
	\label{eqn:poincare_mollify}
	\int_{\Rd} (g(x) - g^r(x))^2 \,dx \leq r^2 \int_{\Rd} \norm{\nabla g(x)}^2 \,dx
	\end{equation}
\end{lemma}
\begin{proof}
	This Lemma is essentially a reproduction of part of the proof of the Rellich-Kondrachov Compactness Theorem from \textcolor{red}{Evans}. Note that it is sufficient to prove in the case when $g$ is smooth. To see this, for the moment assume \eqref{eqn:poincare_mollify} holds for all $u \in C^{\infty}(V)$, and let $g \in W^{1,2}(V)$. By Theorem \ref{thm:local_approx_smooth_functions}, we may take a sequence $g_m \in C^{\infty}(V)$ such that
	\begin{equation*}
	\norm{g - g_m}_{L^2(V)} \to 0, \quad \textrm{and} \quad \norm{\nabla g - \nabla g_m}_{L^2(V)} \to 0,
	\end{equation*}
	Then we have
	\begin{align*}
	\int_{\Rd} (g(x) - g^r(x))^2 \,dx & \leq \int_{\Rd} (g(x) - g_m(x))^2 \,dx + \int_{\Rd}(g_m(x) - g_m^r(x))^2 \,dx + \int_{\Rd}(g_m^r(x) - g^r(x))^2 \,dx \\
	& \leq \int_{\Rd} (g(x) - g_m(x))^2 \,dx + r^2 \int_{\Rd} \norm{\nabla g_m(x)}^2 \,dx + \int_{\Rd}(g_m^r(x) - g^r(x))^2 \,dx
	\end{align*}
	and taking the limit as $m$ goes to infinity, the right hand side converges to $\int_{\Rd} \norm{\nabla g(x)}^2 \,dx$. 
	
	It remains to show \eqref{eqn:poincare_mollify} in the case where $g$ is smooth. In this case, we have
	\begin{align*}
	g^{r}(x) -  g(x) & = \frac{1}{r^d} \int_{B(x,r)} \kappa \left(\frac{x - z}{r}\right)\bigl(g(z) - g(x)\bigr) \,dz \\
	& = \int_{B(0,1)} \kappa(y) \bigl(g(x - ry) - g(x)\bigr) \,dy \\
	& = \int_{B(0,1)} \kappa(y) \int_{0}^{1} \frac{d}{dt} \bigl(g(x - try) \bigr) \,dt \,dy \\
	& = -r \int_{B(0,1)} \kappa(y) \int_{0}^{1} \bigl(\nabla g (x - try) \bigr) \cdot y \,dt \,dy.
	\end{align*} 
	Therefore, by Jensen's and Cauchy-Schwarz inequalities, we have
	\begin{align*}
	\int_{\Rd} (g(x) - g^r(x))^2 \,dx & \leq r^2 \int_{\Rd} \int_{B(0,1)} \kappa(y) \int_{0}^{1} \norm{\nabla g (x - try)}^2 \norm{y}^2 \,dt \,dy \,dx \\
	& \leq r^2 \int_{B(0,1)} \kappa(y) \int_{0}^{1} \int_{\Rd} \norm{\nabla g (x - try)}^2 \,dx \,dt \,dy \\
	& = r^2 \int_{\Rd} \norm{\nabla g(z)}^2 \,dz
	\end{align*}
\end{proof}

The following theorem is Theorem 1 in Section 5.3 of \textcolor{red}{Evans}.
\begin{theorem}[Local approximation by smooth functions.]
	\label{thm:local_approx_smooth_functions}
	Assume $U$ is bounded, and $u \in W^{k,p}(U)$ for some $1 \leq p < \infty$. Then there exists functions $u_m \in C^{\infty}(U) \cap W^{k,p}(U)$ such that
	\begin{equation*}
	\norm{u_m - u}_{W^{k,p}(U)} \overset{m}{\to} 0.
	\end{equation*}
\end{theorem}

\begin{lemma}
	\label{lem:gradient_mollify_commute}
	For any $u \in W^{1,2}(\Rd)$, we have
	\begin{equation*}
	\int_{\Reals^d} \norm{\nabla u^r(x)}_2^2 \,dx \leq \int_{\Reals^d} \norm{\nabla u(x)}_2^2 \,dx
	\end{equation*}
\end{lemma}
\begin{proof}
	Observe that $\norm{\nabla u(x)}_2^2 = \sum_{j = 1}^{d} (D^{e_j}u(x))^2 $. Therefore,
	\begin{align*}
	\int_{\Rd} \norm{\nabla u^r(x)}_2^2 \,dP(x) & = \sum_{j = 1}^{d} \int_{\Rd} (D^{e_j}u^r(x))^2 \,dx \\
	& = \sum_{j = 1}^{d} \int_{\Rd} ((D^{e_j}u)^r(x))^2 \,dx
	\end{align*}
	where the second equality follows from equation (1) in Section 5.3 of \textcolor{red}{Evans}. Then, for any $v \in L^2(\Rd)$, we have that
	\begin{align*}
	\abs{v^r(x)} & = \int_{\Rd} \kappa_r^{1/2}(x - y) \kappa_r^{1/2}(x - y) v(y) \,dy \\
	& \leq \left(\int_{\Rd} \kappa_r^(x - y) \,dy\right)^{1/2} \left(\int_{\Rd} \kappa_r(x - y) v^2(y) \right)^{1/2} \\
	& = \left(\int_{\Rd} \kappa_r(x - y) v^2(y) \right)^{1/2}
	\end{align*}
	and therefore
	\begin{align*}
	\int_{\Rd} (v^r(x))^2 \,dx & \leq \int_{\Rd} \int_{\Rd } \kappa_r(x - y) v^2(y) \,dy \,dx \\
	& = \int_{\Rd} \int_{\Rd } \kappa_r(x - y) v^2(y) \,dy \,dx \\
	& = \int_{\Rd} v^2(y) \,dy
	\end{align*}
	
	Applying this to $D^{e_j}u \in L^2(\Rd)$, we have that
	\begin{equation*}
	\sum_{j = 1}^{d} \int_{\Rd} ((D^{e_j}u)^r(x))^2 \,dx \leq \sum_{j = 1}^{d} \int_{\Rd} ((D^{e_j}u(x))^2 \,dx = \int_{\Rd} \norm{\nabla u(x)}^2 \,dx.
	\end{equation*}
\end{proof}

\begin{lemma}
	\label{lem:paley_zygmund}
	Let $0 < p < q$, and let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. Then, for all $0 \leq \overline{s} \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund}
	\mathbb{P}(Z > \overline{s} \mathbb{E}(Z^p)) \geq \left[(1 - \overline{s}^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
\end{lemma}

\begin{lemma}[Poincare inequality for path graphs.]
	\label{lem: path_poincare}
	Fix $m \geq 0$. For vertices $V = \set{1, \ldots,m}$ define the path $P(1 \to m) = ((1,2),(2,3),\ldots, (m-1,m))$ and $G_{(1,m)}$ to be the graph consisting only of an edge between $1$ and $m$. Then,
	\begin{equation*}
	(m - 1) \cdot P(1 \to m) \succeq G_{(1,m)}
	\end{equation*}
\end{lemma}

\end{document}