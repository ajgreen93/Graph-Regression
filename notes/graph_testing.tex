\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Graph Testing}
\author{Alden Green}
\date{\today}
\maketitle

This document details the current status of the graph testing project. We divide by section based on the testing problem under consideration. At the end of each section, we list some areas we are interested in investigating. All proofs are left until the end.

We establish some notation that we will use throughout. A graph $G = (V,W)$ consists of a collection of vertices $V = \{v_1,\ldots,v_n\}$, and a weight matrix $W \in \Reals^{\abs{V} \times \abs{V}}$ with $(ij)$th entry $W_{ij}$ encoding affinity between $v_i$ and $v_j$. We will sometimes write $W_G$ to make it clear which graph a weight matrix is associated with. When the weight matrix $W_G$ consists only of $0$s and $1$s, we will use the equivalent representation $G = (V,E(G))$ when convenient; here $E(G) = V \times V \cap \{(v_i,v_j): W_{ij} = 1\}$. 

We will focus our attention on neighborhood graphs, and operators associated with them. For a kernel $K:\Reals \to \Reals$ and radius $r > 0$, we define $G_{n,K}=(V,W)$ to be the neighborhood graph with vertices $V = X$, and weights $W_{ij} = K(\norm{x_i - x_j}/r)$. Of particular interest to us will be the \emph{random geometric graph} $G_{n,r}$, the neighborhood graph constructed using the uniform kernel $K(z) := \1\{z \leq 1\}$. The weight matrix $W(G_{n,r})$ consists only of $0$s and $1$s, and we will thus use the equivalent representation $G_{n,r} = (X,E(G_{n,r}))$.

The primary operator we are interested in (at this point) is the graph Laplacian. Let $D_G$ be the diagonal degree
matrix of a graph $G$, with entries $D_{uu} := \sum_{v \in V} W_{uv}$. The graph Laplacian is then $L_G = D_G - W_G$. Let $\Lambda(G)$ consist of the eigenvalues $0 = \lambda_1(G) \leq \lambda_2(G) \leq \cdots \leq \lambda_n(G)$ of $L_G$, with $v_k(G) = (v_{k,1}(G),\ldots,v_{k,n}(G)) \in \Reals^n$ denoting the eigenvector corresponding to the $k$th eigenvalue $\lambda_k(G)$.  

\section{Regression goodness-of-fit testing.}

Suppose we observe samples $(y_i,x_i)$ for $i = 1,\ldots,n$, where conditional on $X$ the responses $Y = \{y_1,\ldots,y_n\}$ are assumed to follow the model
\begin{equation}
\label{eqn:regression_known_variance}
y_i = f(x_i) + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation} 
and our task is to distinguish
\begin{equation*}
\mathbf{H}_0: f = f_0 := 0 \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: f \neq f_0
\end{equation*}

We will evaluate our performance using worst-case risk: for a given function class $\mathcal{H}$ and test function $\phi: \Reals^n \to \set{0,1}$, let
\begin{equation*}
\mathcal{R}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, f \neq f_0} \Ebb_f(1 - \phi).
\end{equation*}
(Not that the expectation here may taken over the random responses $Y$ when $X$ is assumed to be fixed, or jointly over the randomness of $(X,Y)$ when $X$ is assumed random.) The worst-case risk may be quite close to $1$ unless we enforce some separation between null and alternative spaces. A more realistic measure of performance is therefore
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, \norm{f - f_0}_{\Leb^2} \geq \epsilon} \Ebb_f(1 - \phi).
\end{equation*}
For a given level $\alpha$ of tolerated error, the minimax critical radius
\begin{equation*}
\epsilon^{\star}(\mathcal{H},\alpha) := \inf \Bigl\{\epsilon > 0: \inf_{\phi} R_{\epsilon}(\mathcal{H}) \leq \alpha \Bigr\}
\end{equation*}
then provides a lens through which one may examine the hardness of testing over $\mathcal{H}$. We will sometimes suppress the dependence of the minimax critical radius on $\alpha$ and simply write $\epsilon^{\star}(\mathcal{H})$.

\subsection{Test Statistics.}
We list the test statistics we use for the regression testing problem.

\paragraph{Eigenvector projection test statistic.}
The graph Laplacian eigenvector projection test is a truncated-series test. Suppose we are given a graph $G = (X,W)$ defined on the sample points $X$.  Letting $\kappa$ be some integer between $1$ and $n$, our test projects $Y$ onto the eigenvectors $v_1(G),\ldots,v_{\kappa}(G)$, and then takes the empirical norm of this projection to be the test statistic. Formally, let
\begin{equation*}
\Pi_{\kappa,G}(f) := \frac{1}{n}\sum_{k = 1}^{\kappa} \Biggl(\sum_{i = 1}^{n} f(x_i) v_{k,i}(G)\Biggr) v_{k}(G),~~ T_{\mathrm{spec}}(G) := \norm{\Pi_{\kappa,G}(f)}_n^2
\end{equation*}
where $\norm{\theta}_n^2 := \frac{1}{n} \sum_{i = 1}^{n} \theta_i^2$ for $\theta \in \Reals^n$.
Then the test $\phi_{\spec}(G) := \1\{T_{\spec}(G) \geq \tau\}$ rejects the null hypothesis when the test statistic $T_{\mathrm{spec}}(G)$ is greater than a pre-specified cutoff $\tau$.

\subsection{Random design.}
In the random design case, we assume that each design point $x_i$ is sampled independently from some distribution $P$. Formally, our model for the samples is the following:
\begin{equation}
\label{eqn:regression_random_design_known_variance}
x_1,\ldots,x_n \overset{\textrm{i.i.d}}{\sim} P,~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)~\textrm{and}~ \varepsilon \perp X,~~ y_i = f(x_i) + \varepsilon_i.
\end{equation}

In Theorem~\ref{thm:sobolev_testing_rate_order1} we show that under some typical regularity conditions on $P$, the neighborhood graph Laplacian eigenvector projection test $\phi_{\textrm{spec}}(G_{n,r})$ is a minimax optimal test over the Sobolev ball $H^1(\mathcal{X};R)$ for $d = 1,2,3$ and $\Xset = [0,1]^d$. Let $p_d$ equal $3/4$ when $d = 2$ and equal $1/d$ for $d \geq 3$.

\begin{theorem}
	\label{thm:sobolev_testing_rate_order1}
	Suppose we observe samples $(x_i,y_i)_{i = 1}^{n}$ according to the model~\eqref{eqn:regression_random_design_known_variance}. Let $R > 0$ and $b \geq 1$ be fixed constants, and $d = 1,2$ or $3$. Suppose that $P$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p(x)$ bounded away from zero and infinity, 
	\begin{equation*}
	0 < p_{\min} < p(x) < p_{\max} < \infty,~~ \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	and the test $\phi_{\spec}(G_{n,r})$ is performed with parameter choices
	\begin{equation*}
	c \frac{(\log n)^{p_d}}{n^{1/d}} \leq r(n) \leq n^{-4/((4 + d)(2+d))}, ~\kappa = (nR)^{2d/(4 + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	for $c$ a constant that depends only on $\Xset, p_{\min}$ and $p_{\max}$. Then the following statements holds for every $n$ sufficiently large: there exists constants $c_1,c_2$ which do not depend on $n,b$ or $R$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate_order1}
	\epsilon^2 \geq c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4 + d)}\} \cdot n^{-4/(4 + d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_order1_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}(G_{n,r}); H^1(\mathcal{X};R)) \leq \frac{c_2}{b}.
	\end{equation}
\end{theorem}

The testing problem in the random design case with known variance fundamentally changes in the low-smoothness regime, where the number of bounded derivatives is small compared to the dimension $d$. In the case where $f \in H^1(\Xset;R)$ is assumed to have one derivative which is bounded in $\Leb^2(\Xset)$ norm, this transition occurs at $d = 4$. We shall discuss testing in the low-smoothness regime shortly. First, let's consider the situation when we are willing assume $f$ possesses additional derivatives bounded in $\Leb^2$ norm.

Theorem~\ref{thm:sobolev_testing_rate_order1} establishes that a truncated series test using graph eigenvectors performs just as well, in a minimax sense, as a truncated series test using a classic Fourier series; for analysis of the latter test as well as derivation of the minimax rate of nonparametric goodness-of-fit testing over Sobolev classes see \textcolor{red}{(Ingster)}. Intuitively, we can understand this by thinking of the graph eigenvectors as estimates of eigenfunctions of a density-weighted continuum Laplacian operator. When the density is uniform, and the domain $\Xset$ is the unit cube, under appropriate boundary conditions these eigenfunctions are themselves elements of the Fourier basis. When the density is merely close to uniform, the eigenfunctions no longer belong to the Fourier basis; nevertheless, they share enough properties with the latter that the resulting minimax rate is unchanged. Theorem~\ref{thm:sobolev_testing_rate_order1} shows that the additional error due to using \textit{estimates} of these eigenfunctions does not fundamentally change the minimax rate. 

When we are additionally willing to assume $H^s(\Xset)$, we know (again from \textcolor{red}{Ingster}) that the minimax testing rate is $n^{-4s/(4s + d)}$. Now, however, our estimates of the eigenvectors no longer track the derivatives finely enough at the boundaries of $\Xset$ to establish the same minimax rate over all $H^s(\mathcal{X})$. However, if we assume $f$ and its derivatives are compactly supported on $\Xset$, we recover the usual rate. Theorem~\ref{thm:sobolev_testing_rate} presents our formal result, that $\phi_{\textrm{spec}}(G_{n,r})$ is a minimax optimal test over the Sobolev balls $H_0^s(\mathcal{X};R)$ whenever $4s > d$ and $\Xset = [0,1]^d$.

\begin{theorem}
	\label{thm:sobolev_testing_rate}
	Suppose we observe samples $(x_i,y_i)_{i = 1}^{n}$ according to the model~\eqref{eqn:regression_random_design_known_variance}. Let $R > 0$, $b,s,d \geq 1$ be fixed constants, with $s$ and $d$ integers. Suppose that $P$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p \in C^{s-1}(\Xset;p_{\max})$ for some $p_{\max} < \infty$, and further that $p(x)$ is bounded away from zero, i.e. there exists $p_{\min} > 0$ such that 
	\begin{equation*}
	p_{\min} < p(x),~~ \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds for all $n$ sufficiently large: if the test $\phi_{\spec}(G_{n,r})$ is performed with parameter choice
	\begin{equation*}
	n^{-1/(2(s-1) + d)} \leq r(n) \leq n^{-4/((4s + d)(2+d))}, ~\kappa = (Rn)^{2d/(4s + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists constants $c_1,c_2$ which do not depend on $n,b$ or $R$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate}
	\epsilon^2 \geq c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4s + d)}\} \cdot n^{-{4s}/(4s + d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; H_0^{s}(\mathcal{X};R)) \leq \frac{c_2}{b}.
	\end{equation}
\end{theorem}

The restriction that $p$ be supported on the unit cube is mostly for convenience. If instead $p$ is supported on any compact set with Lipschitz boundary, the Theorem statement should hold (up to constants), with only a few modifications to our proofs. However, we do not work through the details.

The restriction $4s > d$ is a fundamental consequence of the tightness of the Sobolev embedding theorem. When $4s \geq d$ the compact embedding
\begin{equation*}
H^{s}(\mathcal{X}) \subseteq \Leb^4(\mathcal{X}) 
\end{equation*}
does not hold (as it does when $d < 4s$). However, if we directly assume $f \in \mathcal{L}_d^4(\mathcal{X};R)$ (regardless of $d$) we obtain the following result.
\begin{proposition}
	\label{prop:L4_testing_rate}
	If $f \in \Leb^4(\mathcal{X};R)$, there exists a constant $c$ such that if
	\begin{equation}
	\label{eqn:L4_testing_rate}
	\epsilon^2 > c^2 b \cdot n^{-1/2}
	\end{equation}
	then the test
	\begin{equation*}
	\phi_{\mathrm{mean}} = \1\Bigl\{\frac{1}{n}\sum_{i = 1}^{n} y_i^2 \geq 1\Bigr\}
	\end{equation*}
	has worst-case risk
	\begin{equation*}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; H^1(\mathcal{X};R)) \lesssim \frac{1}{b}. 
	\end{equation*}
\end{proposition}
Note that when $4s > d$ the critical radius~\eqref{eqn:sobolev_testing_rate} is smaller than \eqref{eqn:L4_testing_rate}. 

\subsection{Fixed Design}
Frequently in nonparametric regression problems, it is assumed that the design points are fixed, and, in the simplest case, uniformly spaced on the unit cube. For convenience in this setting we let $n = N^d$ for some integer $N$. The fixed grid design $\wb{X}$ consists of $n$ total evenly spaced grid points on $[0,1]^d$,
\begin{equation*}
\wb{X} := \Bigl\{\Bigl(\frac{2i_1 - 1}{2N},\ldots,\frac{2i_d - 1}{2N}\Bigr): i \in [N]^d\Bigr\}
\end{equation*}
Suppose we observe $n$ samples according to regression model
\begin{equation}
\label{eqn:grid_regression_model}
y_i = f(\wb{x}_i) + \varepsilon_i, ~\varepsilon_i \overset{\mathrm{i.i.d}}{\sim} \mathcal{N}(0,1) ~~\textrm{for each $i \in [N]^d$,}
\end{equation}
Clearly model~\eqref{eqn:grid_regression_model} bears some similarity with model~\eqref{eqn:regression_random_design_known_variance}, the main difference being that the design $\wb{X}$ is now fixed instead of random. 

We study the behavior of $\phi_{\spec}(\wb{G})$, where $\wb{G}$ is the lattice
\begin{equation*}
\wb{G} := \bigl(\wb{X},\wb{E}\bigr),~~ \wb{E} := \set{(\wb{x}_i,\wb{x}_j): i,j \in [N]^d, \norm{i - j}_1 = 1},
\end{equation*}
a very natural graph to build given our assumed grid design. Note that for a given $d$, we also have the recursive relation
\begin{equation*}
\wb{G} := \wb{G}_1 \otimes \ldots \otimes \wb{G}_1
\end{equation*}
where the tensor product is taken $d$ total times, and $\wb{G}_1$ is the $1$d lattice (i.e. the path) graph on $N$ vertices. For this reason it will be convenient to change notation slightly, and index by $d$-tuples of numbers rather than by numbers. In particular, we index the eigenvalues and eigenvectors of $L_{\wb{G}}$ as
\begin{equation*}
\lambda_k(\wb{G}) = \prod_{j = 1}^{d} \lambda_{k_j}(\wb{G}_1), v_{k,i}(\wb{G}) = \prod_{j = 1}^{d} v_{k_j,i_j}(\wb{G}_1)~~\textrm{for $k,i \in [N]^d$.}
\end{equation*}
Then, for a given $\kappa \in [n]$, letting $K = \kappa^{1/d}$, we see that our graph spectral test statistic may be written equivalently as
\begin{equation*}
T_{\spec}(\wb{G}) = \frac{1}{n} \sum_{k \in [K]^d} \Biggl(\sum_{i \in [N]^d} y_i v_{k,i}(G)\Biggr)^2
\end{equation*}

We see the test $\phi_{\spec}(\wb{G})$ achieves the same rate over the Holder ball $C^{1}(\Xset;L)$ when $d \leq 4$, as the test $\phi_{\spec}(G_{n,r})$ does over the Holder ball $C^1(\Xset;L)$.
\begin{theorem}
	\label{thm:holder_testing_rate_grid}
	Let $L > 0$ be a fixed constant, and let $d \leq 4$. Suppose we observe data according to the model~\eqref{eqn:grid_regression_model}. Then when the test $\phi_{\spec}(\wb{G})$ is performed with the parameter choices
	\begin{equation*}
	\kappa = (Ln)^{2d/(4 + d)},~~ \tau(b) = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	for some $b \geq 1$, the following statement is true: there exists a constant $c$ such that for 
	\begin{equation}
	\label{eqn:holder_testing_rate_grid_1}
	\epsilon^2 \geq c \cdot \max\{L^{d/(4+d)}, L^{d/(4 + d) + 1}\} \cdot b n^{-4/(4 + d)}
	\end{equation}
	the worst-case risk of $\phi_{\spec}(\wb{G})$ on $C^{1}(\Xset;L)$ is upper bounded
	\begin{equation}
	\label{eqn:holder_testing_rate_grid_2}
	\mathcal{R}_{\epsilon}\Bigl(\phi_{\spec}(\wb{G});C^{1}(\Xset;L)\Bigr) \leq \frac{4}{b^2} + \frac{8}{b\sqrt{2\kappa}}.
	\end{equation}
\end{theorem}

As was the case with the neighborhood graph in the random design setting, the grid Sobolev norm fails to track the higher order derivatives of $f$ near the boundary of $\Xset$. We therefore will again insist that $f$ be compactly supported within $\Xset$. Under this condition, we find that the test $\phi_{\spec}(\wb{G})$ achieves the same rate over the Holder ball $C_c^{s}(\Xset;L)$ when $4s \geq d$ as did the test $\phi_{\spec}(G_{n,r})$ over the Holder ball $C_c^s(\Xset;L)$.

\begin{theorem}
	\label{thm:holder_testing_rate_grid_higher_order}
	Let $L > 0$ be a fixed constant, $s \geq 2$ be a fixed integer, and let $d \leq 4s$.  Suppose we observe data according to the model~\eqref{eqn:grid_regression_model}. Then when the test $\phi_{\spec}(\wb{G})$ is performed with the parameter choices
	\begin{equation*}
	\kappa = (Ln)^{2d/(4s + d)},~~ \tau(b) =  \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*} 
	for some $b \geq 1$, the following statement is true: there exists a constant $c$ such that for any
	\begin{equation*}
	\epsilon^2 \geq c L^2 b n^{-4s/(4s + d)},
	\end{equation*}
	the worst-case risk of $\phi_{\spec}(\wb{G})$ is upper bounded
	\begin{equation*}
	\mathcal{R}_{\epsilon}\Bigl(\phi_{\spec}(\wb{G});C_c^{s}(\Xset;L)\Bigr) \leq \frac{4}{b^2} + \frac{8}{b\sqrt{2\kappa}}.
	\end{equation*}
\end{theorem}

Roughly speaking, we have shown that in the high-smoothness regime $4s > d$, testing over Holder classes in the grid design and random design cases is an equally difficult problem from the minimax perspective. The same is not true for the corresponding Sobolev spaces. When $2s > d$, the Sobolev spaces $H^s(\Xset;L)$ embed compactly into (lower-order) Holder classes, implying that every equivalence class $u \in H^s(\Xset;L)$ contains a favored representer which is bounded and Holder continuous, with bounded Holder norm. When $2s < d$, the situation is less nice; the spaces $H^s(\Xset;L)$ now contain (equivalence classes of) bump functions of arbitrarily large height and small width. By placing such bump functions at design points, it is possible to make the sampling model~\eqref{eqn:grid_regression_model} contain arbitrarily little information about the overall behavior of the function $f$ over $\Xset$. It is not hard to construct a function $f \in H^s(\Xset;L)$ for which, say, $\norm{f}_{\Leb^2(\Xset)} = 1$ yet $f(\wb{x}) = 0$ for all $\wb{x} \in \wb{X}$, implying that the critical radius $\epsilon^{\star}(H^s(\Xset;L)) = \Omega(1)$. We note that this is not an issue in the random design setting, because the design points are (randomly) chosen only after the function $f$ is fixed.

\textcolor{red}{(TODO): Make this discussion more rigorous. Perhaps appeal to the concept of precise representatives defined in (Evans and Gariepy).}

The above discussion leaves hope that when $2s > d$, consistent testing might still be possible. We can in fact go beyond this and show that the graph spectral test $\phi_{\spec}(\wb{G})$ achieves the same rate over the Sobolev space $\wt{H}^s(\Xset;L)$ as the test $\phi_{\spec}(G_{n,r})$ did over $H_0^s(\Xset;L)$.

\begin{theorem}
	\label{thm:sobolev_testing_rate_grid}
	Let $L > 0$ be a fixed constant, $s \geq 1$ be a fixed integer, and let $2s > d$.  Suppose we observe data according to the model~\eqref{eqn:grid_regression_model}. Then when the test $\phi_{\spec}(\wb{G})$ is performed with the parameter choices
	\begin{equation*}
	\kappa = n^{2d/(4s + d)},~~ \tau(b) =  \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*} 
	for some $b \geq 1$, the following statement is true: there exists a constant $c$ such that for any
	\begin{equation*}
	\epsilon^2 \geq c L^2 b n^{-4s/(4s + d)},
	\end{equation*}
	the worst-case risk of $\phi_{\spec}(\wb{G})$ is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_grid_1}
	\mathcal{R}_{\epsilon}\Bigl(\phi_{\spec}(\wb{G});\wt{H}^s(\Xset;L)\Bigr) \leq \frac{4}{b^2} + \frac{8}{b\sqrt{2\kappa}}
	\end{equation}
	for all $n$ sufficiently large.
\end{theorem}

\subsubsection{Low-smoothness regime with fixed design}
In the low-smoothness regime $d > 4s$, the coupling between empirical norm over $\wb{X}$ and the continuum $\Leb^2$ norm is sufficiently weak that the discretization error can become the dominant source of error, as we show in Lemma~\ref{lem:holder_testing_rate_grid_low_smoothness_lb}.

\begin{lemma}
	\label{lem:holder_testing_rate_grid_low_smoothness_lb}
	For any $L > 0$ and any integers $s$ and $d$, there exists a function $g \in C^s(\Xset;L)$ such that
	\begin{equation*}
	\norm{g}_{\Leb^2}^2 \geq \frac{L^2}{2^d \pi^{2s} (d + 1)^{2s}}n^{-2s/d} 
	\end{equation*}
	but $g(\wb{x}) = 0$ for all $\wb{x} \in \wb{X}$. 
\end{lemma}
Lemma~\ref{lem:holder_testing_rate_grid_low_smoothness_lb} implies the critical radius over the Holder classes $C^s(\Xset;L)$ must be at least on the order of $\epsilon^{\star}(C^s(\Xset;L)) \gtrsim n^{-s/d}$. When $4s \geq d$, this term is neglible relative to $n^{-2s/(4s + d)}$, and we arrive at the ``usual'' rates of minimax testing we see in Theorems~\ref{thm:holder_testing_rate_grid} and~\ref{thm:holder_testing_rate_grid_higher_order}. On the other hand when $4s < d$, this becomes the dominant term, and we can no longer achieve the typical rate $\epsilon^2 \asymp n^{-4s/(4s + d)}$. 

In this setting, when we enforce the wider radius $\epsilon^2 \asymp n^{-2s/d}$, Proposition~\ref{prop:holder_testing_rate_grid_low_smoothness_ub} shows that a very simply test has small worst-case risk $\mathcal{R}_{\epsilon}$. 

\begin{proposition}
	\label{prop:holder_testing_rate_grid_low_smoothness_ub}
	Let $L > 0$ be a fixed constant, $s \geq 1$ a fixed integer, and let $d \geq 4s$. Suppose we observe data according to the grid design regression model~\eqref{eqn:grid_regression_model}. Then when the test $\phi_{\spec}(\wb{G})$ is performed with the parameter choices
	\begin{equation*}
	\kappa = n,~~ \tau(b) = 1 + b\sqrt{\frac{2}{n}}
	\end{equation*}
	for some $b \geq 1$, the following statement is true: there exists a constant $c$ such that for any 
	\begin{equation*}
	\epsilon^2 \geq c b L^2 n^{-2s/d}
	\end{equation*}
	the worst case risk of $\phi_{\spec}(\wb{G})$ is upper bounded
	\begin{equation*}
	\mathcal{R}_{\epsilon}(\phi_{\spec}(\wb{G}); C^s(\Xset;L)) \leq \frac{4}{b^2} + \frac{8}{b\sqrt{2n}}
	\end{equation*}
\end{proposition}

\begin{itemize}
	\item The test statistic $T_{\spec}(\wb{G})$ is simply the empirical norm of $Y$. 
	\item Together, Lemma~\ref{lem:holder_testing_rate_grid_low_smoothness_lb} and Proposition~\eqref{prop:holder_testing_rate_grid_low_smoothness_ub} characterize the minimax rate of the grid design regression testing problem when $4s > d$.
	\item This minimax rate does not match the upper bound established in Proposition~\ref{prop:L4_testing_rate}---the fixed design problem is harder. 
	\item Note that the usual minimax rate of estimation error over Holder classes is always $n^{-2s/(2s + d)}$, regardless of the relation between $s$ and $d$, and regardless of whether loss is measured in $\Leb^2$-norm or empirical norm. The explanation for this is that the rate $n^{-2s/(2s + d)}$ is always larger than $n^{-2s/d}$, and so the discretization error is never the dominant source of error in estimation. This reveals an interesting distinction between the testing and estimation problems, as testing is easy enough in a statistical sense that error incurred by the discrete nature of the problem may be the bottleneck.
\end{itemize}



\subsection{Analysis}

To prove Theorem~\ref{thm:sobolev_testing_rate}, we show that there exists a high-probability set $E \subseteq \Xset^n$ such that conditional on $X \in E$, the test $\phi_{\spec}(G_{n,r})$ has small risk. Since $G_{n,r}$ is a function only of $X$ and not of $Y$, this amounts to reasoning about the behavior of the test $\phi_{\spec}$ over a fixed graph $G = (X,E)$, where we observe
\begin{equation}
\label{eqn:fixed_graph_regression_model}
y_i = \beta_i + \varepsilon_i,~~\varepsilon_i \sim \mathcal{N}(0,1)
\end{equation}
for some fixed $\beta \in \Reals^n$.  In Lemma~\ref{lem:fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}(G)$. Our bound on the Type II error will be stated as a function of $\beta^T L^s \beta$--a measure of the smoothness the signal $\beta$ displays over the graph $G$--as well as the $\kappa$th eigenvalue $\lambda_{\kappa}$.

\begin{lemma}
	\label{lem:fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer. Suppose we observe data according to the model~\eqref{eqn:fixed_graph_regression_model}, and perform the test $\phi_{\spec}(G)$ with $\tau(b) = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}$.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}(G)$ is upper bounded
		\begin{equation}
		\label{eqn:graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{2}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b \geq 1$ and $\beta$ such that
		\begin{equation}
		\label{eqn:fixed_graph_testing_critical_radius_1}
		\norm{\Pi_{\kappa,G}(\beta)}_n^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}}
		\end{equation}
		the Type II error of $\phi_{\spec}(G)$ is upper bounded,
		\begin{equation}
		\label{eqn:graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{2}{b^2} + \frac{8}{b\sqrt{2\kappa}}.
		\end{equation}
		In particular if
		\begin{equation}
		\label{eqn:fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{\beta^T L^s \beta}{n\lambda_{\kappa}^s}
		\end{equation}
		then~\eqref{eqn:fixed_graph_testing_critical_radius_1} and thus~\eqref{eqn:graph_spectral_type_II_error} follow.
	\end{enumerate}
\end{lemma}

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $d \geq 4$, is it true that there does not exist a uniformly consistent
	test over the Sobolev ball $W_d^{1,2}(\mathcal{X};R)$?
	\item Assume the distribution $P$ is supported on a manifold $\mathcal{M}$ of intrinsic dimension $s < d$. Does $\mathrm{\phi_{\mathrm{spec}}}$ display adaptivity to the intrinsic dimension of $\mathcal{M}$?
	\item Assume that $f$ belongs to the Holder space $C_d^s(\mathcal{X})$. Moreover, suppose that instead of observing ${y_i}$ according to the regression testing model \eqref{eqn:regression_known_variance}, we observe
	\begin{equation*}
	y_i = f(x_i) + \sigma \varepsilon_i, ~\varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
	\end{equation*}
	where $\sigma > 0$ is unknown. When $d \geq 4$, what are the minimax regression testing rates over $C_d^1(\mathcal{X};R)$? Is the test $\phi_{\mathrm{spec}}$ minimax optimal, when the tuning parameters $r$ and $\kappa$ are appropriately chosen?
\end{enumerate}

\section{Two-sample density testing.}
In the two-sample density testing problem, we observe independent samples $Z = z_1,\ldots,z_N \sim P$ and $Y = y_1,\ldots,y_M \sim Q$, where $P$ and $Q$ are distributions over $\Reals^d$ with densities $p$ and $q$, respectively, and $N \sim \textrm{Bin}(n,1/2)$. Our goal is to distinguish the hypotheses
\begin{equation*}
\mathbf{H}_0: P = Q \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: P \neq Q
\end{equation*}
and we again evaluate our performance using worst-case risk; letting $\phi:\Reals^{N + M} \to \{0,1\}$, 
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \inf_{p \in \mathcal{H}}\Ebb_{p,p}(\phi) + \sup_{\substack{p,q \in \mathcal{H} \\ \norm{p - q}_{\Leb^2} \geq \epsilon}} \Ebb_{p,q}(1 - \phi).
\end{equation*}

\subsection{Test statistics.}

We suggest several two-sample test statistics. 

\paragraph{Eigenvector projection test statistic.}

It is straightforward to adapt the test statistic $T_{\mathrm{spec}}$ to the two-sample testing problem. Concatenate the samples $Z$ and $Y$ in $X = (z_1,\ldots,z_N,y_1,\ldots,y_M)$, and define $T_{\mathrm{spec}}^{(2)}$ to be
\begin{equation}
\label{eqn:graph_spectral_projections_2}
T_{\mathrm{spec}}^{(2)} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2, ~~\textrm{where}~~ a = (\underbrace{N^{-1},\ldots,{N^{-1}}}_{\textrm{length } N},\underbrace{-M^{-1},\ldots,-M^{-1}}_{\textrm{length } M})
\end{equation}

For convenience, we state our following two test statistics with respect to the empirical norm $\norm{\theta}_n = n^{-1/2}\norm{\theta}_2$ for $\theta \in \Reals^n$. They will each depend on a tuning parameter $\lambda > 0$.
\paragraph{Graph Sobolev IPM.}
Letting $C_n := nr^{(d + 2)/2}$ and
\begin{equation*}
\Theta_{1,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B\theta}_2 \leq 1\} 
\end{equation*}
we define the \emph{graph Sobolev IPM} to be
\begin{equation}
\label{eqn:sobolev_IPM}
T_{\textrm{sob}} := \sup_{\substack{\theta \in \Theta_{1,2} \\ \lambda \norm{\theta}_n \leq 1}} \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}. 
\end{equation}

\paragraph{Graph Total Variation IPM.}
Letting $C_n' := n^{2}r^{(d + 1)}$ and 
\begin{equation*}
\Theta_{1,1} := \{\theta \in \Reals^n:~ (C_n')^{-1} \norm{B\theta}_1 \leq 1\}
\end{equation*}
we define the \emph{graph Total Variation} IPM to be
\begin{equation}
\label{eqn:total_variation_IPM}
T_{\mathrm{TV}} := \sup_{\substack{\theta \in \Theta_{1,1}, \\ \lambda \norm{\theta}_n \leq 1} } \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}, \quad
\end{equation}

\subsection{Current results.}

In Theorem~\ref{thm:twosample_sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}}^{(2)} := \1\{T_{\mathrm{spec}}^{(2)} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $H^1(\mathcal{X};R)$ when $d = 1$.

\begin{theorem}
	\label{thm:twosample_sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d = 1$.  Suppose that $\mu = (P + Q)/2$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]$ with density functions $\rho$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < \rho_{\min} < \rho(x) < \rho_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}^{(2)}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right), ~\kappa = n^{2/5}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $R,p_{\max},q_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/5} (\log n)^{h(a,1)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}

We prove Theorem~\ref{thm:twosample_sobolev_testing_rate} by relating the density testing problem to a regression testing problem with a certain type of structured noise, and then proceeding along similar lines to the proof of Theorem~\ref{thm:sobolev_testing_rate}. To pursue this strategy, we require the eigenvectors to satisfy a certain type of incoherence condition; this is in constrast to the regression testing problem with known variance, where we did not require the eigenvectors to be smooth in any sense.

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $1 < d < 4$, is the test $\phi_{\spec}^{(2)}$ minimax optimal?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $C_d^1(\mathcal{X};R)$ for all values of $d$?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $W_d^{1,2}(\mathcal{X};R)$ when $d \leq 4$?
	\item Is the test statistic \eqref{eqn:graph_spectral_projections_2}, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $H^s$? 
	\item Modify the test statistic \eqref{eqn:sobolev_IPM} by replacing the function class $\Theta_{1,2}$ with
	\begin{equation}
	\Theta_{s,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B^{(s)}\theta}_2 \leq 1\} 
	\end{equation}
	Is the modified test statistic, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $H^s$?
	\item What is the minimax testing rate over $BV_d^{1}(\mathcal{X};R)$? Does it exhibit a phase transition analogous to the minimax estimation rate over bounded variation spaces?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over $BV_d^{1}(\mathcal{X};R)$?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over Sobolev and Holder function classes?
\end{enumerate}

\section{Definitions and Notation}

Here we collect definitions of some common function spaces and graph operators.

\subsection{Function Spaces}

Here we formally define, discuss some function spaces and associated norms.

\subsubsection{Lebesgue spaces}

We say a Borel measurable function $f: \mathcal{X} \to \Reals$ is in the space $\mathcal{L}^p(\mathcal{X})$ for $1 \leq p < \infty$ if 
$$\norm{f}_{\mathcal{L}^p(\mathcal{X})} := \int_{\mathcal{X}} \abs{f(x)}^p \,dx < \infty$$
and we let 
\begin{equation*}
\mathcal{L}^p(\mathcal{X};R) = \set{f \in \mathcal{L}^p(\mathcal{X}): \norm{f}_{\mathcal{L}^p(\mathcal{X})} < R}
\end{equation*}
be a ball in the Lebesgue space.

\subsubsection{Holder spaces}

For a given $s > 0$, let $\ell = \floor{s}$ be the largest integer strictly less than $s$. Then the $s$th Holder norm is given by
\begin{equation*}
\norm{f}_{C^{s}(\mathcal{X})} := \sum_{\abs{\alpha} < s} \norm{D^{\alpha}f}_{\infty} + \sum_{\abs{\alpha} = \ell} \sup_{x,y \in \mathcal{X}} \frac{\abs{D^{\alpha}f(y) - D^{\alpha}f(x)}}{\norm{x - y}_2^{s - \ell}}
\end{equation*}
and the $s$th Holder space $C^{s}(\mathcal{X})$ consists of all functions which are $\ell$ times continuously differentiable with finite Holder norm $\norm{\cdot}_{C^{s}(\mathcal{X})}$. Denote the Holder ball by $C_d^{s}(\mathcal{X},R) = \set{f \in C^{s}(\mathcal{X}): \norm{f}_{C_d^{s}(\mathcal{X})} \leq R}$. Let $C_c^{s}(\Xset)$ consist of those functions in $C^s(\Xset)$ which are compactly supported within $\Xset$; formally $f \in C_c^{s}(\Xset)$ if and only if $f \in C^s(\Xset)$ and there exists an open set $V \subset \ol{V} \subset \Xset$ such that $\mathrm{supp}(f) \subset V$. 

\subsubsection{Sobolev spaces}

For a given $s$ and $p > 0$, the Sobolev space $W^{s,p}(\mathcal{X})$ consists of all functions $f \in \Leb^2(\mathcal{X})$ such that for each $\alpha = (\alpha_1,\ldots,\alpha_d)$ with $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ belongs to $\mathcal{L}^2(\mathcal{X})$. The Sobolev $\{s,p\}$ norm is then 
\begin{equation*}
\norm{f}_{W^{s,p}(\mathcal{X})}^2 = \sum_{\abs{\alpha} \leq s} \int_{\mathcal{X}} \abs{D^{\alpha}f}^p \,dx
\end{equation*}
and for a given $R > 0$, the corresponding ball is $W^{s,p}(\Xset; R) = \set{f: \norm{f}_{H^s(\Xset)} \leq R}$. In the special case when $p = 2$, the Sobolev space $W$ is a Hilbert space; we adopt the usual convention of writing $H^s(\Xset) = H^s(\Xset)$. 

We write $W_0^{s,p}(\Xset)$ for the completion of $C_c^{\infty}(\Xset)$, and let $H_0^s(\Xset) = W_0^{s,2}(\Xset)$.  

\paragraph{Defining Sobolev spaces using Fourier series.}

Let the cosine basis on $\Leb^2([0,1])$ be defined as 
\begin{equation*}
\varphi_k(x) = 
\begin{cases*}
1,~~\textrm{for $k = 1$.} \,
\sqrt{2}\cos\Bigl(\pi (k - 1) x\Bigr),~~\textrm{for $k \in \Nbb_1$}
\end{cases*}
\end{equation*}
for any $x \in [0,1]$. Let the tensor product cosine basis on $\Leb^2([0,1]^d)$ be defined as
\begin{equation*}
\varphi_k(x) = \prod_{j = 1}^{d} \varphi_{k_j}(x_j), ~~\textrm{for $k \in \Nbb^d$},
\end{equation*}
for any $x \in [0,1]^d$. Observe that $\varphi_k(\wb{x}_i) = \sqrt{n} v_{k,i}(\wb{G}_d)$ for all $k,i \in [N]^d$.

We let the space 
\begin{equation*}
\wt{H}^{s}(\Xset;L) = \Bigl\{ \sum_{k \in \mathbb{N}^d} \theta_k \varphi_k: \sum_{k \in \mathbb{N}^d} \theta_k^2 a_k^{2} \leq L \Bigr\}
\end{equation*}
where in the one-dimensional case $a_{k} = (k - 1)^s$ and for general $d$, $a_k = \sqrt{\sum_{j = 1}^{d} a_{k_j}^2}$. It is not hard to show that, when $s$ is an integer, $\wt{H}^s([0,1]^d)$ consists of those functions in $H^s([0,1]^d)$ which satisfy some Neumann-type boundary conditions. For example when $d = 1$,
\begin{equation*}
\wt{H}^s([0,1];L) = \Bigl\{f \in H^{s}([0,1];L \pi^{s}): f^{(\ell)}(0) = f^{(\ell)}(1) = 0,~~\textrm{for all odd integers $0 < \ell < s$} \Bigr \} \supseteq H_0^s([0,1];L'),
\end{equation*}
a fact which we prove in Section~\ref{subsec:sobolev_class_equivalence}.

\paragraph{Bounded Variation spaces.}

For a function $f \in L^1(\mathcal{X})$ the \emph{total variation} semi-norm of $f$ is
\begin{equation*}
TV(f;\mathcal{X}) := \sup \left\{ \int_{\mathcal{X}} f \, \Xsetive \, \psi \,dx : \psi \in C_c^1(\mathcal{X}; \Reals^d), \abs{\psi} \leq 1 \right\};
\end{equation*}
and we write $BV_d(\mathcal{X})$ for the subset of functions $f \in L^1(\mathcal{X})$ which have bounded norm
\begin{equation*}
\norm{f}_{BV_d(\mathcal{X})} := \norm{f}_{\infty} + TV(f;\mathcal{X}).
\end{equation*}
For a given $R > 0$, the corresponding ball is $BV_d^{1}(\mathcal{X};R) = \set{f: \norm{f}_{BV_d(\mathcal{X})} \leq R}$. 

\subsection{Graph Operators}
Let $s \geq 1$ be an integer. The $s$th-order difference operator on $G_{n,r}$, denoted $B^{(s)}$, is defined by
\begin{equation*}
B^{(s)} :=
\begin{cases}
L^{s/2},& ~~ s \textrm{ even} \\
BL^{(s - 1)/2},& ~~ s \textrm{ odd.}
\end{cases}
\end{equation*}

\subsection{Kernels}
We say a kernel function $K(\cdot)$ is a $2$nd order kernel if $K$ is compactly supported on $B(0,1)$, uniformly upper bound $\abs{K(x)} \leq K_{\max} < \infty$ for all $x \in B(0,1)$, and
\begin{equation*}
\int K(x) \,dx = \nu_d,~~ \int x K(x) \,dx = 0.
\end{equation*}
Note that the uniform kernel, defined as $K(x) = \1\{\norm{x} \leq 1\}$, is a $2$nd order kernel. 

\subsection{Notation}

We will treat $f$ interchangeably as a function $f:\Rd \to \Reals$, and as a vector $f = (f(x_1),\ldots,f(x_n))$. More generally, for $U \subset V$ and $f:V \to \Reals$, the action of $f$ on $U$ is naturally defined via the restriction operator $R_U$; we will forego additional notation and simply treat $f$ as a function on $U$, defined as $f(x) = R_Uf(x)$ for $x \in U$. It should always be clear from context how we are using $f$. 

For sequences $(a_n), (b_n)$, we say $a_n = O(b_n)$ if there exists a constant $c$ such that $a_n \leq cb_n$ for all $c$. We use the notation $a \wedge b$ for the minimum of $a$ and $b$.

We use the notation $\Lambda(H)$ to denote the spectrum of a matrix $H$, and $\lambda_k(H)$ to denote the $k$th smallest eigenvalue of $H$.

We sometimes -- and currently not in a consistent manner -- denote the lattice Laplacian $L_{\wb{G}} =: \wb{L}$. 

We write $\mathbb{N}_k$ for the natural numbers excluding $0,\ldots,k - 1$. We write $\mathbb{N}_0$ for the natural numbers including $0$, and $\mathbb{N} = \mathbb{N}_1$.


\section{Proofs}

\subsection{Proof of Lemma~\ref{lem:fixed_graph_testing}}

In this proof, we will drop all notational dependence on the graph $G$ for ease of reading. 

To prove Lemma~\ref{lem:fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\paragraph{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 \\
& = \frac{\kappa}{n} + \norm{\Pi_{\kappa,G}(\beta)}_n^2.
\end{align*}

\paragraph{Variance of $T_{\mathrm{spec}}$:}
Note that $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvector $v_k$ as its $k$th column. Therefore,
\begin{align*}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa) \\
& = \frac{1}{n^2}(4 n \norm{\Pi_{\kappa,G}(\beta)}_n^2 + 2 \kappa)
\end{align*}
where the last inequality follows from standard properties of the Gaussian distribution, and the last equality follows since $V_{\kappa} V_{\kappa}^T$ is idempotent. We now move on to showing the desired inequalities \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}. Let $t(b) := b\sqrt{\frac{\kappa}{n^2}}$.

\paragraph{Proof of~\eqref{eqn:graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{2}{b^2}.
\end{align*}

\paragraph{Proof of~\eqref{eqn:graph_spectral_type_II_error}:} For simplicity, we introduce the notation
\begin{equation*}
\Delta = \norm{\Pi_{\kappa,G}(\beta)}_n^2
\end{equation*}
Assumption~\eqref{eqn:fixed_graph_testing_critical_radius} implies $\Delta \geq 2 t(b)$, and we have already shown that $\Ebb_{\beta}(T_{\spec}) = \Delta + \kappa/n$. Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & \leq \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\Delta/n}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term we have
\begin{equation}
\label{eqn:spectral_type_II_error_pf2}
\frac{4}{n\Delta} \leq \frac{2}{nt(b)} = \frac{2}{b\sqrt{2\kappa}}, 
\end{equation}
and combining~\eqref{eqn:spectral_type_II_error_pf1} and~\eqref{eqn:spectral_type_II_error_pf2} yields~\eqref{eqn:graph_spectral_type_II_error}.

Finally the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{\lambda_{\kappa}^s}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \lambda_k^s \\
& \geq \norm{\beta}_2^2 - \frac{\beta^T L^s \beta}{\lambda_{\kappa}^s},
\end{align*}
shows that~\eqref{eqn:fixed_graph_testing_critical_radius} implies~\eqref{eqn:fixed_graph_testing_critical_radius_1}.

\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate_order1}}

We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = G_{n,r}$. In order to do this, we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that the following statements each hold with probability at least $1 - c/b$ for sufficiently large $n$ (Here and in what follows, $c$ denotes a constant which is fixed in $n$ and $f$ and does not depend on $b$, but may depend on other fixed quantities such as $d$, $s$, etc.): 
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm_order1}
	\textbf{Graph Sobolev norm:} For any $n^{-1/(2 + d)}\leq r \leq 1$,
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm_order1}
	f^T L f \leq c \cdot b \cdot \norm{f}_{H^1(\Xset)}^2 \cdot n^{2} r^{d + 2} 
	\end{equation}
	\item 
	\label{event:eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} Let $t = n^{1/d}$, and let
	\begin{equation*}
	\overline{X} := \Bigl\{\frac{1}{t}(j_1,\ldots,j_d): k \in [t]^d\Bigr\}
	\end{equation*} 
	consist of $n$ evenly spaced grid points on $[0,1]^d$. The transportation distance between $X$ and $\ol{X}$ is
	\begin{equation*}
	d_{\infty}(X,\overline{X}) := \inf_{\pi} \sup\Bigl\{\norm{\pi(\ol{x}_i) - \ol{x}_i}: \ol{x}_i \in \ol{X}\Bigr\}
	\end{equation*}
	where the infimum is taken over all bijections $\pi: \ol{X} \to X$. 
	There exists a constant $c$ such that for any $\kappa = 1,\ldots,n$ if $\max\{ 3 d_{\infty}(X,\ol{X}),n^{-1/d}\} \leq r \leq \kappa^{-2/(d(2 +d))}$, 
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound}
	\lambda_{\kappa} \geq c \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	Theorem~\ref{thm:slepcev_transport_distance} (Theorem 1.1 of \textcolor{red}{(Garcia Trillos and Slepcev)}) establishes that there exists a constant $c$ such that 
	\begin{equation*}
	d_{\infty}(X,\ol{X}) \leq c\biggl(\frac{\log n}{n}\biggr)^{1/d}
	\end{equation*}
	with probability $1  - O(1/n)$. Therefore in particular, the inequality~\eqref{eqn:eigenvalue_tail_bound} is satisfied for sufficiently large $n$ when $\kappa = n^{2d/(4 + d)}$ and $c(\log n/n)^{1/d} \leq r \leq n^{-4/((2+d)(4s + d))}$. 
	\item 
	\label{event:l2_norm}
	\textbf{Empirical norm of $f$:} There exists a constant $c_1$ such that if $\norm{f}_{\Leb^2(\Xset)} \geq c_1 \cdot b \cdot n^{-2/(4 + d)}$, and $d = 1,2$ or $3$,
	\begin{equation}
	\label{eqn:l2_to_empirical_norm}
	\norm{f}_n^2 \geq \frac{1}{b} \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

\subsubsection{Proof of \eqref{eqn:continuous_to_discrete_sobolev_norm}}

The proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows from Lemma~\ref{lem:expected_first_order_seminorm}--which upper bounds the expected first order graph Sobolev seminorm-- and then an application of Markov's inequality.

\subsubsection{Proof of \eqref{eqn:eigenvalue_tail_bound}}
\label{subsubsec:eigenvalue_tail_bound_pf}

We prove~\eqref{eqn:eigenvalue_tail_bound} by comparing $G_{n,r}$ to the tensor product of a $d$-dimensional lattice and a complete graph. The latter is a highly structured graph with known eigenvalues, which as we will see are sufficiently lower bounded for our purposes.

Let $\wt{r} = r/(3(\sqrt{d} + 1)), M = (1/\wt{r})^d$ and $N = n\wt{r}^d$; assume without loss of generality that $M$ and $N$ are integers. Additionally for $m = M^{1/d}$ define
\begin{equation*}
\overline{Z} = \set{\frac{1}{m}(j_1,\ldots,j_d): j \in [m]^d}
\end{equation*}
to be the $M$ evenly spaced grid points over $[0,1]^d$.
For a given $\overline{z}_j \in \overline{Z}$, we write $Q(z_j) = m^{-1}[j_1 - 1,j_1] \times \cdots \times m^{-1}[j_d - 1,j_d]$ for the cube of side length $1/m$ with $z_j$ at one corner. 

Consider the graph $H = (\overline{X}, E_H)$, where $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$ if
\begin{equation*}
\textrm{there exists}~\ol{z}_i, \ol{z}_j \in \ol{Z}~\textrm{such that}~\ol{x}_k \in Q(\ol{z}_i),~ \ol{x}_\ell \in Q(\ol{z}_j),~\textrm{and}~\norm{i - j}_1 \leq 1.
\end{equation*}
On the one hand $H \cong \ol{G}^M_d \otimes K_N$ where $\ol{G}^M_d$ is the $d$-dimensional lattice on $M$ nodes, and $K_N$ is the complete graph on $N$ nodes. On the other hand, we now show that when $\max\{3d_{\infty}(X,\ol{X}), n^{-1/d}\} \leq r$ then $G_{n,r} \succeq H$ as a result of the triangle inequality. If $(\ol{x}_k, \ol{x}_{\ell}) \in E(H)$, by definition there exist $\ol{z}_i, \ol{z}_j$ connected in $\ol{G}_d^M$ such that $\ol{x}_k \in Q(\ol{z}_i)$ and $\ol{x}_{\ell} \in Q(\ol{z}_j)$. This implies that $\ol{x}_k$ and $\ol{x}_{\ell}$ must themselves be close together, since
\begin{equation*}
\norm{\ol{x}_k - \ol{x}_{\ell}}_2 \leq \norm{\ol{x}_k - \ol{z}_{i}}_2 + \norm{\ol{z}_i - \ol{z}_j}_2 +  \norm{\ol{z}_{j} - \ol{x}_{\ell} }_2 \leq \wt{r}(1 + 2\sqrt{d}) = r/3.
\end{equation*}
Since we also assume $r/3 \geq d_{\infty}(X,\ol{X})$, another application of the triangle inequality gives
\begin{equation*}
\norm{\pi(\ol{x}_k) - \pi(\ol{x}_\ell)}_2 \leq \norm{\pi(\ol{x}_k) - \ol{x}_\ell}_2 + \norm{\ol{x}_k - \ol{x}_\ell}_2 \leq \norm{\ol{x}_{\ell} - \pi(\ol{x}_\ell)}_2 \leq r,
\end{equation*}
implying that $(\pi(\ol{x}_k), \pi(\ol{x}_{\ell})) \in E$ and consequently that $G_{n,r} \succeq \ol{G}^M_d \otimes K_N$.

The eigenvalues of lattices and complete graphs are known to satisfy, respectively
\begin{equation*}
\lambda_k(\ol{G}^{M}_d) \geq \frac{k^{2/d}}{M^{2/d}}~\textrm{for $k = 0,\ldots,M - 1$},~~ \textrm{and}~\lambda_{j}(K_N) \geq N\1\{j > 0\}~\textrm{for $j = 0,\ldots,N-1$.}
\end{equation*}
and by standard facts regarding the eigenvalues of tensor product graphs, we have that the spectrum $\Lambda(H)$ satisfies
\begin{equation*}
\Lambda(H) = \set{N\lambda_k(\ol{G}^{M}_d) + M\lambda_j(K_N): \textrm{for $k = 0,\ldots,M - 1$ and $j = 0,\ldots,N-1$}}
\end{equation*}
For all $j = 1,\ldots,N-1$, we have that $M\lambda_j(K_N) = MN = n$. Therefore,
\begin{align*}
\lambda_{\kappa}(H) & \geq \{n \wedge N\lambda_{\kappa}(\ol{G}^{M}_d)\} \\
& \geq \{n \wedge n\wt{r}^d\frac{\kappa^{2/d}}{M^{2/d}}\} \\
& \geq \{n \wedge (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d}\} \\
& \geq (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d},
\end{align*}
where the last inequality is satisfied since $r \leq \kappa^{-2/(d(d + 2))}$, completing the proof of~\eqref{eqn:eigenvalue_tail_bound}.

\subsubsection{Proof of \eqref{eqn:l2_to_empirical_norm_order1}}
The inequality~\eqref{eqn:l2_to_empirical_norm_order1} is implied by the more general Lemma~\ref{lem:empirical_norm_sobolev}, which we state in Section~\ref{subsec:sobolev_testing_rate_pf} and use to prove a similar inequality when $f \in H^s(\Xset)$ for general $s$. 

\subsubsection{Putting the pieces together}

The final proof of Theorem~\ref{thm:sobolev_testing_rate_order1} proceeds by taking $s = 1$ and proceeding as in Section~\ref{subsubsec:sobolev_testing_rate_pf_conclusion}.


\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate}}
\label{subsec:sobolev_testing_rate_pf}

We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = G_{n,r}$. In order to do this, we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that the following statements each hold with probability at least $1 - c/b$ for sufficiently large $n$ (Here and in what follows, $c$ denotes a constant which is fixed in $n$ and $f$ and does not depend on $b$, but may depend on other fixed quantities such as $d$, $s$, etc.): 
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} There exists an $r^{\star}$ (which does not depend on $f$) such that for any $n^{-1/(2(s - 1) + d)}\leq r \leq r^{\star}$,
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm}
	f^T L^s f \leq c \cdot b \cdot \norm{f}_{H^s(\Xset)}^2 \cdot n^{s + 1} r^{s(d + 2)}.
	\end{equation}
	Here $L = L_{G_{n,r}}$ is the Laplacian matrix of the neighborhood graph $G_{n,r}$.
	\item 
	\label{event:eigenvalue_tail_decay_2}
	\textbf{Eigenvalue tail bound:} There exists a constant $c$ such that for any $\kappa = 1,\ldots,n$, if $\max\{ 3 d_{\infty}(X,\ol{X}),n^{-1/d}\} \leq r \leq \kappa^{-2/(d(2 +d))}$ then,
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound_2}
	\lambda_{\kappa} \geq c \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	In particular, the inequality~\eqref{eqn:eigenvalue_tail_bound_2} is satisfied for sufficiently large $n$ when $\kappa = n^{2d/(4s + d)}$ and $c(\log n/n)^{1/d} \leq r \leq n^{-4/((2+d)(4s + d))}$. 
	\item 
	\label{event:l2_norm_order1}
	\textbf{Empirical norm of $f$:} There exists a constant $c_1$ such that if $\norm{f}_{\Leb^2(\Xset)} \geq c_1 \cdot b \cdot n^{-2s/(4s + d)}$ and $4s > d$, then
	\begin{equation}
	\label{eqn:l2_to_empirical_norm_order1}
	\norm{f}_n^2 \geq \frac{1}{b} \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

\subsubsection{Proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm}}

The probabilistic bound~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows from the more general Lemma~\ref{lem:roughness_functional_expectation_sobolev} by Markov's inequality. 
\begin{lemma}
	\label{lem:roughness_functional_expectation_sobolev}
	Let $\Xset$ be a Lipschitz domain and let $s \geq 1$ be an integer. Suppose that $f \in H_0^{s}(\Xset)$, and further that $p \in C^{s-1}(\Xset;p_{\max})$ for some constant $p_{\max} < \infty$. Then for any $2$nd-order kernel $K$ and any $n^{-1/(2(s - 1) + d)} \leq r(n)$ such that $r \to 0$ as $n \to \infty$, the expected graph Sobolev seminorm is upper bounded
	\begin{equation}
	\label{eqn:roughness_functional_expectation_sobolev}
	\Ebb\bigl[f^T L^s f\bigr] \leq c \cdot \norm{f}_{H^s(\Xset)}^2 \cdot n^{s + 1}r^{s(d + 2)}
	\end{equation}
	for all $n$ sufficiently large.
\end{lemma}

We note that the proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev} is where we rely on the fact that $f$ and its derivatives are compactly supported on $\Xset$. The proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev} is lengthy, and we defer it to Section~\ref{sec:technical_lemma_proofs}.

\subsubsection{Proof of~\eqref{eqn:eigenvalue_tail_bound}}
See Section~\ref{subsubsec:eigenvalue_tail_bound_pf}.

\subsubsection{Proof of~\eqref{eqn:l2_to_empirical_norm}}

\begin{lemma}
	\label{lem:empirical_norm_sobolev}
	Let $\Xset$ be a Lipschitz domain over which the density is upper and lower bounded 
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty ~~\textrm{for all $x \in \Xset$,}
	\end{equation*}
	and let $f \in H^s(\Xset)$.Then for any $b \geq 1$, there exists $c_1$ such that if 
	\begin{equation}
	\norm{f}_{\Leb^2(\Xset)} \geq 
	\begin{cases*}
	c_1 \cdot b \cdot \norm{f}_{H^s(\Xset)} \cdot \max\{n^{-1/2},n^{-s/d}\},~~\textrm{if}~2s \neq d \\
	c_1 \cdot b \cdot \norm{f}_{H^s(\Xset)} \cdot n^{-a/2},~~\textrm{if}~ 2s = d ~\textrm{for any}~ 0 < a < 1
	\end{cases*}
	\end{equation}
	then,
	\begin{equation}
	\label{eqn:l2_to_empirical_norm_sobolev}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}
	\end{equation}
	where $c_1$ and $c_2$ are constants which may depend only on $s$, $\Xset$, $d$, $p_{\min}$ and $p_{\max}$.
\end{lemma}
The lower bound~\eqref{eqn:l2_to_empirical_norm} results from the more general Lemma~\ref{lem:empirical_norm_sobolev}, which can be verified by checking the various orderings of $2s/(4s + d)$, $s/d$ and $1/2$ whenever $4s < d$. 

\paragraph{Proof of Lemma~\ref{lem:empirical_norm_sobolev}:}

To prove~\eqref{eqn:l2_to_empirical_norm_sobolev} we will show
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot \left(\mathbb{E}\bigl[\norm{f}_n^2\bigr]\right)^2
\end{equation*}
whence the claim follows from the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). Since $p \leq p_{\max}$ is uniformly bounded, we can relate $\mathbb{E}\bigl[\norm{f}_n^4\bigr]$ to the $\Leb^4$ norm,
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] = \frac{(n-1)}{n}\left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + \frac{\mathbb{E}\Bigl[\bigl(f(x_1)\bigr)^4\Bigr]}{n} \leq \left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + p_{\max}^2\frac{\norm{f}_{\Leb^4}^4}{n}.
\end{equation*}
We will use a Sobolev inequality to relate $\norm{f}_{\Leb^4}$ to $\norm{f}_{W_d^{s,2}(\Xset)}$. The nature of this inequality depends on the relationship between $s$ and $d$ (see Theorem 6 in Section 5.6.3 of \textcolor{red}{Evans} for a formal statement), so from this point on we divide our analysis into three cases: (i) the case where $2s > d$, (ii) the case where $2s < d$, and (iii) the borderline case $2s = d$.

\textit{Case 1: $2s > d$.}
When $2s > d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in C^{\gamma}(\overline{\Xset})$ for some $\gamma > 0$ which depends on $s$ and $d$, with the accompanying estimate
\begin{equation*}
\sup_{x \in \Xset} \abs{f(x)} \leq \norm{f}_{C^{\gamma}(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Therefore,
\begin{align*}
\norm{f}_{\Leb^4}^4 & = \int_{\Xset} [f(x)]^4 \,dx \\
& \leq \left(\sup_{x \in \Xset} \abs{f(x)}\right)^2 \cdot \int_{\Xset} [f(x)]^2 \,dx \\
& \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \norm{f}_{\Leb^2(\Xset)}^2.
\end{align*}
Since by assumption
\begin{equation*}
\norm{f}_{\Leb^2(\Xset)}^2 \geq c_1^2 \cdot b^2 \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2 \cdot \frac{1}{n},
\end{equation*}
we have
\begin{equation*}
p_{\max}^2\frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \frac{\norm{f}_{\Leb^2(\Xset)}^4}{n \norm{f}_{\Leb^2(\Xset)}^2} \leq c \frac{\norm{f}_{\Leb^2(\Xset)}^4}{c_1^2 b^2} \leq \frac{\Ebb\bigl[\norm{f}_n^2\bigr]}{b^2},
\end{equation*}
where the last inequality follows by taking $c_1$ sufficiently large.

\textit{Case 2: $2s < d$.}
When $2s < d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in \Leb^q(\Xset)$ for $q = 2d/(d - 2s)$, and moreover that
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Since $4 = 2\theta + (1 - \theta)q$ for $\theta = 2 - d/(2s)$, Lyapunov's inequality implies
\begin{equation*}
\norm{f}_{\Leb^4(\Xset)}^4 \leq \norm{f}_{\Leb^2}^{2\theta} \cdot \norm{f}_{\Leb^q(\Xset)}^{(1 - \theta)q} \leq c \norm{f}_{\Leb^2(\Xset)}^{4} \cdot \left(\frac{\norm{f}_{H^s(\Xset)}}{\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s}.
\end{equation*}
By assumption, $\norm{f}_{\Leb^2(\Xset)} \geq c_1 b \norm{f}_{H^s(\Xset)} n^{-s/d}$, and therefore
\begin{equation*}
p_{\max}^2 \frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c\norm{f}_{\Leb^2(\Xset)}^4 \left(\frac{\norm{f}_{H^s(\Xset)}}{n^{s/d}\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s} \leq \frac{c\norm{f}_{\Leb^2(\Xset)}^4}{c_1b^{d/s}} \leq \frac{\norm{f}_{\Leb^2(\Xset)}^4}{b^2}.
\end{equation*}
where the last inequality follows when $c_1$ is sufficiently large, and keeping in mind that $d/s > 2$ and $b \geq 1$. 

\textit{Case 3: $2s = d$.}
Assume $f$ satisfies~\eqref{eqn:paley_zygmund_1} for a given $0 < a < 1$. When $2s = d$, since $\Xset$ is a Lipschitz domain we have that $f \in L^q(\Xset)$ for any $q < \infty$, with the accompanying estimate
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
In particular the above holds for $q = 2/(1 - a)$ when $1/2 < a < 1$, and for any $q > 4$ when $0 < a < 1/2$. Using Lyapunov's inequality as in the previous case then implies the desired result.

\subsubsection{Putting the pieces together}
\label{subsubsec:sobolev_testing_rate_pf_conclusion}

We note that for all possible values of $X \in \Xset^n$, under the null hypothesis $f = f_0 = 0$ and therefore $\beta = (f(x_1),\ldots,f(x_n)) = 0$ as well. Therefore by~\eqref{eqn:graph_spectral_type_I_error}, we have the following bound on Type I error:
\begin{equation}
\Ebb_{f_0}(\phi_{\mathrm{spec}}) = \mathbb{E}(\mathbb{E}_{\beta = 0}(\phi_{\spec}) | X) \leq \frac{1}{b^2}.
\end{equation}

Now, we bound Type II error under the assumption $f \in H_0^{s}(\mathcal{X};R), p \in C^{s - 1}(\Xset;p_{\max})$ uniformly bounded away from $0$ and $\infty$ over $\Xset$, and 
\begin{equation}
\label{eqn:critical_radius_1}
\norm{f}_{\Leb^2(\Xset)}^2 \geq \epsilon^2 = c_1^2 \cdot b^2 \cdot \max\{R^2,R^{2d/(4s + d)}\} \cdot n^{-4s/(4s + d)}.
\end{equation}
Choosing $n^{-1/(2(s - 1) + d)}\leq r(n) \leq n^{-4/((2+d)(4s + d))}$, we may therefore apply our conclusions in Step 2; namely, that for every possible choice of $f \in H^s(\Xset;R)$ there exists a good set $\mathcal{E}_f \subseteq \Xset^n$ with $\Pbb(\mathcal{E}_f) \geq 1 - c/b$ such that each of \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, and \eqref{eqn:l2_to_empirical_norm_sobolev} hold for all $X \subseteq \mathcal{E}_f$. The choice $\kappa = (nR^2)^{2d/(4s + d)}$ balances the squared bias and variance terms on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius}, and we have that for all $X \subseteq \mathcal{E}_f$
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{f^T L^s f}{n\lambda_{\kappa}^s} & \leq 2bR^{2d/(4s + d)}n^{-4s/(4s+d)} + cbR^2\frac{n^{s} r^{s(d + 2)}}{\lambda_{\kappa}^s} \tag{by \eqref{eqn:continuous_to_discrete_sobolev_norm}} \\
& \leq cbR^{2d/(4s + d)}n^{-4s/(4s+d)} \tag{by \eqref{eqn:eigenvalue_tail_bound}} \\
& \leq \frac{1}{b}\norm{f}_{\Leb^2(\Xset)}^2 \\
& \leq \frac{1}{n}\sum_{i = 1} \bigl[f(x_i)\bigr]^2. \tag{by \eqref{eqn:l2_to_empirical_norm_sobolev}}
\end{align*}
where the last two inequalities follow for a suitably large choice of $c_1$ in~\eqref{eqn:critical_radius_1}.
We conclude that for all $X \subseteq \mathcal{E}_f$, the inequality \eqref{eqn:fixed_graph_testing_critical_radius} is satisfied with respect to $\beta = (f(x_1),\ldots,f(x_n))$ and $G = G_{n,r}$. As a result the worst-case Type II error is bounded
\begin{equation*}
\sup_{\substack{f \in H^s(\Xset;R), \\ \norm{f}_{\Leb^2(\Xset)} \geq \epsilon}}\mathbb{E}_{f}(1 - \phi_{\spec}) \leq \sup_{\substack{f \in H^s(\Xset;R), \\ \norm{f}_{\Leb^2(\Xset)} \geq \epsilon}} \mathbb{E}\bigl[\mathbb{E}_{\beta}(1 - \phi_{\spec}|X \in \mathcal{E}_f)\bigr] + \frac{c}{b} \leq \frac{3 + c}{b},
\end{equation*}
completing the proof of Theorem~\ref{thm:sobolev_testing_rate} upon proper choice of constants $c_1,c_2$ in that Theorem.

\subsection{Proof of Theorem~\ref{thm:holder_testing_rate_grid}}
\label{subsec:holder_testing_rate_grid_pf}

We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = \wb{G}$ and $s = 1$. In order to do this, we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = \wb{G}$ and $\beta = (f(\wb{x}_i): i \in [N]^d)$. In particular, we will show that each of the following statements hold:

\begin{enumerate}[label=(E\arabic*)]
	\item
	\label{event:discrete_sobolev_norm_grid}
	\textbf{Lattice Sobolev norm:}
	For every $f \in C^1(\Xset)$,
	\begin{equation}
	\label{eqn:discrete_sobolev_norm_grid}
	f^T L f \leq 2d \norm{f}_{C^1(\Xset)}^2 \cdot n^{1 - 2/d}
	\end{equation}
	Here $L = L_{\wb{G}}$ is the Laplacian matrix of the lattice graph.
	
	\item
	\label{event:eigenvalue_tail_bound_grid}
	\textbf{Lattice eigenvalue tail bound:}
	Letting $\mathbf{K} = (K,\ldots,K)$, 
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound_grid}
	\lambda_{\mathbf{K}}(\wb{G}) \geq \frac{K^2}{N^2}
	\end{equation}
	for every $2 \leq K \leq N$.
	
	\item 
	\label{event:l2_to_empirical_norm_grid}
	\textbf{Grid empirical norm:}
	When $d \leq 4$, for every $f \in C^1(\Xset)$ satisfying $\norm{f}_{\Leb^2(\Xset)} \geq \sqrt{d/4} \cdot \norm{f}_{C^1(\Xset)} \cdot n^{-2/(4 + d)}$, additionally
	\begin{equation}
	\label{eqn:l2_to_empirical_norm_grid}
	\frac{1}{n}\sum_{i = 1}^{n} \bigl(f(x_i)\bigr)^2 \geq \frac{1}{4}\norm{f}_{\Leb^2(\Xset)}^2
	\end{equation}
\end{enumerate}
Once we have shown these inequalities, some basic algebra yields that for the specific choice $\kappa = (Ln)^{2d/(4 + d)}$ and a sufficiently large choice of $c$ in \eqref{eqn:holder_testing_rate_grid_1},
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{f^T L f}{n\lambda_{\kappa}(\wb{G})} \leq \frac{1}{n}\sum_{i = 1}^{n}\bigl[f(x_i)\bigr]^2
\end{align*}
and~\eqref{eqn:holder_testing_rate_grid_2} follows from Lemma~\ref{lem:fixed_graph_testing}.

\subsubsection{Proof of~\eqref{eqn:discrete_sobolev_norm_grid}.}
The upper bound~\eqref{eqn:discrete_sobolev_norm_grid} follows straightforwardly from the Holder property,
\begin{align*}
f^T L f & = \sum_{i \in [N]^d} \sum_{j \in [N]^d} \bigl(f(\wb{x}_i) - f(\wb{x}_j\bigr)^2 \1\{\norm{i - j}_1 = 1\} \\
& \leq \norm{f}_{C^1(\Xset)}^2 \sum_{i \in [N]^d} \sum_{j \in [N]^d} \norm{\wb{x}_i - \wb{x}_j}_2^2 \cdot  \1\{\norm{i - j}_1 = 1\} \\
& = \norm{f}_{C^1(\Xset)}^2 n^{-2/d} \sum_{i \in [N]^d} \sum_{j \in [N]^d} \1\{\norm{i - j}_1 = 1\}  \\
& \leq 2d \norm{f}_{C^1(\Xset)}^2 n^{1 - 2/d}.
\end{align*}

\subsubsection{Proof of~\eqref{eqn:eigenvalue_tail_bound_grid}.}
The eigenvalues of $\wb{G}$ are known to have the analytic form
\begin{equation*}
\lambda_k(\wb{G}) = 4 \sum_{i = 1}^{d} \sin^2\Bigl(\frac{\pi(i_d - 1)}{2N}\Bigr)
\end{equation*}
whence the statement follows since $\sin(\pi x /2) \geq \min\{\pi x/2,\sqrt{2}/2 \}$ for all $x \in [0,1]$.

\subsubsection{Proof of~\eqref{eqn:l2_to_empirical_norm_grid}.}
For $x \in \Rd$, let $Q(x)$ be the $d$-dimensional cube of side length $1/N$ with $x$ at its center,
\begin{equation*}
Q(x) = [x_1 - 1/2N,x_1 + 1/2N] \otimes \cdots \otimes [x_d - 1/2N,x_d + 1/2N]
\end{equation*}
The difference between $f(\wb{x})$ and the average of $f$ over the cube $Q(\wb{x})$ can be bounded using the Holder property,
\begin{equation*}
\int_{Q(\wb{x})} (f(x) - f(\wb{x}))^2 \,dx \leq \mathrm{Leb}\Bigl(Q(\wb{x})\Bigr) \norm{f}_{C^1(\Xset)}^2 \frac{d}{4N^2} = \frac{d}{4n^{1 + 2/d}} \norm{f}_{C^1(\Xset)}^2
\end{equation*}
Dividing $\Xset$ into cubes and summing over the cubes, we have
\begin{align*}
\norm{f}_{\Leb^2(\Xset)}^2 & = \sum_{i \in [N]^d} \int_{Q(\wb{x}_i)} \bigl(f(x)\bigr)^2 \,dx \\
& \leq 2 \sum_{i \in [N]^d} \Bigl\{\mathrm{Leb}\Bigl(Q(\wb{x})\Bigr) \bigl(f(\wb{x}_i)\bigr)^2 + \int_{Q(\wb{x}_i)} \bigl(f(x) - f(\wb{x}_i)\bigr)^2 \,dx \Bigr\} \\
& \leq \frac{2}{n}\sum_{i \in [N]^d} \Bigl\{ \bigl(f(\wb{x}_i)\bigr)^2\Bigr\} + \frac{d}{4n^{2/d}} \norm{f}_{C^1(\Xset)}^2 \\
& \leq \frac{2}{n}\sum_{i \in [N]^d} \Bigl\{ \bigl(f(\wb{x}_i)\bigr)^2\Bigr\} + \frac{1}{2}\norm{f}_{\Leb^2(\Xset)}^2
\end{align*}
where the last inequality follows by the assumption $\norm{f}_{\Leb^2(\Xset)}^2 \geq (d/4) \norm{f}_{C^1(\Xset)}^2 n^{-4/(4 + d)}$ along with the fact that $4/(4 + d) \leq 2/d$ when $d \leq 4$.

\subsection{Proof of Theorem~\ref{thm:holder_testing_rate_grid_higher_order}}
\label{subsec:holder_testing_rate_grid_ho_pf}
We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = \wb{G}$ and $s \geq 2$. In order to do this, we will need to show that when $\kappa$ is appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = \wb{G}$ and $\beta = (f(\wb{x}_i): i \in [N]^d)$. We already know  from~\eqref{eqn:eigenvalue_tail_bound_grid}, that
\begin{equation*}
\lambda_{\mathbf{K}}(\wb{G}) \geq \frac{K^2}{N^2}
\end{equation*}
for every $2 \leq K \leq N$. In addition, we will show that each of the following statements hold:
\begin{enumerate}[label=(E\arabic*)]
	\item
	\textbf{Lattice Sobolev norm:}
	\label{event:grid_sobolev_norm_higher_order}
	There exists a constant $c$ such that for any $f \in C_c^s(\Xset)$,
	\begin{equation}
	\label{eqn:grid_sobolev_norm_higher_order}
	f^T \wb{L}^s f \leq c \norm{f}_{C^s(\Xset)}^2 n^{1 - 2s/d}.
	\end{equation}	
	\item 
	\textbf{Grid empirical norm:}
	\label{event:grid_empirical_norm_higher_order}
	There exists a sufficiently small constant $c_1$ and a sufficiently large constant $c_2$ such that for any $f \in C^s(\Xset;L)$,
	\begin{equation}
	\label{eqn:grid_empirical_norm_higher_order}
	c_1 \norm{f}_n^2 \geq \norm{f}_{\Leb^2(\Xset)}^2 - c_2 L n^{-2s/d}
	\end{equation}
	for all $n$ sufficiently large.
\end{enumerate}
Then for the specific choice $\kappa = n^{2d/(4s + d)}$ and a sufficiently large choice of constant $c$ in \eqref{eqn:holder_testing_rate_grid_1},
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{f^T \wb{L}^s f}{n\bigl(\lambda_{\kappa}(\wb{G})\bigr)^s} \leq \frac{1}{n}\sum_{i = 1}^{n}\bigl[f(x_i)\bigr]^2
\end{align*}
and~\eqref{eqn:holder_testing_rate_grid_2} follows from Lemma~\ref{lem:fixed_graph_testing}.

\subsubsection{Proof of~\eqref{eqn:grid_sobolev_norm_higher_order}.}
To prove~\eqref{eqn:grid_sobolev_norm_higher_order}, we approximate the action $\wb{L}f(\wb{x})$ to within a factor of $h^s$ for all $\wb{x} \in \wb{X}$. The manner in which we do this depends on whether $\wb{x}$ is sufficiently in the interior, or conversely is near the boundary, of $\wb{X}$. With an appropriate approximation of $\wb{L} f(\wb{x})$ in hand, we then find the desired estimate on the discrete semi-norm $f^T \wb{L}^s f$.

\paragraph{Approximation at interior points.}
Put $h = 1/N$, and for $j = 1,\ldots,d$, let $D_{e_j^2}f := f(\wb{x} + he_i) + f(\wb{x} - he_i) - 2f(\wb{x})$. At any point $\wb{x} \in \Xset_{h} \cap \wb{X} =: \wb{X}_{h}$, by taking a $2$nd order Taylor expansion of $f$ at $\wb{x}$ we can express the action of the lattice Laplacian as
\begin{align}
\wb{L}f(\wb{x}) & = \sum_{i = 1}^{d} D_{e_i^2}f(\wb{x}) \nonumber \\
& = \sum_{i = 1}^{d} \biggl[\frac{\partial}{\partial e_i} f(\wb{x})(e_ih - e_ih) + \frac{h^2}{2} \int_{-h}^{h} \frac{\partial^2}{\partial e_i^2} f(\wb{x} + e_it) \,dt \biggr] \nonumber \\
& = \frac{h^2}{2} \int_{-h}^{h} \sum_{i = 1}^{d} \frac{\partial^2}{\partial e_i^2} f(\wb{x} + e_it) \,dt \label{eqn:grid_sobolev_norm_higher_order_pf1}
\end{align} 
as a second-order differential operator which roughly approximates the continuum Laplace operator $\Delta f = -\sum_{i = 1}^{d} \frac{\partial^2}{\partial e_i^2} f$. Motivated by this, in Lemma~\ref{lem:grid_laplacian_approximation_error} we provide estimates on the absolute difference between the action of the iterated grid Laplacian and the iterated continuum Laplace operator.

\begin{lemma}
	\label{lem:grid_laplacian_approximation_error}
	Let $f \in C^s(\Xset;L)$ for $s \geq 3$, and let $q = (s - 1)/2$ or $q = (s - 2)/2$. At all grid points $\wb{x} \in \wb{X}_{qh}$,
	\begin{equation*}
	\abs{\wb{L}^{q}f(\wb{x}) - h^{2q}\Delta^{q}f(\wb{x})} \leq cLh^s.
	\end{equation*}
\end{lemma} 
We prove Lemma~\ref{lem:grid_laplacian_approximation_error} in Section~\ref{subsec:grid_laplacian_approximation_error_pf}.

\paragraph{Approximation at boundary points.}
For those remaining points $\wb{x} \in \wb{X} \cap \Xset_{qh}$ sufficiently near the boundary of $\Xset$, as might be expected to analyze $f(\wb{x})$ we will use the boundary assumptions we've made on $f$. Since $f$ is compactly supported in $\Xset$ there exists a point $x_0$ and some $\delta > 0$ such that $\mathrm{dist}(\wb{x},x_0) < qh$, and $f(z) = 0$ for all $z \in B(x_0,\delta)$. Therefore the partial derivatives $f^{(q)}(x_0) = 0$ for all $\abs{q} = 0,\ldots,s - 1$, and by taking a Taylor expansion of $f(x)$ around $f(x_0)$ we obtain
\begin{equation*}
\abs{f(x)} \leq q L h^{s}
\end{equation*}
Using this, we obtain the crude bound
\begin{equation*}
\abs{\wb{L}^s f(\wb{x})} \leq 2^s \cdot \max_{\wb{z} \in Q_{2qh}(\wb{x})} \bigl\{\abs{f(\wb{z})} \bigr\} \leq 2^s q L h^{s},
\end{equation*}
which will suffice for our purposes. 

\paragraph{Bounding the semi-norm.}
When $s$ is even, our previous manipulations imply that
\begin{align*}
f^T \wb{L}^s f & \leq \sum_{\wb{x} \in \wb{X}_{qh}} \Bigl\{\sum_{i = 1}^d D_{e_i^2} \wb{L}^{(s - 2)/2}f(\wb{x}) \Bigr\}^2 + cL^2h^{2s}n^{1 - 1/d} \\
& \leq h^{2s - 4}\sum_{\wb{x} \in \wb{X}_{qh}} \Bigl\{\sum_{i = 1}^d D_{e_i^2} \Delta^{(s - 2)/2}f(\wb{x}) \Bigr\}^2 + cL^2h^{2s}n^{1 - 1/d} \\
& \leq \frac{h^{2s}}{4}\sum_{\wb{x} \in \wb{X}_{qh}} \Bigl\{\sum_{i = 1}^d \int_{-h}^{h} \Bigl(\frac{\partial^2}{\partial e_i^2}\Delta^{(s - 2)/2}f\Bigr)(\wb{x} + te_i) \,dt \Bigr\}^2 + cL^2h^{2s}n^{1 - 1/d} \\
& \leq d^2 h^{2s} L^2 n + cL^2h^{2s}n^{1 - 1/d}
\end{align*}
where we have used, in order, our bounds on the absolute value of $\wb{L}^sf(\wb{x})$ and $\abs{f}(\wb{x})$ at points $\wb{x}$ near the boundary, our estimate on the approximation error between the grid Laplacian and continuum Laplace operator, 
the representation~\eqref{eqn:grid_sobolev_norm_higher_order_pf1}, and finally the Holder property
\begin{equation*}
\norm{\Delta^{(s - 2)/2}f}_{C^2(\Xset)} \leq \norm{f}_{C^s(\Xset)} \leq L.
\end{equation*}
Similar manipulations supply an equivalent bound when $s$ is odd
\begin{align*}
f^T \wb{L}^s f & \leq \sum_{\wb{x} \in \wb{X}_{qh}} \Bigl\{\sum_{i = 1}^d \wb{L}^{(s - 1)/2}f(\wb{x} + he_i) - \wb{L}^{(s - 1)/2}f(\wb{x}) \Bigr\}^2 + cL^2h^{2s}n^{1 - 1/d} \\
& \leq h^{2s - 2}\sum_{\wb{x} \in \wb{X}_{qh}} \Bigl\{\sum_{i = 1}^d \wb{\Delta}^{(s - 1)/2}f(\wb{x} + he_i) - \wb{\Delta}^{(s - 1)/2}f(\wb{x}) \Bigr\}^2 + cL^2h^{2s}n^{1 - 1/d} \\
& \leq d^2 h^{2s} L^2 n + cL^2h^{2s}n^{1 - 1/d}
\end{align*}
where this time we have invoked the Holder property $\norm{\Delta^{(s - 1)/2}f}_{C^1(\Xset)} \leq \norm{f}_{C^s(\Xset)} \leq L$. This establishes~\eqref{eqn:grid_sobolev_norm_higher_order}.

\subsubsection{Proof of~\eqref{eqn:grid_empirical_norm_higher_order}.}
\label{subsubsec:grid_empirical_norm_ho_pf}
Take $\eta$ to be the tensorization of a univariate $(s - 1)$-order kernel $\nu$. More specifically, we let $\nu: \Reals \to \Reals$ satisfy
\begin{equation*}
\int_{\Reals} \nu(x) \,dx = 1,~~ \int x^{\ell} \nu(x) \,dx = 0~~\textrm{for $\ell = 1,\ldots,s-1$},~~ \abs{\nu(x)} \leq 1~\textrm{for all $x \in [0,1^d]$}
\end{equation*}
additionally be compactly supported on $[-1/2,1/2]$ but otherwise arbitrary, and define $\eta:\Reals^d \to \Reals$ to be $\eta(z) = \prod_{i = 1}^{d} \nu(x_i)$ for $z \in \Reals^d$. We denote $\eta_h(x) = \frac{1}{h^d} \eta(x/h)$, and note that for any polynomial $u$ of degree at most $s - 1$, $\bigl(u \ast \eta_h\bigr)(x) = u(x)$. 

Fix an odd integer $m \geq 1$ to be chosen large enough, but fixed, later on, and assume without loss of generality that $N/m =: M \in \mathbb{N}$. For $i \in [n]^d$, let $u_{i}$ be the $(s - 1)$-order Taylor expansion of $f$ around $\wb{x}_i$, and further let
\begin{align*}
u(x) = \sum_{i \in [M]^d} u_{mi - 1}(x) \cdot \1\{x \in Q_{hm}(\wb{x}_{mi - 1})  \},~~ \wt{u}(x) = \sum_{i \in [M]^d} \sum_{\wb{x} \in  Q_{hm}(\wb{x}_{mi - 1})} \bigl(u_{mi - 1} \ast \eta_h\bigr)(\wb{x}) \cdot \1\{x \in Q_{h}(\wb{x})  \}.
\end{align*}
be piecewise polynomial and piecewise constant functions, respectively.

Standard properties of Taylor expansions imply that $\abs{f(x) - u_{mi - 1}(\wb{x})} \leq (L\sqrt{d}mh)^{s}$ for all $x \in Q_{mh}(\wb{x}_{mi - 1})$, and as a result
\begin{equation*}
\norm{u - f}_{\Leb^2([0,1]^d)}^2,~~\norm{u - f}_{n}^2 \leq ch^{2s}
\end{equation*}
Since $u_{mi- 1}$ is an $(s - 1)$-order polynomial, we have that $\bigl(u_{mi - 1} \ast \eta_h\bigr)(\wb{x}) = u_{mi - 1}(\wb{x})$ for each $\wb{x} \in \wb{X}$; therefore $\norm{\wt{u}}_n = \norm{u}_n$.
Additionally $\wt{u}$ is piecewise constant over the grid cells $Q_h(\wb{x})$, and so we have
\begin{equation*}
\norm{\wt{u}}_{\Leb^2([0,1]^d)}^2 = \sum_{\wb{x} \in \wb{X}} h^d \cdot  [\wt{u}(\wb{x})]^2 = \norm{\wt{u}}_n^2.
\end{equation*}
It remains only to show that
\begin{equation*}
c \norm{u}_{\Leb^2([0,1]^d)} \leq \norm{\wt{u}}_{\Leb^2([0,1]^d)},
\end{equation*}
for a constant $c$ which does not depend on $\wt{u}$. In Lemma~\ref{lem:riemann_sums_polynomials}, we show that the statement holds true if $u$ is a polynomial on $[0,1]^d$; and the width of the partition used to form $\wt{u}$ is taken to be a sufficiently small constant. By translating and rescaling the cubes $Q_{mi - 1}(\wb{x}_{mi} - 1)$ into $[0,1]^d$ as in \textcolor{red}{(Arias-Castro)}, we have that the same result holds for the piecewise polynomial $u$ when $m$ is taken to be a sufficiently large constant. Putting the pieces together, we have that whenever $n$ is great enough such that $M \in \mathbb{N}$, 
\begin{align*}
\norm{f}_{\Leb^2([0,1]^d)}^2 & \leq cn^{-2s/d} + 2\norm{u}_{\Leb^2([0,1]^d)}^2 \\ & \leq cn^{-2s/d} + c\norm{\wt{u}}_{\Leb^2([0,1]^d)}^2 \\
& = cn^{-2s/d} + c\norm{u}_{n}^2 \\
& \leq cn^{-2s/d} + c\norm{f}_n^2.
\end{align*}
which completes the proof of~\eqref{eqn:grid_empirical_norm_higher_order}.

\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate_grid}}

We want to apply Lemma~\ref{lem:fixed_graph_testing} to the case where $G = \wb{G}$. In order to do this, we will need to show that when $\kappa$ is appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius_1} holds with respect to $G = \wb{G}$ and $\beta = (f(\wb{x}_i): i \in [N]^d)$. We will show that for all $f = \sum_{k \in \Nbb^d} \theta_k \varphi_k \in \wt{H}^s(\Xset;L)$,
\begin{equation}
\label{eqn:grid_sobolev_continuum_sobolev_approximation}
\norm{\Pi_{\kappa,G}(f)}_n^2 \geq \norm{f}_{\Leb^2}^2 - c L \norm{f}_{\Leb^2(\Xset)} \cdot n^{-s/d} - \sum_{k \in \mathbb{N}^d/[K]^d} \theta_k^2 
\end{equation}
The tail sum of $\theta$ on the right hand side of~\eqref{eqn:grid_sobolev_continuum_sobolev_approximation} can be upper bounded,
\begin{equation*}
 \sum_{k \in \mathbb{N}^d/[K]^d} \theta_k^2  \leq L^2 K^{-2s} = L^2 \kappa^{-2s/d}
\end{equation*}
and since $2s > d$, we have that for sufficiently large $n$
\begin{equation*}
c L \norm{f}_{\Leb^2(\Xset)} \cdot n^{-s/d} \leq \frac{1}{2}\norm{f}_{\Leb^2(\Xset)}^2.
\end{equation*}
Along with~\eqref{eqn:grid_sobolev_continuum_sobolev_approximation} this implies
\begin{equation*}
\norm{\Pi_{\kappa,G}(f)}_n^2 \geq \frac{1}{2} \norm{f}_{\Leb^2}^2 - L^2 \kappa^{-2s/d}.
\end{equation*}
Then for the specific choice $\kappa = n^{2d/(4s + d)}$ and a sufficiently large choice of constant $c$ in~\eqref{eqn:sobolev_testing_rate_grid_1}, 
\begin{equation*}
\norm{f}_{\Leb^2}^2 \geq L^2 \kappa^{-2s/d} + 2b\sqrt{\frac{2\kappa}{n^2}}
\end{equation*}
which establishes~\eqref{eqn:fixed_graph_testing_critical_radius_1}, allowing us to apply Lemma~\ref{lem:fixed_graph_testing} and completing the proof of Theorem~\ref{thm:sobolev_testing_rate_grid}.

\subsubsection{Proof of~\eqref{eqn:grid_sobolev_continuum_sobolev_approximation}.}

Recall that the eigenvectors of the grid graph $v_k := v_k(\wb{G}_d)$ satisfy
\begin{equation*}
v_{k}(\wb{x}_i) = \frac{1}{\sqrt{n}} \varphi_{k}(\wb{x}_i)
\end{equation*}
for each $i \in [N]^d$ and $k \in \mathbb{N}^d$. Therefore we may rewrite $\Pi_{\kappa,\wb{G}_d}$ as a function of the tensor product cosine Fourier basis,
\begin{align*}
\Pi_{\kappa,\wb{G}_d}(f) & = \frac{1}{n} \sum_{k \in [K]^d} \Bigl\{\sum_{i \in [N]^d} f(\wb{x}_i) \varphi_k(\wb{x}_i)\Bigr\} \varphi_k \\
& :=  \sum_{k \in [K]^d} \wt{\theta}_k \varphi_k
\end{align*}
where we interpret the right hand side as a function over $\wb{X}$. Since $(\varphi_k)$ is an $L_2(\wb{X})$ orthonormal sequence, we obtain
\begin{align*}
\norm{\Pi_{\kappa,\wb{G}_d}(f)}_n^2 & = \sum_{k \in [K]^d} \wt{\theta}_k^2 \\ 
& = \sum_{k \in [K]^d} {\theta}_k^2 + \sum_{k \in [K]^d} \wt{\theta}_k^2 - \theta_k^2 \\
& = \norm{f}_{\Leb^2([0,1]^d)}^2 - \sum_{k \in \Nbb^d \setminus [K]^d} \theta_k^2 + \sum_{k \in [K]^d} \wt{\theta}_k^2 - \theta_k^2
\end{align*}
or equivalently
\begin{equation*}
\norm{f}_{\Leb^2([0,1]^d)}^2 - \norm{\Pi_{\kappa,\wb{G}_d}(f)}_n^2 = \sum_{k \in [K]^d} \theta_k^2 - \wt{\theta}_k^2 + \sum_{k \in \Nbb^d \setminus [K]^d} \theta_k^2
\end{equation*}
The second term in the above equation is exactly the third term in~\eqref{eqn:grid_sobolev_continuum_sobolev_approximation}, and so we focus our attention on the first term, which represents discretization error. Applying the parallelogram law and the Cauchy-Schwarz inequality gives
\begin{equation}
\label{eqn:grid_sobolev_approximation_error6_pf1}
\begin{aligned}
\sum_{k = 1}^{\kappa} \theta_k^2 - \wt{\theta}_k^2 & \leq \sum_{k \in [K]^d} \max\Bigl\{(\theta_k + \wt{\theta}_k)(\theta_k - \wt{\theta}_k),0\Bigr\} \\
& \leq \left(\sum_{k \in [K]^d} \max\Bigl\{(\theta_k + \wt{\theta}_k)^2,0\Bigr\}\right)^{1/2} \left(\sum_{k \in [K]^d} (\theta_k - \wt{\theta}_k)^2\right)^{1/2} \\
& \leq \sqrt{2} \norm{f}_{\Leb^2([0,1])} \left(\sum_{k \in [K]^d} (\theta_k - \wt{\theta}_k)^2\right)^{1/2}
\end{aligned}
\end{equation}

It is well known that the error $\theta_k - \wt{\theta}_k$ is due to the aliasing phenomenon, where high frequency sinusoidal waves agree with low frequency waves at evenly spaced points. This phenomenon has a very precise periodic nature---which we formally state in Lemma~\ref{lem:alias}---which allows us to rewrite $\wt{\theta}_k$. Note that for every $\ell \in \mathbb{N}^d$, there exists exactly one $m \in \mathbb{N}_0^d$, $o \in [N]^d$ and $b \in \{0,1\}^d$ such that $\ell = 2mN + (1 - b)o - b((o - 1))$ (where for $a,b \in \Reals^d$ we denote $ab := (a_1b_1,\ldots,a_db_d)$). Let
\begin{equation*}
I_N(m,b,o) = 
\begin{cases*}
2mn + k,& ~~\textrm{if $b = 0, k \in [N]$,} \\
2mn - (k - 2),& ~~\textrm{if $b = 1, k \in [N], k > 1$}, \\
0,& ~~\textrm{if $b = 1$ and $k = 1$.}
\end{cases*}
\end{equation*}
Adopting the convention $\theta_{\ell} := 0$ if $\ell$ contains a non-positive index, we have
\begin{align*}
\wt{\theta}_k & = \frac{1}{n} \sum_{i \in [N]^d} f(\wb{x}_i) \varphi_k(\wb{x}_i) \\
& = \frac{1}{n} \sum_{i \in [N]^d} \Bigl(\sum_{\ell \in \Nbb^d} \theta_{\ell} \varphi_{\ell}(\wb{x}_i) \Bigr) \varphi_k(\wb{x}_i) \\
& = \sum_{\ell \in \mathbb{N}^d} \theta_{\ell} \Bigl(\frac{1}{n} \sum_{i \in [N]^d} \varphi_{\ell}(\wb{x}_i) \varphi_{k}(\wb{x}_i)\Bigr) \\
& = \sum_{m \in \mathbb{N}_0^d} \sum_{o \in [N]^d}\sum_{b \in \{0,1\}^d} \theta_{I_N(m,b,o)}  \Bigl(\frac{1}{n} \sum_{i \in [N]^d} \varphi_{I_N(m,b,o)}(\wb{x}_i) \varphi_{k}(\wb{x}_i)\Bigr).
\end{align*}
We now derive an upper bound on the magnitude of the error $|\theta_k - \wt{\theta}_k|$ by using first Lemma~\ref{lem:alias}, second the Cauchy-Schwarz inequality, and third the fact that whenever $I_N(m,b,k) > 0$, we have $I_N(m,b,k) \geq mN$ (where inequality between two vectors is interpreted entrywise).
\begin{align*}
\abs{\wt{\theta}_k - \theta_k} & \leq 2^{d/2} \sum_{m \in \Nbb_0^d \setminus \{0\}} \sum_{b \in \{0,1\}^d} \abs{\theta_{I_N(m,b,k)}} \\
& \leq 2^{d/2} \biggl(\sum_{m \in \Nbb_0^d \setminus \{0\}} \sum_{b \in \{0,1\}^d} \Bigl(\theta_{I_N(m,b,k)}a_{I_N(m,b,k)}\Bigr)^2 \biggr)^{1/2} \biggl(\sum_{m \in \Nbb_0^d \setminus \{0\}} \sum_{b \in \{0,1\}^d} \Bigl(a_{I_N(m,b,k)}\Bigr)^{-2} \biggr)^{1/2} \\
& \leq 2^{d}N^{-s } \biggl(\sum_{m \in \Nbb_0^d \setminus \{0\}} \sum_{b \in \{0,1\}^d} \Bigl(\theta_{I_N(m,b,k)}a_{I_N(m,b,k)}\Bigr)^2 \biggr)^{1/2} \biggl(\sum_{m \in \Nbb_0^d \setminus \{0\}}\sum_{j = 1}^{d} m_j^{-2s} \biggr)^{1/2} 
\end{align*}
Letting $c(d,s)$ be a finite constant which may change from line to line but can depend only on $d$ and $s$, and recalling the notation $\Nbb_2 = \Nbb_0 \setminus \{0,1\}$, we can upper bound the second sum in the previous display,
\begin{align*}
\sum_{m \in \Nbb_0^d \setminus \{0\}}\sum_{j = 1}^{d} m_j^{-2s} & \overset{(i)}{\leq} d^{2(s + 1)} \sum_{m \in \Nbb_0^d \setminus \{0\}} \Bigl(\sum_{j = 1}^{d} m_j^{-2}\Bigr)^s \\
& \overset{(ii)}{\leq} d^{2(s + 1)} \sum_{\wt{d} = 1}^{d} 2^{\wt{d}} \frac{d!}{\wt{d}!} \sum_{m \in \Nbb_2^{\wt{d}}} \Bigl(\sum_{j = 1}^{d} m_j^{-2}\Bigr)^s \\
& \overset{(iii)}{\leq} d^{2(s + 1)} \sum_{\wt{d} = 1}^{d} 2^{\wt{d}} \frac{d!}{\wt{d}!} \int_{[1,\infty)^{\wt{d}}} \Bigl(\sum_{j = 1}^{\wt{d}} x_j^{-2}\Bigr)^{s} \,dx_1 \ldots \,dx_{\wt{d}} \\
& \leq \sum_{\wt{d} = 1}^{d} c(d,s) \int_{\sqrt{\wt{d}}}^{\infty} r^{-2s + d - 1} \,dr \\
& \overset{(iv)}{\leq} c(d,s) < \infty.
\end{align*}
where $(i)$ is an equality when $d = 1$, and otherwise follows from Jensen's inequality when $d \geq 2$ and $s \geq 1$; $(ii)$ follows from specially considering the edge cases where $m_j = 0$ or $m_j = 1$ for $j = \wt{d} + 1,\ldots, d$; $(iii)$ uses the fact that the Riemann sum of a monotone non-increasing function evaluated at right end points is always no greater than the corresponding integral; and $(iv)$ follows since $2s > d$. We are now in a position to conclude that
\begin{align}
\sum_{k \in [K]^d} (\theta_k - \wt{\theta}_k)^2 & \leq c(d,s) N^{-2s} \sum_{k \in [K]^d} \sum_{m \in \Nbb_0^d \setminus \{0\}} \sum_{b \in \{0,1\}^d} \Bigl(\theta_{I_N(m,b,k)}a_{I_N(m,b,k)}\Bigr)^2 \nonumber \\
& \leq c(d,s) N^{-2s} \sum_{k \in \Nbb^d \setminus [N]^d} \theta_k^2 a_k^2 \nonumber \\
& \leq L c(d,s) N^{-2s} \label{eqn:grid_sobolev_approximation_error6_pf2}
\end{align}
where the second inequality in the preceding display follows since for any $(m,k,b) \neq (m',k',b')$, either $I_N(m,b,k) \neq I_N(m',b',k')$ or $I_N(m,b,k) = I_N(m',b',k') = 0$. We plug~\eqref{eqn:grid_sobolev_approximation_error6_pf2} back in to~\eqref{eqn:grid_sobolev_approximation_error6_pf1} to obtain~\eqref{eqn:grid_sobolev_continuum_sobolev_approximation}.

\subsection{Proof of Lemma~\ref{lem:holder_testing_rate_grid_low_smoothness_lb}}
We note that when $d = 1$, for all $i \in [n]$
\begin{equation*}
\varphi_{n + 1}(\wb{x}_i) = \sqrt{2}\cos\Bigl(\frac{\pi}{2}(2i - 1)\Bigr) = 0.
\end{equation*}
and therefore for $\mathbf{M} = (N + 1,\ldots,N + 1)$ and all $i \in [N]^d$,
\begin{equation*}
\varphi_{\mathbf{M}}(\wb{x}_i) = 0.
\end{equation*}
Letting 
\begin{equation*}
a = \frac{L}{2^{d/2}\pi^s(d+1)^s}N^{-s},~~ g = a \varphi_{N+1}
\end{equation*}
some simple calculations show that $g$ satisfies the conditions of Lemma~\ref{lem:holder_testing_rate_grid_low_smoothness_lb}.

\subsection{Proof of Proposition~\ref{prop:holder_testing_rate_grid_low_smoothness_ub}}
Let $f \in C^s(\Xset;L)$ satisfy
\begin{equation*}
\norm{f}_{\Leb}^2 \geq c b L^2 n^{-2s/d}
\end{equation*}
for a sufficiently large constant $c$. Note that when $\kappa = n$, $\pi_{\kappa,\wb{G}}(f) = f$ when the latter is considered as a function on $\wb{X}$, i.e. a vector. Therefore the claimed result follows from Lemma~\ref{lem:fixed_graph_testing}, once we show
\begin{equation*}
\frac{1}{n} \sum_{i \in [N]^d} \bigl(f(\wb{x}_i)\bigr)^2 \geq 2b \sqrt{\frac{2}{n}}
\end{equation*}
However, it follows from~\eqref{eqn:grid_empirical_norm_higher_order} that the above inequality is satisfied whenever
\begin{equation*}
\frac{1}{n} \sum_{i \in [N]^d} \bigl(f(\wb{x}_i)\bigr)^2 \geq \max\{c L^2 n^{-2s/d}, b n^{-1/2}\} \leq c b L^2 n^{-2s/d}
\end{equation*}
when $4s \leq d$.



\clearpage


\subsection{Proof of Proposition~\ref{prop:L4_testing_rate}}
Let $T_{\mathrm{mean}} = \frac{1}{n}\sum_{i = 1}^{n} y_i^2$. The expectation of $T_{\mathrm{mean}}$ is
\begin{equation*}
\Ebb(T_{\mathrm{mean}}) = \mathbb{E}(f^2(x_1)) + 1,
\end{equation*}
and the variance can be upper bounded
\begin{equation*}
\Var(T_{\mathrm{mean}}) \leq \frac{1}{n}(3 + p_{\max} \norm{f}_{\Leb^4}^4 + p_{\max}\norm{f}_{\Leb^2}^2) = \frac{c}{n}.
\end{equation*}
Since $\mathbb{E}(f^2(x_1)) \geq p_{\min} \epsilon^2$, when $\epsilon^2 \gtrsim b\cdot n^{-1/2}$ we can apply Chebyshev's inequality to obtain the claimed result.

\subsection{Proof of Theorem~\ref{thm:twosample_sobolev_testing_rate}}

As mentioned previously, to prove Theorem~\ref{thm:twosample_sobolev_testing_rate}, we relate the two sample model to the following regression model: we observe
$X = \{x_1,\ldots,x_n\} \sim \mu$ (where we recall $\mu = (P + Q)/2$), and associated labels
\begin{equation*}
a_i = 
\begin{cases}
1, & \textrm{with probability}~ \frac{p(x_i)}{p(x_i) + q(x_i)} \\
-1, & \textrm{with probability}~ \frac{q(x_i)}{p(x_i) + q(x_i)}
\end{cases}
\end{equation*}
where $a_i$ is conditionally independent of $x_j,a_j$ given $x_i$. 

\textcolor{red}{TODO:} Complete the above argument.

Our analysis will trace a similar path to the proof of Theorem~\ref{thm:sobolev_testing_rate}, proceeding according to the following steps:
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $a$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded. One of these functionals will be a measure of eigenvector incoherence.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta^{(p)} = (\beta_1^{(p)},\ldots,\beta_n^{(p)}) \in \Reals^n$, $\beta^{(q)} = (\beta_1^{(q)},\ldots,\beta_n^{(q)}) \in \Reals^n$ be non-negative signals over the vertices $V$. We observe labels $a = (a_1,\ldots,a_n)$ according to the model
\begin{equation*}
a_i = 
\begin{cases*}
1, & \textrm{with probability $\frac{\betap_i}{\betap_i + \betaq_i}$} \\
-1, & \textrm{with probability $\frac{\betaq_i}{\betap_i + \betaq_i}$}
\end{cases*}
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec}^{(2)} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec}^{(2)} = \1\{T_{\spec}^{(2)} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $\Pi_{\max}(\kappa;G)$ be a measure of the incoherence of the eigenvectors $v_1,\ldots,v_\kappa$, given by
\begin{equation*}
\Pi_{\max}(\kappa;G) := \max_{i = 1,\ldots,n} \left\{\sum_{k = 1}^{\kappa} v_{k,i}^2\right\}
\end{equation*}
In Lemma~\ref{lem:twosample_fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}^{(2)}$. Our bound on the Type II error will be stated as a function of $\Pi_{\max}(\kappa;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$ and the smoothness functional $S_2(\beta;G)$. We use the notation $\varDelta^{(p,q)} = (\betap - \betaq)/(\betap + \betaq)$.

\begin{lemma}
	\label{lem:twosample_fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\betap = \betaq$, the Type I error of $\phi_{\spec}^{(2)}$ is upper bounded
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_I_error}
		\mathbb{E}_{\betap,\betap}(\phi_{\spec}^{(2)}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} Suppose $\Pi_{\max}(\kappa;G) \leq 1$. Then for any $b$, $\betap$ and $\betaq$ such that
		\begin{equation}
		\label{eqn:twosample_fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} (\varDelta_i^{(p,q)})^2 \geq \frac{1}{1 - \Pi_{\max}(\kappa;G)}\left(2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\varDelta;G)}{ns_{\kappa}}\right)
		\end{equation}
		the Type II error of $\phi_{\spec}^{(2)}$ is upper bounded,
		\begin{equation}
		\label{eqn:twosample_graph_spectral_type_II_error}
		\mathbb{E}_{\betap,\betaq}(1 - \phi_{\spec}^{(2)}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:twosample_fixed_graph_testing}.}

To prove Lemma~\ref{lem:twosample_fixed_graph_testing} we will compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}^{(2)}$. The inequalities \eqref{eqn:twosample_graph_spectral_type_I_error} and \eqref{eqn:twosample_graph_spectral_type_II_error} can then be computed using Chebyshev's inequality in a manner very similar to the proof of Lemma~\ref{lem:fixed_graph_testing}, and we omit the details. Let $w_i := a_i - \vardeltapq_{i}$, and note that $\Ebb(w_i) = 0$ and
\begin{equation*}
\Ebb(w_i^2) = \frac{\betap_i \betaq_i}{(\betap_i + \betaq_i)^2} = 1 - (\vardeltapq_{i})^2,~~ \Ebb(w_i^3) = \frac{1}{2}(1 - (\vardeltapq_{i})^2)\vardeltapq_{i}
\end{equation*}

\vspace{.2 in}

\textit{Mean of $T_{\spec}^{(2)}$}:

We have
\begin{align*}
\Ebb(T_{\spec}^{(2)}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\vardeltapq}{v_k}^2 + \Ebb\bigl(\dotp{w}{v_k}^2 + 2 \dotp{w}{v_k} \dotp{\vardeltapq}{v_k}\bigr)\right) \\
& = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{1 - (\vardeltapq)^2}{v_k^2} + \dotp{\vardeltapq}{v_k}\right) \\
& \geq \frac{\kappa}{n} + \left(\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} - \dotp{(\vardeltapq)^2}{v_k^2}\right)
\end{align*}
When $\vardeltapq = 0$, this equals $\kappa/n$. Otherwise, we have the following upper bound:
\begin{align*}
\sum_{k = 1}^{\kappa} \dotp{(\vardeltapq)^2}{v_k^2}  & = \sum_{i = 1}^{n} (\vardeltapq_{i})^2 \left(\sum_{k = 1}^{\kappa} v_{k,i}^2 \right)\\
& \leq \Pi_{\max}(\kappa,G) \cdot \sum_{i = 1}^{n} (\vardeltapq_{i})^2 = \Pi_{\max}(\kappa,G) \cdot \norm{\vardeltapq}_2^2.
\end{align*}
Additionally, $\sum_{k = 1}^{\kappa}\dotp{\vardeltapq}{v_k} \geq \norm{\vardeltapq}_2^2 - S_2(\beta;G)/s_{\kappa}$ by reasoning given in the proof of Lemma~\ref{lem:fixed_graph_testing}, and as a result 
\begin{equation*}
\Ebb(T_{\spec}^{(2)}) \geq \frac{\kappa}{n} -  \frac{\bigl(1 - \Pi_{\max}(\kappa;G)\bigr)}{n} \norm{\vardeltapq}_2^2 - \frac{S_2(\beta;G)}{ns_{\kappa}}
\end{equation*}

\vspace{.2 in}
\textit{Variance of $T_{\spec}^{(2)}$:}

We have
\begin{align}
\Var(T_{\spec}^{(2)}) & = \frac{1}{n^2}\Var(2\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w} + \dotp{V_{\kappa}V_{\kappa}^Tw}{w}) \nonumber \\
& = \frac{1}{n^2}\Bigl\{4\underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w})}_{=: V_1} + 2\underbrace{\Cov(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w},\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=: K_1} + \underbrace{\Var(\dotp{V_{\kappa}V_{\kappa}^Tw}{w})}_{=:V_2}\Bigr\}, \label{eqn:var}
\end{align}
and we now upper bound each of the three terms on the right hand side of the previous display.

\textbf{Upper bound on $V_1$:}
Let $\Sigma := \Cov(w)$ be the covariance matrix of $w$. Noting that $\Sigma \preceq I$, we have
\begin{equation}
\label{eqn:var1}
\Var(\dotp{V_{\kappa}V_{\kappa}^T\vardeltapq}{w}) =(\vardeltapq)^T V_{\kappa}V_{\kappa}^T \Sigma V_{\kappa}V_{\kappa}^T \vardeltapq \leq \norm{V_{\kappa}V_{\kappa}^T\vardeltapq}^2.
\end{equation}

\textbf{Upper bound on $K_1$:}
Noting that $\Ebb(\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w}) = 0$, we have that
\begin{align}
K_1 & = \Ebb\left[\dotp{V_{\kappa}V_{\kappa}^T \vardeltapq}{w} \dotp{V_{\kappa}V_{\kappa}^T w}{w}\right] \nonumber \\
& = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \sum_{i' = 1}^{n} \Ebb\left[w_i w_j (V_{\kappa}V_{\kappa}^T)_{ij} w_{i'} (V_{\kappa}V_{\kappa}^T\vardeltapq)_i\right] \nonumber \\
& = \sum_{i = 1}^{n} \Ebb\left[w_i^3\right] (V_{\kappa}V_{\kappa}^T)_{ii} (V_{\kappa}V_{\kappa}^T \vardeltapq)_i \nonumber \\
& = \frac{1}{2}\sum_{i = 1}^{n} (1 - (\vardeltapq_{i})^2)(\vardeltapq_{i}) (V_{\kappa} V_{\kappa}^T)_{ii}  (V_{\kappa}V_{\kappa}^T\vardeltapq)_i \nonumber \\
& \leq \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \left(\sum_{i = 1}^{n} (\vardeltapq_{i})^2 (V_{\kappa} V_{\kappa}^T)_{ii}^2\right)^{1/2} \nonumber \\
& \leq \Pi(\kappa,G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq}  \label{eqn:var2} 
\end{align}

\textbf{Upper bound on $V_2$:}
$V_2$ is a variance of a sum, which we re-express as the sum of covariances:
\begin{align*}
V_2 & = \Var(\dotp{V_{\kappa}V_{\kappa}^T w}{w}) \\
& = \sum_{i,j,i',j' = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij} (V_{\kappa}V_{\kappa}^T)_{i'j'} \Cov(w_i w_j, w_{i'} w_{j'}).
\end{align*}
This covariance will be non-zero only when $i = i' \neq j = j'$, $i = j' \neq j = i'$, or $i = i' = j = j'$, and therefore
\begin{align}
V_2 & = 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 \Var(w_iw_j) + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \Var(w_i^2) \nonumber\\
& \leq 2\sum_{i = 1}^{n}\sum_{j = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ij}^2 + \sum_{i = 1}^{n} (V_{\kappa}V_{\kappa}^T)_{ii}^2 \leq 3 \mathrm{tr}\bigl((V_{\kappa}V_{\kappa}^T)^2\bigr) = 3\kappa. \label{eqn:var3}
\end{align}

Now, plugging \eqref{eqn:var1}, \eqref{eqn:var2}, and \eqref{eqn:var3} back into \eqref{eqn:var}, we obtain
\begin{equation}
\label{eqn:var_5}
\Var(T_{\spec}^{(2)}) \leq \frac{1}{n^2}\left\{4\norm{V_{\kappa}V_{\kappa}^T \vardeltapq}^2 + \Pi(\kappa;G) \cdot \norm{V_{\kappa}V_{\kappa}^T \vardeltapq} \cdot \norm{\vardeltapq} + 3\kappa \right\}.
\end{equation}


\subsubsection{Step 2: Bounding neighborhood graph functionals}

Note that since $p,q \in H^1(\mathcal{X};R)$, we have $f := p - q \in H^1(\mathcal{X};2R)$. Therefore the bounds~\eqref{eqn:continuous_to_discrete_sobolev_norm} and \eqref{eqn:l2_to_empirical_norm} apply to $S_2(\varDelta,G_{n,r})$ and $(1/n)\sum_{i = 1}^{n}\varDelta_i^2$, respectively. The bound~\eqref{eqn:eigenvalue_tail_bound} continues to apply to $s_{\kappa}$. What remains is to upper bound the incoherence parameter
\begin{equation}
\label{eqn:pi_max}
\Pi_{\max}(\kappa,G_{n,r}) \leq \frac{1}{2}~~\textrm{when}~ \kappa = n^{2/5}~\textrm{and}~ r = \log^a n \cdot \left(\frac{\log n}{n}\right)
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$.

Our proof of~\eqref{eqn:pi_max} will proceed by relating $G_{n,r}$ to a graph which satsifies a strict notion of incoherence, the chain graph. When $G = \overline{G}$, the incoherence parameter $\Pi_{\max}$ is upper bounded
\begin{equation}
\Pi_{\max}(\kappa,\overline{G}) \leq \frac{\kappa}{n}.
\end{equation}
To prove that the weaker bound \eqref{eqn:pi_max} holds with respect to $G_{n,r}$, we will show that the two graphs $\overline{G}$ and $G_{n,r}$ are $\delta$-spectrally similar, meaning
\begin{equation}
\label{eqn:spectral_similarity}
(1 - \delta_n) x^T L x \leq x^T \overline{L} x \leq (1 + \delta_n) x^T L x~~\textrm{for all}~x \in \Reals^n, \delta_n = c \cdot (\log n)^{h(a,1) + da}\cdot \frac{\log n}{n}
\end{equation}
with probability $1 - o(1)$ as $n \to \infty$. The proof of~\eqref{eqn:spectral_similarity} relies heavily on bounding the transport distance between $X$ and $\overline{X}$, i.e. \eqref{eqn:slepcev_transport_distance}. Then the following technical Lemma along with \eqref{eqn:spectral_similarity} will imply \eqref{eqn:pi_max}.
\begin{lemma}
	\label{lem:pi_max_pf_1}
	Let $G$ satisfy~\eqref{eqn:spectral_similarity} for a given $0 \leq \delta < 1$. Then for any $R \gtrsim n \delta^{1/2} (s_{n}^{1/2} \vee 1)$ and $k \leq n/8$,
	\begin{equation*}
	v_{k,i}^2 \lesssim \frac{1}{n}\left(R + \frac{n^4 \delta^2 s_{n}^2}{R^3}\right) \quad \textrm{for every $i = 1,\ldots,n$.}
	\end{equation*} 
\end{lemma}

\textcolor{red}{TODO}: Explain how Lemma~\ref{lem:pi_max_pf_1} implies~\eqref{eqn:pi_max}.


At a high level, Lemma~\ref{lem:pi_max_pf_1} is proved through repeated applications of the Davis-Kahan Theorem. The full proof is long and we delay presenting it until Section~\ref{sec:technical_lemma_proofs}.

\subsubsection{Step 3: Conclusion}

With Lemma~\ref{lem:twosample_fixed_graph_testing} as well as \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, \eqref{eqn:l2_to_empirical_norm} and \eqref{eqn:pi_max} in hand, the proof of Theorem~\ref{thm:twosample_sobolev_testing_rate} follows by similar reasoning to the conclusion of Theorem~\ref{thm:sobolev_testing_rate}.

\section{Proofs of Technical Results}
\label{sec:technical_lemma_proofs}

Here we give in full detail the proofs of Lemma~\ref{lem:roughness_functional_expectation_sobolev}, \eqref{eqn:spectral_similarity}, and Lemma~\ref{lem:pi_max_pf_1}.

\subsection{Proof of Lemma~\ref{lem:roughness_functional_expectation_sobolev}}
To simplify exposition, we introduce the iterated difference operator, defined recursively as
\begin{equation*}
D_{jk}f(x) = (D_{k}f(x_j) - D_{k}f(x))\frac{K_r(x_j,x)}{r^d},~~ D_jf(x) = (f(x_j) - f(x))\frac{K_r(x_j,x)}{r^d}~~ \textrm{for $j \in [n], k \in [n]^q$}
\end{equation*}
We will also use the notation $d_jf(x) := (f(x_j) - f(x))$. We split our analysis into cases based on whether $s$ is even or odd. 

\subsubsection{Case 1: $s$ is even.}
When $s$ is even, letting $q = s/2$ we have the decomposition
\begin{equation}
\label{eqn:continuous_to_discrete_sobolev_norm_pf1}
f^T L^s f =  r^{ds} \cdot \sum_{i = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} D_kf(x_i) D_{\ell}f(x_i). 
\end{equation}

For given index vectors $k,\ell \in [n]^q$ and indices $i,j$, let $I = \abs{k \cup \ell \cup i}$ be the total number of unique indices. We separate our analysis into cases based on the magnitude of $I$, specifically whether $I = s + 1$ (the leading terms where all indices are distinct) $I < s + 1$ (the terms where at least one index is repeated) and show that
\begin{equation}
\label{eqn:expected_difference_operators_1}
\Ebb(D_kf(x_i) D_\ell f(x_i)) =
\begin{cases*}
O(r^{2s}) \cdot \norm{f}_{H^s(\Xset)}^2, & ~~\textrm{if $I = s + 1$} \\
O(r^{2} r^{d(I - (s + 1))}) \cdot [f]_{H^1(\Xset)}^2, & ~~\textrm{if $I < s + 1$}~ 
\end{cases*}
\end{equation}
We will prove~\eqref{eqn:expected_difference_operators_1} in Section~\ref{subsec:expected_difference_operators_pf}. First, we verify that~\eqref{eqn:continuous_to_discrete_sobolev_norm_pf1} and \eqref{eqn:expected_difference_operators_1} are together enough to show Lemma~\ref{lem:roughness_functional_expectation_sobolev} when $s$ is even. In the sum on the right hand side of~\eqref{eqn:continuous_to_discrete_sobolev_norm_pf1}, there are $O(n^{I})$ terms with exactly $I$ distinct indices. When $I < s + 1$, by~\eqref{eqn:expected_difference_operators_1} the total contribution of such terms to the sum is $O(n^{I}r^{d(I - 1) + 2}) \cdot [f]_{H^1(\Xset)}^2$. Since by assumption $r \geq n^{-1/d}$, this increases with $I$. Taking $I = s$ to be the largest integer less than $s + 1$, the contribution of these terms to the sum is therefore $O(n^sr^{d(s - 1) + 2}) \cdot [f]_{H^1(\Xset)}^2$ which in light of the restriction $r \geq n^{-1/(2(s - 1) + d)}$ is $O(n^{s+1}r^{s(d +2)}) \cdot [f]_{H^1(\Xset)}^2$. On the other hand when $I = s + 1$, by~\eqref{eqn:expected_difference_operators_1} we immediately have that the total contribution of these terms is $O(n^{s + 1}r^{2(s + d)}) \cdot \norm{f}_{H^s(\Xset)}$. Therefore,
\begin{equation*}
\Ebb(f^T L^s f) = O(n^{s+1}r^{s(d+2)}) \cdot \norm{f}_{H^s(\Xset)}^2.
\end{equation*}

\subsubsection{Case 2: $s$ is odd.}
When $s$ is odd, letting $q = (s - 1)/2$ we have
\begin{equation}
\label{eqn:roughness_functional_representation_odd}
f^T L^s f =  r^{ds} \cdot \sum_{i,j = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} \bigl(d_jD_kf(x_i)\bigr) \cdot  \bigl(d_jD_{\ell}f(x_i)\bigr) \cdot K_r(x_i,x_j).
\end{equation}
For given index vectors $k,\ell \in [n]^q$ and indices $i,j \in [n]$, let $I = \abs{k \cup \ell \cup i \cup j}$ be the total number of unique indices. Similar to the case when $s$ is even, we show that 
\begin{equation}
\label{eqn:expected_difference_operators_2}
\Ebb(d_iD_kf(x_j) d_iD_\ell f(x_j)) =
\begin{cases*}
O(r^{2s}) \cdot \norm{f}_{H^s(\Xset)}^2, & ~~\textrm{if $I = s + 1$} \\
O(r^{2} \cdot r^{d(I - (s + 1))}) \cdot [f]_{H^1(\Xset)}^2, & ~~\textrm{if $I < s + 1$}~ 
\end{cases*}
\end{equation}
Then Lemma~\ref{lem:roughness_functional_expectation_sobolev} follows from similar reasoning to the case where $s$ was even.

\subsubsection{Proof of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2}}
\label{subsec:expected_difference_operators_pf}

We first prove the desired bounds in the case when some indices are repeated, and then the desired bounds in the case when all indices are distinct. 

\paragraph{Repeated indices.}

Since the proofs of~\eqref{eqn:expected_difference_operators_1} and~\eqref{eqn:expected_difference_operators_2} are essentially the same for the case where some index is repeated, we will assume without loss of generality that $s$ is even. Let $k,\ell \in [n]^q$ be index vectors for $q = s/2$. 

When at least one index is repeated, we obtain a sufficient upper bound by reducing the problem of upper bounding the iterated difference operator to that of upper bounding a single difference operator. Letting $k = (k_1,\ldots,k_q)$, we can show by induction that the absolute value of the iterated difference operator $\abs{D_kf(x_i)}$ is upper bounded by
\begin{equation*}
\abs{D_kf(x_i)} \leq \left(\frac{2K_{\max}}{r^d}\right)^{q-1} \sum_{h \in k \cup i} \abs{D_{k_q}f(x_h)} \cdot \1\{G_{n,r}[X_{k \cup i}]~\textrm{is a connected graph} \}.
\end{equation*}
Therefore,
\begin{align}
\abs{D_kf(x_i)} \cdot \abs{D_{\ell}f(x_i)} & \leq \left(\frac{2K_{\max}}{r^d}\right)^{2(q - 1)} \sum_{h,j \in k \cup \ell \cup i} \abs{D_{k_q}f(x_h)} \cdot \abs{D_{\ell_q}f(x_j)} \cdot \1\{G_{n,r}[X_{k \cup i}], G_{n,r}[X_{\ell \cup i}]~\textrm{are connected graphs.} \} \nonumber \\
& =  \left(\frac{2K_{\max}}{r^d}\right)^{2(q - 1)} \sum_{h,j \in k \cup \ell \cup i} \abs{D_{k_q}f(x_h)} \cdot \abs{D_{\ell_q}f(x_j)} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is a connected graph.} \} \label{eqn:expected_difference_operators_sobolev_pf0}
\end{align}

We now break our analysis into three cases, based on the number of distinct indices in $k_q,\ell_q,h,j$. In each case we will obtain the same rate
\begin{equation*}
\Ebb\Bigl[\abs{D_{k_q}f(x_h)} \cdot \abs{D_{\ell_q}f(x_j)}\Bigr] = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [f]_{H^1(\Xset)}^2,
\end{equation*}
and plugging this back in to~\eqref{eqn:expected_difference_operators_sobolev_pf0} we have that for any $k, \ell \in [n]^q$
\begin{equation*}
\Ebb\Bigl[\bigl|D_{k}f(x_i)\bigr| \cdot \bigl|D_{\ell}f(x_i)\bigr|\Bigr] = O(r^{(\abs{k \cup \ell \cup i} - (2q + 1))d + 2}) \cdot [f]_{H^1(\Rd)}^2.
\end{equation*}

\textit{Case 1: Two distinct indices.}
Let $k_q = \ell_q = i$, and $h = j$. Using the law of iterated expectation, we obtain
\begin{align*}
\Ebb \left[\bigl(D_{i}f(x_j)\bigr)^2 \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\right] &= \Ebb \left[\bigl(D_{i}f(x_j)\bigr)^2 \cdot \Pbb\bigl[\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \} |x_i,x_j\bigr]\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 2)d}) \cdot \Ebb\left[\bigl(D_{i}f(x_j)\bigr)^2\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d}) \cdot \Ebb\left[\bigl(d_{i}f(x_j)\bigr)^2K_r(x_i,x_j)\right] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [f]_{H^1(\Xset)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm}.

\textit{Case 2: Three distinct indices.}
Let $k_q = \ell_q = i$, for some $i \neq j \neq h$. Using the law of iterated expectation, we obtain
\begin{align*}
\Ebb \Bigl[ & \abs{D_{i}f(x_j)} \cdot \abs{D_if(x_h)} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\Bigr] = \nonumber \\
& \Ebb\Bigl[\abs{D_{i}f(x_j)} \cdot \abs{D_if(x_h)} \cdot \Pbb\bigl[\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \} |x_i,x_j,x_h\bigr]\Bigr] \nonumber \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d}) \cdot \Ebb\Bigl[\abs{D_{i}f(x_j)} \cdot \abs{D_if(x_h)} \Bigr] \nonumber \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2})\cdot[f]_{H^1(\Xset)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm_2}.


\textit{Case 3: Four distinct indices.}
Using the law of iterated expectation, we find that
\begin{align*}
\Ebb\Bigl[ &\abs{D_{k_q}f(x_i)}\cdot{\abs{D_{\ell_q}f(x_j)}} \cdot \1\{G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected} \}\Bigr] \\
& = \Ebb\Bigl[\abs{D_{k_q}f(x_i)}\cdot{\abs{D_{\ell_q}f(x_j)}} \cdot\Pbb\bigl[G_{n,r}[X_{k \cup \ell \cup i}] ~\textrm{is connected}|x_i,x_j,x_{k_q},x_{\ell_q}\bigr]\Bigr] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 4)d}) \cdot \Ebb\Bigl[\abs{D_{k_q}f(x_i)}\cdot{\abs{D_{\ell_q}f(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \\
& = O(r^{(\abs{k \cup \ell \cup i} - 3)d + 2}) \cdot [f]_{H^1(\Xset)}^2
\end{align*}
where the last equality follows from Lemma~\ref{lem:expected_first_order_seminorm_3}.

\paragraph{All indices distinct.}

We first show the desired result when $s$ is even, and then when $s$ is odd. 

\textit{Case 1: $s$ is even.}

The result follows from the law of iterated expectation, the condition that the density be upper bounded, and equations~\eqref{eqn:leading_term_sobolev_compact_1} and \eqref{eqn:leading_term_sobolev_compact_2} in Lemma~\ref{lem:leading_term_sobolev_compact}
\begin{equation*}
\Ebb\bigl[D_kf(x_i)D_kf(x_j)\bigr] = \Ebb\Bigl[\bigl(\Ebb[D_kf(x_i)|x_i]\bigr)^2\Bigr] \leq p_{\max} \norm{\Ebb\bigl[D_kf\bigr]}_{\Leb^2(\Xset)}^2 \leq c r^{2s} \norm{f}_{H^s(\Xset)}^2.
\end{equation*}

\textit{Case 2: $s$ is odd.}

By Lemma~\ref{lem:leading_term_sobolev_compact}, we have that for $k \in (n)^{(s - 1)/2}$, there exists a function $f_{s - 1,q} \in H_0^1(\Xset)$ such that
\begin{equation*}
\Bigl\|\Ebb\bigl[D_kf\bigr] - r^{2{s - 1}}f_{s - 1,q}\Bigr\|_{\Leb^2(\Xset)} \leq c r^{2s} \norm{f}_{H^s(\Rd)}^2
\end{equation*}
and $\norm{f_{s - 1,q}}_{H^1(\Xset)} \leq c \norm{f}_{H^s(\Xset)}$. By the law of iterated expectation, and writing $\varDelta := \Ebb\bigl[D_kf\bigr] - r^{2{s - 1}}f_{s - 1,q}$, we have
\begin{align*}
\Ebb\biggl[\Bigl(d_iD_kf(x_j)\Bigr) \Bigl(d_iD_{\ell}f(x_j)\Bigr) K_r(x_i,x_j)\biggr] & =  \Ebb\biggl[\Bigl(d_i\bigl(\Ebb[D_kf|x_i,x_j]\bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \nonumber \\
& = \Ebb\biggl[\Bigl(d_i\bigl(r^{s - 1}\cdot f_{s - 1,q} + \varDelta \bigr)(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \\
& \leq 2r^{2(s - 1)}\Ebb\biggl[\Bigl(d_i\bigl(f_{s - 1,q}\bigr)\Bigr)^2K_r(x_i,x_j)\biggr]  + 2 \Ebb \biggl[\Bigl(d_i\varDelta(x_j)\Bigr)^2K_r(x_i,x_j)\biggr] \\
& \leq 2r^{2(s - 1)}\Ebb\biggl[\Bigl(d_i\bigl(f_{s - 1,q}\bigr)\Bigr)^2K_r(x_i,x_j)\biggr] + 2p_{\max}^2\norm{\varDelta}_{\Leb^2(\Xset)}^2 \\
& \leq c r^{2s} \norm{f}_{H^s(\Xset)}^2
\end{align*}
where the last inequality follows by Lemma~\ref{lem:expected_first_order_seminorm}.

\subsection{Proof of Lemma~\ref{lem:grid_laplacian_approximation_error}}
\label{subsec:grid_laplacian_approximation_error_pf}
We prove by induction on $q$. In the base case, $q = 1$, $s = 3$ or $s = 4$, and we assume $\wb{x} \in \wb{X}_{h}$. By taking Taylor expansions of $f(\wb{x} + e_ih)$ and $f(\wb{x} - e_ih)$ around $f(\wb{x})$ for $i = 1,\ldots,d$, we obtain
\begin{align}
\wb{L}f(\wb{x}) & = \sum_{i = 1}^{d} D_{e_i^2}f(\wb{x}) \nonumber \\
& = \sum_{i = 1}^{d} \Biggl[\sum_{\alpha = 1}^{s - 1}\frac{1}{\alpha!}\frac{\partial^{\alpha}}{\partial e_i^{\alpha}} f(\wb{x})\bigl(h^{\alpha} + (-h)^{\alpha}\bigr) + R(\wb{x}) \Biggr] \nonumber \\
& = \sum_{i = 1}^{d} \Biggl[\sum_{\beta = 1}^{\floor{(s - 1)/2}}\frac{2}{(2\beta)!}\frac{\partial^{2\beta}}{\partial e_i^{2\beta}} f(\wb{x}) h^{2\beta} + R(\wb{x})\Biggr] \label{eqn:grid_laplacian_approximation_error_pf1}
\end{align} 
where the remainder term is upper bounded
\begin{equation*}
\abs{R(\wb{x})} = \abs{\frac{1}{s!}\sum_{i = 1}^d \int_{-h}^{h} \frac{\partial^s}{\partial e_i^s} f(\wb{x} + e_it) t^{s - 1} \,dt} \leq \frac{d}{(s + 1)!} L h^s 
\end{equation*}
When $s = 3$ or $s = 4$, the first sum includes only second-order derivatives,
\begin{equation*}
\sum_{i = 1}^{d} \sum_{\beta = 1}^{\floor{(s - 1)/2}}\frac{2}{(2\beta)!}\frac{\partial^{2\beta}}{\partial e_i^{2\beta}} f(\wb{x}) h^{2\beta} = h^2 \sum_{i = 1}^d \frac{\partial^{2\beta}}{\partial e_i^{2\beta}} f(\wb{x}) = h^2 \Delta f(\wb{x}),
\end{equation*}
and the claim is shown.

When $q \geq 2$, we use the prior analysis to rewrite the $q$th-order iterated Laplacian
\begin{align}
\wb{L}^qf(\wb{x}) & = \wb{L}^{q - 1} \Biggl(\sum_{i = 1}^d \sum_{\beta = 1}^{\floor{(s - 1)/2}} \frac{2}{(2\beta)!} \frac{\partial^{2\beta}}{\partial e_i^{2\beta}} f h^{2 \beta} + R\Biggr)(\wb{x}) \nonumber \\
& = \sum_{\beta = 1}^{\floor{(s - 1)/2}} \frac{2h^{2\beta}}{(2\beta)!} \wb{L}^{q - 1} \biggl(\sum_{i = 1}^d\frac{\partial^{2\beta}}{\partial e_i^{2\beta}} f \biggr)(\wb{x}) + \wb{L}^{q - 1}R(\wb{x}) \label{eqn:grid_laplacian_approximation_error_pf5}
\end{align}
We deal with the remainder term first. Noting that $Q_{2(q - 1)h}(\wb{x}) \subset \Xset_{h}$, we have
\begin{align}
\abs{\wb{L}^{q - 1}R(\wb{x})} & \leq \bigl(2(q - 1)d\bigr)^{q - 1} \cdot \max_{\wb{z} \in Q_{2(q - 1)h}(\wb{x})} \bigl\{\abs{R(\wb{z})}\bigr\} \nonumber \\
& \leq \frac{\bigl(2(q - 1)d\bigr)^{q - 1}d}{(s + 1)!} L h^s \label{eqn:grid_laplacian_approximation_error_pf2}
\end{align}
Next we deal with the higher-order terms in the first sum, meaning the terms where $\beta \geq 2$. For convenience, denote $f_\beta := \sum_{i = 1}^d\frac{\partial^{2\beta}}{\partial e_i^{2\beta}} f$, and note that $f_{\beta} \in C^{s - 2\beta}(\Xset;L)$.  We will show that for any $\wb{x} \in \wb{X}_{qh}$,
\begin{equation}
\label{eqn:grid_laplacian_approximation_error_pf3}
\abs{\wb{L}^{q - 1}f_{\beta}(\wb{x})} \leq c L h^{s - 2\beta}.
\end{equation}
which will be sufficient for the purposes of proving the Lemma. 

Let us assume $s - 2\beta$ is odd. Set $s' = (s - 2\beta - 1)/2$, and $s'' = q - 1 - (s' + 1)$, and note that $s''$ is a non-negative integer. By hypothesis, for any $\wb{z} \in \wb{X}_{hs'}$
\begin{equation*}
\abs{\wb{L}^{s'}f_{\beta}(\wb{z}) - h^{2s'}\Delta^{s'}f_{\beta}(\wb{z})} \leq cLh^{s - 2\beta}
\end{equation*}
and consequently,
\begin{align*}
\abs{\wb{L}^{q - 1}f_{\beta}(\wb{x})} & \leq (2s''d)^{s''} \cdot \max_{\wb{z} \in Q_{2s''h}(\wb{x})} \Bigl\{\abs{\wb{L}^{s' + 1}f(\wb{z})}\Bigr\} \\
& \leq (2s''d)^{s''} \cdot \max_{\wb{z} \in \wb{X}_{(s' + 1)h}} \Bigl\{\abs{\wb{L}^{s' + 1}f(\wb{z})}\Bigr\}  \\
& \leq (2s''d)^{s''} \Bigl( h^{2s} \cdot \max_{\wb{z} \in \wb{X}_{(s' + 1)h}} \Bigl\{ \abs{\wb{L} \Delta^{s'} f_{\beta}(\wb{z})} \Bigr\} + cLh^{s - 2\beta}\Bigr) \\
& \leq (2s''d)^{s''} \Bigl( 2dL h^{2s' + 1}  + cLh^{s - 2\beta}\Bigr)
\end{align*}
where the last line follows since $\Delta^{s'}{f_{\beta}} \in C^1(\ol{X};L)$. Since $2s' + 1 = s - 2\beta$, we have shown the desired result~\eqref{eqn:grid_laplacian_approximation_error_pf3}. Similar manipulations prove the result when $s - 2\beta$ is even. 

Finally, we consider the second order term, where $\beta = 1$ and $f_\beta = \Delta f \in C^{s - 2}(\Xset;L)$. Then, since either $q - 1 = (s - 3)/2$ or $q - 1 = (s - 4)/2$, by hypothesis for any $\wb{x} \in \wb{X}_{qh} \subset \wb{X}_{(q - 1)h} $,
\begin{equation}
\label{eqn:grid_laplacian_approximation_error_pf4}
h^{2}\abs{ \wb{L}^{q - 1} \Delta f(\wb{x}) - h^{2(q - 1)} \Delta^{q} f(\wb{x})  } \leq cL h^{2}h^{s - 2} = cLh^s
\end{equation}
The claimed result follows upon plugging~\eqref{eqn:grid_laplacian_approximation_error_pf2},\eqref{eqn:grid_laplacian_approximation_error_pf3}, and \eqref{eqn:grid_laplacian_approximation_error_pf4} back into~\eqref{eqn:grid_laplacian_approximation_error_pf5}.

\subsection{Proof of~\eqref{eqn:spectral_similarity}}

Assuming~\eqref{eqn:slepcev_transport_distance} holds and $r$ is chosen according to~\eqref{eqn:pi_max}, we have already shown that the upper bound in~\eqref{eqn:spectral_similarity} holds for $\delta = 0$ (see the proof of ~\eqref{eqn:eigenvalue_tail_bound}). Now we need only to show the lower bound, which we will do following these steps:
\begin{enumerate}[(i)]
	\item First, we consider the case of two fixed graphs $G$ and $\wt{G}$, and establish a Poincare inequality. In other words, we show that if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$, then the Laplacian matrix $L$ cannot be much larger $\wt{L}$, in the usual sense of partial ordering of matrices.
	\item Then, we treat the case of $G = G_{n,r}$ and $\wt{G} = \overline{G}$. We assume that~\eqref{eqn:slepcev_transport_distance} holds, and exhibit such a mapping between edges in $G_{n,r}$ and paths in $\overline{G}$. The resulting Poincare inequality will imply~\eqref{eqn:spectral_similarity}.
\end{enumerate}
In step (ii) we assume that~\eqref{eqn:slepcev_transport_distance} holds. Since \eqref{eqn:slepcev_transport_distance} holds with probability at least $1 - c/n$, this will imply that~\eqref{eqn:spectral_similarity} holds with at least the same probability.

\subsubsection{Step 1: Poincare inequality}

Let $G = (V,E)$ and $\wt{G} = (V,\wt{E})$ be two graphs over a common vertex set $V$. The set $\mathcal{P}_{\wt{G}}$ consists of all paths $P$ over $\wt{G}$, meaning all tuples
\begin{equation*}
P = (\wt{e}_1,\wt{e}_2,\ldots,\wt{e}_m),~~m \in \mathbb{N}~, \wt{e}_j \in \wt{E}~ \forall~ j \in [m], (\wt{e}_j)_2 = (\wt{e}_{j + 1})_1 \forall~j \in [m - 1].
\end{equation*}
Let $\gamma:E \to \mathcal{P}_{\wt{G}}$ be a mapping from edges in $G$ to paths in $\wt{G}$. The maximum path length $M(\gamma)$ is defined
\begin{equation*}
M(\gamma) = \max_{e \in E} \abs{\gamma(e)}
\end{equation*} 
and the bottleneck $b(\gamma)$ is defined
\begin{equation*}
b(\gamma) = \max_{\wt{e} \in \wt{E}} \abs{\{e \in E:  \wt{e} \in \gamma(e)\}}.
\end{equation*}
As mentioned previously, the Laplacian matrix $L$ cannot be much larger than $\wt{L}$ if edges in $G$ can be mapped to short, non-overlapping paths in $\wt{G}$. Formally, we will show the following: for any $\gamma: E \to \mathcal{P}_{\wt{E}}$,
\begin{equation}
\label{eqn:poincare_inequality}
x^T L x \leq M(\gamma)\cdot b(\gamma) \cdot (x^T \wt{L} x)~\forall{x \in \Reals^n}
\end{equation} 

\paragraph{Proof of~\eqref{eqn:poincare_inequality}:}

Letting $c \in \Reals$, we will use the notation $G \preceq c \cdot  \wt{G}$ as shorthand for
\begin{equation*}
x^T L x \leq c \cdot  x^T \wt{L} x,~~\textrm{for all $x \in \Reals^n$}.
\end{equation*}


Let $G_e = (V, \set{e})$ and $P_e = (V, \set{\widetilde{e}: \widetilde{e} \in \gamma(e)})$ be the graphs associated with $e$ and $\gamma(e)$, respectively. By Lemma \ref{lem: path_poincare}, we have
\begin{equation*}
G_{e} \preceq \abs{P_e} P_e
\end{equation*}
Summing over all $e \in E_G$, we obtain
\begin{align*}
G & \preceq \sum_{e \in E_G} \abs{P_e} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} \sum_{e \in E_G} P_e \\
& \preceq \max_{e \in E_G} \abs{\gamma(e)} b_{\gamma}\cdot \widetilde{G}
\end{align*}

\subsubsection{Step 2: Mapping $E_{G_{n,r}}$ to $\mathcal{P}_{\overline{G}}$}
We assume there exists a mapping $\pi:\overline{X} \to X$ which satisfies \eqref{eqn:slepcev_transport_distance}. We will apply the Poincare inequality to $\pi^{-1}(G_{n,r})$, the isomorphism of $G_{n,r}$ induced by the mapping of vertices $\pi^{-1}: X \to \overline{X}$. We will map a given $(\overline{x}_{k},\overline{x}_{\ell}) \in \pi^{-1}(E)$ to a shortest path $P \in \mathcal{P}_{\overline{G}}$ (measured by Manhattan distance) between $\overline{x}_k$ and $\overline{x}_{\ell}$. Recalling the notation $t = n^{1/d}$ and letting
\begin{equation*}
s_i = \frac{1}{t}(\textrm{sign}(k_i - \ell_i),0,\ldots,0),~~\textrm{for $i = 1,\ldots,d$,}
\end{equation*}
our mapping $\gamma: \pi^{-1}(E) \to \mathcal{P}_{\overline{G}}$ is given by
\begin{align*}
\gamma((\overline{x}_{k},\overline{x}_{\ell})) = \bigl(& (\overline{x}_k, \overline{x}_{k} + s_1), (\overline{x}_{k} + s_1,\overline{x}_{k} + 2s_1),\ldots,(\overline{x}_{k} + (\abs{k_1 - \ell_1} - 1)s_1, \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1)) \\
& (\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1), \overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + s_2),\ldots,(\overline{x}_{k} + \frac{1}{t}(k_1 - \ell_1) + (\abs{k_2 - \ell_2} - 1)s_2, \overline{x}_{k}  + \frac{1}{t}((k_1 - \ell_1) + (k_2 - \ell_2)) \\
& \vdots \\
& (\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + s_d,\overline{x}_{k} + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + 2s_d), \ldots, (\overline{x}_k + \frac{1}{t}\sum_{i = 1}^{d - 1}(k_i - \ell_i) + (\abs{k_d - \ell_d} - 1)s_d, \overline{x}_{\ell})\bigr)
\end{align*}
As a result of~\eqref{eqn:slepcev_transport_distance} we obtain the following bounds on $M(\gamma)$ and $b(\gamma)$:
\begin{align}
M(\gamma) & \leq c n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right) \label{eqn:maximum_path_length}\\
b(\gamma) & \leq c \left(n^{1/d} \left(\left(\frac{\log n}{n}\right)^{1/d} + r\right)\right)^{2d}, \label{eqn:bottleneck}
\end{align}
which we now prove. 

\paragraph{Proof of~\eqref{eqn:maximum_path_length}:}

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \pi^{-1}(E)$. Then
\begin{align*}
\gamma(\overline{x}_k, \overline{x}_{\ell})) = \norm{\overline{x}_k - \overline{x}_{\ell}}_1 & \leq \sqrt{d} \norm{\overline{x}_k - \overline{x}_{\ell}}_2 \\
& \leq \sqrt{d}\bigl(\norm{\overline{x}_k - \pi(\ol{x}_{k})}_2 + \norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + \norm{\overline{x}_\ell - \pi(\ol{x}_{\ell})}_2\bigr) \\
& \leq \sqrt{d}\left(\norm{\pi(\ol{x}_k) - \pi(\ol{x}_{\ell})}_2 + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{by \eqref{eqn:transport_distance}} \\
& \leq \sqrt{d}\left(r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right) \tag{since $(\pi(\ol{x}_k),\pi(\ol{x}_{\ell})) \in E$}
\end{align*} 

\paragraph{Proof of~\eqref{eqn:bottleneck}:} 

Suppose $(\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)$ for some $e = (\overline{x}_i, \overline{x}_j) \in \pi^{-1}(E)$.  Then, by the construction of $\gamma$,
\begin{equation}
\label{eqn:bottleneck_pf_1}
\norm{\overline{x}_k - \overline{x}_i}_2 \wedge \norm{\overline{x}_\ell - \overline{x}_j}_2  \leq \norm{\overline{x}_j - \overline{x}_i}_2
\end{equation}
As shown in the preceding paragraph, assuming~\eqref{eqn:transport_distance} we have
\begin{equation*}
\norm{\overline{x}_j - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}
\end{equation*}
and therefore by~\eqref{eqn:bottleneck_pf_1},
\begin{align*}
\abs{\set{e \in E: (\overline{x}_k, \overline{x}_{\ell}) \in \gamma(e)}} & \leq \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \cdot \abs{\set{\overline{x}_i: \norm{\overline{x}_k - \overline{x}_i}_2 \leq r + 2c\left(\frac{\log n}{n}\right)^{1/d}}} \\
& \leq \left(n^{1/d}\left( r + 2c\left(\frac{\log n}{n}\right)^{1/d}\right)\right)^2.
\end{align*}

Putting the pieces together, we have that when $d = 1$ and $r = (\log n)^a (\log n/n)$, then
\begin{equation*}
G_{n,r} \preceq c \cdot (\log n)^{(1/d + a) + 2 + 2da}\cdot \overline{G}
\end{equation*}
with probability at least $1 - o(1)$.

\subsection{Proof of Lemma~\ref{lem:pi_max_pf_1}}

Write $\overline{L} = \overline{V}\overline{S} \overline{V}^T$ for the spectral decomposition of $\overline{L}$. The proof of Lemma~\ref{lem:pi_max_pf_1} will proceed according to the following steps.
\begin{enumerate}
	\item We show that the incoherence $\max_{i} v_{k,i}^2$ can be upper bounded
	\begin{equation}
	\label{eqn:incoherence_proof_1}
	\max_{i} v_{k,i}^2 \leq \frac{1}{n} \left(\sum_{j = 1}^{n} \abs{\dotp{v_k}{\overline{v}_j}}\right)^2
	\end{equation}
	replacing the norm $\norm{v_k^2}_{\infty}$ by the inner products $\dotp{v_k}{\overline{v}_j}$.
	\item Let $I(k,r) = \set{j \geq 0: \abs{j - k} \leq r}$. Using Davis-Kahan, we establish that the following upper bounds
	\begin{equation}
	\label{eqn:incoherence_proof_2}
	\sum_{j \not\in I(k,r)}^{n} (\dotp{v_k}{\overline{v}_j})^2 \leq 400 \frac{n^4 \delta^2 s_{\max}^2}{R^4}
	\end{equation}
	hold for each $k \leq n/8$ and any $r \geq 8k\sqrt{\delta}$. 
	\item We carefully upper bound the $L_1$ norm present in \eqref{eqn:incoherence_proof_1} given the various bounds on $L_2$ norm we've established in \eqref{eqn:incoherence_proof_2}.
\end{enumerate}

\paragraph{Step 1: Proof of~\eqref{eqn:incoherence_proof_1}.}

We can re-express $v_k$ in the basis $(\overline{v}_j)$ as
\begin{equation*}
v_k = \sum_{j = 1}^{n} \dotp{v_k}{\overline{v}_j} \overline{v}_j,
\end{equation*}
whereupon the bound~\eqref{eqn:incoherence_proof_1} follows from the incoherence property of the chain
\begin{equation*}
\max_{i,k = 1,\ldots,n} \overline{v}_{k,i}^2 \leq \frac{1}{n}.
\end{equation*}
\paragraph{Step 2: Proof of~\eqref{eqn:incoherence_proof_2}.}

Let $\widetilde{L} = L + H$. By Davis Kahan, we have that
\begin{equation}
\label{eqn:davis_kahan}
\sum_{j \not\in I(k,r)} (\dotp{v_k}{\overline{v}_j})^2 \leq \left(\frac{\norm{H}_{\textrm{op}}}{\min{\abs{\overline{s}_j - s_k}:j \in I(k,r)}}\right)^2
\end{equation}
To upper bound the numerator, we use~\eqref{eqn:spectral_similarity} to get
\begin{equation*}
\norm{H}_{op} \leq \delta s_{n}.
\end{equation*}

To lower bound the denominator, we note
\begin{align}
\abs{\overline{s}_j - s_k} & \geq \abs{\overline{s}_j - \overline{s}_k} - \abs{\overline{s}_k - s_k} \nonumber \\
& \geq \abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k. \label{eqn:incoherence_proof_8}
\end{align}
The eigenvalues of the chain graph are well known to be
\begin{equation*}
\overline{s}_j = 2\left(1 - \cos\left(\frac{j\pi}{n}\right)\right) = 4\sin^2\left(\frac{k\pi}{n}\right) ~~\textrm{for $k = 0,\ldots,n - 1$.}
\end{equation*}
By Taylor expansion, for $k \leq j \leq n/(2\pi)$ we have
\begin{equation}
\label{eqn:incoherence_proof_5}
\overline{s}_j - \overline{s}_k = 2\left(\cos\left(\frac{k\pi}{n}\right) - \cos\left(\frac{j\pi}{n}\right)\right) \geq \frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
and when $j \geq n/(2\pi)$ we have $\overline{s}_j - \overline{s}_k \geq 2(\cos\left(\frac{\pi}{8}\right) - \cos\left(\frac{1}{2}\right) > .09$. Additionally since $\sin(x) \leq x$ for all $x \geq 0$, we have
\begin{equation}
\label{eqn:incoherence_proof_6}
\overline{s}_k \leq 4\frac{k^2\pi^2}{n^2}.
\end{equation}
Combining~\eqref{eqn:incoherence_proof_5},\eqref{eqn:incoherence_proof_6}, and the lower bound $\abs{k - j} \geq R \geq 8k\sqrt{\delta}$, we have that
\begin{equation}
\label{eqn:incoherence_proof_7}
\abs{\overline{s}_j - \overline{s}_k} - \delta \overline{s}_k \geq .04\frac{(k - j)^2\pi^2}{n^2} - 4 \delta\frac{k^2\pi^2}{n^2} \geq .02\frac{(k - j)^2\pi^2}{n^2}.
\end{equation}
The result follows from \eqref{eqn:incoherence_proof_7}, \eqref{eqn:incoherence_proof_8} and \eqref{eqn:davis_kahan}.

\paragraph{Step 3: $L_1$ norm to $L_2$ norm.}

Let $u \in \Reals^n$, and suppose we know that the $L_2$ norm of $u$ is bounded,
\begin{equation}
\label{eqn:incoherence_proof_3}
\norm{u}_2^2 \leq 1.
\end{equation}
Under no other conditions on $u$, the upper bound $\norm{u}_1 \leq \sqrt{n}$ is the best that can be hoped for (achieved when $u = (n^{-1/2},\ldots,n^{-1/2})$). However, suppose we also know that for some $B_2^2 \geq B_3^2 \geq \ldots \geq B_n^2$, we have that there exists some $R \in [n]$ such that
\begin{equation}
\label{eqn:incoherence_proof_4}
\sum_{j = r}^{n} v_j^2 \leq B_r^2 ~~\textrm{for each $r = R,\ldots,n$.}
\end{equation}
Clearly, if $B_n^2 \leq \frac{1}{n}$ then $u$ cannot be equal to $(n^{-1/2},\ldots,n^{-1/2})$. We might hope for more general improvements on the bound $\norm{u}_1 \leq \sqrt{n}$ if $B_R^2$ is quite small once $R$ gets sufficiently large. The following Lemma gives such a result.
\begin{lemma}
	\label{lem:pi_max_pf_1_util_1}
	Suppose $u \in \Reals^n$ satisfies \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} for some $1 = B_1^2 = B_2^2 \geq \ldots \geq B_n^2 \geq B_{n+1}^2 = 0$. Assume additionally that there exists an $R \in [n]$ such that
	\begin{equation}
	\label{eqn:incoherence_util_1}
	B_r^2 - B_{r + 1}^2 \leq \frac{1}{r}~\textrm{for each $r = R, R+1,\ldots, n - 1$.}
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:incoherence_util_2}
	\norm{u}_1 \leq \sqrt{R}\sqrt{1 - B_R^2} + \sum_{j = R}^{n} \sqrt{B_j^2 - B_{j+1}^2}.
	\end{equation}
\end{lemma}
\begin{proof}
	Set
	\begin{equation*}
	(u_\star)_j^2 = 
	\begin{cases*}
	\frac{1}{R}(1 - B_R^2), ~ j \leq R \\
	B_j^2 - B_{j + 1}^2, j > R
	\end{cases*}
	\end{equation*}
	so that $\norm{u_\star}_1$ is equal to the right hand side of~\eqref{eqn:incoherence_util_2}. We now prove by contradiction that $u_\star$ maximizes $\norm{\cdot}_1$ under the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4}. Suppose this were not true, and that the maximum is achieved for some $u \neq u_{\star}$.
	\begin{itemize}
		\item since $\norm{u}_1 > \norm{u_{\star}}$ then there exists some index $j$ such that
		\begin{equation}
		\label{eqn:incoherence_util_3}
		u_j > (u_{\star})_j
		\end{equation} 
		\item by~\eqref{eqn:incoherence_proof_3} $\norm{u}_2 \leq 1$. Since $\norm{u_{\star}}_2 = 1$, by \eqref{eqn:incoherence_util_3} there must exist some index $k$ such that
		\begin{equation}
		\label{eqn:incoherence_util_4}
		(u_{\star})_k > u_k.
		\end{equation} 
		Choose $k$ to be the largest of all such indices.
		\item suppose $k < j$ and $j > R$. Since $k$ was chosen to be the largest such index which satisfied~\eqref{eqn:incoherence_util_4}, clearly $v_i \geq (u_{\star})_i$ for all $i > j$, and by \eqref{eqn:incoherence_util_3} $u_j > (u_{\star})_j$. But then 
		\begin{equation*}
		B_j^2 = \sum_{i = j}^{N} (u_{\star})_i^2 < \sum_{i = j}^{N} u_i^2 
		\end{equation*}
		and so $v$ does not satisfy~\eqref{eqn:incoherence_proof_4}. Therefore either $k > j$ or $j \leq R$.
		\item In either case, we have
		\begin{equation*}
		u_j > (u_{\star})_j \geq (u_{\star})_k > u_k.
		\end{equation*}
		Let $j \leq l < k$ be the largest index for which $v_j > (a_{\star})_j$. 
		
		We now construct a vector $\wt{u}$ which satisfies the constraints \eqref{eqn:incoherence_proof_3} and \eqref{eqn:incoherence_proof_4} such that $\norm{\wt{u}}_1 > \norm{u}_1$. Once we have shown this, we will have established a contradiction, and the proof of Lemma~\ref{lem:pi_max_pf_1_util_1} will be complete. Let $\wt{u} = (\wt{u}_i)$ be given as follows:
		\begin{equation*}
		\wt{u}_i = 
		\begin{cases*}
		u_i, ~~\textrm{if $i \neq k,l$}, \\
		u_{\star,k}, ~~\textrm{if $i = k$}, \\
		\sqrt{u_l^2 - (u_{\star,k}^2 - a_k^2)}, ~~\textrm{if $i = l$.}
		\end{cases*}
		\end{equation*}
		Clearly $\norm{\wt{u}}_2 = \norm{u}_2 = 1$ and so $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_3}. To see that $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4} we separate the analysis into cases. When $r \geq l + 1$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		When $l < r \leq k$, we have that
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq \sum_{i = r}^{k} u_{\star,i}^2 + \sum_{i = k +1}^{n} u_{i}^2 \leq B_r^2 - B_{k + 1}^2 + B_{k + 1}^2 = B_r^2
		\end{equation*}
		Finally, when $R \leq r \leq l$, we have
		\begin{equation*}
		\sum_{i = r}^{n} \wt{u}_i^2 \leq u_l^2 - (u_{\star,k}^2 - u_k^2) + (u_{\star,k}^2) + \sum_{i \neq k,l} u_i^2 = \sum_{i = r}^{n} u_i^2 \leq B_r^2.
		\end{equation*}
		Therefore $\wt{u}$ satisfies~\eqref{eqn:incoherence_proof_4}. Finally, we have that
		\begin{equation*}
		\norm{\wt{u}}_1 = \norm{u}_1 + (u_{\star,k}  - u_k) - (\sqrt{u_l^2 - (u_{\star,k}^2 - u_k^2)} - u_l) > \norm{u}_1 + (u_{\star,k}  - u_k) - (u_{\star,k}  - u_k) = \norm{u}_1,
		\end{equation*}
		so we have established the desired contradiction.
	\end{itemize}
\end{proof}

\paragraph{Putting the pieces together.}
We apply Lemma~\ref{lem:pi_max_pf_1_util_1} to the vector $u = (u_i)_{i = 1}^{n}$, where
\begin{equation*}
u_i = \frac{1}{\sqrt{2}}\left(\abs{\dotp{v_k}{\overline{v}_{k - i}}} + \abs{\dotp{v_k}{\overline{v}_{k + i}}}\right).
\end{equation*}
Note the following:
\begin{itemize}
	\item $\norm{u}_2 \leq 1$.
	\item By \eqref{eqn:incoherence_proof_2}, the vector $u$ satisfies the constraint~\eqref{eqn:incoherence_proof_4} with
	\begin{equation*}
	B_r^2 = 400\frac{n^4\delta^2s_{n}^2}{r^4} ~~\textrm{when $r \geq 8 k \sqrt{\delta}$.}
	\end{equation*}
	\item Taylor expanding $f(r + 1) = (r + 1)^{-4}$ around $r$, we have that
	\begin{equation*}
	B_r^2 - B_{r + 1}^2 \leq 3200 \frac{n^4 \delta^2 s_{n}^2}{r^5} ~~\textrm{when $r \geq 2$.}
	\end{equation*}
	Therefore, the sequence $(B_r)$ satisfies \eqref{eqn:incoherence_util_1} for all $r$ large enough such that 
	\begin{equation*}
	r \geq 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}
	\end{equation*}
	\item Since $u$ and $(B_r)$ satisfy the constraints \eqref{eqn:incoherence_proof_3}, \eqref{eqn:incoherence_proof_4} and \eqref{eqn:incoherence_proof_5}, we may apply Lemma~\ref{lem:pi_max_pf_1_util_1} to $u$, obtaining that
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{v_j}{\overline{v}_k}} = \sqrt{2}\norm{u}_1 \leq \sqrt{2R} + 60\sqrt{2} n^2 \delta s_{n} \sum_{r = R}^{n} r^{-5/2}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{n}}$. Bounding sum by integral, we have that if $R \geq 2$ then
	\begin{equation*}
	\sum_{r = R}^{n} r^{-5/2} \leq \int_{R - 1}^{n - 1} x^{-5/2} \,dx \leq \frac{2^{3/2}2}{3} R^{-3/2}.
	\end{equation*}
	Plugging this into the previous expression, we arrive at
	\begin{equation*}
	\sum_{j = 1}^{n} \abs{\dotp{\overline{v}_j}{v_k}} \leq \sqrt{R} + 60 \sqrt{2} n^2 \delta s_{\max} \frac{2^{3/2}2}{3R^{3/2}}
	\end{equation*}
	for any $R \geq 2 \vee k\sqrt{\delta} \vee 2^{5/4} 100^{1/4} n \sqrt{\delta s_{\max}}$. Lemma~\ref{lem:pi_max_pf_1} then follows from~\eqref{eqn:incoherence_proof_1}.
\end{itemize}

\section{Auxiliary Results}

\subsection{Results of Others}

Let $\overline{X}$ denote the $n$ evenly spaced grid points on $[0,1]^d$; formally, letting $t = n^{1/d}$
\begin{equation*}
\overline{X} = \biggl\{\frac{1}{t}(k_1,\ldots,k_d): k \in [t]^d\biggr\}
\end{equation*}

\begin{theorem}[Theorem 1 of \textcolor{red}{Garcia-Trillos and Slepcev}]
	\label{thm:slepcev_transport_distance}
	With probability at least $1 - c/n$ there exists a bijection $\pi: \overline{X} \to X$ such that
	\begin{equation}
	\label{eqn:slepcev_transport_distance}
	\max_{k \in [t]^d} \abs{\overline{x}_k - \pi(\overline{x}_k)} \leq c \left(\frac{\log n}{n}\right)^{1/d}
	\end{equation}
\end{theorem}

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.4, Theorem 1]
	\label{thm:evans_extension}
	Assume $U$ is bounded and $\partial U$ is $C^1$. Select a bounded open set $V$ such that $U \subset \subset V$ ($U$ is compactly contained in $V$). Then there exists a bounded linear operator $E: H^1(U) \to H^1(\Rd)$ such that for each $u \in H^1(U)$:
	\begin{enumerate}
		\item $Eu = u$ a.e. in $U$,
		\item $Eu$ has support within $V$, and 
		\item 
		\begin{equation*}
		\norm{Eu}_{H^1(\Rd)} \leq C \norm{u}_{H^1(\Rd)}
		\end{equation*}
		the constant $C$ depending only on $U$ and $V$.
	\end{enumerate}
\end{theorem}

\begin{lemma}[Poincare inequality for path graphs.]
	\label{lem: path_poincare}
	Fix $m \geq 0$. For vertices $V = \set{1, \ldots,m}$ define the path $P(1 \to m) = ((1,2),(2,3),\ldots, (m-1,m))$ and $G_{(1,m)}$ to be the graph consisting only of an edge between $1$ and $m$. Then,
	\begin{equation*}
	(m - 1) \cdot P(1 \to m) \succeq G_{(1,m)}
	\end{equation*}
\end{lemma}

\subsection{Integrals}

For Lemmas~\ref{lem:expected_first_order_seminorm} - \ref{lem:expected_first_order_seminorm_3}, we will assume that $K$ is a kernel function compactly supported on $B(0,1)$ satisfying the upper bound $K(x) \leq K_{\max}$ for all $x \in B(0,1)$. Additionally, we note that although each Lemma assumes $g \in C^1(\Xset)$, in each case if we assume merely $g \in H^1(\Xset)$ the results still hold due to the density of $C^{\infty}$ in $H^1(\Xset)$.

\begin{lemma}
	\label{lem:expected_first_order_seminorm}
	Suppose $g \in C^{1}(\Xset)$ for $\Xset$ a Lipschitz domain and that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Xset$. Then
	\begin{equation*}
	\Ebb\Bigl[(g(x_j) - g(x_i))^2K_r(x_i,x_j)\Bigr] \leq c K_{\max} p_{\max}^2 r^2 [g]_{H^1(\Xset)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	Since $\Xset$ is a Lipschitz domain, we may take $h \in C^1(\Rd)$ to be an extension of $g$ such that $h = g$ a.e. on $\Xset$, and additionally
	\begin{equation*}
	[h]_{H^1(\Rd)} \leq c[g]_{H^1(\Xset)}
	\end{equation*}
	Since $h = g$ a.e on $\Xset$, the expectations satisfy
	\begin{equation*}
	\Ebb\Bigl[(g(x_j) - g(x_i))^2K_r(x_i,x_j)\Bigr] = \Ebb\Bigl[(h(x_j) - h(x_i))^2K_r(x_i,x_j)\Bigr].
	\end{equation*}
	By the fundamental theorem of calculus we have for any $y,x \in \Rd$,
	\begin{equation}
	\label{eqn:expected_first_order_seminorm_pf1}
	h(y) - h(x) = \int_{0}^{1} \frac{d}{dt}\bigl[h(x + t(y - x))\bigr] \,dt = \int_{0}^{1} \dotp{\nabla(h(x + t(y - x)))}{y - x} \,dt
	\end{equation}
	where the integral is well-defined as $\nabla h(z)$ exists almost everywhere since $h \in C^1(\Rd)$. We now perform some standard calculus:
	\begin{align*}
	\Ebb[(h(x_j) - h(x_i))^2K_r(x_i,x_j)] & \leq p_{\max}^2 \int_{\Rd} \int_{\Rd} (h(y) - h(x))^2 K_r(y,x) \,dy \,dx\\
	& = p_{\max}^2 \int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \dotp{\nabla(h(x + t(y - x)))}{y - x} \,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(i)}{\leq} p_{\max}^2 \int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \norm{\nabla(h(x + t(y - x)))}\norm{y - x} \,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(ii)}{\leq} p_{\max}^2 r^2\int_{\Rd} \int_{\Rd} \left(\int_{0}^{1} \norm{\nabla(h(x + t(y - x)))}\,dt\right)^2 K_r(y,x) \,dy \,dx \\
	& \overset{(iii)}{\leq} p_{\max}^2 r^2\int_{\Rd} \int_{\Rd} \int_{0}^{1} \norm{\nabla(h(x + t(y - x)))}^2 \,dt K_r(y,x) \,dy \,dx \\
	& \overset{(iv)}{\leq} p_{\max}^2 K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(x,r)} \norm{\nabla(h(x + t(y - x)))}^2 \,dy \,dt \,dx \\
	& \overset{(v)}{\leq}  p_{\max}^2 K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(0,r)} \norm{\nabla(h(x + z))}^2  \,dz \,dt \,dx
	\end{align*}
	where $(i)$ follows by Cauchy-Schwarz, $(ii)$ follows since either $\norm{y - x} \leq r$ or $K_r(y,x) = 0$, $(iii)$ follows by Jensen's, $(iv)$ follows by the assumption $K \leq K_{\max}$ supported on $B(0,1)$, and $(v)$ follows from the change of variables $z = x + t(y - x)$. Finally, again using Fubini's Theorem, we have
	\begin{align*}
	K_{\max} r^{2 - d}\int_{\Rd} \int_{0}^{1} \int_{B(0,r)} \norm{\nabla(h(x + z))}^2  \,dz \,dt \,dx & = r^{2 - d}\int_{B(0,r)} \int_{0}^{1} \int_{\Rd} \norm{\nabla(h(x + z))}^2  \,dz \,dt \,dx \\
	& = K_{\max} r^2 [h]_{W_d^{1,2}(\Rd)}.
	\end{align*}
\end{proof}

\begin{lemma}
	\label{lem:expected_first_order_seminorm_2}
	Suppose $f \in C^{1}(\Xset)$ for $\Xset$ a Lipschitz domain and that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Xset$. Then
	\begin{equation*}
	\Ebb\Bigl[\abs{D_if(x_h)}\cdot\abs{D_if(x_j)} \Bigr] \leq c K_{\max}^3 p_{\max}^3 r^2 [f]_{H^1(\Xset)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	Since $\Xset$ is a Lipschitz domain, we may take $g \in C^1(\Rd)$ to be an extension of $f$ such that $g = f$ a.e. on $\Xset$, and additionally
	\begin{equation*}
	[g]_{H^1(\Rd)} \leq c[f]_{H^1(\Xset)}
	\end{equation*}
	Since $g = f$ a.e on $\Xset$, the expectations satisfy
	\begin{equation*}
	\Ebb\Bigl[\abs{D_if(x_h)}\cdot\abs{D_if(x_j)} \Bigr] =\Ebb\Bigl[\abs{D_ig(x_h)}\cdot\abs{D_ig(x_j)} \Bigr]
	\end{equation*}
	
	We rewrite $\Ebb\bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \bigr]$ as follows,
	\begin{align*}
	\Ebb\Bigl[\abs{D_{i}g(x_j)} \cdot \abs{D_ig(x_h)} \Bigr] & = \int \int \int \abs{g(z) - g(x)} \cdot \abs{g(z) - g(y)} K_r(z,y) K_r(z,x) \,dP(x) \,dP(y) \,dP(x) \\
	& = \int \left[\int \abs{g(z) - g(x)} K_r(z,x) \,dP(x)\right]^2 \,dP(z) \\
	& \leq p_{\max}^3 \int_{\Xset} \left[\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx\right]^2 \,dz
	\end{align*}
	Applying~\eqref{eqn:expected_first_order_seminorm_pf1} inside the integral gives
	\begin{align*}
	\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx & \leq \int_{\Rd} \abs{g(z) - g(x)} K_r(z,x) \,dx \\
	& = \int_{\Rd} \abs{\int_{0}^{1} \dotp{\nabla g(x + t(z - x))}{z - x} \,dt} K_r(z,x) \,dx \\
	& \leq \int_{\Rd} \int_{0}^{1} \norm{\nabla g(x + t(z - x))}\cdot\norm{z - x} \,dt K_r(z,x) \,dx \\
	& \leq r \int_{\Rd} \int_{0}^{1} \norm{\nabla g(x + t(z - x))} \,dt K_r(z,x) \,dx \\
	& \leq r \frac{K_{\max}}{r^d} \int_{B(z,r)} \int_{0}^{1} \norm{\nabla g(x + t(z - x))} \,dt  \,dx \\
	& \leq r K_{\max} \int_{B(0,1)} \int_{0}^{1} \norm{\nabla g(x - try)} \,dt  \,dy,
	\end{align*}
	and as a result, 
	\begin{equation*}
	p_{\max}^3 \int_{\Xset} \left[\int_{\Xset} \abs{g(z) - g(x)} K_r(z,x) \,dx\right]^2 \,dz \leq c\cdot p_{\max}^3 r^2 K_{\max}^3 [f]_{W_d^{1,2}(\Rd)}^2.
	\end{equation*}
\end{proof}

\begin{lemma}
	\label{lem:expected_first_order_seminorm_3}
	Suppose $f \in C^{1}(\Xset)$ and that $\abs{p(x)} \leq p_{\max}$ for all $x \in \Xset$. Then for any distinct $i,j,k,\ell$ each in $[n]$,
	\begin{equation*}
	\Ebb\Bigl[\abs{D_{k}f(x_i)}\cdot{\abs{D_{\ell}f(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \leq c K_{\max} p_{\max}^2 r^{2 + d} [f]_{H^1(\Rd)}^2
	\end{equation*}
	for a constant $c$ which depends only on $\Xset$ and $d$.
\end{lemma}
\begin{proof}
	Since $\Xset$ is a Lipschitz domain, we may take $g \in C^1(\Rd)$ to be an extension of $f$ such that $g = f$ a.e. on $\Xset$, and additionally
	\begin{equation*}
	[g]_{H^1(\Rd)} \leq c[f]_{H^1(\Xset)}
	\end{equation*}
	Since $g = f$ a.e on $\Xset$, the expectations satisfy
	\begin{equation*}
	\Ebb\Bigl[\abs{D_{k}f(x_i)}\cdot{\abs{D_{\ell}f(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] = 	\Ebb\Bigl[\abs{D_{k}g(x_i)}\cdot{\abs{D_{\ell}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr].
	\end{equation*}
	We rewrite the expectation as an integral,
	\begin{align*}
	\Ebb\Bigl[& \abs{D_{k}g(x_i)}\cdot{\abs{D_{\ell}g(x_j)}}\cdot\1\{\norm{x_i - x_j} \leq (2q + 1)r\}\Bigr] \\
	& \leq p_{\max}^4 \int_{\Xset^4} \abs{g(x) - g(y)} \cdot \abs{g(u) - g(v)} \cdot  K_r(x,y) K_r(u,v) \1\{\norm{y - v} \leq (2q + 1)r\} \,dy \,dx \,du \,dv
	\end{align*}
	By substituting $z_1 = (y - v)/r$, $z_2 = (u - v)/r$, and $z_3 = (x - y)/r = (x - v)/r + z_1$, we can simplify the integral in the previous display,
	\begin{align*}
	\int_{\Xset^4} & \abs{g(x) - g(y)} \cdot \abs{g(u) - g(v)} \cdot K_r(x,y) K_r(u,v) \1\{\norm{y - v} \leq (2q + 1)r\} \,dy \,dx \,du \,dv \\
	& \leq K_{\max}^2 r^d \int_{\Xset} \int_{[B(0,1)]^3} \abs{g\bigl((z_3 + z_1)r + v\bigr) - g(z_1r + v)} \cdot \bigl|g(z_2r + v) - g(v)\bigr| \,dz_1 \,dz_2 \,dz_3 \,dv \\
	& \leq  K_{\max}^2 r^{d + 2} \int_{[B(0,1)]^3} \int_{[0,1]^2} \int_{\Xset} \norm{\nabla g(t z_3 r + z_1r + v)} \cdot \norm{\nabla g(t z_2 r + v)} \,dv \,dt_1 \,dt_2 \,dz_1 \,dz_2 \,dz_3 \\
	& \leq c \nu_d^3 K_{\max}^2 r^{d + 2} [f]_{W_{d}^{1,2}(\Xset)}^2.
	\end{align*}
\end{proof}


\begin{lemma}
	\label{lem:remainder_term}
	Suppose that $f \in \Leb^2(U)$ for some open domain $U$, and that $h:U \times U \times [0,1] \to \Reals$ is uniformly bounded. Then, the function $g(x) = \int_{0}^{1} \int_{B(0,1)} f(x + aty) h(y,x, t) \,dy \,dt$ also belongs to $\Leb^2(U)$, with norm
	\begin{equation*}
	\norm{g}_{\Leb^2(U)} \leq \nu_d \cdot \norm{f}_{\Leb^2(U)} \cdot \norm{h}_{\infty}
	\end{equation*}
\end{lemma}
\begin{proof}
	We compute the squared norm of $g$,
	\begin{align*}
	\norm{g}_{\Leb^2(\Rd)}^2 & = \int_{U} \left(\int_{0}^{1} \int_{B(0,1)} f(x + aty) h(y,x,t) \,dt \,dy \right)^2 \,dx \\
	& \leq \norm{h}_{\infty}^2 \int_{U} \left(\int_{0}^{1} \int_{B(0,1)} f(x + aty) \,dt \,dy \right)^2 \,dx \\
	& \leq \nu_d^2 \norm{h}_{\infty}^2 \int_{U} \int_{0}^{1} \frac{1}{\nu_d}\int_{B(0,1)} f^2(x + aty) \,dt \,dy \,dx \tag{Jensen's inequality} \\
	& = \nu_d^2 \norm{h}_{\infty}^2 \int_{0}^{1} \int_{B(0,1)} \frac{1}{\nu_d}\int_{U}f^2(x + aty) \,dt \,dy \,dx \tag{Fubini's theorem} \\
	& = \nu_d^2 \norm{h}_{\infty}^2 \norm{f}_{\Leb^2(U)}^2.
	\end{align*}
\end{proof}

Lemma~\ref{lem:leading_term_sobolev_compact} supplies an equivalent result when $f \in H_0^{s}(\Xset)$. In this Lemma, we write $\Ebb[D_kf]:\Xset \to \Reals$ for the mapping $x \mapsto \Ebb[D_kf(x)]$. We will also denote $U_r = \Xset \setminus \Xset_{r}$.
\begin{lemma}
	\label{lem:leading_term_sobolev_compact}
	Fix integers $s \geq 0$ and $q \geq 1$, and an index vector $k \in (n)^q$. Suppose that $p \in C^0(\Xset;p_{\max})$, and additionally that $p \in C^{s-1}(\Xset;p_{\max})$ if $s \geq 2$. Then there exists an $r' > 0$ such that for all $0 < r < r'$, the following statements hold for all $f \in H_0^{s}(\Xset)$:
	\begin{itemize}
		\item The expected difference operator $\Ebb[D_kf]$ belongs to $\Leb^2(U_{qr})$, with norm
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_1}
		\Bigl\|\Ebb\bigl[D_kf\bigr]\Bigr\|_{\Leb^2(U_{qr})} \leq c r^s \norm{f}_{H^s(\Xset)}
		\end{equation}
		\item If $2q \geq s$, then additionally $\Ebb[D_kf]$ belongs to $\Leb^2(\Xset_{qr})$, with norm
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_2}
		\Bigl\|\Ebb\bigl[D_kf\bigr]\Bigr\|_{\Leb^2(\Xset_{qr})} \leq c r^s \norm{f}_{H^s(\Xset)}.
		\end{equation}
		Otherwise there exist functions $f_{\ell} \in H_0^{s - \ell}(\Xset)$, which additionally satisfy
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_3}
		\norm{f_{\ell}}_{H^{s - \ell}(\Xset)} \leq c \norm{f}_{H^s(\Xset)}
		\end{equation}
		for each $\ell = 2q,\ldots,s-1$, such that
		\begin{equation}
		\label{eqn:leading_term_sobolev_compact_4}
		\Bigl\|\Ebb\bigl[D_kf\bigr] - \sum_{\ell = 2q}^{s - 1} r^{\ell} f_{\ell}\Bigr\|_{\Leb^2(\Xset_{qr})} \leq c r^s \norm{f}_{H^s(\Xset)}
		\end{equation}
	\end{itemize}
\end{lemma}
\begin{proof}
	One can interpret the conclusions of Lemma~\ref{lem:leading_term_sobolev_compact} as demonstrating that expected difference operators behave similarly to derivatives over $H_0^{s}(\Xset)$. The proof of Lemma~\ref{lem:leading_term_sobolev_compact} is therefore naturally centered on taking Taylor expansions, but in order to do this, we must relate $f$ to a function $g$ which has classical derivatives. 
	
	Since $f \in H_0^s(\Xset)$, there exists a sequence $(f_m) \subset C_c^s(\Xset)$ such that $\norm{f_m - f}_{H^s(\Xset)} \to 0$ as $m \to \infty$ (Indeed $f_m$ will be smooth for each $m$, but we will not need that fact.) Picking $m$ large enough so that
	\begin{equation*}
	\norm{f_m - f}_{H^s(\Xset)} \leq r^s \norm{f}_{H^s(\Xset)}
	\end{equation*}
	we have that for any $\wt{f} \in \Leb^2(\Xset)$,
	\begin{align*}
	\norm{\Ebb\Bigl[D_kf\Bigr] - \wt{f}}_{\Leb^2(\Xset)} & \leq \norm{\Ebb\Bigl[D_kf_m\Bigr] - \wt{f}}_{\Leb^2(\Xset)} + \norm{\Ebb\Bigl[D_k(f - f_m)\Bigr]}_{\Leb^2(\Xset)} \\
	& \leq \norm{\Ebb\Bigl[D_kf_m\Bigr] - \wt{f}}_{\Leb^2(\Xset)} + c \norm{f - f_m}_{\Leb^2(\Xset)} \\
	& \leq \norm{\Ebb\Bigl[D_kf_m\Bigr] - \wt{f}}_{\Leb^2(\Xset)} + c r^s \norm{f}_{H^s(\Xset)}
	\end{align*}
	Since $f_m \in C_c^s(\Xset)$, it can be continuously extended to $g: \Rd \to \Reals,~ g \in C_c^s(\Xset)$ by taking $g(x) = 0$ for all $x \in \Rd \setminus \Xset$, such that $\norm{g}_{H^s(\Rd)} = \norm{f_m}_{H^s(\Xset)}$ and $g = f$ everywhere on $\Xset$. Therefore $\Ebb[D_kg] =\Ebb[D_kf_m]$, and it suffices to prove the estimates~\eqref{eqn:leading_term_sobolev_compact_1}-\eqref{eqn:leading_term_sobolev_compact_4} hold with respect to $g$. 
	
	Before we do so, let us establish some notation. When $s \geq 1$, since $g \in C_c^s(\Rd)$ it admits a Taylor expansion of the form
	\begin{equation*}
	g(y) =  \sum_{\abs{\alpha} = 0}^{s - 1} g^{(\alpha)}(x) (z - x)^{\alpha} + \sum_{\abs{\alpha} = s} (y - x)^{\alpha} G_{\alpha}(x,y),
	\end{equation*}
	for any $y,x \in \Rd$. When $s \geq 2$, since $p \in C_c^{s - 1}(\Xset)$ it also admits a Taylor expansion,
	\begin{equation*}
	p(y) = \sum_{\abs{\beta} = 0}^{s - 2} p^{\beta}(x) (y - x)^{\beta} + \sum_{\abs{\beta} = s - 1} (x - y)^s P_{\beta}(x,y).
	\end{equation*}
	for any $y,x \in \Xset$.
	In both cases we use the integral form of the remainders:
	\begin{align*}
	G_{\alpha}(x,y) & = \int_{0}^{1} g^{(\alpha)}\bigl(x + t(y - x)\bigr)(1 - t)^{\abs{\alpha}} \,dt \\
	P_{\beta}(x,y) & = \int_{0}^{1} p^{(\beta)}\bigl(x + t(y - x)\big) (1 - t)^{\abs{\beta}}  \,dt 
	\end{align*}
	where by Rademacher's Theorem $g^{(\alpha)}$ and $p^{(\beta)}$ exist almost everywhere, and the preceding integrals are therefore well defined.
	
	It will also be helpful to introduce some notation. For $G: \Xset \times \Xset \to \Reals$, let
	\begin{align*}
	\Bigl(\Ebb_{\alpha}[G]\Bigr)(x) & := \int_{\Xset} (y - x)^{\alpha} G(x,y) K_r(y,x) p(y) \,dy,~~ && \Ebb_{\alpha}(x) := \Bigl(\Ebb_{\alpha}[1]\Bigr)(x) \\
	\Bigl(\Ibb_{\alpha}[G]\Bigr)(x) & := \int_{\Xset} (y - x)^{\alpha} G(x,y) K_r(y,x) \,dy,~~ && \Ibb_{\alpha}(x)  := \Bigl(\Ibb_{\alpha}[1]\Bigr)(x)
	\end{align*}
	Additionally we define
	\begin{equation*}
	I_{\alpha} := \int z^{\alpha} K\bigl(\norm{z}\bigr) \,dz.
	\end{equation*}
	and note that when $B(x,r) \subset \Xset$, the following two facts are true: first, that $\Ibb_{\alpha,x} = r^{\abs{\alpha}} I_{\alpha}$, and second that $I_{\alpha} = 0$ when $\abs{\alpha} = 1$.
	
	We begin with $s = 0$. When $q = 1$, we have
	\begin{align*}
	\norm{\Ebb\bigl[D_kg\bigr]}_{\Leb^2(\Xset)} & = \int_{\Xset} \biggl[\int_{\Xset} \Bigl(g(y) - g(x)\Bigr)K_r(y,x) p(y) \,dy \biggr]^2 \,dx \\
	& \leq p_{\max}^2 \int_{\Rd} \biggl[\int_{\Rd} \Bigl(\abs{g(y)} + \abs{g(x)}\Bigr)K_r(y,x) \,dy \biggr]^2 \,dx \\
	& \leq K_{\max}^2 p_{\max}^2 \int_{\Rd} \biggl[\int_{B(0,1)} \abs{g(zr + x)} + \abs{g(x)} \,dz \biggr]^2 \,dx
	\end{align*}
	and the statement follows by Lemma~\ref{lem:remainder_term}. The same result holds (up to different constants) for $s = 0$ and general $q$ by induction.
	
	For $s \geq 1$, we first show the desired estimate over $U_{qr}$.
	
	\subsubsection{Boundary region}
	
	Take $q = 1$, and $k \in [n]$. We begin by relating the $\Leb^2$ norm of $\Ebb[D_kg]$ over $U_r$ to the $\Leb^2$ norm of $g$ over $U_{2r}$, as follows:
	\begin{align*}
	\Bigl\|\Ebb\bigl[D_kg\bigr]\Bigr\|_{\Leb^2(U_r)}^2 & = \int_{U_r} \biggl[\int_{\Xset} \bigl(g(y) - g(x)\bigr)K_r(x,y) p(y) \,dy \biggr]^2 \,dx \\
	& \leq p_{\max}^2 \int_{U_r} \biggl[\int_{\Xset} \bigl(\abs{g(y)} +  \abs{g(x)}\bigr)K_r(x,y) \,dy \biggr]^2 \,dx \\
	& \overset{(i)}{\leq}  p_{\max}^2 K_{\max}^2 \int_{U_r} \biggl[\int_{B(0,1) \cap (\Xset - x)/r} \bigl(\abs{g(zr + x)} +  \abs{g(x)}\bigr)\,dz \biggr]^2 \,dx \\
	& \overset{(ii)}{\leq} 2 p_{\max}^2 K_{\max}^2 \int_{U_r} \nu_d^2 \bigl(g(x)\bigr)^2 \,dx + 2 p_{\max}^2 K_{\max}^2 \nu_d \int_{U_r}\biggl[\int_{B(0,1) \cap (\Xset - x)/r} \bigl(g(zr + x)\bigr)^2\,dz\biggr] \,dx \\
	& \leq 2 p_{\max}^2 K_{\max}^2 \int_{U_r} \nu_d^2 \bigl(g(x)\bigr)^2 \,dx + 2 p_{\max}^2 K_{\max}^2 \nu_d \int_{B(0,1)} \int_{U_r} \bigl(g(zr + x)\bigr)^2\,dz \,dx \\
	& \leq 4 p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{g}_{\Leb^2(U_{2r})}^2
	\end{align*}
	where $(i)$ follows from change of variables and $(ii)$ from Young's and Jensen's inequality. Reasoning by induction, we see that it suffices to show that
	\begin{equation*}
	\norm{g}_{\Leb^2(U_{(q + 1)r})}^2 \leq c r^{2s} \norm{g}_{H^s(\Xset)}^2
	\end{equation*}
	to establish~\eqref{eqn:leading_term_sobolev_compact_1}. The previous inequality is established in Lemma~\ref{lem:boundary_term_sobolev} for all $r > 0$ sufficiently small, and we have therefore proved the desired estimate over the boundary. 
	
	\subsubsection{Interior region}
	
	To show the desired bounds on $\Xset_{qr}$ when $s \geq 1$, we reason by induction on $q$.
	
	\paragraph{Base case.}
	In the base case $q = 1$, meaning $D_kg$ is only a single-difference operator.
	Since $s \geq 1$, replacing $g$ by its Taylor expansion inside the first order expected difference operator $\Ebb[D_kg(x)]$ yields
	\begin{equation}
	\label{eqn:leading_term_sobolev_compact_pf1}
	\Ebb\Bigl[D_kg(x)\Bigr] = \sum_{1 \leq \abs{\alpha} < s} \Ebb_{\alpha}(x) \cdot g^{(\alpha)}(x)  + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x)
	\end{equation}
	When $s = 1$ only the second term in the previous expression is non-zero, and we therefore begin by analyzing this term, obtaining that for each $\abs{\alpha} = s$,
	\begin{align}
	\biggl\|\Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)\biggr\|_{\Leb^2(\Xset_{r})}^2 & = \biggl\|\int (y - \cdot)^{\alpha} G_{\alpha}(\cdot,y) K_r(y,\cdot) p(y) \,dy\biggr\|_{\Leb^2(\Xset_{r})}^2 \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2  \biggl\|\int_{B(0,1)} G_{\alpha}(\cdot,zr + \cdot) \,dy\biggr\|_{\Leb^2(\Xset_{r})}^2 \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2 \nu_d \int_{B(0,1)} \Bigl\|G_{\alpha}(\cdot,zr + \cdot)\Bigr\|_{\Leb^2(\Xset_{r})}^2 \,dz \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{g^{(\alpha)}}_{\Leb^2(\Xset)}^2 \nonumber \\
	& \leq r^{2s} p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{g}_{H^s(\Xset)}^2; \nonumber
	\end{align}
	hence~\eqref{eqn:leading_term_sobolev_compact_2} follows when $q = 1, s = 1$.
	
	When $s \geq 2$ we must analyze $\Ebb_{\alpha}(x) \cdot g^{(\alpha)}(x)$, which we do by using the Taylor expansion of $p$. Since $B(x,r) \subset \Xset$, we recall that $\Ibb_{\alpha + \beta,x} = r^{\abs{\alpha} + \abs{\beta}}I_{\alpha,\beta}$; thus
	\begin{align*}
	\Ebb_{\alpha}(x) & = \int(y - x)^{\alpha} K_r(y,x) p(y) \,dy \\
	& = \sum_{\abs{\beta} = 0}^{s - 2} p^{(\beta)}(x) \Ibb_{\alpha + \beta,x} + \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x) \\
	& = \sum_{\abs{\beta} = 0}^{s - 2} p^{(\beta)}(x) r^{\abs{\alpha} + \abs{\beta}} I_{\alpha + \beta} + \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x).
	\end{align*}
	Replacing $\Ebb_{\alpha}(x)$ by this expansion in~\eqref{eqn:leading_term_sobolev_compact_pf1} gives
	\begin{equation*}
	\Ebb\Bigl[D_kg(x)\Bigr] = \sum_{\abs{\alpha} = 1}^{s - 1} \sum_{\abs{\beta} = 0}^{s - 2} r^{\abs{\alpha} + \abs{\beta}} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x)  + \sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = s - 1} g^{(\alpha)}(x) \Bigl( \Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x)  + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x)
	\end{equation*}
	We now divide the sum in the first term based on the size of $\abs{\alpha} + \abs{\beta}$. The critical fact is that $I_{\alpha + \beta} = 0$ when $\abs{\alpha} + \abs{\beta} = 1$. When $s = 2$ this leaves
	\begin{equation*}
	\Ebb\Bigl[D_kg(x)\Bigr] = \sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x) g^{(\alpha)}(x) + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x).
	\end{equation*}
	We have already shown that the second term belongs to $\Leb^2(\Xset)$, and provided an appropriate upper bound on its norm. The first term is similarly upper bounded, since for each term inside the sum
	\begin{equation*}
	\norm{\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]g^{(\alpha)}}_{\Leb^2(\Xset)}^2 \leq r^{2(\abs{\alpha} + \abs{\beta})} p_{\max}^2 \norm{g^{(\alpha)}}_{\Leb^2(\Xset)}^2 \leq r^{2(\abs{\alpha} + \abs{\beta})} p_{\max}^2 \norm{g}_{H^s(\Xset)}^2 \nonumber
	\end{equation*}
	thus establishing~\eqref{eqn:leading_term_sobolev_compact_2} when $s = 2$. Otherwise when $s > 2$, we rearrange
	\begin{equation*}
	\sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = 0}^{s - 2} r^{\abs{\alpha} + \abs{\beta}} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x) = \sum_{\ell = 2}^{s - 1} r^{\ell} \Biggl\{\underbrace{\sum_{\abs{\alpha} + \abs{\beta} = \ell} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x)}_{:=g_{\ell,1}(x)}\Biggr\} + \sum_{\ell = s + 1}^{2s - 2} r^{\ell} \sum_{\abs{\alpha} + \abs{\beta} = \ell} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x).
	\end{equation*}
	and therefore
	\begin{align*}
	& \Ebb\Bigl[D_kg(x)\Bigr] - \sum_{\ell = 2}^{s - 1} r^{\ell} g_{\ell,1}(x) = \\ & ~~~~ \sum_{\ell = s + 1}^{2s - 2} r^{\ell} \sum_{\abs{\alpha} + \abs{\beta} = \ell} I_{\alpha + \beta} g^{(\alpha)}(x) p^{(\beta)}(x) +  \sum_{\abs{\alpha} = 1}^{s} \sum_{\abs{\beta} = s - 1} \Bigl(\Ibb_{\alpha + \beta}\bigl[P_{\beta}\bigr]\Bigr)(x) g^{(\alpha)}(x) + \sum_{\abs{\alpha} = s} \Bigl(\Ebb_{\alpha}\bigl[G_{\alpha}\bigr]\Bigr)(x)
	\end{align*}
	On the left hand side, taking $\ell = \abs{\alpha} + \abs{\beta}$, note that for each $\ell < s$ the function $g_{\ell,1} \in C_c^{s - \ell}(\Xset) \subset H_0^{s - \ell}(\Xset)$ and further
	\begin{equation}
	\label{eqn:leading_term_sobolev_compact_pf3}
	\norm{g_{\ell,1}}_{H^{s - \ell}(\Xset)} \leq c p_{\max} \norm{g}_{H^s(\Xset)}.
	\end{equation}
	
	The right hand side consists of three terms, and we have already obtained sufficient estimates on the second and third term, so it remains to deal with the first term. We have that $g^{(\alpha)}\cdot p^{(\beta)} \in C_c^{0}(\Xset) \subset \Leb_0^2(\Xset)$ and
	\begin{equation}
	\label{eqn:leading_term_sobolev_compact_pf4}
	\norm{g^{(\alpha)}p^{(\beta)}}_{\Leb^2(\Xset)} \leq p_{\max} \norm{g}_{H^s(\Xset)},
	\end{equation}
	establishing~\eqref{eqn:leading_term_sobolev_compact_1} when $s > 2$.
	
	\paragraph{Induction step.}
	We now assume that~\eqref{eqn:leading_term_sobolev_compact_2}-\eqref{eqn:leading_term_sobolev_compact_4} hold with respect to $g$ for all $k \in (n)^q$, and show the desired estimates on $\Ebb[D_jD_kg]$ for all $(kj) \in (n)^{q + 1}$.
	
	We first consider the case where $s \leq 2q$. Then,
	\begin{align*}
	\norm{\Ebb\bigl[D_jD_kg\bigr]}_{\Leb^2(X_{(q + 1)r})}^2 \leq 2 p_{\max}^2 K_{\max}^2 \nu_d^2 \norm{D_kg}_{\Leb^2(X_{qr})}^2 \leq c r^{2s} \norm{g}_{H^s(\Xset)}
	\end{align*}
	where the final inequality follows by hypothesis, and gives the desired estimate.
	
	Otherwise $s \geq 2q + 1$. We make use of the inductive hypothesis through the following three facts:
	\begin{enumerate}
		\item There exist functions $g_{2q,q}, \ldots, g_{s - 1,q}$ satisfying~\eqref{eqn:leading_term_sobolev_compact_4} such that
		\begin{equation*}
		\norm{\Ebb\bigl[D_kg\bigr] - \sum_{\ell = 2q}^{s - 1}r^{\ell}g_{\ell,q}}_{\Leb^2(X_{qr})} \leq c r^s \norm{g}_{H^s(\Xset)}
		\end{equation*}
		\item The functions $f_{\ell,q}$ belong to $H_0^{s - \ell}(\Xset)$. Thus by hypothesis, when $\ell = s - 1$ or $\ell = s - 2$,
		\begin{equation*}
		\norm{\Ebb\bigl[D_jg_{\ell,q}\bigr]}_{\Leb^2(\Xset_r)} \leq c r^s \norm{f}_{H^s(\Xset)}.
		\end{equation*}
		\item Otherwise if $s - \ell > 2$, there exist further functions $g_{\ell,m,q}$ for $m = 2,\ldots,s - \ell - 1$ such that
		\begin{equation*}
		\norm{\Ebb\bigl[D_jf\bigr] - \sum_{m = 2}^{s - \ell - 1}r^{m}g_{\ell,m,q}}_{\Leb^2(\Xset_r)} \leq  c r^s \norm{g}_{H^s(\Xset)}
		\end{equation*}
		The functions $g_{\ell,m,q} \in H_0^{s - (\ell + m)}(\Xset)$ additionally satisfy
		\begin{equation*}
		\norm{g_{\ell,m,q}}_{H^{s - (\ell + m)}(\Xset)} \leq c \norm{g_{\ell}}_{H^{s - \ell}(\Xset)} \leq c \norm{g}_{H^s(\Xset)}.
		\end{equation*}
	\end{enumerate}
	Making use of the law of iterated expectation and the Fact 1, we have
	\begin{align}
	\Ebb\Bigl[D_jD_kg(x)\Bigr] & = \Ebb\biggl[\Bigl(\Ebb\bigl[D_kg(x_j)|x_j\bigr] - \Ebb\bigl[D_kg(x)\bigr]\Bigr)K_r(x_j,x)\biggr] \nonumber \\
	& = \sum_{\ell = 2q}^{s - 1} r^{\ell} \Ebb\bigl[D_jg_{\ell}(x)\bigr] + \Ebb\biggl[\Bigl(\Ebb\bigl[D_kg\bigr](x_j) - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}(x_j)\Bigr)K_r(x_j,x)\biggr] + \Ebb\bigl[D_kg\bigr](x) - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}(x). \label{eqn:leading_term_sobolev_compact_pf6}
	\end{align}
	The above expansion consists of three terms. By Fact 1, the third term has bounded norm
	\begin{equation*}
	\norm{\Ebb\bigl[D_kg\bigr] - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}}_{\Leb^2(\Xset_{(q + 1)r})} \leq \norm{\Ebb\bigl[D_kg\bigr] - \sum_{\ell = 2q}^{s - 1}r^{\ell} g_{\ell}}_{\Leb^2(\Xset_{(q)r})} \leq c r^{s} \norm{g}_{H^s(\Xset)}
	\end{equation*}
	By Fact 1 and Lemma~\ref{lem:remainder_term_sobolev}, the same estimate holds with respect to the second term (up to constants). 
	
	When $s = (2q + 1)$ or $s = (2q + 2)$, by Fact 2
	\begin{equation*}
	\norm{ \sum_{\ell = 2q}^{s - 1} r^{\ell} \Ebb\bigl[D_jg_{\ell}\bigr]}_{\Leb^2(\Xset_r)} \leq c r^s \norm{g}_{H^s(\Xset)}
	\end{equation*}
	and the desired result~\eqref{eqn:leading_term_sobolev_compact_2} follows from the triangle inequality. Finally when $s > (2q + 2)$, by using Facts 2 and 3 we obtain
	\begin{align*}
	\norm{\sum_{\ell = 2q}^{s - 1} r^{\ell} \Ebb\bigl[D_jg_{\ell}\bigr] - \sum_{\ell = 2q}^{s - 3} \sum_{m = 2}^{s - \ell - 1} r^{\ell + m} g_{\ell,m,q}}_{\Leb^2(X_r)} \leq cr^s \norm{g}_{H^s(X)}
	\end{align*}
	Rewriting the double sum as a single sum over $\ell + m = 2q,\ldots,s - 1$ and plugging back in to~\eqref{eqn:leading_term_sobolev_compact_pf6} gives the desired result~\eqref{eqn:leading_term_sobolev_compact_4}.
\end{proof}

Here are a couple of Lemmas which helped us with the proof of Lemma~\ref{lem:leading_term_sobolev_compact}.

\begin{lemma}
	\label{lem:boundary_term_sobolev}
	Let $\Xset \subset \Rd$ be a bounded open set with Lipschitz boundary. For any $g \in C_c^{\infty}(\Xset)$, we have that 
	\begin{equation*}
	\norm{g}_{\Leb^2(U_r)}^2 \leq c r^s \norm{g}_{H^s(\Xset)}
	\end{equation*}
	for all $r > 0$ sufficiently small.
\end{lemma}
\begin{proof}
	Fix $x_0 \in \partial \Xset$, and let $Q_d(x_0,r)$ be the $d$-dimensional cube centered at $x_0$ of side length $r$. We will show that for all sufficiently small $r > 0$,
	\begin{equation}
	\norm{g}_{\Xset \cap \Leb^2(Q_d(x_0,r))} \leq c r^s \norm{g}_{H^s(Q_d(x_0,r))}
	\end{equation}
	The Lemma then follows by taking a finite covering of $\partial X$ -- possible since $X$ is assumed to be bounded -- in a similar manner to e.g. Theorem 18.1 of \textcolor{red}{(Leoni)} or Theorem 1 in 5.5 of \textcolor{red}{(Evans)}.
	
	We begin by straightening the boundary. Since $\Xset$ has a Lipschitz boundary, for any $x_0 \in \partial X$ there exists a rigid motion $\Phi: \Rd \to \Rd$ with $T(x_0) = 0$, a Lipschitz continuous function $\gamma: \Reals^{d-1} \to \Reals^d$, and a radius $r_0' > 0$ such that, setting $y = \Phi(x)$ and writing $y = (y',y_d)$, we have that for all $r < r_0$,
	\begin{equation*}
	\Phi(\Xset \cap Q(x_0,r)) = \bigl\{y \in Q(0,r): y_d > \gamma(y')\bigr\}.
	\end{equation*}
	Fixing $0 < r < r_0/[\mathrm{Lip}(\gamma)\sqrt{d}]$, we have that $\gamma(y') > -r$ for all $y' \in Q_{d - 1}(x_0,r)$; writing $\wt{g}(y) = g(T^{-1}(y))$, taking a Taylor expansion of $\wt{g}(y)$ around $\wt{g}((y',\gamma(y')))$ thus yields
	\begin{align*}
	\wt{g}(y) & = \sum_{\ell = 1}^{s - 1} g^{(\ell e_d)}\bigl((y',\gamma(y'))\bigr) (y_d - \gamma(y'))^{\ell} + \int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr) h^{s - 1}\,dh \\
	& = \int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr) h^{s - 1}\,dh
	\end{align*}
	where the second equality follows from the assumption $g \in C_c^{\infty}(\Xset)$. We now analyze the $\Leb^2$ norm over $Q_d(x_0,r)$,
	\begin{align*}
	\norm{g}_{\Xset \cap \Leb^2(Q_d(x_0,r))}^2 & = \norm{\wt{g}}_{\Leb^2(\Phi(\Xset \cap Q(0,r)))}^2 \\
	& = \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \biggl[\int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr)h^{s - 1} \,dh\biggr]^2 \,dy_d \,dy' \\
	& \overset{(i)}{\leq} (2r)^{2(s - 1)} \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \bigl(y_d - \gamma(y')\bigr)^2 \biggl[\frac{1}{y_d - \gamma(y')}\int_{\gamma(y')}^{y_d} \wt{g}^{(se_d)}\bigl((y',he_d)\bigr) \,dh\biggr]^2 \,dy_d \,dy' \\
	& \overset{(ii)}{\leq}  (2r)^{2(s - 1)} \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \bigl(y_d - \gamma(y')\bigr) \int_{\gamma(y')}^{y_d} \biggl[\wt{g}^{(se_d)}\bigl((y',he_d)\bigr)\biggr]^2 \,dh \,dy_d \,dy' \\
	& \overset{(iii)}{\leq}  (2r)^{2s - 1} \int_{Q_{d - 1}(0,r)} \int_{\gamma(y')}^{r} \int_{\gamma(y')}^{r} \biggl[\wt{g}^{(se_d)}\bigl((y',he_d)\bigr)\biggr]^2 \,dh \,dy_d \,dy' \\
	& \leq  (2r)^{2s} \int_{Q_{d - 1}(0,r)}  \int_{\gamma(y')}^{r} \biggl[\wt{g}^{(se_d)}\bigl((y',he_d)\bigr)\biggr]^2 \,dh  \,dy' \\
	& \overset{(iv)}{\leq} (2r)^{2s} \int_{Q(x_0,r)} \bigl[g^{(se_d)}(x)\bigr]^2 \,dx \leq (2r)^{2s} \norm{g}_{H^s(B(x_0,r))}^2
	\end{align*} 
	where $(i)$ follows since $0 < y_d - \gamma(y') < r - \gamma(y') < 2r$, $(ii)$ follows by Jensen's inequality, $(iii)$ follows since $y_d < r$, and $(iv)$ follows from a change of variables. This completes the proof of Lemma~\ref{lem:boundary_term_sobolev}.
\end{proof}

The following Lemma helps us deal with remainder terms in the Sobolev case.
\begin{lemma}
	\label{lem:remainder_term_sobolev}
	Let $U \subset \Rd$ be an open set, and $U_r = \set{x \in \Rd: \mathrm{dist}(x,U) < x}$. Suppose $f \in \Leb^2(U_r)$ and $k \in (n)$. Then,
	\begin{equation*}
	\norm{\Ebb\bigl[D_kf\bigr]}_{\Leb^2(U)}, \norm{\Ebb\bigl[fK_r(x_k,\cdot)\bigr]}_{\Leb^2(U)} \leq c \norm{f}_{\Leb^2(U_r)}
	\end{equation*}
\end{lemma}

To simplify the statement of the following Lemma---which we use in the proof of~\eqref{eqn:grid_empirical_norm_higher_order}---for $j \in \mathbb{N}$ let
\begin{equation*}
\mathcal{K}_j(u) := \sum_{[j]^d} \bigl(u \ast \eta_h\bigr)(\wb{x}) \cdot \1\{x \in Q_{1/j}(\wb{x})\}.
\end{equation*}
where $\eta_h$ is taken as in Section~\ref{subsubsec:grid_empirical_norm_ho_pf}. Denote the class of polynomials on $\Reals^d$ of degree at most $m$ by $\mathcal{P}_m^d$.

\begin{lemma}
	\label{lem:riemann_sums_polynomials}
	There exists a constant $c$ such that for any $j > 1/c$, 
	\begin{equation*}
	\norm{\mathcal{K}_j(u)}_{\Leb^2([0,1]^d)} \geq c \norm{u}_{\Leb^2([0,1]^d)}
	\end{equation*}
	for all polynomials $u \in \mathcal{P}_{s - 1}^d$.
\end{lemma}
\begin{proof}
	We proceed by contradiction, following the proof of~\textcolor{red}{(Arias-Castro)} closely and making necessary adjustments. Suppose there exists a sequence $(u_j) \subset \mathcal{P}_{s - 1}^d$ such that $\norm{\mathcal{K}_j(u)}_{\Leb^2([0,1]^d)} \leq 1/j \norm{u_j}_{\Leb^2([0,1]^d)}$ for each $j \in \mathcal{N}$. Assume without loss of generality that $\norm{u_j}_{\Leb^2([0,1]^d)} = 1$; then there exists an accumulation point $u_{\infty}$ of $(u_j)$, and $\norm{u_{\infty}}_{\Leb^2([0,1]^d)} = 1$. However, by Jensen's inequality, and the boundedness and compact support of $\eta$, we have
	\begin{align*}
	\Bigl[\bigl(u \ast \eta_h\bigr)(\wb{x})\Bigr]^2 & = \Bigl[\int u(y) \eta_h(y - \wb{x}) \,dy \Bigr]^2 \\
	& = \biggl[\frac{1}{h^d}\int u(y) \eta\Bigl(\frac{y - \wb{x}}{h}\Bigr) \,dy \biggr]^2 \\
	& \leq \frac{1}{h^d}  \biggl[u(y) \eta\Bigl(\frac{y - \wb{x}}{h}\Bigr)\biggr]^2 \,dy \\
	& \leq \frac{1}{h^d} \int_{Q_h(\wb{x})} \bigl[u(y)\bigr]^2 \,dy
	\end{align*}
	which in turn gives 
	\begin{align*}
	\norm{\mathcal{K}_j(u)}_{\Leb^2([0,1]^d)}^2 & \leq \sum_{\wb{x} \in \wb{X}} h^d\Bigl[\bigl(u \ast \eta_h\bigr)(\wb{x})\Bigr]^2 \\
	& \leq \sum_{\wb{x} \in \wb{X}}\int_{Q_h(\wb{x})} \bigl[u(y)\bigr]^2 \\
	& = \norm{u}_{\Leb^2([0,1]^d)}^2.
	\end{align*}
	As a result,
	\begin{align*}
	\norm{\mathcal{K}_j(u_{\infty})}_{\Leb^2([0,1]^d)} & \leq \norm{\mathcal{K}_j(u_{\infty} - u_j)}_{\Leb^2([0,1]^d)} + \norm{\mathcal{K}_j(u_j)}_{\Leb^2([0,1]^d)} \\
	& \leq \norm{u_\infty - u_j}_{\Leb^2([0,1]^d)} + 1/j \to 0
	\end{align*}
	as $j \to \infty$. On the other hand, $\mathcal{K}_j(u_{\infty}) \to u_{\infty}$ as $j \to \infty$. In light of the boundedness of $\eta$ and the boundedness of $u_{\infty}$ on $[0,1]^d$, we may apply the dominated convergence theorem and obtain that $\norm{\mathcal{K}_j(u_{\infty})}_{\Leb^2([0,1]^d)} \to \norm{u_{\infty}}_{\Leb^2([0,1]^d)}$. This establishes a contradiction, thus proving the claim.
\end{proof}


\subsection{One-Sided Concentration}
	
The proof of Lemma~\ref{lem:empirical_norm_sobolev} relies on (a variant of) the Paley-Zygmund Inequality.
\begin{lemma}
	\label{lem:paley_zygmund}
	Let $f$ satisfy the following moment inequality for some $b \geq 1$:
	\begin{equation}
	\label{eqn:paley_zygmund_1}
	\Ebb\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot\Bigl(\Ebb\bigl[\norm{f}_n^2\bigr]\Bigr)^2.
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:paley_zygmund_2}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}.
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. The Paley-Zygmund inequality says that for all $0 \leq \lambda \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund_pf1}
	\mathbb{P}(Z > \lambda \mathbb{E}(Z^p)) \geq \left[(1 - \lambda^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
	Applying~\eqref{eqn:paley_zygmund_pf1} with $Z = \norm{f}_n^2$, $p = 1$, $q = 2$ and $\lambda = \frac{1}{b}$, by assumption~\eqref{eqn:paley_zygmund_1} we have
	\begin{equation*}
	\mathbb{P}\Bigl(\norm{f}_n^2 > \frac{1}{b} \mathbb{E}[\norm{f}_n^2]\Bigr) \geq \Bigl(1 - \frac{1}{b}\Bigr)^2 \cdot  \frac{\bigl(\mathbb{E}[\norm{f}_n^2]\bigr)^2}{\mathbb{E}[\norm{f}_n^4]} \geq \frac{\Bigl(1 - \frac{2}{b}\Bigr)}{\Bigl(1 + \frac{1}{b^2}\Bigr)} \geq 1 - \frac{5}{b}.
	\end{equation*}
\end{proof}

\subsection{Discrete norms}
\begin{lemma}
	\label{lem:alias}
	Let $k \in [n]$ and let $\ell \in \Nbb$. There exists a unique $m \in \Nbb_{0}$, $j \in [n]$ and $b \in \{0,1\}$ such that $\ell = 2mn + (1 - b)j - b(j - 1)$. Then
	\begin{equation*}
	\abs{\frac{1}{n}\sum_{i = 1}^{n} \varphi_{k}(\wb{x}_i)\varphi_{\ell}(\wb{x}_i)} \leq  
	\begin{cases*}
	2^{d/2},& ~~ \textrm{if $b = 0$ and $j = k$, or $b = 1$ and $j = k - 1$} \\
	0,& ~~ \textrm{otherwise.}
	\end{cases*}
	\end{equation*}
\end{lemma}

Note that it is possible to sharpen the factor $2^{d/2}$ to $1$ whenever $k \neq 1$. Since we are not too concerned with constants which depend only $d$, we simply lump everything together into the weaker bound.

\begin{proof}
	Note that for $k = 1,\ldots,n$, and $i = 1,\ldots,n$,
	\begin{equation*}
	\sqrt{n}v_{k,i}(\wb{G}_1) = \varphi_k(\wb{x}_i).
	\end{equation*}
	Additionally, it is easy to check that
	\begin{equation*}
	\varphi_{\ell}(\wb{x}_i) \propto
	\begin{cases*}
	\varphi_{j}(\wb{x}_i),& ~~\textrm{if $b = 0$} \\
	\varphi_{j + 1}(\wb{x}_i),& ~~\textrm{if $b = 1$.}
	\end{cases*}
	\end{equation*}
	where the constant of proportionality is always no greater than $2^{d/2}$.
	The claim follows since the eigenvectors $\sqrt{n} v_k(\wb{G}_1)$ are orthonormal in empirical norm.	
\end{proof}

\subsection{Equivalence between Sobolev classes}
\label{subsec:sobolev_class_equivalence}

We will show that when $s \geq 1$,
\begin{equation*}
\wt{H}^s([0,1];L) = \Bigl\{f \in H^{s}([0,1];L'): f^{(\ell)}(0) = f^{(\ell)}(1) = 0,~~\textrm{for each $\ell < s$ odd.} \Bigr \}
\end{equation*}
First, we show the $\subseteq$ direction. Let $f = \sum_{k = 1}^{\infty} \theta_k \varphi_k$ satisfy $\sum_{k = 1}^{\infty} \theta_k^2 a_k^2 \leq L^2$, and let $\phi_k$ be the cosine basis over $\Leb^2([-1,1])$, i.e.
\begin{equation*}
\phi_k(x) = 
\begin{cases*}
1/\sqrt{2},~~\textrm{for $k = 1$.} \,
\cos\Bigl(\pi (k - 1) x\Bigr),~~\textrm{for $k \in \Nbb_1$}
\end{cases*}
\end{equation*}
for any $x \in [-1,1]$. Put
\begin{equation*}
f_{\diamond} := \sqrt{2} \sum_{k = 1}^{\infty} \theta_k \phi_k.
\end{equation*}
Since $\sum_{k = 1}^{\infty} (\sqrt{2}\theta_k)^2 a_k^2 \leq 2L^2$, we know $f_{\diamond} \in H_{\mathrm{per}}^s([-1,1];2L)$. Moreover since $f_{\diamond}$ is an even function, we have that
\begin{equation*}
\sum_{\ell = 0}^{s}\int_{0}^{1} \bigl(f_{\diamond}^{(\ell)}(x)\bigr)^2 \,dx = \frac{1}{2} \sum_{\ell = 0}^{s}\int_{-1}^{1} \bigl(f_{\diamond}^{(\ell)}(x)\bigr)^2 \,dx \leq \frac{1}{\pi^{2s}}L^2
\end{equation*}
where the latter inequality follows from \textcolor{red}{(Tsybakov)}.
As $f(x) = f_{\diamond}(x)$ for all $x \in [0,1]$, we have established that $f \in H^s([0,1],L')$ for $L' = L/\pi^s$. As for the boundary cases $x = 0$ and $x = 1$, putting
\begin{equation*}
f_N := \sum_{k = 1}^{N} \theta_k \varphi_k
\end{equation*}
we observe the following facts: first that $f_N^{(\ell)}(0) = f_N^{(\ell)}(1) = 0$ for all $N \in \Nbb$ and $\ell$ odd, and second that $f_N^{(\ell)}$ converges uniformly for all $0 \leq \ell < s$. Therefore we have $f^{(\ell)}(x) = \lim_{N \to \infty} f_N^{(\ell)}(x)$ for all $x \in [0,1]$ and $0 \leq \ell < s$, and in particular $f^{(\ell)}(0) = f^{(\ell)}(1) = 0$ for all $0 < \ell < s$ odd. 

Now for the $\supseteq$ direction. Let $f \in H^s([0,1];L)$ satisfy $f^{(\ell)}(0)$ for all $\ell = 1,\ldots,s - 1$ odd. Put
\begin{equation*}
f_{\diamond} = f(\abs{x})~~\textrm{for $x \in [-1,1]$}
\end{equation*}
and note the following facts: first that $f_{\diamond}^{(\ell)}(0)$ is well-defined for each $\ell = 0,\ldots, s - 1$, and second that $f_{\diamond}^{(s - 1)}$ is absolutely continuous on $[-1,1]$. As a result, $f_{\diamond}$ is $s$-times weakly differentiable (and $s - 1$ times classically differentiable), and by construction satisfies
\begin{equation*}
\norm{f_{\diamond}}_{H^s([-1,1])} = \sqrt{2} \norm{f}_{H^s([0,1])} \leq 2L
\end{equation*} 
Since clearly $f_{\diamond}^{(\ell)}(-1) = f_{\diamond}^{(\ell)}(1)$ for each $\ell = 0,\ldots, s - 1$, we have that $f_{\diamond} \in H_{\mathrm{per}}^s([-1,1];\sqrt{2}L)$. Since $f_{\diamond}$ is even, it may be expressed as $f_{\diamond} = \sum_{k = 1}^{\infty} \theta_k \phi_k$ where $\sum_{k = 1}^{\infty} \theta_k^2 a_k^{2} \leq 2L^2/\pi^{2s}$. Since $f(x) = f_{\diamond}(x)$ for $x \in [0,1]$, this implies
\begin{equation*}
f(x) = \sum_{k = 1}^{\infty} \theta_k \phi_k(x) = \sum_{k = 1}^{\infty} \frac{1}{\sqrt{2}}\theta_k \varphi_k(x) 
\end{equation*}
and the claim is proved.
\end{document}