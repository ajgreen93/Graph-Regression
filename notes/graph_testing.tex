\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\spec}{\mathrm{spec}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Graph Testing}
\author{Alden Green}
\date{\today}
\maketitle

This document details the current status of the graph testing project. We divide by section based on the testing problem under consideration. At the end of each section, we list some areas we are interested in investigating. All proofs are left until the end.

Throughout, let $X = \{x_1,
\ldots, x_n\}$ be a sample drawn i.i.d. from a distribution $P$ on $\Rd$,
with density~$p$.  For a radius $r > 0$, we define $G_{n,r}=(V,E)$ to be the
\emph{$r$-neighborhood graph} of $X$, an unweighted, undirected graph with
vertices $V=X$, and an edge $(x_i,x_j) \in E$ if and only if $K_r(x_i,x_j) = \norm{x_i -x_j} \leq r$, where $\norm{\cdot}$ is the Euclidean norm. We denote by $A \in
\Reals^{n \times n}$ the adjacency matrix, with entries $A_{uv} = 1$ if
$(u,v) \in E$ and $0$ otherwise.  We also denote by $D$ the diagonal degree
matrix, with entries $D_{uu} := \sum_{v \in V} A_{uv}$. The graph Laplacian is $L = D - A$, and we write its spectral decomposition as $L = V S V^T$. 

\section{Regression goodness-of-fit testing with random design.}

Let $P$ be a distribution with density $p$ supported on $\mathcal{X} \subseteq \Reals^d$. Suppose we observe random design points $X = \set{x_1,\ldots,x_n} \sim P$, and additionally responses
\begin{equation}
\label{eqn:regression_known_variance}
y_i = f(x_i) + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation} 

We wish to distinguish
\begin{equation*}
\mathbf{H}_0: f = f_0 := 0 \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: f \neq f_0
\end{equation*}
We will evaluate our performance using worst-case risk: for a given function class $\mathcal{H}$ and test function $\phi: \Reals^n \to \set{0,1}$, let
\begin{equation*}
\mathcal{R}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, f \neq f_0} \Ebb_f(1 - \phi).
\end{equation*}
The worst-case risk may be quite close to $1$ unless we require some separation between null and alternative spaces. A more realistic measure of performance is therefore
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \Ebb_{f = f_0}(\phi) + \sup_{f \in \mathcal{H}, \norm{f - f_0}_2 \geq \epsilon} \Ebb_f(1 - \phi).
\end{equation*}


\subsection{Test Statistics.}
We list the test statistics we use for the regression testing problem.

\paragraph{Eigenvector projection test statistic.}
Let $VSV^T$ be the spectral decomposition of the Laplacian matrix $L$ of the neighborhood graph $G_{n,r}$. To test whether $f = f_0$, we propose the following \emph{eigenvector projection} test statistic:
\begin{equation}
\label{eqn:graph_spectral_projections}
T_{\mathrm{spec}} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1} v_i y_i\right)^2
\end{equation}

\subsection{Current Results.}

In Theorem~\ref{thm:sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}} := \1\{T_{\mathrm{spec}} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $W^{1,2}(\mathcal{X};R)$. To conveniently state our results we introduce the notation
\begin{equation*}
h(a,d) = a(d+2) + (1 + 2/d),~~\textrm{for $a > 0$}
\end{equation*}

\begin{theorem}
	\label{thm:sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d < 4$. Suppose that $\Pbb$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < p_{\min} < p(x) < p_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right)^{1/d}, ~\kappa = n^{2d/(4 + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $d,R,p_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/(4 + d)} (\log n)^{h(a,d)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}
The testing rate~\eqref{eqn:sobolev_testing_rate} matches the minimax critical radius up to a factor of $\log^{h(a,d)}n$.

When $d \geq 4$ the compact embedding
\begin{equation*}
W_d^{1,2}(\mathcal{X}) \subseteq \mathcal{L}_d^4(\mathcal{X}) 
\end{equation*}
does not hold (as it does when $d < 4$). However, if we directly assume $f \in \mathcal{L}_d^4(\mathcal{X};R)$ (regardless of $d$) we obtain the following result.
\begin{proposition}
	\label{prop:L4_testing_rate}
	If $f \in \mathcal{L}_d^4(\mathcal{X};R)$, there exists a constant $c$ such that if
	\begin{equation}
	\label{eqn:L4_testing_rate}
	\epsilon^2 > b \cdot n^{-1/2}
	\end{equation}
	then the test
	\begin{equation*}
	\phi_{\mathrm{mean}} = \1\{\frac{1}{n}\sum_{i = 1}^{n} y_i^2 \geq 1\}
	\end{equation*}
	has worst-case risk
	\begin{equation*}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; W^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}. 
	\end{equation*}
\end{proposition}

Note that when $d < 4$ the rate~\eqref{eqn:sobolev_testing_rate} is sharper than \eqref{eqn:L4_testing_rate}. 

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $d \geq 4$, is it true that there does not exist a uniformly consistent
	test over the Sobolev ball $W_d^{1,2}(\mathcal{X};R)$?
	\item Does the testing rate of $\phi_{\mathrm{spec}}$ over Sobolev spaces $W_d^{s,2}(\mathcal{X};R)$ match the minimax rate, for an appropriate choice of kernel $K$?
	\item Assume the distribution $P$ is supported on a manifold $\mathcal{M}$ of intrinsic dimension $s < d$. Does $\mathrm{\phi_{\mathrm{spec}}}$ display adaptivity to the intrinsic dimension of $\mathcal{M}$?
	\item Assume that $f$ belongs to the Holder space $C_d^s(\mathcal{X})$. Moreover, suppose that instead of observing ${y_i}$ according to the regression testing model \eqref{eqn:regression_known_variance}, we observe
	\begin{equation*}
	y_i = f(x_i) + \sigma \varepsilon_i, ~\varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
	\end{equation*}
	where $\sigma > 0$ is unknown. When $d \geq 4$, what are the minimax regression testing rates over $C_d^1(\mathcal{X};R)$? Is the test $\phi_{\mathrm{spec}}$ minimax optimal, when the tuning parameters $r$ and $\kappa$ are appropriately chosen?
\end{enumerate}

\section{Two-sample density testing.}
In the two-sample density testing problem, we observe independent samples $Z = z_1,\ldots,z_N \sim P$ and $Y = y_1,\ldots,y_M \sim Q$, where $P$ and $Q$ are distributions over $\Reals^d$ with densities $p$ and $q$, respectively, and $N \sim \textrm{Bin}(n,1/2)$. Our goal is to distinguish the hypotheses
\begin{equation*}
\mathbf{H}_0: P = Q \quad \textrm{vs} \quad \mathbf{H}_{\textrm{a}}: P \neq Q
\end{equation*}
and we again evaluate our performance using worst-case risk; letting $\phi:\Reals^{N + M} \to \{0,1\}$, 
\begin{equation*}
\mathcal{R}_{\epsilon}(\phi; \mathcal{H}) = \inf_{p \in \mathcal{H}}\Ebb_{p,p}(\phi) + \sup_{\substack{p,q \in \mathcal{H} \\ \norm{p - q}_{\Leb^2} \geq \epsilon}} \Ebb_{p,q}(1 - \phi).
\end{equation*}

\subsection{Test statistics.}

We suggest several two-sample test statistics. 

\paragraph{Eigenvector projection test statistic.}

It is straightforward to adapt the test statistic $T_{\mathrm{spec}}$ to the two-sample testing problem. Concatenate the samples $Z$ and $Y$ in $X = (z_1,\ldots,z_N,y_1,\ldots,y_M)$, and define $T_{\mathrm{spec}}^{(2)}$ to be
\begin{equation}
\label{eqn:graph_spectral_projections_2}
T_{\mathrm{spec}}^{(2)} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_i a_i\right)^2, ~~\textrm{where}~~ a = (\underbrace{N^{-1},\ldots,{N^{-1}}}_{\textrm{length } N},\underbrace{-M^{-1},\ldots,-M^{-1}}_{\textrm{length } M})
\end{equation}

For convenience, we state our following two test statistics with respect to the empirical norm $\norm{\theta}_n = n^{-1/2}\norm{\theta}_2$ for $\theta \in \Reals^n$. They will each depend on a tuning parameter $\lambda > 0$.
\paragraph{Graph Sobolev IPM.}
Letting $C_n := nr^{(d + 2)/2}$ and
\begin{equation*}
\Theta_{1,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B\theta}_2 \leq 1\} 
\end{equation*}
we define the \emph{graph Sobolev IPM} to be
\begin{equation}
\label{eqn:sobolev_IPM}
T_{\textrm{sob}} := \sup_{\substack{\theta \in \Theta_{1,2} \\ \lambda \norm{\theta}_n \leq 1}} \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}. 
\end{equation}

\paragraph{Graph Total Variation IPM.}
Letting $C_n' := n^{2}r^{(d + 1)}$ and 
\begin{equation*}
\Theta_{1,1} := \{\theta \in \Reals^n:~ (C_n')^{-1} \norm{B\theta}_1 \leq 1\}
\end{equation*}
we define the \emph{graph Total Variation} IPM to be
\begin{equation}
\label{eqn:total_variation_IPM}
T_{\mathrm{TV}} := \sup_{\substack{\theta \in \Theta_{1,1}, \\ \lambda \norm{\theta}_n \leq 1} } \abs{\frac{1}{n}\sum_{i = 1}^{n} a_i \theta_i}, \quad
\end{equation}

\subsection{Current results.}

\textcolor{red}{WARNING:} While I have proved all the parts I believe are required for the below theorem, I haven't actually put together the pieces yet.

In Theorem~\ref{thm:twosample_sobolev_testing_rate} we show that under some typical regularity conditions on $P$, the test $\phi_{\textrm{spec}}^{(2)} := \1\{T_{\mathrm{spec}}^{(2)} \geq \tau\}$ is, up to log factors, a minimax optimal test over the Sobolev ball $W^{1,2}(\mathcal{X};R)$ when $d = 1$.

\begin{theorem}
	\label{thm:twosample_sobolev_testing_rate}
	Let $b \geq 1$ and $a > 0$ be fixed constants, and let $d = 1$.  Suppose that $P$ and $Q$ are absolutely continuous probability measures over $\mathcal{X} = [0,1]$ with density functions $p$ and $q$ bounded above and below by constants, i.e
	\begin{equation*}
	0 < p_{\min},q_{\min} < p(x),q(x) < p_{\max},q_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*}
	Then the following statement holds: if the test $\phi_{\spec}^{(2)}$ is performed with parameter choices 
	\begin{equation*}
	r = \log^a n \cdot \left(\frac{\log n}{n}\right), ~\kappa = n^{2/5}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
	\end{equation*}
	then there exists a constant $c$ which may depend on $R,p_{\max},q_{\max},b$ and $a$ but is independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate}
	\epsilon^2 \geq c \cdot b \cdot n^{-4/5} (\log n)^{h(a,1)}
	\end{equation}
	the worst-case risk is upper bounded
	\begin{equation}
	\label{eqn:twosample_sobolev_testing_rate_1}
	\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; \mathcal{W}^{1,2}(\mathcal{X};R)) \lesssim \frac{1}{b}.
	\end{equation}
\end{theorem}

We prove Theorem~\ref{thm:twosample_sobolev_testing_rate} by relating the density testing problem to a regression testing problem with a certain type of structured noise, and then proceeding along similar lines to the proof of Theorem~\ref{thm:sobolev_testing_rate}. To pursue this strategy, we require the eigenvectors to satisfy a certain type of incoherence condition; this is in constrast to the regression testing problem with known variance, where we did not require the eigenvectors to be smooth in any sense.

\subsection{Areas to Investigate}

\begin{enumerate}[(i)]
	\item When $1 < d < 4$, is the test $\phi_{\spec}^{(2)}$ minimax optimal?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $C_d^1(\mathcal{X};R)$ for all values of $d$?
	\item Are either or both of the test statistics \eqref{eqn:graph_spectral_projections_2}-\eqref{eqn:sobolev_IPM} minimax optimal over $W_d^{1,2}(\mathcal{X};R)$ when $d \leq 4$?
	\item Is the test statistic \eqref{eqn:graph_spectral_projections_2}, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $W^{s,2}$? 
	\item Modify the test statistic \eqref{eqn:sobolev_IPM} by replacing the function class $\Theta_{1,2}$ with
	\begin{equation}
	\Theta_{s,2} := \{\theta \in \Reals^n:~ C_n^{-1} \norm{B^{(s)}\theta}_2 \leq 1\} 
	\end{equation}
	Is the modified test statistic, computed over a graph with suitable choice of kernel $K$, minimax optimal over higher order derivative classes $C_d^s$ and $W^{s,2}$?
	\item What is the minimax testing rate over $BV_d^{1}(\mathcal{X};R)$? Does it exhibit a phase transition analogous to the minimax estimation rate over bounded variation spaces?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over $BV_d^{1}(\mathcal{X};R)$?
	\item Is the test statistic \eqref{eqn:total_variation_IPM} minimax optimal over Sobolev and Holder function classes?
\end{enumerate}

\section{Definitions}

Here we collect definitions of some common function spaces and graph operators.

\subsection{Function Spaces}

\paragraph{Lebesgue spaces.}

We say a Borel measurable function $f: \mathcal{X} \to \Reals$ is in the space $\mathcal{L}^p(\mathcal{X})$ for $1 \leq p < \infty$ if 
$$\norm{f}_{\mathcal{L}^p(\mathcal{X})} := \int_{\mathcal{X}} \abs{f(x)}^p \,dx < \infty$$
and we let 
\begin{equation*}
\mathcal{L}^p(\mathcal{X};R) = \set{f \in \mathcal{L}^p(\mathcal{X}): \norm{f}_{\mathcal{L}^p(\mathcal{X})} < R}
\end{equation*}
be a ball in the Lebesgue space.


\paragraph{Holder spaces.}

For a given $s > 0$, the $s$th Holder norm is given by
\begin{equation*}
\norm{f}_{C_d^{s}(\mathcal{X})} := \sum_{\abs{\alpha} \leq s} \norm{D^{\alpha}f}_{\infty} + \sum_{\abs{\alpha} = s} \sup_{x,y \in \mathcal{X}} \frac{\abs{D^{\alpha}f(y) - D^{\alpha}f(x)}}{\norm{x - y}_2}
\end{equation*}
and the $s$th Holder space $C_d^{s}(\mathcal{X})$ consists of all functions which are $s$ times continuously differentiable with finite $s$ Holder norm. Denote the Holder  ball by $C_d^{s}(\mathcal{X},R) = \set{f \in C_d^{s}(\mathcal{X}): \norm{f}_{C_d^{s}(\mathcal{X})} \leq R}$.

\paragraph{Sobolev spaces.}

For a given $s > 0$, the Sobolev space $W_d^{s,2}(\mathcal{X})$ consists of all functions $f \in \mathcal{L}^2(\mathcal{X})$ such that for each $\alpha = (\alpha_1,\ldots,\alpha_d)$ with $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ belongs to $\mathcal{L}^2(\mathcal{X})$. The Sobolev $\{s,2\}$ norm is then 
\begin{equation*}
\norm{f}_{W_d^{s,2}(\mathcal{X})}^2 = \sum_{\abs{\alpha} \leq s} \int_{\mathcal{X}} \abs{D^{\alpha}f}^2 \,dx
\end{equation*}
and for a given $L > 0$, the corresponding ball is $W_d^{s,2}(\Xset; L) = \set{f: \norm{f}_{W^{s,2}(\Xset)} \leq L}$.

\paragraph{Bounded Variation spaces.}

For a function $f \in L^1(\mathcal{X})$ the \emph{total variation} semi-norm of $f$ is
\begin{equation*}
TV(f;\mathcal{X}) := \sup \left\{ \int_{\mathcal{X}} f \, \Xsetive \, \psi \,dx : \psi \in C_c^1(\mathcal{X}; \Reals^d), \abs{\psi} \leq 1 \right\};
\end{equation*}
and we write $BV_d(\mathcal{X})$ for the subset of functions $f \in L^1(\mathcal{X})$ which have bounded norm
\begin{equation*}
\norm{f}_{BV_d(\mathcal{X})} := \norm{f}_{\infty} + TV(f;\mathcal{X}).
\end{equation*}
For a given $R > 0$, the corresponding ball is $BV_d^{1}(\mathcal{X};R) = \set{f: \norm{f}_{BV_d(\mathcal{X})} \leq R}$. 

\subsection{Graph Operators.}
Let $s \geq 1$ be an integer. The $s$th-order difference operator on $G_{n,r}$, denoted $B^{(s)}$, is defined by
\begin{equation*}
B^{(s)} :=
\begin{cases}
L^{s/2},& ~~ s \textrm{ even} \\
BL^{(s - 1)/2},& ~~ s \textrm{ odd.}
\end{cases}
\end{equation*}

\section{Proofs}

\subsection{Proof of Theorem~\ref{thm:sobolev_testing_rate}}

To prove Theorem~\ref{thm:sobolev_testing_rate}, we will show that when
\begin{equation}
\label{eqn:critical_radius}
\epsilon^2 \geq c \cdot b \cdot (\log n)^{h(\beta,d)} \cdot n^{-4/(4 + d)}
\end{equation}
and we choose tuning parameters as specified in Theorem~\ref{thm:sobolev_testing_rate}, the test $\phi_{\mathrm{spec}}$ satisfies
\begin{equation}
\mathcal{R}_{\epsilon}(\phi_{\spec}; W^{1,2}(\mathcal{X};R)) = \Ebb_{f_0}(\phi_{\spec}) + \sup_{\substack{f \in W^{1,2}(\mathcal{X};R) \\ \norm{f}_{\Leb^2(\mathcal{X})} > \epsilon}} \Ebb_{f}(1 - \phi_{\spec}) \leq \left(\frac{1}{b^2} + \frac{3}{b}\right) + o(1).
\end{equation}

Our analysis will proceed according to the following steps.
\begin{enumerate}
	\item We upper bound the testing error of our eigenvector projection test statistic when $Y$ are viewed as random responses defined over the vertices of a fixed graph $G$. This upper bound will hold whenever certain functionals on the graph $G$ are themselves bounded.
	\item We analyze the behavior of these functionals with respect to the random graph $G_{n,r}$, and bound them with high probability.
	\item We condition on $X \subseteq \mathcal{E}$, where $\mathcal{E}$ is a high-probability set over which the relevant functionals on $G_{n,r}$ are bounded. We conclude that the upper bound on testing error derived in our first step holds whenever $X \subseteq \mathcal{E}$. 
\end{enumerate}

\subsubsection{Step 1: Testing error on a fixed graph}

Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta = (\beta_1,\ldots,\beta_n) \in \Reals^n$ be a signal over the vertices $V$. We observe responses $Y = (y_1,\ldots,y_n)$ according to the model
\begin{equation*}
y_i = \beta_i + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i y_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec} = \1\{T_{\spec} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $S_2(\beta;G)$ be a measure of smoothness the signal $\beta$ displays over the graph $G$, given by
\begin{equation*}
S_2(\beta;G) = \sum_{i = 1}^{n} \sum_{j = 1}^{n} (\beta_i - \beta_j)^2 \1((v_i,v_j) \in E).
\end{equation*}
In Lemma~, we upper bound the Type I and Type II error of the test $\phi_{\spec}$. Our bound on the Type II error will be stated as a function of $S_2(\beta;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$.

\begin{lemma}
	\label{lem:fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}$ is upper bounded
		\begin{equation}
		\label{eqn:graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b$ and $\beta$ such that
		\begin{equation}
		\label{eqn:fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\beta;G)}{ns_{\kappa}}
		\end{equation}
		the Type II error of $\phi_{\beta}$ is upper bounded,
		\begin{equation}
		\label{eqn:graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

The first term on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius} grows with $\kappa$, while the second term shrinks. These can be thought of as the variance and squared bias terms, respectively, of the error.

\paragraph{Proof of Lemma~\ref{lem:fixed_graph_testing}.}

To prove Lemma~\ref{lem:fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2.
\end{align*}
When $\beta = 0$, this equals $\kappa/n$. Otherwise, we have the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{s_{\kappa}}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 s_k \\
& \geq \norm{\beta}_2^2 - \frac{S_2(\beta;G)}{s_{\kappa}},
\end{align*}
and therefore $\Ebb(T_{\spec}) \geq \kappa/n + n^{-1}(\norm{\beta}_2^2 - S_2(\beta;G)/s_{\kappa})$. 

\vspace{.2 in}

\textit{Variance of $T_{\mathrm{spec}}$:}
We write $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvectors $v_1,\ldots,v_{\kappa}$ as columns. Consequently,
\begin{align}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa)
\end{align}
where the last inequality follows from standard properties of the Gaussian distribution. We now move on to showing the desired inequalities \eqref{eqn:graph_spectral_type_I_error} and \eqref{eqn:graph_spectral_type_II_error}.
For simplicity, we introduce the notation
\begin{equation*}
t(b) = b\sqrt{\frac{2\kappa}{n^2}}, ~~ \Delta = \norm{\beta}_2^2 - \frac{S_2(\beta;G)}{s_{\kappa}}.
\end{equation*}

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{1}{b^2}.
\end{align*}

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:graph_spectral_type_II_error}:} Assumption~\eqref{eqn:fixed_graph_testing_critical_radius} implies
\begin{equation*}
\Delta \geq \frac{\norm{\beta}_2^2}{n} - \frac{C_n^2}{nC^2} \geq 2 t(b).
\end{equation*}
Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & = \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\beta^T V_{\kappa} V_{\kappa}^T \beta /n}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term, noting that $\Delta = \beta^T V_{\kappa} V_{\kappa}^T \beta$, we have
\begin{align}
\frac{\beta^T V_{\kappa} V_{\kappa}^T \beta/n}{\Delta^2} & \leq \frac{1}{n\Delta} \nonumber \\
& \leq \frac{1}{2nt(b)} \nonumber \\
& = \frac{1}{2b\sqrt{2\kappa}}, \label{eqn:spectral_type_II_error_pf2}
\end{align}
and combining~\eqref{eqn:spectral_type_II_error_pf1} and~\eqref{eqn:spectral_type_II_error_pf2} yields~\eqref{eqn:graph_spectral_type_II_error}.


\subsubsection{Step 2: Bounding neighborhood graph functionals}

To make use of Lemma~\ref{lem:fixed_graph_testing} we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that for some constants $c_1,c_2,c_3$, the following statements:
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} When $\beta = (f(x_1),\ldots,f(x_n))$ for $f \in W^{1,2}(\mathcal{X};R)$:
	\begin{equation}
	\label{eqn:continuous_to_discrete_sobolev_norm}
	S_2(\beta;G_{n,r}) \leq c_1 \cdot n^2 r^{d + 2} 
	\end{equation}
	\item 
	\label{event:eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} If $r = (\log^{a}n)\cdot(\log n/n)^{1/d}$ for any $a > 0$, then for all $\kappa \in [n]$:
	\begin{equation}
	\label{eqn:eigenvalue_tail_bound}
	s_{\kappa} \geq c_2 \cdot \frac{\kappa^{2/d}}{n^{2/d}}
	\end{equation}
	\item 
	\label{event:l2_norm}
	\textbf{Empirical norm of $f$:} When $f \in W^{1,2}(\mathcal{X};R)$ and one of (a) $d = 1,2$, or (b) $d = 3$ and $\norm{f}_{\Leb^2}^2 \geq n^{-2/(d - 2)}$, then
	\begin{equation}
	\label{eqn:l2_to_empirical_norm}
	\frac{1}{n}\sum_{i = 1}^{n}\beta_i^2 = \norm{f}_n^2 \geq c_3 \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

each hold with probability $1 - o(1)$ as $n \to \infty$.

\paragraph{Proof of~\eqref{eqn:continuous_to_discrete_sobolev_norm}:}

Observe that
\begin{align*}
\frac{1}{n^2}\Ebb(\beta^T L \beta) = \Ebb\left(\frac{1}{n^2} \sum_{i,j = 1}^{n} (\beta_i - \beta_j)^2 A_{ij}\right) & = \frac{(n - 1)}{n} \int_{\Xset} \int_{\Xset} (f(x) - f(y))^2K_r(x,y) \,dP(y) \,dP(x) \\
& \leq p_{\max}^2 \frac{(n - 1)}{n} \int_{\Xset} \int_{\X} (f(x) - f(y))^2K_r(x,y) \,dy \,dx.
\end{align*}
We will show that for any $f \in \mathcal{W}^{1,2}(\X;R)$
\begin{equation}
\label{eqn:nonlocal_continuous_sobolev_norm}
\int_{\Xset} \int_{\Xset} (f(x) - f(y))^2K_r(x,y) \,dy \,dx \leq c r^{d + 2} \norm{f}_{W^{1,2}(\mathcal{X})}
\end{equation}
whence the desired result~\eqref{eqn:continuous_to_discrete_sobolev_norm} follows by Markov's inequality and the assumption $f \in W^{1,2}(\mathcal{X};R)$.

We begin by dealing with complications due to smoothing at the boundary of $\mathcal{X} = [0,1]^d$. Let $U$ be any bounded open set such that $\Xset \subset U$ is compactly contained in $U$. Note that as $\partial \Xset$ is $C^1$, by Theorem \ref{thm:evans_extension} there exists $g \in W^{1,2}(\Reals^d)$ such that
\begin{enumerate}[(i)]
	\item
	\label{eqn:evans_extension_1}
	$g = f$, $P$-almost-everywhere in $\Xset$
	\item 
	$g$ has support within $V$, and  
	\item 
	\label{eqn:sobolev_1_bound_2}
	$\norm{g}_{W^{1,2}(\Rd)} \leq c \norm{f}_{W^{1,2}(\Xset)}$ for a constant $c$ which depends only on $\Xset$.
\end{enumerate}
As a result of \ref{eqn:evans_extension_1},
\begin{equation}
\label{eqn:sobolev_1_bound_1}
\int_{\Xset} \int_{\Xset} (f(x) - f(y))^2K_r(x,y) \,dy \,dx \leq \int_{\Rd} \int_{\Rd} (g(x) - g(y))^2K_r(x,y) \,dy \,dx.
\end{equation}

Next we smooth $g$, so that we may work with ordinary partial derivatives.
We let $\eta \in C^{\infty}(\Rd)$ be given by
\begin{equation*}
\eta(x) :=
\begin{cases}
C \exp \left\{\frac{1}{\norm{x}^2 - 1}\right\}& \quad \textrm{if $\norm{x}_2 \leq 1$} \\
0 & \quad \textrm{if $\norm{x}_2 \geq 1$}
\end{cases}
\end{equation*}
where the normalizing constant $C > 0$ is chosen so that $\int_{\Rd} \eta dx = 1$. Let $\eta_r(x) := (1/r^d) \eta(x/r)$. Then, the mollification of $g$ by $\eta_r$ is given by
\begin{align*}
g^r & := g \ast \eta_r \\
& = \int_{\Rd} \eta_r(x - y) g(y) dy
\end{align*}
(Refer to \textcolor{red}{Evans}, Appendix C, Theorem 7 for a proof that $g^r \in C^{\infty}(\Rd)$.)
Adding and subtracting within \eqref{eqn:sobolev_1_bound_1}, we have
\begin{align}
\int_{\Rd} & \int_{\Rd} (g(x) - g(y))^2K_r(x,y) \,dy \,dx \\
& \leq 3 \int_{\Rd} \int_{\Rd} \bigl((g(x) - g^r(x))^2 + (g^r(x) - g^r(y))^2 + (g^r(y) - g(y))^2\bigr)K_r(x,y) \,dy \,dx \nonumber \\
& = 6 \int_{\Rd} \int_{\Rd} \bigl((g(y) - g^r(y))^2 K_r(x,y) \,dy \,dx + 2 \int_{\Rd} \int_{\Rd} \bigl((g^r(x) - g^r(y))^2 K_r(x,y) \,dy \,dx \label{eqn:sobolev_1_bound_5}
\end{align}
We deal with each summand individually, beginning with the first one. Let $\nu_d = \pi^{d/2}/\Gamma(d/2 + 1)$ denote the volume of a ball with unit radius in $\Rd$. Then, we have
\begin{align}
\int_{\Rd} \int_{\Rd} \bigl((g(y) - g^r(y))^2 K_r(x,y) \,dy \,dx & = \int_{\Rd} \int_{B(x,r)} \bigl((g(y) - g^r(y))^2\,dy \,dx \nonumber \\
& = \int_{\Rd} \int_{B(y,r)} \bigl((g(y) - g^r(y))^2\,dx \,dy \tag{by Tonelli's Theorem} \nonumber \\
& = \nu_d r^d \int_{\Rd}\bigl((g(y) - g^r(y))^2 \,dy \\
& \leq \nu_d r^{d + 2} \int_{\Rd} \norm{\nabla g(y)}^2 \,dy \label{eqn:sobolev_1_bound_4}
\end{align}
where the last line follows from Lemma \ref{lem:poincare_mollify}.

We now turn our attention to the second summand. Note that as $g^r \in C^{\infty}(\Rd)$, we may apply Theorem \ref{thm:taylor_expansion} and obtain
\begin{align*}
(g^r(x) - g^r(y))^2 & = \left(\int_{0}^{1} \nabla g^r(x + t(y - x)) \cdot (y - x) \,dt\right)^2 \\
& \leq \int_{0}^{1} \bigl(\nabla g^r(x + t(y - x)) \cdot (y - x)\bigr)^2 \,dt  \tag{Jensen's inequality} \\
& \leq \norm{y - x}^2 \int_{0}^{1} \norm{\nabla g^r(x + t(y - x))}^2 \,dt. \tag{Cauchy-Schwarz inequality}
\end{align*}
As a result, we have
\begin{align*}
\int_{\Rd} \int_{\Rd} \bigl((g^r(x) - g^r(y))^2 K_r(x,y) \,dy) \,dx & \leq \int_{\Rd} \int_{\Rd} \norm{y - x}^2  \int_{0}^{1} \norm{\nabla g^r(x + t(y - x))}^2 K_r(x,y) \,dt \,dy \,dx \\
& \leq r^{2} \int_{\Rd} \int_{\Rd} \int_{0}^{1} \norm{\nabla g^r(x + t(y - x))}^2 K_r(x,y) \,dt \,dy \,dx \\
& = r^{2} \int_{\Rd} \int_{0}^{1} \int_{\Rd} \norm{\nabla g^r(x + t(y - x))}^2 K_r(x,y) \,dx \,dt \,dy \\
& = r^{2} \int_{\Rd} \int_{0}^{1} \int_{\Rd} \norm{\nabla g^r(x + tz)}^2 K_r(z) \,dx \,dt \,dz \tag{$z = y - x$}
\end{align*}

where we write $K_r(z) = \mathbf{1}(\norm{z} \leq r)$ in an abuse of notation. Next, we note that
\begin{equation*}
\int_{\Rd} \norm{\nabla g^r(x + tz)}^2 \,dx = \int_{\Rd} \norm{\nabla g^r(x)}^2 \,dx \leq \int_{\Rd} \norm{\nabla g(x)}^2 \,dx
\end{equation*}
with the inequality following from Lemma \ref{lem:gradient_mollify_commute}. Therefore,
\begin{align}
r^{2} \int_{\Rd} \int_{0}^{1} \int_{\Rd} \norm{\nabla g^r(x + tz)}^2 K_r(z) \,dx \,dt \,dz & \leq r^{2} \int_{\Rd} K_r(z) \int_{0}^{1} \int_{\Rd} \norm{\nabla g(x)}^2 \,dx  \,dt \,dz \nonumber \\
& = \nu_d r^{2 + d} \int_{\Rd} \norm{\nabla g(x)}^2 \,dx. \label{eqn:sobolev_1_bound_3}
\end{align}
By \eqref{eqn:sobolev_1_bound_5}, \eqref{eqn:sobolev_1_bound_4} and \eqref{eqn:sobolev_1_bound_3}, we have that 
\begin{equation*}
\int_{\Rd} \int_{\Rd} (g(x) - g(y))^2 K_r(x,y) \,dx \,dy \leq 9 \nu_d r^{d + 2} \int_{\Rd} \norm{\nabla g(x)}^2 \,dx
\end{equation*}
Then by \ref{eqn:sobolev_1_bound_2}, $\int_{\Rd} \norm{\nabla g(x)}^2 \,dx \leq c \int_{\Xset} \norm{\nabla f(x)}^2 \,dx$, and the desired result of \eqref{eqn:sobolev_1_bound_1} follows.

\paragraph{Proof of~\eqref{eqn:eigenvalue_tail_bound}:}

For ease of notation we write $t = n^{1/d}$, and assume without loss of generality that $t$ is whole number. Let $\overline{G} = (\overline{V},\overline{E})$ be the lattice over $[0,1]^d$, formally
\begin{align*}
\overline{V} & = \bigl\{\overline{x}_k := \frac{1}{n}(k_1,\ldots,k_d): k \in [t]^d\bigr\} \\
\overline{E} & = \bigl\{(\overline{x}_k, \overline{x}_{\ell}): \norm{k - \ell}_1 = 1\bigr\}.
\end{align*}

By Theorem 1.1 of \textcolor{red}{Garcia Trillos and Slepcev}, with probability at least $1 - n^{-1}$ there exists a bijection $\pi: \overline{V} \to X$ such that
\begin{equation*}
\max_{k \in [t]^d} \abs{\overline{x}_k - \pi(\overline{x}_k)} \leq c \left(\frac{\log n}{n}\right)^{1/d}
\end{equation*}
Since our choice $r = \log^a(n) \cdot (\log n/n)^{1/d}$ implies $r = \omega(\left(\frac{\log n}{n}\right)^{1/d})$, for $n$ sufficiently large with probability $1 - n^{-1} = 1 - o(1)$ we have
\begin{equation}
\label{eqn:eigenvalue_tail_bound_pf1}
(\overline{x}_k,\overline{x}_{\ell}) \in \overline{E} \Longrightarrow (\pi(\overline{x}_k),\pi(\overline{x}_{\ell})) \in E.
\end{equation}
Let $\overline{L}$ be the Laplacian of $\overline{G}$, with eigenvalues $\lambda_1 < \cdots \leq \lambda_n$. If \eqref{eqn:eigenvalue_tail_bound_pf1} holds, then for all $x \in \Reals^n$,
\begin{equation*}
x^T \overline{L} x \geq x^T L x \Longrightarrow \lambda_j \leq s_j~~ \textrm{for all}~ j \in [n].
\end{equation*}
The eigenvalues of the $d$-dimensional lattice are explicitly known. In particular when $j = h^d$ for some whole number $h$, we have $s_j \geq \lambda_j = 4 d \sin^2(\pi h/2t) \geq \pi^2 h^2/ (4 t^2)$, with a similar inequality (for possible different constants) holding when $j^{1/d}$ is not integral; this shows~\eqref{eqn:eigenvalue_tail_bound}.


\paragraph{Proof of~\eqref{eqn:l2_to_empirical_norm}:}

To prove~\eqref{eqn:l2_to_empirical_norm} we will upper bound
\begin{equation*}
\mathbb{E}\biggl[\Bigl(\frac{1}{n}\sum_{i = 1}^{n}f^2(x_i)\Bigr)^2\biggr],
\end{equation*}
and then apply the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). We have
\begin{align*}
\mathbb{E}\biggl[\Bigl(\frac{1}{n}\sum_{i = 1}^{n}f^2(x_i)\Bigr)^2\biggr] & = \frac{(n-1)}{n}\mathbb{E}(f^2(x_1)) + \frac{\mathbb{E}[f^4(x_1)]}{n} \\
& \leq p_{\max}^2 \left(\norm{f}_{\Leb^2}^2 + \frac{\norm{f}_{\Leb^4}^4}{n}\right) \\
& \overset{(i)}{\leq} p_{\max}^2 \left(\norm{f}_{\Leb^2}^2 + c\frac{\norm{f}_{\Leb^2}^{(4 - d)}}{n}\right) \\
& \overset{(ii)}{\leq}  p_{\max}^2 \left(\norm{f}_{\Leb^2}^2 + c\norm{f}_{\Leb^2}^2\right).
\end{align*}
Here $(i)$ follows from two facts, the first being the embedding $W^{1,2}(\mathcal{X};R) \subseteq \Leb^q(\mathcal{X};R')$ for $q = 2d/(d - 2)$ when $d > 2$ and $q = \infty$ when $d \leq 2$ and some constant $R'$, and the second being a standard result on interpolation of $\Leb^p$ spaces, namely that since $f \in \Leb^2(\mathcal{X}) \cap \Leb^q(\mathcal{X})$, then 
\begin{equation*}
\norm{f}_{\Leb^4} \leq \norm{f}_{\Leb^2}^{\lambda} \cdot \norm{f}_{\Leb^q}^{1 - \lambda},~~\textrm{where}~ \frac{1}{4} = \frac{\lambda}{2} + \frac{1 - \lambda}{q}.
\end{equation*} 
Then $(ii)$ follows from our assumptions on $\norm{f}_{\Leb^2}^2$: either 
\begin{enumerate}[(a)]
	\item $d \leq 2$ and $\norm{f}_{\Leb^2}^2 \geq 1/n$ so $\norm{f}_{\Leb^2}^{(4 - d)} \leq \max(R^{4 - d}/n, \norm{f}_{\Leb^2}^2) \leq c\norm{f}_{\Leb^2}^2$, or
	\item $d = 3$ and $\norm{f}_{\Leb^2}^2 \geq n^{-2/(d - 2)}$.
\end{enumerate} 
Applying Lemma~\ref{lem:paley_zygmund} to  $\norm{f}_n^2$ yields~\eqref{eqn:l2_to_empirical_norm}.

\subsubsection{Step 3: Conclusion}

We note that for all possible values of $X \in \Xset^n$, under the null hypothesis $f = f_0 = 0$ and therefore $\beta = (f(x_1),\ldots,f(x_n)) = 0$ as well. Therefore by~\eqref{eqn:graph_spectral_type_I_error}, we have the following bound on Type I error:
\begin{equation}
\Ebb_{f_0}(\phi_{\mathrm{spec}}) = \mathbb{E}(\mathbb{E}_{\beta = 0}(\phi_{\spec}) | X) \leq \frac{1}{b^2}.
\end{equation}

Now, we bound Type II error under the assumption $f \in W^{1,2}(\mathcal{X};R)$ and 
\begin{equation}
\label{eqn:critical_radius_1}
\norm{f}_{\Leb^2}^2 \geq \epsilon^2 = c \cdot b \cdot (\log n)^{h(\beta,d)} \cdot n^{-4/(4 + d)}.
\end{equation}
We verify that the conditions required to apply Step 2 are satisfied. In particular, we choose $r = (\log^a n) \cdot (\log n/n)^{1/d}$, and observe that~\eqref{eqn:critical_radius_1} implies $\norm{f}_{\Leb^2}^2 \geq n^{-1}$ when $d \leq 2$ and $\norm{f}_{\Leb^2}^2 \geq n^{-2/(d - 2)}$ when $2 < d < 4$. We may therefore apply our conclusions in Step 2; namely, that for every possible choice of $f$ there exists a good set $\mathcal{E}_f \subseteq \Xset^n$ with $\Pbb(\mathcal{E}_f) \geq 1 - o(1)$ such that each of \eqref{eqn:continuous_to_discrete_sobolev_norm}, \eqref{eqn:eigenvalue_tail_bound}, and \eqref{eqn:l2_to_empirical_norm} hold for all $X \subseteq \mathcal{E}_f$. Choosing $\kappa = n^{2d/(4 + d)}$ to balance the squared bias and variance terms on the right hand side of~\eqref{eqn:fixed_graph_testing_critical_radius}, we have that for all $X \subseteq \mathcal{E}_f$
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_2(\beta;G)}{ns_{\kappa}} & \leq 2bn^{-4/(4+d)} + \frac{n r^{d + 2}}{s_{\kappa}} \tag{by \eqref{eqn:continuous_to_discrete_sobolev_norm}} \\
& \leq 2bn^{-4/(4+d)} + c\cdot\frac{n r^{d + 2}}{n^{4/(4+d)}} \tag{by \eqref{eqn:eigenvalue_tail_bound}} \\
& \leq 2bn^{-4/(4+d)} + c\cdot(\log^{a(d + 2) + (1 + 2/d)}n)\cdot n^{-4/(4+d)} \\
& \leq \frac{1}{10}\norm{f}_{\Leb^2} \tag{for a suitably large choice of $c$ in \eqref{eqn:critical_radius_1}}\\
& \leq \frac{1}{n}\sum_{i = 1} \beta_i^2. \tag{by \eqref{eqn:l2_to_empirical_norm}}
\end{align*}
We conclude that for all $X \subseteq \mathcal{E}_f$, the inequality \eqref{eqn:fixed_graph_testing_critical_radius} is satisfied with respect to $\beta = (f(x_1),\ldots,f(x_n))$ and $G = G_{n,r}$. As a result the worst-case Type II error is bounded
\begin{equation*}
\sup_{\substack{f \in W^{1,2}(\mathcal{X;R}) \\ \norm{f}_{\Leb^2} \geq \epsilon}}\mathbb{E}_{f}(1 - \phi_{\spec}) \leq \sup_{\substack{f \in W^{1,2}(\mathcal{X;R}) \\ \norm{f}_{\Leb^2} \geq \epsilon}} \mathbb{E}\bigl[\mathbb{E}_{\beta}(1 - \phi_{\spec}|X \in \mathcal{E}_f)\bigr] + o(1) \leq \frac{3}{b} + o(1).
\end{equation*}

\subsection{Proof of Proposition~\ref{prop:L4_testing_rate}}

Let $T_{\mathrm{mean}} = \frac{1}{n}\sum_{i = 1}^{n} y_i^2$. The expectation of $T_{\mathrm{mean}}$ is
\begin{equation*}
\Ebb(T_{\mathrm{mean}}) = \mathbb{E}(f^2(x_1)) + 1,
\end{equation*}
and the variance can be upper bounded
\begin{equation*}
\Var(T_{\mathrm{mean}}) \leq \frac{1}{n}(3 + p_{\max} \norm{f}_{\Leb^4}^4 + p_{\max}\norm{f}_{\Leb^2}^2) = \frac{c}{n}.
\end{equation*}
Since $\mathbb{E}(f^2(x_1)) \geq p_{\min} \epsilon^2$, when $\epsilon^2 \gtrsim b\cdot n^{-1/2}$ we can apply Chebyshev's inequality to obtain the claimed result.
\section{Auxiliary Results}

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.4, Theorem 1]
	\label{thm:evans_extension}
	Assume $U$ is bounded and $\partial U$ is $C^1$. Select a bounded open set $V$ such that $U \subset \subset V$ ($U$ is compactly contained in $V$). Then there exists a bounded linear operator $E: W^{1,2}(U) \to W^{1,2}(\Rd)$ such that for each $u \in W^{1,2}(U)$:
	\begin{enumerate}
		\item $Eu = u$ a.e. in $U$,
		\item $Eu$ has support within $V$, and 
		\item 
		\begin{equation*}
		\norm{Eu}_{W^{1,2}(\Rd)} \leq C \norm{u}_{W^{1,2}(\Rd)}
		\end{equation*}
		the constant $C$ depending only on $U$ and $V$.
	\end{enumerate}
\end{theorem}

For $u \in W^{1,2}(\Rd)$ and $x \in \Rd$, write $\nabla u(x) = (D^{e_1}(x),\ldots,D^{e_d}(x)$, for the gradient of $u$.

\begin{theorem}[\textcolor{red}{Evans} Chapter 5.8.1, Theorem 2]
	\label{thm:evans_poincare}
	There exists a constant $C$, depending only on $d$, such that
	\begin{equation*}
	\norm{u - u^r}_{L^2(B(x,r))} \leq Cr\norm{\nabla u}_{L^2(B(x,r))}
	\end{equation*}
\end{theorem}

\begin{theorem}[Taylor expansion.]
	\label{thm:taylor_expansion}
	For any function $u \in C^1$, and any $x,y \in \Rd$, 
	\begin{equation*}
	u(y) - u(x) = \int_{0}^{1} \nabla(u(x + t(y - x))) \cdot (y - x) \,dt
	\end{equation*}
\end{theorem}

\begin{lemma}
	\label{lem:poincare_mollify}
	For any function $g \in W^{1,2}(\Rd)$ compactly supported in a bounded open set $V \subset \Rd$, we have
	\begin{equation}
	\label{eqn:poincare_mollify}
	\int_{\Rd} (g(x) - g^r(x))^2 \,dx \leq r^2 \int_{\Rd} \norm{\nabla g(x)}^2 \,dx
	\end{equation}
\end{lemma}
\begin{proof}
	This Lemma is essentially a reproduction of part of the proof of the Rellich-Kondrachov Compactness Theorem from \textcolor{red}{Evans}. Note that it is sufficient to prove in the case when $g$ is smooth. To see this, for the moment assume \eqref{eqn:poincare_mollify} holds for all $u \in C^{\infty}(V)$, and let $g \in W^{1,2}(V)$. By Theorem \ref{thm:local_approx_smooth_functions}, we may take a sequence $g_m \in C^{\infty}(V)$ such that
	\begin{equation*}
	\norm{g - g_m}_{L^2(V)} \to 0, \quad \textrm{and} \quad \norm{\nabla g - \nabla g_m}_{L^2(V)} \to 0,
	\end{equation*}
	Then we have
	\begin{align*}
	\int_{\Rd} (g(x) - g^r(x))^2 \,dx & \leq \int_{\Rd} (g(x) - g_m(x))^2 \,dx + \int_{\Rd}(g_m(x) - g_m^r(x))^2 \,dx + \int_{\Rd}(g_m^r(x) - g^r(x))^2 \,dx \\
	& \leq \int_{\Rd} (g(x) - g_m(x))^2 \,dx + r^2 \int_{\Rd} \norm{\nabla g_m(x)}^2 \,dx + \int_{\Rd}(g_m^r(x) - g^r(x))^2 \,dx
	\end{align*}
	and taking the limit as $m$ goes to infinity, the right hand side converges to $\int_{\Rd} \norm{\nabla g(x)}^2 \,dx$. 
	
	It remains to show \eqref{eqn:poincare_mollify} in the case where $g$ is smooth. In this case, we have
	\begin{align*}
	g^{r}(x) -  g(x) & = \frac{1}{r^d} \int_{B(x,r)} \kappa \left(\frac{x - z}{r}\right)\bigl(g(z) - g(x)\bigr) \,dz \\
	& = \int_{B(0,1)} \kappa(y) \bigl(g(x - ry) - g(x)\bigr) \,dy \\
	& = \int_{B(0,1)} \kappa(y) \int_{0}^{1} \frac{d}{dt} \bigl(g(x - try) \bigr) \,dt \,dy \\
	& = -r \int_{B(0,1)} \kappa(y) \int_{0}^{1} \bigl(\nabla g (x - try) \bigr) \cdot y \,dt \,dy.
	\end{align*} 
	Therefore, by Jensen's and Cauchy-Schwarz inequalities, we have
	\begin{align*}
	\int_{\Rd} (g(x) - g^r(x))^2 \,dx & \leq r^2 \int_{\Rd} \int_{B(0,1)} \kappa(y) \int_{0}^{1} \norm{\nabla g (x - try)}^2 \norm{y}^2 \,dt \,dy \,dx \\
	& \leq r^2 \int_{B(0,1)} \kappa(y) \int_{0}^{1} \int_{\Rd} \norm{\nabla g (x - try)}^2 \,dx \,dt \,dy \\
	& = r^2 \int_{\Rd} \norm{\nabla g(z)}^2 \,dz
	\end{align*}
\end{proof}

The following theorem is Theorem 1 in Section 5.3 of \textcolor{red}{Evans}.
\begin{theorem}[Local approximation by smooth functions.]
	\label{thm:local_approx_smooth_functions}
	Assume $U$ is bounded, and $u \in W^{k,p}(U)$ for some $1 \leq p < \infty$. Then there exists functions $u_m \in C^{\infty}(U) \cap W^{k,p}(U)$ such that
	\begin{equation*}
	\norm{u_m - u}_{W^{k,p}(U)} \overset{m}{\to} 0.
	\end{equation*}
\end{theorem}

\begin{lemma}
	\label{lem:gradient_mollify_commute}
	For any $u \in W^{1,2}(\Rd)$, we have
	\begin{equation*}
	\int_{\Reals^d} \norm{\nabla u^r(x)}_2^2 \,dx \leq \int_{\Reals^d} \norm{\nabla u(x)}_2^2 \,dx
	\end{equation*}
\end{lemma}
\begin{proof}
	Observe that $\norm{\nabla u(x)}_2^2 = \sum_{j = 1}^{d} (D^{e_j}u(x))^2 $. Therefore,
	\begin{align*}
	\int_{\Rd} \norm{\nabla u^r(x)}_2^2 \,dP(x) & = \sum_{j = 1}^{d} \int_{\Rd} (D^{e_j}u^r(x))^2 \,dx \\
	& = \sum_{j = 1}^{d} \int_{\Rd} ((D^{e_j}u)^r(x))^2 \,dx
	\end{align*}
	where the second equality follows from equation (1) in Section 5.3 of \textcolor{red}{Evans}. Then, for any $v \in L^2(\Rd)$, we have that
	\begin{align*}
	\abs{v^r(x)} & = \int_{\Rd} \kappa_r^{1/2}(x - y) \kappa_r^{1/2}(x - y) v(y) \,dy \\
	& \leq \left(\int_{\Rd} \kappa_r^(x - y) \,dy\right)^{1/2} \left(\int_{\Rd} \kappa_r(x - y) v^2(y) \right)^{1/2} \\
	& = \left(\int_{\Rd} \kappa_r(x - y) v^2(y) \right)^{1/2}
	\end{align*}
	and therefore
	\begin{align*}
	\int_{\Rd} (v^r(x))^2 \,dx & \leq \int_{\Rd} \int_{\Rd } \kappa_r(x - y) v^2(y) \,dy \,dx \\
	& = \int_{\Rd} \int_{\Rd } \kappa_r(x - y) v^2(y) \,dy \,dx \\
	& = \int_{\Rd} v^2(y) \,dy
	\end{align*}
	
	Applying this to $D^{e_j}u \in L^2(\Rd)$, we have that
	\begin{equation*}
	\sum_{j = 1}^{d} \int_{\Rd} ((D^{e_j}u)^r(x))^2 \,dx \leq \sum_{j = 1}^{d} \int_{\Rd} ((D^{e_j}u(x))^2 \,dx = \int_{\Rd} \norm{\nabla u(x)}^2 \,dx.
	\end{equation*}
\end{proof}

\begin{lemma}
	\label{lem:paley_zygmund}
	Let $0 < p < q$, and let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. Then, for all $0 \leq \lambda \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund}
	\mathbb{P}(Z > \lambda \mathbb{E}(Z^p)) \geq \left[(1 - \lambda^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
\end{lemma}

\end{document}