\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left( #1 \right)}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\dotp}[2]{\langle #1 , #2 \rangle}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\Perp}{\perp \! \! \! \perp}


\newcommand{\Linv}{L^{\dagger}}
\newcommand{\tr}{\text{tr}}
\newcommand{\h}{\textbf{h}}
% \newcommand{\l}{\ell}
\newcommand{\x}{\textbf{x}}
\newcommand{\y}{\textbf{y}}
\newcommand{\Lx}{\mathcal{L}_X}
\newcommand{\Ly}{\mathcal{L}_Y}
\DeclareMathOperator*{\argmin}{argmin}


\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Rd}{\Reals^d}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\begin{document}
	
\title{Graph Testing Notes}
\author{Alden Green}
\date{\today}
\maketitle

\section{Setup}

\paragraph{Data model.}

We are given two distributions, $P$ and $Q$, with the ability to sample from either one. Our goal is to test the hypothesis $H_0: P = Q$ vs. the alternative $H_a: P \neq Q$. 

Under the \textbf{binomial data model}, our sampling procedure is to draw i.i.d Rademacher labels $L_i \in \set{1, -1}$ for $i \in \set{1, \ldots, N}$, and then sample 
$Z_i \sim P$ if $L_i = 1$ and $Z_i \sim Q$ otherwise. Define $1_X$ to be the length-$N$ indicator vector for $L_i = 1$
\begin{equation*}
1_X[i] = 
\begin{cases}
1, L_i = 1\\
0 \text{ otherwise } 
\end{cases}
\end{equation*}
and similarly for $1_Y$
\begin{equation*}
1_Y[j] = 
\begin{cases}
1, L_i= -1 \\
0 \text{ otherwise } 
\end{cases}
\end{equation*}
and define $a = \frac{1_X}{N / 2} - \frac{1_Y}{N / 2}$. 

Under the \textbf{fixed label data model} we use the same data generating process as above, except fix $\mathcal{L}_X = \set{1, \ldots, N/2}$ and $\mathcal{L}_Y = \set{N/2, \ldots, N}$. Say that $L_i = 1$ for $i \in \mathcal{L}_X$ and $L_i = -1$ for $i \in \mathcal{L}_Y$, and call $\set{ X_1, \ldots, X_{\abs{\Lx}} } = \set{Z_i: i \in \mathcal{L}_X}$ and likewise for $Y$.

\paragraph{Graph.}

Form an $N \times N$ Gram matrix $A$, where $A_{ij} = K(Z_i, Z_j)$ for \textbf{kernel function} $K$. Let $G = (V,E)$ with $V = \set{Z_1, \ldots, Z_n}$ and $E = \set{A_{ij}: 1 \leq i < j \leq n}$. Take $L = D - A$ to be the (unnormalized) \textbf{Laplacian matrix} of $A$ (where $D$ is the diagonal degree matrix with $D_{ii} = \sum_{j \in [n + m]} A_{ij}$). Denote by $B$ the $M \times N$ \textbf{incidence matrix} of $A$, where we denote the $i$th row of $B$ as $B_i$ and set $B_i$ to have entry $A_{ij}$ in position $i$, $-A_{ij}$ in position $j$, and $0$ everywhere else. 

\paragraph{Resistance distances.}

There are many distances one can define over nodes in a graph. The \textbf{resistance distance between nodes $u$ and $v$}, $R_{uv}$, is defined as
\begin{equation*}
R_{uv} = (e_u - e_v)^T \Linv (e_u - e_v).
\end{equation*}

\paragraph{Test statistics.}

We begin by defining our \textbf{laplacian smooth} test statistic. 
\begin{equation*}
T_2 = \left(\max_{\theta: \norm{B\theta}_2 \leq 1} a^T \theta \right)^2 = a^T \Linv a. 
\end{equation*}

(Bhattacharya 2018) defines a general notion of 2-sample \textbf{graph-based test statistics}
\begin{equation*}
T_{\mathcal{G}} = \frac{1}{N^2}\sum_{i = 1}^{n} \sum_{j = n + 1}^{n + m} A_{ij}
\end{equation*}
Although he develops theory for this statistic in the context of $k$NN and minimum spanning tree graphs, we will at present consider it for the complete weighted similarity graph defined by $A$ above. Then, we can write 
\begin{equation*}
T_{\mathcal{G}} = a^T L a.
\end{equation*}

Finally, define $\mathcal{H}$ to be a \textbf{reproducing kernel Hilbert space} with $K$ the associated kernel. Let $\mathcal{F}$ be the unit ball of $\mathcal{H}$, and let the evaluation of $f \in \mathcal{F}$ at the sample points $Z_1, \ldots, Z_N$ be denoted by $\textbf{f} = f(Z_1, \ldots, Z_N)$. Then, the statistic $\text{MMD}_b$ of (Gretton 2012) can be written as
\begin{equation*}
T_{\mathcal{K}} = \sup_{f \in \mathcal{F}} a^T \textbf{f} = a^T K a. 
\end{equation*}

\paragraph{Distances between probability measures.} 

We will need distances between probability measures for two different purposes. The first is that they are self-evidently useful in analyzing limiting distributions of statistics (in particular in this case, our test statistics).

For a function $f$, define its \textbf{Lipschitz norm} $\norm{f}_{L}$ to be
\begin{equation*}
\inf \set{K : \abs{f(x) - f(y)} \leq K \norm{x - y}}.
\end{equation*}
Define the \textbf{Wasserstein distance} between two measures $\mu$ and $\nu$ to be
\begin{equation*}
\mathcal{W}(\mu, \nu) := \sup \left \{ \abs{\int h \, d\mu - \int h \, d\nu}: h \text{ Lipschitz, with } \norm{h}_{L} \leq 1 \right\}.
\end{equation*}

If the measures $\mu$ and $\nu$ have corresponding cumulative distribution functions $F_{\mu}$ and $F_{\nu}$ then we can define the \textbf{Kolmogorov-Smirnov distance} to be
\begin{equation*}
\norm{F_\mu - F_\nu}_{\infty} := \sup_{t} \abs{F_{\mu}(t) - F_{\nu}(t)}.
\end{equation*}

The second reason we will use distances between probability measures is that they themselves make for good test statistics!

An \textbf{integral probability metric} (IPM) with respect to a function class $\F$ is defined
\begin{equation*}
\underset{f \in \F}{\sup} ~ \Expect{f(X)} - \Expect{f(Y)}
\end{equation*}
for $X \sim P$, $Y \sim Q$. 

Hereafter, we will assume $P$ and $Q$ are absolutely continuous with respect to Lebesgue measure, with density functions $p$ and $q$, respectively. Denote the \textbf{mixture density} by $\mu = \frac{p + q}{2}$. 

Denote the \textbf{gradient} of a function $f$ by $\nabla_x$. Then we can define the \textbf{Sobolev semi-norm} and \textbf{dot product}, $\norm{f}_{W_0^{1,2}(\X, \mu^2)}$ and $\dotp{f}{g}_{W_0^{1,2}(\X, \mu^2)}$, by
\begin{equation*}
\dotp{f}{g}_{W_0^{1,2}(\X, \mu)} = \int_{\X} \dotp{\nabla_x f(x)}{\nabla_x g(x)}_{\Rd} \mu^2(x), ~~ \norm{f}_{W_0^{1,2}(\X, \mu)} = \sqrt{\int_{\X} \norm{\nabla_x f(x)}^2 \mu^2(x) dx }
\end{equation*}

Let the \textbf{Sobolev space}, $W^{1,2}(\mathcal{X}, \mu^2)$, be 
\begin{equation*}
W^{1,2}(\mathcal{X},\mu^2) = \set{f: \X \to \Reals, \int_{\X} \norm{\nabla_x f(x)}^2 \mu^2(x) dx < \infty}.
\end{equation*}
and denote by $W_0^{1,2}(\mathcal{X}, \mu^2)$ the restriction of $W^{1,2}(\mathcal{X}, \mu^2)$ to functions which vanish at the boundary of $\X$. Note that $\norm{f}_{W_0^{1,2}(\X, \mu^2)}$ defines a semi-norm over $W_0^{1,2}(\X, \mu^2)$. Finally, let $B_W(\X, \mu^2)$ be the \textbf{unit ball} of $W_0^{1,2}(\X, \mu^2)$, meaning
\begin{equation*}
B_W(\X, \mu^2) = \set{f \in W_0^{1,2}(\X, \mu^2): \norm{f}_{W_0^{1,2}(\X, \mu^2)} \leq 1}
\end{equation*}

Now we can define the \textbf{Sobolev IPM}, $\mathcal{S}_{\mu^2}(P,Q)$ It is simply an IPM where the function class is the Sobolev unit ball with respect to $\mu^2$.
\begin{equation*}
\mathcal{S}_{\mu^2}(P,Q) \overset{\mathrm{def}}{=} \sup_{f \in B_W} \biggl\{ \Expect{f(X)} - \Expect{f(Y)}\biggr\}
\end{equation*}

\paragraph{Holder functions.}
We will show that the Laplacian constraint $\norm{B \theta}_2 \leq 1$ is very similar to the constraint $f_{\theta} \in B_W(X,\mu^2)$ for the right choice of $K$, over all Holder functions. 

For mapping $f: \Rd \to \Reals$ and $\beta$ a positive integer, we say $f$ is a \textbf{$\beta$-Holder function} if there exists $C > 0$ such that for all $x, y \in \X$
\begin{equation*}
\abs{f^{(\beta - 1)}(x) - f^{(\beta - 1)}(y)} \leq K \norm{x - y}
\end{equation*}
Roughly speaking, this means the functions have bounded $\beta$ partial derivatives.

\section{Conjectures}

Conjectures \ref{conj: spectral_measure_conv} and \ref{conj: diagonal_entries} will be needed for Theorem \ref{thm: asymptotic_null_distribution}.

\begin{conjecture}
	\label{conj: spectral_measure_conv}
	There exists a sequence of scaling factors $(\rho_n)_{n = 1}^{\infty}$ such that the \textbf{spectral measure} $\mu_n$ of $\rho_n \Linv$ converges weakly in probability
	\begin{equation*}
	\mu_n(\rho_n \Linv) \overset{\ast}{\rightharpoonup} \nu_{\infty}.
	\end{equation*}
	
	
	where $V \sim \nu_{\infty}$ and $V_n \sim \mu_n$ are bounded almost surely for all $n$ by some constant $C$. 
\end{conjecture}

\begin{conjecture}
	\label{conj: diagonal_entries}
	For all $\epsilon > 0$, there exists $N$ such that
	\begin{equation*}
	\Prob{\underset{i \in [n]}{\max} \, \frac{1}{n} \left(\{ \rho_n \Linv\}^2 \right)_{ii} \leq \epsilon} \geq 1 - \epsilon
	\end{equation*}
	for all $n \geq N$.
\end{conjecture}

\section{DESIRED RESULTS}

\begin{theorem}
	\label{thm:convprob_of_T2}
	For bandwidth parameter $h >0$ and decreasing function $k(\cdot,\cdot)$, write
	\begin{equation*}
	K(Z_i,Z_j) = \frac{1}{h^m} k(\norm{Z_i - Z_j}^2 / h^2).
	\end{equation*}
	
	For Sobolev IPM $\mathcal{S}_{\mu^2}(P,Q)$ as defined above,
	\begin{equation*}
	\sqrt{T_2} \convprob \mathcal{S}_{\mu^2}(P,Q)
	\end{equation*}
\end{theorem}

\begin{proof}[Proof attempt of Proposition \ref{thm:convprob_of_T2}]
	Recall that, for incidence matrix $B$, 
	\begin{equation*}
	\sqrt{T_2} = \left(\max_{\theta: \norm{B\theta}_2 \leq 1} a^T \theta \right).
	\end{equation*}
	
	We expand $\abs{\sqrt{T_2} - \mathcal{S}_{\mu^2}(P,Q)}$,
	\begin{align}
	\label{eqn: bias_variance_expansion}
	\abs{\sqrt{T_2} - \mathcal{S}_{\mu^2}(P,Q)} & \leq \abs{\max_{\theta: \norm{B\theta}_2 \leq 1} \bigl\{a^T \theta \bigr\} - \sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} } \nonumber \\
	& + \abs{\sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} - \sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}(f) - \mathbb{Q}(f) \right\} }
	\end{align}
	
	\textcolor{red}{(The following statement would hold only if Proposition \ref{prop: sobolev_donsker} held over $B_W(\X, \mu^2)$, rather than over $B_W([0,1], \lambda)$ for $\lambda$ Lebesgue measure.)}
	
	By Proposition \ref{prop: sobolev_donsker}, the second term in the summand on the right hand side of (\ref{eqn: bias_variance_expansion}) is $o_P(1)$.
	
	\textcolor{red}{(The following statement would hold only if Proposition \ref{prop: convprob_regularization_functional} were uniform over $B_W(\X, \mu^2)$ rather than over the class of $\alpha$-Holder functions $\F_{\alpha}$)}
	
	Then, Proposition \ref{prop: convprob_regularization_functional} implies that for any $\epsilon > 0$, there exists $N$ such that for $n \geq N$, 
	\begin{equation*}
	\sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} - \max_{\theta: \norm{B\theta}_2 \leq 1} \bigl\{a^T \theta \bigr\} \leq \epsilon
	\end{equation*}
	with high probability. 
	
	To complete the proof, we will have to show that for any $\epsilon > 0$, there exists $N$ such that for $n \geq N$, 
	\begin{equation*}
	\max_{\theta: \norm{B\theta}_2 \leq 1} \bigl\{a^T \theta \bigr\} - \sup_{f \in B_W(\X, \mu^2)} \left\{ \mathbb{P}_n(f) - \mathbb{Q}_n(f) \right\} \leq \epsilon
	\end{equation*}
	with high probability.
\end{proof}

\section{Results}

\paragraph{Expectation of two-sample test statistics.}

The expectation of the above statistics is potentially a good way to understand their large sample behavior, as quadratic forms often satisfy laws of large numbers assuming the matrices are well-conditioned.

\begin{proposition}
	\label{prop: expectation_of_TG}
	Draw $Z$ and $a$ under the binomial data model, and assume both $P$ and $Q$ are absolutely continuous with respect to Lebesgue measure over Euclidean space $\Reals^d$. Write $h_0(x) = p(x) - q(x)$ with empirical analogue $\mathbf{h_0} = (h_0(Z_1), \ldots, h_0(Z_n) )$. Then
	\begin{align}
	\label{eqn: expectation_of_TG}
	\Expect{T_{\mathcal{G}}} = & \int \int K(\norm{\x - \y}) \left[p(\x) + q(\x)\right] \left[p(\y) + q(\y)\right] d\x d\y \nonumber \\
	- & \frac{N}{N - 1} \int \int K(\norm{\x - \y}) \left[h_0(\x)\right]^2 \frac{p(\y) + q(\y)}{p(\x) + q(\x)} d\x d\y \nonumber \\
	+ & \frac{N}{N - 1} \int \int K(\norm{\x - \y}) \left[h_0(\x) - h_0(\y)\right]^2 \left[p(\y) + q(\y)\right] \left[p(\x) + q(\x)\right] d\x d\y.
	\end{align}
\end{proposition}

Note that even under the null hypothesis, where $h_0 = 0$,
\begin{equation*}
\Expect{T_{\mathcal{G}}} = \int \int K(\norm{\x - \y}) \left[p(\x) + q(\x)\right] \left[p(\y) + q(\y)\right] d\x d\y
\end{equation*}
which is not distribution-free, unlike in the case of the $k$NN graph. 

Another interesting consequence of Proposition \ref{prop: expectation_of_TG} comes when we take $K(\x,\y) = K(\frac{\norm{\x - \y}}{t})$ and let $t \to 0$. 

\begin{proposition}
	\label{prop: expectation_of_TG_with_kde}
	If $p$ and $q$ are Lipschitz continuous functions with bounded hessians, and $K$ is a continuous function on $R^+$ such that $x^{2 + d} K(x) \in L_2$, then under the same setup as in Proposition \ref{prop: expectation_of_TG},
	\begin{equation}
	\label{eqn: expectation_of_TG_with_kde}
	\frac{N - 1}{N}\underset{t \to 0}{\lim} \frac{1}{t^d} \Expect{T_{\mathcal{G}}} = \int  h_0(\x)^2 d\x + \int (p(\x) + q(\x))^2 d\x
	\end{equation}
\end{proposition}

We turn now to the expectation of the Laplacian smooth statistic.
\begin{proposition}
	\label{prop: expectation_of_T2}
	Under the fixed label data model
	\begin{equation}
	\label{eqn: expectation_of_T2}
	\Expect{a^T \Linv a} = \Expect{R_{X_1 Y_1}} - \frac{N - 1}{2N} \Expect{R_{X_1,X_2}} - \frac{N - 1}{2N} \Expect{R_{Y_1, Y_2}}
	\end{equation}
\end{proposition}

Note that, although the resistance distances are between only two nodes, in each case the expectation is over the entire (random) graph $G$. 

\paragraph{Asymptotic null distribution for $T_2$.}

We can compute an asymptotic null distribution for $T_2$, although its formulation depends on the eigenvalues of the matrix $\Linv$ which themselves are not obvious.

\begin{theorem}
	\label{thm: asymptotic_null_distribution}
	Denote the scaled version of the Laplacian smooth test statistic
	\begin{equation*}
	W_n = \sqrt{\frac{N^4}{32 \cdot \tr((\Linv)^2)}}\Bigl(T_2^2 - \frac{\tr(\Linv)}{4N^2} \Bigr).
	\end{equation*}
	
	If Conjectures \ref{conj: spectral_measure_conv} and \ref{conj: diagonal_entries} hold,
	\begin{equation*}
	\lim_{n \to \infty} \sup_{t} \abs{\Prob{W_n \leq t} - \Phi(t)} = 0.
	\end{equation*}
\end{theorem}

To prove Theorem \ref{thm: asymptotic_null_distribution}, we will need the following calculations of moments under $H_0$. 

\begin{lemma}
	\label{lem: cond_expectation_of_T2}
	Under $H_0$, the conditional expectation $\Expect{T_2 \vert Z} = \frac{\tr(\Linv)}{N^2}$. 
\end{lemma}

\begin{lemma}
	\label{lem: cond_var_of_T2}
	Under $H_0$, the conditional variance $\Var{T_2 \vert Z} = \frac{32 \tr[(\Linv)^2]}{N^4}$. 
\end{lemma}

\section{Supplemental Results}

\paragraph{Empirical process over Sobolev classes.}

The following theorem is a stand-in; it handles only functions with domain on the unit interval, and is stated specifically with respect to Lebesgue measure.
\begin{proposition}
	\label{prop: sobolev_donsker}
	Let $\F$ be the set of all absolutely continuous functions $f: [0,1] \to \Reals$ such that $\norm{f}_{\infty} \leq 1$ such that $\int (f'(x))^2 dx \leq 1$. Then, there exists a constant $K$ such that for every $\epsilon > 0$,
	\begin{equation*}
	\log N_{[]}(\epsilon, \mathcal{F}, \norm{\cdot}_{\infty}) \leq K\left(\frac{1}{\epsilon}\right).
	\end{equation*} 
	
	Thus, the class $\mathcal{F}$ is $P$-Donsker (and $P$-Glivenko-Cantelli) for all $P$.
\end{proposition}

\paragraph{Regularization functionals.}

When taking the supremum over functions which satisfy $\norm{B \theta}_2 \leq 1$, we will argue that this constraint is well-behaved in the limit, i.e. that it converges to the \textbf{regularization functional} $\norm{\cdot}_{W_0^{1,2}(\X, \mu^2)}$. Proposition \ref{prop: convprob_regularization_functional} makes this convergence uniform over the set of $3$-Holder functions (essentially functions with bounded $3rd$ derivative). Proposition \ref{prop: bousquet04} makes this convergence only pointwise, but merely requires that $f$ have bounded 2nd derivative. 

\begin{proposition}
	\label{prop: convprob_regularization_functional}
	Let $\mathcal{F}_{\alpha}$ be a unit ball in the space of $\alpha$-Holder functions, and define $k(\cdot,\cdot)$ as in Theorem \ref{thm:convprob_of_T2}. For function $f \in \F_{\alpha}$, denote $f$ evaluated on the data, $\mathbf{f} = (f(Z_1), \ldots, f(Z_N))$. Then, there exists a constant $c$ depending only on $k$ such that for $\alpha \geq 3$ and a sequence $(h_n) \to 0$ such that
	\begin{equation*}
	\sup_{f \in \mathcal{F}_{\alpha}} \abs{ \norm{B \mathbf{f}_2} - \norm{f}_{W_0^{1,2}(\X, \mu^2)} } \convprob 0
	\end{equation*}
\end{proposition}

\begin{proposition}[Bousquet 04]
	\label{prop: bousquet04}
	If $p$ and $q$ are Lipschitz continuous functions with bounded hessians, and $K$ is a continuous function on $R^+$ such that $x^{2 + d} K(x) \in L_2$, then 
	\begin{align}
	\label{eqn: bousquet04}
	& \underset{t \to 0}{\lim} \frac{d}{Ct^{d+2}} \int  K(\norm{\x - \y}/ t)(h_0(\x) - h_0(\y))^2 (p(\x) + q(\x))(p(\y) + q(\y)) d\x d \y \nonumber \\
	& = \int \norm{\nabla h_0(\x)}^2 (p(\x) + q(\x))^2 d\x
	\end{align}
\end{proposition}

\begin{lemma}[von Luxburg 12]
	\label{lem: vonluxburg12}
	Assume $P$ and $Q$ are absolutely continuous with respect to Lebesgue measure on Euclidean space $\Reals^d$, with density functions $p$ and $q$, respectively. Let $K(x,y) = \frac{1}{(2\pi \sigma^2)}^{d/2} \exp{-\frac{\norm{x - y}^2}{2 \sigma^2}}$. 
	
	Under some regularity assumptions on $p$ and $q$, if $n \to \infty$, $\sigma \to 0$ , and $n\sigma^{d+2}/ \log(n) \to \infty$, then
	\begin{align*}
	n R_{XY} \to \frac{2}{p(X) + q(X)} + \frac{2}{p(Y) + q(Y)} \textit{ almost surely }
	\end{align*}
	with equivalent statements holding for $X_1, X_2$ and $Y_1, Y_2$.  
\end{lemma}

\paragraph{Central limit theorem for quadratic forms.}

\begin{theorem}[Chatterjee 08]
	\label{thm: clt_quadratic_forms}
	Let $a = (a_1, \ldots, a_n)$ be i.i.d random variables with with $\Prob{X_i = 1} = \Prob{X_i = -1} = 1/2$. For some fixed real valued symmetric matrix $M = (M_{ij})_{1 \leq i, j \leq n}$, define
	\begin{equation*}
	W = a^T M a.
	\end{equation*}
	with $\mu$ denoting the law of $(W - EW) / \sqrt{\Var{W}}$.
	
	Then, letting $\mathcal{G}$ be the standard Gaussian measure 
	\begin{equation}
	\label{eqn: wass_distance_to_normal}
	\mathcal{W}(\mu, \mathcal{G} ) \leq \left(\frac{\tr(M^4)}{\tr(M^2)^2} \right)^{1/2} + \left( \frac{5 \, \underset{i}{\max} \, (M_{ii})^2}{\tr(M^2)} \right)^{1/2}.
	\end{equation}
\end{theorem}

\paragraph{Translating from Wasserstein to Kolmogorov distance.}

\begin{lemma}[Wasserstein to Kolmogorov distance]
	\label{lem: wass_to_ks_distance}
	For any probability measures $\mu$, $\nu$ with corresponding cdfs $F_{\mu}$ and $F_{\nu}$ and any $\epsilon' > 0$, there exists some $\epsilon > 0$ such that
	\begin{equation*}
	\mathcal{W}(\mu, \nu ) < \epsilon \implies \sup_{t} \abs{F_{\mu}(t) - F_{\nu}(t)} \leq \epsilon'.
	\end{equation*}
\end{lemma}

\section{Proofs}

\begin{proof}[Proof of Proposition \ref{prop: expectation_of_TG}]
	Throughout, we will use the fact that $a_i | Z_i \sim \text{Rademacher}(\frac{p(Z_i)}{p(Z_i) + q(Z_i)})$, which is easily seen by an application of Bayes rule.
	
	
	Begin by rewriting
	\begin{equation*}
	a^T L a = (\mathbf{h_0} + a - \mathbf{h_0})^T L (\mathbf{h_0} + a - \mathbf{h_0}) := (\mathbf{h_0} + \mathbf{\epsilon})^T L (\mathbf{h_0} + \mathbf{\epsilon}).
	\end{equation*}
	Expanding the quadratic form yields
	\begin{equation*}
	a^T L a = \mathbf{h_0}^T L \mathbf{h_0} + \mathbf{\epsilon}^T L \mathbf{\epsilon} + 2 \mathbf{h_0}^T L \mathbf{\epsilon}.
	\end{equation*}
	
	Going from back to front, we have that the first term has expectation $0$, because
	\begin{equation*}
	\Expect{L_{ij} h_0(Z_i) \epsilon_{j}} = \Expect{L_{ij} h_0(Z_i) \Expect{\epsilon_{j} | Z} } = \Expect{L_{ij} h_0(Z_i) 0} = 0.
	\end{equation*}
	
	For the middle term, only the diagonal terms have non-zero expectation.
	\begin{align*}
	\Expect{ \mathbf{\epsilon}^T L \mathbf{\epsilon} } & = \sum_{i,j = 1}^{N} \Expect{L_{ij} \Expect{\epsilon_{j} \epsilon_i | Z}} \\
	& \overset{(i)}{=} \sum_{1 \leq i < j \leq N} \Expect{L_{ij} \Expect{\epsilon_{j} | Z} \Expect{\epsilon_i | Z}} + \sum_{i = 1}^n \Expect{L_{ii}^2 \Expect{\epsilon_i^2 | Z}} \\
	& = \sum_{i = 1}^N \Expect{L_{ii}^2 \Expect{\epsilon_i^2 | Z}}.
	\end{align*}
	where $(i)$ follows from the conditional independence relation $a_i \Perp a_j | Z$. 
	
	Then 
	\begin{equation*}
	\Expect{\epsilon_i^2 | Z} = \Expect{(a(Z_i) - h_0(Z_i))^2 | Z} = \Var{a(Z_i) | Z_i} = \frac{4}{N^2} \left(\frac{4 p(Z_i) q(Z_i)}{(p(Z_i) + q(Z_i))^2}\right)
	\end{equation*}
	and plugging this in, we have
	\begin{align*}
	\Expect{ \mathbf{\epsilon}^T L \mathbf{\epsilon} } & = \frac{16}{N^2}\sum_{i = 1}^{N} \Expect{L_{ii} \left(\frac{p(Z_i) q(Z_i)}{(p(Z_i) + q(Z_i))^2}\right)} \\
	& = \frac{16}{N^2} \sum_{i = 1}^{N} \Expect{ \sum_{j \neq i}K(\norm{Z_i - Z_j}) \left(\frac{p(Z_i) q(Z_i)}{(p(Z_i) + q(Z_i))^2}\right)} \\
	& = \frac{4 N}{N - 1} \int \int K(\norm{\x - \y}) \left[p(\x) q(\x)\right] \frac{p(\y) + q(\y)}{p(\x) + q(\x)} d\x d\y \\
	\end{align*}
	
	Using the relation $\frac{(a + b)^2 - (a - b)^2}{4} = ab$ yields the 1st and 2nd integrals of (\ref{eqn: expectation_of_TG}). The 3rd integral is exactly $\Expect{h_0^T L h_0}$. 
	
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop: expectation_of_TG_with_kde}]
	Write $\mathbf{h} = \frac{\x - \y}{t}$. Via Taylor expansion, we can write
	\begin{align*}
	& \int K\left(\frac{\norm{\x - \y}}{t}\right) (p(\y) + q(\y) ) d\y \\
	& \overset{(i)}{=} \int K(\norm{\mathbf{h}}) (p(\x) + q(\x) + \mathcal{O}(t \norm{h})) t^d d \mathbf{h} \\
	& \overset{(ii)}{=} (p(\x) + q(\x)) + \mathcal{O}(t^{d + 1})
	\end{align*}
	where $(i)$ follows from the Lipschitz continuity of $p$ and $q$, and $(ii)$ follows from the integrability condition on $K$. 
	
	Applying this to the 2nd and 3rd integrals of (\ref{eqn: expectation_of_TG}) yields the two integrals of (\ref{eqn: expectation_of_TG_with_kde}). The 3rd integral is $\mathcal{O}(t^{d + 1})$ by Lemma \ref{prop: bousquet04}.
\end{proof}

\begin{proof}
	First, we rewrite $T_2$, using the fact that $a = \frac{2}{N} \left(\sum_{i \in \Lx} e_i - \sum_{i \in \Ly} e_i \right)$. 
	\begin{align*}
	a^T \Linv a & = \frac{4}{N^2} \left(\sum_{i, j \in \Lx }e_i \Linv e_j + \sum_{i, j \in \Ly }e_i \Linv e_j - 2\sum_{i \in \Lx, j \in \Ly} e_i \Linv e_j \right)\\
	\end{align*}
	Via this expression, we see that in the above summations
	\begin{itemize}
		\item For $i = j$, $e_i^T \Linv e_i$ appears exactly once.
		\item For $i \neq  j$ and $i, j \in \Lx$ or $i,j \in \Ly$, $e_i^T \Linv e_j$ appears exactly twice.
		\item For $i \in \Lx$, $j \in \Ly$, $- e_i^T \Linv e_j$ appears exactly twice.
	\end{itemize}

	Now, consider the expression
	\begin{equation*}
	 \sum_{u \in \Lx, v \in \Ly} R_{uv} - \sum_{u < v \in \Ly }R_{uv} - \sum_{u < v \in \Lx }R_{uv}.
	\end{equation*}
	Going from bottom to top, we have
	\begin{itemize}
		\item When $i \in \Lx$ and $j \in \Ly$, $R_{ij}$ will contribute $-2 e_i \Linv e_j$. No other $R_{uv}$ will contribute anything to this term.
		\item When $i < j \in \Lx$ or $i < j \in \Ly$, the term $R_{ij}$ in the 2nd or 3rd sum will appear exactly once and will contribute $2 e_i \Linv e_j$. No other $R_{uv}$ will contribute anything to this term.
		\item When $i = j \in \Lx$, $- R_{ik}$ will contribute $ - e_i \Linv e_i$ for each $k \neq i \in \Lx$, and will contribute $ e_i \Linv e_i$ for each $k \in \Ly$. The total contribution will be $ \left(\abs{\Ly} - \abs{\Lx} + 1\right) (e_i \Linv e_i) = e_i \Linv e_i$. The same reasoning holds for $i = j \in \Ly$. 
	\end{itemize}

	All contributions from all $R_{uv}$ can be put into one of the three proceeding categories. Therefore,
	\begin{equation*}
	a^T \Linv a = \frac{4}{N^2} \left(\sum_{u \in \Lx, v \in \Ly} R_{uv} - \sum_{u < v \in \Ly }R_{uv} - \sum_{u < v \in \Lx }R_{uv}\right)
	\end{equation*}
	
	(\ref{eqn: expectation_of_T2}) follows from taking expectation and noting that $X_i$ and $X_j$ are identically distributed for all $i$ and $j$. 
	
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm: asymptotic_null_distribution}]
	We will proceed by
	\begin{enumerate}
		\item Conditioning on the high-probability outcome that the Laplacian converges to a limiting object in the right sense.
		\item Showing that, under such convergence of the Laplacian, both terms in Theorem \ref{thm: clt_quadratic_forms} grow small with $n$.
		\item Converting from Wasserstein distance to Kolmogorov distance. 
	\end{enumerate}
	
	\paragraph{Step 1.}
	Fix $\epsilon > 0$. Throughout, let $P_Z$ denote the distribution of $Z$, and likewise $P_a$ denote the distribution of $a$.
	
	For  $V_n \sim \nu_n(\rho_n \Linv)$, and $V \sim \nu_{\infty}$ let
	\begin{equation*}
	A_n = \biggl \{z \in \mathbb{R}^n: \abs{ E V_n^p - E V^p } \leq \epsilon \text{ for } p = 1, 2, 4 \biggr \} \bigcup \set{ z \in \mathbb{R}^n: \underset{i \in [n]}{\max} \, \frac{1}{n} \left(\{ \rho_n \Linv\}^2 \right)_{ii} \leq \epsilon}.
	\end{equation*}
	
	It is not hard to see that our Conjectures \ref{conj: spectral_measure_conv} and \ref{conj: diagonal_entries} imply $A_n$ will eventually have high probability.
	\begin{align}
	\label{eqn: good_event_probability}
	\Prob{A_n} & \geq \Prob{\biggl \{z \in \mathbb{R}^n: \abs{ E V_n^p - E V^p } \leq \epsilon \biggr \}} + \Prob{\set{ z \in \mathbb{R}^n: \underset{i \in [n]}{\max} \, \frac{1}{n} \left(\{ \rho_n \Linv\}^2 \right)_{ii} \leq \epsilon}} \nonumber \\
	& \overset{(i)}{\geq} 1 - 2 \epsilon \text{ for all } n \geq N.
	\end{align}
	where $(i)$ follows from Conjecture \ref{conj: diagonal_entries} (for the second term), and Conjecture \ref{conj: spectral_measure_conv} (for the first term).
	
	
	Writing $W_n := W_n(z,a)$ to emphasize that it is a function of $z$ and $a$, we have by Tonelli's theorem that
	\begin{align}
	\label{eqn: conditional_ks_distance}
	\sup_{t} \abs{\Prob{W_n \leq t} - \Phi(t)} & \overset{(i)}{=} \sup_{t} \abs{ \int_{\Reals^N} \left( \int_{\{-1,1\}^N} 1(W_n(z,a) \leq t) dP_a\right) dP_z - \Phi(t) } \nonumber \\
	& = \sup_{t} \abs{ \int_{\Reals^N} \left( \int_{\{-1,1\}^N} 1(W_n(z,a) \leq t) dP_a\right) - \Phi(t)dP_z } \nonumber \\
	& \leq \int_{\Reals^N} \sup_{t} \abs{\left( \int_{\{-1,1\}^N} 1(W_n(z,a) \leq t)   dP_a\right) - \Phi(t) } dP_z \nonumber \\
	& \overset{(ii)}{\leq} \int_{A_n} \sup_{t} \abs{\left( \int_{\{-1,1\}^N} 1(W_n(z,a) \leq t) dP_a\right) - \Phi(t)} dP_z + 2 \epsilon
	\end{align}
	where $(i)$ follows from Tonelli's theorem and $(ii)$ from (\ref{eqn: good_event_probability}).
	
	\paragraph{Step 2.}
	
	Denote as
	\begin{equation*}
	F_{a|z}(z, t) := \left( \int_{\{-1,1\}^N} 1(W_n(z,a) \leq t) dP_a\right)
	\end{equation*}
	and note that for any $z$ this defines a measure over the Borel $\sigma$-algebra $\mathcal{B}(\Reals)$, which we will call $\mu_{a|Z}(z)$.
	
	We wish to upper bound $\mathcal{W}(\mu_{a|Z}(z), \mathcal{G})$. To do so, we will compute upper bounds for each present in (\ref{eqn: wass_distance_to_normal}). For the first term, we have
	\begin{align*}
	\frac{\tr(\{\Linv \}^4)}{\tr(\{ \Linv \}^2)^2} & = \frac{1}{n} \frac{\frac{1}{n} \tr(\rho_n^4 \{\Linv \}^4)}{ \frac{1}{n^2} \rho_n^4 \tr(\{ \Linv \}^2)^2} \\
	& \leq \frac{1}{n} \frac{\Expect{V^4} + \epsilon}{\Expect{V^2}^2 - \epsilon}.
	\end{align*}
	For the second term, we have
	\begin{align*}
	\frac{ \underset{i}{\max}(\{\Linv\}^2)_{ii} }{ \tr(\{\Linv\}^2) } & = \frac{ \frac{\rho_n^2}{n} (\{\Linv\}^2)_{ii}}{ \frac{\rho_n^2}{n} \tr(\{\Linv\}^2) } \\
	& \leq \frac{ \epsilon }{ \Expect{V^2} - \epsilon }.
	\end{align*}
	By Theorem \ref{thm: clt_quadratic_forms} we therefore have
	\begin{equation}
	\label{eqn: wasserstein_conditional_bound}
	\mathcal{W}(\mu_{a|Z}(z), \mathcal{G} ) \leq \frac{1}{n} \frac{\Expect{V^4} + \epsilon}{\Expect{V^2}^2 - \epsilon} + \left(\frac{ \epsilon }{ \Expect{V^2} - \epsilon } \right)^{1/2}.
	\end{equation}
	
	\paragraph{Step 3.}
	Note that the right hand side of (\ref{eqn: wasserstein_conditional_bound}) converges to 0 with $\epsilon$. Therefore, for any $\epsilon$ sufficiently small, by (\ref{eqn: wasserstein_conditional_bound}) and Lemma \ref{lem: wass_to_ks_distance} we have
	\begin{equation*}
	\norm{F_{Z | a} - \Phi}_{\infty} \leq \epsilon'.
	\end{equation*}
	Combined with (\ref{eqn: conditional_ks_distance}) we have
	\begin{equation*}
	\sup_{t} \abs{\Prob(W_n \leq t) - \Phi(t)} \leq 2 \epsilon + \epsilon'.
	\end{equation*}
	for all $n \geq n_0$.
	
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem: cond_expectation_of_T2}]
	\begin{align}
	\Expect{T^2 \vert Z} & = \Expect{a^T \Linv a \vert Z} \nonumber \\
	& \overset{(i)}{=} \sum_{i = 1}^{N} \sum_{j = 1}^{N} \Expect{a_i a_j} \Linv_{ij} \nonumber \\
	& =  \sum_{i = 1}^{N} \frac{1}{4N^2} \Linv_{ii} \nonumber \\
	& = \frac{\tr(\Linv)}{4N^2}.
	\end{align}
	where $(i)$ comes from the independence of $Z$ and $a$ under $H_0$.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem: cond_var_of_T2}]
	First, we re-arrange $T_2$. 
	\begin{align*}
	T_2 & = \sum_{i = 1}^{N} \sum_{j = 1}^{N} a_i a_j \Linv_{ij} \\
	& = 2 \sum_{i \leq j} a_i a_j \Linv_{ij} - \frac{4}{N^2}\sum_{i = 1}^{N} \Linv_{ii}.
	\end{align*}
	Therefore, for $R_i \overset{i.i.d}{\sim} \text{Rademacher}(1/2)$,
	\begin{align*}
	\Var{T_2 \vert Z} & = 4 \Var{\sum_{i \leq j} a_i a_j \Linv_{ij} | Z} \\
	& = \frac{64}{N^4} \Var{\sum_{i \leq j} R_i R_j \Linv_{ij} \vert Z } \\
	& = \frac{32}{N^4} \tr[(\Linv)^2].
	\end{align*}
\end{proof}

\end{document}