\documentclass[twoside]{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{fullpage}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

% Make a widecheck symbol (thanks, Stack Exchange!)
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
% widecheck made

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}
\newcommand{\Lap}{L}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Vset}{\mathcal{V}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\wc}[1]{\widecheck{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

%%% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

%%% So as not to overlap with theorem/lemma numbering from the main text...
\setcounter{theorem}{4}
\setcounter{lemma}{2}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

%%% Reference main text
\usepackage{xr-hyper}
\usepackage[colorlinks]{hyperref}
\externaldocument{minimax_optimal_laplacian_smoothing}

%%% New equation numbering
\renewcommand{\theequation}{A.\arabic{equation}}

\begin{document}
	
\title{Supplementary Materials for Minimax Optimal Regression over Sobolev Spaces via Laplacian Regularization on Neighborhood Graphs}
\date{}
\maketitle

In this supplement, we provide proofs of all results found in the paper ``Minimax Optimal Regression over Sobolev Spaces via Laplacian Regularization on Neighborhood Graphs''. Our main theorems (Theorems~\ref{thm:laplacian_smoothing_estimation1}-\ref{thm:laplacian_smoothing_testing_manifold}) all follow the same general proof strategy, centered on conditioning. In Section~\ref{sec:fixed_graph_error_bounds}, we establish (estimation or testing) error bounds which hold for any graph $G$; these bounds are stated with respect to (functionals of) the graph $G$, and allow us to upper bound the error of $\wh{f}$ and $\wh{\varphi}$ conditional on the design $\mathbf{X} = x$. In Sections~\ref{sec:graph_sobolev_seminorm}, \ref{sec:graph_eigenvalues}, and \ref{sec:empirical_norm}, we give all our necessary probabilistic estimates on these functionals, for the particular random neighborhood graph $G = G_{n,r}$. It is in these sections where we invoke our various assumptions on the distribution $P$ and regression function $f_0$. In Section~\ref{sec:main_results}, we prove our main theorems and some other results. In Section~\ref{sec:concentration}, we state a few concentration bounds that we use repeatedly in our proofs.

\section{Graph-dependent error bounds}
\label{sec:fixed_graph_error_bounds}
In this section, we adopt the fixed design perspective; or equivalently, condition on $X_i = x_i$ for $i = 1,\ldots,n$. Let $G = \bigl([n],W\bigr)$ be a fixed graph on $\{1,\ldots,n\}$ with Laplacian matrix $\Lap = D - W$. The randomness thus all comes from the responses 
\begin{equation}
\label{eqn:fixed_graph_regression_model}
Y_i = f_{0}(x_i) + \varepsilon_i
\end{equation}
where the noise variables $\varepsilon_i$ are independent $N(0,1)$. In the rest of this section, we will mildly abuse notation and write $f_0 = (f_0(x_1),\ldots,f_0(x_n)) \in \Reals^n$. We will also write ${\bf Y} = (Y_1,\ldots,Y_n)$.

Recall~\eqref{eqn:laplacian_smoothing} and~\eqref{eqn:laplacian_smoothing_test}: the Laplacian smoothing estimator of $f_0$ on $G$ is
\begin{equation*}
\label{eqn:ls_G}
\wh{f} := \argmin_{f \in \Reals^n} \biggl\{ \sum_{i = 1}^{n}(Y_i - f_i)^2 + \rho \cdot f^T \Lap f \biggr\} = (\rho \Lap + \Id)^{-1}{\bf Y}.
\end{equation*}
and the Laplacian smoothing test statistic is 
\begin{equation*}
\label{eqn:ls_ts_G}
\wh{T} := \frac{1}{n} \Bigl\|\wh{f}\Bigr\|_2^2.
\end{equation*}
We note that in this section, many of the derivations involved in upper bounding the estimation error of $\wh{f}$ are similar to those of \citep{sadhanala16}, with the difference being that we seek bounds in high probability rather than in expectation. We keep the work here self-contained for purposes of completeness.

\subsection{Error bounds for linear smoothers}
Let $S \in \Reals^{n \times n}$ be a square, symmetric matrix, and let 
\begin{equation*}
\wc{f} := SY
\end{equation*}
be a linear estimator of $f_0$. In  Lemma~\ref{lem:linear_smoother_fixed_graph_estimation} we upper bound the error $\frac{1}{n}\|\wc{f} - f_0\|_2^2$ as a function of the eigenvalues of $S$. Let $\lambdavec(S) = (\lambda_1(S),\ldots,\lambda_n(S)) \in \Reals^n$ denote these eigenvalues, and let $v_k(S)$ denote the corresponding unit-norm eigenvectors, so that $S = \sum_{k = 1}^{n} \lambda_k(S) \cdot v_k(S) v_k(S)^T$. Denote $Z_k = v_k(S)^T \varepsilon$, and observe that ${\bf Z} = (Z_1,\ldots,Z_n) \sim N(0,\Id)$. 

\begin{lemma}
	\label{lem:linear_smoother_fixed_graph_estimation}
	Let $\wc{f} = SY$ for a square, symmetric matrix, $S \in \Reals^{n \times n}$. Then
	\begin{equation*}
	\Pbb_{f_0}\biggl(\frac{1}{n}\bigl\|\wc{f} - f_0\bigr\|_2^2 \geq \frac{10}{n} \bigl\|\lambdavec(S)\bigr\|_2^2 + \frac{2}{n}\bigl\|(S - I)f_0\bigr\|_2^2\biggr) \leq 1 - \exp\Bigl(-\bigl\|\lambdavec(S)\bigr\|_2^2\Bigr)
	\end{equation*}
\end{lemma}
Here we have written $\Pbb_{f_0}(\cdot)$ for the probability law under the regression ``function'' $f_0 \in \Reals^n$. 

In Lemma~\ref{lem:linear_smoother_fixed_graph_testing}, we upper bound the error of a test involving the statistic $\|\wc{f}\|_2^2 = {\bf Y}^T S^2 {\bf Y}$.
\begin{lemma}
	\label{lem:linear_smoother_fixed_graph_testing}
	Let $\wc{T} = Y^T S^2 Y$ for a square, symmetric matrix $S \in \Reals^{n \times n}$. Define the threshold $\wc{t}_b$ to be 
	\begin{equation}
	\wc{t}_{b} := \norm{\lambdavec(S)}_2^2 + 2b \norm{\lambdavec(S)}_4^2
	\end{equation}
	It holds that:
	\begin{itemize}
		\item \textbf{Type I error.}
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_typeI}
		\Pbb_0\bigl(\wc{T} > \wc{t}_b\bigr) \leq \frac{1}{b^2}
		\end{equation}
		\item \textbf{Type II error.} Under the further assumption
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_critical_radius}
		f_0^T S^2 f_0 \geq 4b \norm{\lambdavec(S)}_4^2,
		\end{equation}
		then
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_typeII}
		\Pbb_{f_0}\bigl(\wc{T} \leq \wc{t}_b\bigr) \leq \frac{4}{b^2} + \frac{8}{b \norm{\lambdavec(S)}_4^{2}}.
		\end{equation}
	\end{itemize}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:linear_smoother_fixed_graph_estimation}.}
The expectation $\Ebb_{f_0}[\wc{f}] = Sf_0$, and by the triangle inequality,
\begin{align*}
\frac{1}{n}\bigl\|\wc{f} - f_0\bigr\|_2^2 & \leq \frac{2}{n}\Bigl(\bigl\|\wc{f} - \Ebb_{f_0}[\wc{f}]\bigr\|_2^2 + \bigl\|\Ebb_{f_0}[\wc{f}] - f_0\bigr\|_2^2\Bigr) \\ 
& = \frac{2}{n}\Bigl(\bigl\|S\varepsilon\bigr\|_2^2 + \bigl\|(S - I)f_0\bigr\|_2^2\Bigr)
\end{align*}
Writing $\norm{S\varepsilon}_2^2 = \sum_{k = 1}^{n} \lambda_k(S)^2 Z_k^2$, the claim follows from the result of \cite{laurent00} on concentration of $\chi^2$-random variables, which for completeness we restate in Lemma~\ref{lem:chi_square_bound}. To be explicit, taking $t = \norm{\lambdavec(S)}_2^2$ in Lemma~\ref{lem:chi_square_bound} completes the proof of Lemma~\ref{lem:linear_smoother_fixed_graph_estimation}. 

\paragraph{Proof of Lemma~\ref{lem:linear_smoother_fixed_graph_testing}.}
We compute the mean and variance of $T$ as a function of $f_0$, then apply Chebyshev's inequality.

\textit{Mean.} We make use of the eigendecomposition $S = \sum_{k = 1}^{n} \lambda_k(S) \cdot v_k(S) v_k(S)^T$ to obtain
\begin{equation}
\label{pf:linear_smoother_fixed_graph_testing1}
\begin{aligned}
\wc{T} & = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \varepsilon^T S^2 \varepsilon \\
& = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \sum_{k = 1}^{n}  \bigl(\lambda_k(S)\bigr)^2 (\varepsilon^T v_k(S))^2 \\
& = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \sum_{k = 1}^{n}  \bigl(\lambda_k(S)\bigr)^2 Z_k^2,
\end{aligned}
\end{equation}
implying
\begin{equation}
\label{pf:linear_smoother_fixed_graph_testing_mean}
\Ebb_{f_0}\bigl[\wc{T}\bigr] = f_0^T S^2 f_0 + \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^2.
\end{equation}

\textit{Variance.} Starting from~\eqref{pf:linear_smoother_fixed_graph_testing1} and recalling $\Var(Z_k^2) = 2$, we derive
\begin{equation}
\label{pf:linear_smoother_fixed_graph_testing_var}
\Var_{f_0}\bigl[\wc{T}\bigr] \leq 8 f_0^T S^4 f_0 + 4 \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^4.
\end{equation}

\textit{Bounding Type I and Type II error.} The upper bound~\eqref{eqn:linear_smoother_fixed_graph_testing_typeI} follows immediately from~\eqref{pf:linear_smoother_fixed_graph_testing_mean}, ~\eqref{pf:linear_smoother_fixed_graph_testing_var}, and Chebyshev's inequality.

The bound~\eqref{eqn:linear_smoother_fixed_graph_testing_typeII} also follows from Chebyshev's inequality, as can be seen by the following manipulations,
\begin{equation*}
\begin{aligned}
\Pbb_{f_0}\Bigl(\wc{T} \leq \wc{t}_b\Bigr) & = \Pbb_{f_0}\Bigl(\wc{T} - \Ebb_{f_0}\bigl[\wc{T}\bigr] \leq \wc{t}_b - \Ebb_{f_0}\bigl[\wc{T}\bigr]\Bigr) \\
& \overset{(i)}{\leq} \Pbb_{f_0}\Bigl(\Bigl|\wc{T} - \Ebb_{f_0}\bigl[\wc{T}\bigr]\Bigr| \geq \Bigl|\wc{t}_b - \Ebb_{f_0}\bigl[\wc{T}\bigr]\Bigr|\Bigr) \leq \frac{Var_{f_0}\bigl[\wc{T}\bigr]}{\bigl(\wc{t}_b - \Ebb_{f_0}\bigl[\wc{T}\bigr]\bigr)^2} \\ 
& \overset{(ii)}{\leq} 4 \frac{\Var_{f_0}\bigl[\wc{T}\bigr]}{(f_0^T S^2 f_0)^2} \\
& \overset{(iii)}{\leq} \frac{32}{f_0^T S^2 f_0} + \frac{4}{b^2} \\
& \overset{(iv)}{\leq} \frac{8}{b} \Biggl(\sum_{k = 1}^{n}\bigl(\lambda_k(S)\bigr)^4\Biggr)^{-1/2} + \frac{4}{b^2}
\end{aligned}
\end{equation*}
In the previous expression, $(i)$ and $(ii)$ follow since assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} and equation~\eqref{pf:linear_smoother_fixed_graph_testing_mean} together imply 
\begin{equation*}
\wc{t}_b - \Ebb_{f_0}[\wc{T}] = f_0^T S^2 f_0 - 2b\|{\bf \lambda}(S)\|_4^2 \geq \frac{1}{2}f_0^T S^2 f_0
\end{equation*}
$(iii)$ follows from assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} and the inequality~\eqref{pf:linear_smoother_fixed_graph_testing_var}, and $(iv)$ follows from~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius}. 

\subsection{Analysis of Laplacian Smoothing}
Upper bounds on the mean squared error of $\wh{f}$, and Type I and Type II error of $\wh{T}$, follow from setting $S = (\rho L + I)^{-1}$ in Lemmas~\ref{lem:linear_smoother_fixed_graph_estimation} and~\ref{lem:linear_smoother_fixed_graph_testing}. We give these results in Lemma~\ref{lem:ls_fixed_graph_estimation} and~\ref{lem:ls_fixed_graph_testing}, and prove them immediately. Recall that $\lambda_1,\ldots,\lambda_n$ are the $n$ eigenvalues of $\Lap$ (sorted in ascending order).
\begin{lemma}
	\label{lem:ls_fixed_graph_estimation}
	For any $\rho > 0$,
	\begin{equation}
	\label{eqn:ls_fixed_graph_estimation_prob}
	\frac{1}{n}\bigl\|\wh{f} - f_0\bigr\|_2^2 \leq \frac{2\rho}{n} \bigl(f_0^T \Lap f_0\bigr) + \frac{10}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k} + 1\bigr)^2}
	\end{equation}
	with probability at least $1 - \exp\Bigl(-\sum_{k = 1}^{n}\bigl(\rho \lambda_k + 1\bigr)^{-2}\Bigr)$.
\end{lemma}
Recall that 
\begin{equation*}
\wh{t}_b := \frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k + 1\bigr)^2} + \frac{2b}{n}\sqrt{\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k + 1\bigr)^4}}.
\end{equation*}
\begin{lemma}
	\label{lem:ls_fixed_graph_testing}
	For any $\rho > 0$ and any $b \geq 1$, it holds that:
	\begin{itemize}
		\item \textbf{Type I error.}
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_typeI}
		\Pbb_0\Bigl(\wh{T} > \wh{t}_b\Bigr) \leq \frac{1}{b^2}
		\end{equation}
		\item \textbf{Type II error.} If
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_critical_radius}
		\frac{1}{n}\norm{f_0}_2^2 \geq \frac{2 \rho}{n} \bigl(f_0^T \Lap f_0\bigr) + \frac{4b}{n} \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k + 1)^4} \Biggr)^{1/2}
		\end{equation}
		then
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_typeII}
		\Pbb_{f_0}\Bigl(\wh{T}(G) \leq \wh{t}_b\Bigr) \leq \frac{4}{b^2} + \frac{8}{b} \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k + 1)^4} \Biggr)^{-1/2}
		\end{equation}
	\end{itemize}
\end{lemma}
\paragraph{Proof of Lemma~\ref{lem:ls_fixed_graph_estimation}.}
Letting $\wh{S} = (\Id + \rho \Lap)^{-1}$, the estimator $\wh{f} = \wh{S}Y$, and
\begin{equation*}
\bigl\|\lambdavec(\wh{S})\bigr\|_2^2 = \sum_{k = 1}^{n} \frac{1}{\bigl(1 + \rho \lambda_k\bigr)^2}.
\end{equation*} 
We deduce the following upper bound on the bias term,
\begin{equation*}
\begin{aligned}
\bigl\|(\wh{S} - I)f_0\bigr\|_2^2 & = f_0^T \Lap^{1/2} \Lap^{-1/2}\bigl(\wh{S} - \Id\bigr) \Lap^{-1/2} \Lap^{1/2} f_0 \\
& \leq f_0^T \Lap f_0 \cdot \lambda_n\Bigl(\Lap^{-1/2}\bigl(\wh{S} - \Id\bigr)\Lap^{-1/2}\Bigr) \\
& = f_0^T \Lap f_0 \cdot \max_{k \in [n]} \biggl\{\frac{1}{\lambda_k} \Bigl(1 - \frac{1}{\rho\lambda_k + 1}\Bigr) \biggr\} \\
& = f_0^T \Lap f_0 \cdot \rho.
\end{aligned}
\end{equation*} 
In the above, we have written $\Lap^{-1/2}$ for the square root of the pseudoinverse of $\Lap$, and the maximum is over all indices $k$ such that $\lambda_k > 0$.

\paragraph{Proof of Lemma~\ref{lem:ls_fixed_graph_testing}.}
Let $\wh{S} := (\rho \Lap + \Id)^{-1}$, and note that $\wh{T} = \frac{1}{n}{\bf Y}^T \wh{S}^2 {\bf Y}$. The bound on Type I error~\eqref{eqn:ls_fixed_graph_testing_typeI} follows immediately from \eqref{eqn:linear_smoother_fixed_graph_testing_typeI}. 
To establish the bound on Type II error, we must lower bound $f_0^T \wh{S}^2 f_0$. We first note that by assumption~\eqref{eqn:ls_fixed_graph_testing_critical_radius},
\begin{align*}
f_0^{T} \wh{S}^2 f_0 & = \bigl\|f_0\bigr\|_2^2 - f_0^T(I - \wh{S}^2)f_0 \\
& \geq 2 \rho \bigl(f_0^T \Lap f_0\bigr) - f_0^T\bigl(I - \wh{S}^2\bigr)f_0 + 4b \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k + 1)^4} \Biggr)^{-1/2}.
\end{align*}
Upper bounding $f_0^T\bigl(I - \wh{S}^2\bigr)f_0$ as follows:
\begin{equation*}
\begin{aligned}
f_0^T \Bigl(\Id - \wh{S}^2\Bigr) f_0  & = f_0^T \Lap^{1/2} \Lap^{-1/2}\Bigl(\Id - \wh{S}^2\Bigr) \Lap^{-1/2} \Lap^{1/2} f_0 \\ 
& \leq f_0^T \Lap f_0 \cdot  \lambda_{n}\biggl(\Lap^{-1/2}\Bigl(\Id - \wh{S}^2\Bigr) \Lap^{-1/2}\biggr) \\ 
& \overset{(i)}{=}  f_0^T \Lap f_0 \cdot \max_{k} \biggl\{ \frac{1}{\lambda_k} \Bigl(1 - \frac{1}{(\rho \lambda_k + 1)^2}\Bigr) \biggr\} \\
& \overset{(ii)}{\leq} f_0^T \Lap f_0 \cdot 2\rho,
\end{aligned}
\end{equation*}
---where in the above the maximum is over all indices $k$ such that $\lambda_k > 0$, and the last inequality follows from the basic algebraic identity $1 - 1/(1 + s x)^2 \leq 2 s x$ for any $x, s > 0$---we deduce that
\begin{equation*}
f_0^{T} \wh{S}^2 f_0 \geq 2b \biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k + 1)^4} \biggr)^{-1/2}.
\end{equation*} 
The upper bound on Type II error~\eqref{eqn:ls_fixed_graph_testing_typeII} then follows from Lemma~\ref{lem:linear_smoother_fixed_graph_testing}.

\section{Neighborhood graph Sobolev semi-norm}
\label{sec:graph_sobolev_seminorm}
In this section, we prove Lemma~\ref{lem:graph_sobolev_seminorm}, which states an upper bound on $f^T \Lap_{n,r} f$ holds when $f$ is bounded in Sobolev norm. We also establish stronger bounds in the case when $f$ has a bounded Lipschitz constant; this latter result justifies the remark one of our remarks after Theorem~\ref{thm:laplacian_smoothing_estimation1}. 

To prove Lemma~\ref{lem:graph_sobolev_seminorm}, we will show that for any $f \in H^1(\Xset)$,
\begin{equation*}
\Ebb\Bigl[f^T \Lap_{n,r} f\Bigr] \leq p_{\max}^2 \sigma_K n^2 r^{d + 2} |f|_{H^1(\Xset)}^2
\end{equation*}
whence the claim follows immediately by Markov's inequality (recall that $\Lap_{n,r}$ is positive semi-definite, and therefore $f^T \Lap_{n,r} f$ is a non-negative random variable).

Since
\begin{equation*}
f^T \Lap_{n,r} f = \frac{1}{2}\sum_{i, j = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr)^2 \mathbf{W}_{ij},
\end{equation*}
it follows that
\begin{equation}
\label{pf:first_order_graph_sobolev_seminorm_1}
\Ebb\Bigl[f^T \Lap_{n,r} f\Bigr] = \frac{n(n - 1)}{2} \Ebb\biggl[\Bigl(f(X') - f(X)\Bigr)^2 K\biggl(\frac{\|X' - X\|}{r}\biggr)\biggr]
\end{equation}
where $X$ and $X'$ are random variables independently drawn from $P$. 

For the remainder of this proof, we will assume that $f \in C^{\infty}(\Xset)$. We may do so without loss of generality: $C^{\infty}(\Xset)$ is dense in $H^1(\Xset)$ and the expectation on the right hand side of~\eqref{pf:first_order_graph_sobolev_seminorm_1} is continuous in $H^1(\Xset)$. Obviously
\begin{equation}
\Ebb\biggl[\bigl(f(X') - f(X)\bigr)^2K\biggl(\frac{\|X' - X\|}{r}\biggr)\biggr] \leq p_{\max}^2 \int_{\Xset} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 K_r(x',x) \,dx' \,dx. \label{pf:first_order_graph_sobolev_seminorm_2}
\end{equation}
and it remains now to bound the integral. Taking a first-order Taylor expansion of $f$, we get
\begin{align}
\int_{\Xset} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 K_r(x',x) \,dx' \,dx & = \int_{\Xset} \int_{\Xset} \biggl[\int_{0}^{1} \nabla f\bigl(x + t(x' - x)\bigr)^T (x' - x)\,dt\biggr]^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) \,dx' \,dx \nonumber \\
& \overset{(i)}{\leq} \int_{\Xset} \int_{\Xset} \int_{0}^{1} \biggl[\nabla f\bigl(x + t(x' - x)\bigr)^T (x' - x)\biggr]^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) \,dt\,dx' \,dx \nonumber \\
& \overset{(ii)}{=} r^{d + 2} \int_{\Xset} \int_{\mc{Z}_r(x)} \int_{0}^{1} \biggl[\nabla f\bigl(x + trz\bigr)^T z\biggr]^2 K\bigl(\|z\|\bigr) \,dt\,dz \,dx \\
&  \overset{(iii)}{\leq} r^{d + 2} \int_{\Xset} \int_{\Reals^d} \int_{0}^{1} \Bigl[\nabla f\bigl(\wt{x}\bigr)^T z\Bigr]^2 K\bigl(\|z\|\bigr) \,dt \,dz \,d\wt{x} \nonumber \\
& = r^{d + 2} \int_{\Xset} \int_{\Reals^d} \Bigl[\nabla f\bigl(\wt{x}\bigr)^T z\Bigr]^2 K\bigl(\|z\|\bigr) \,dz \,d\wt{x} \label{pf:first_order_graph_sobolev_seminorm_3}
\end{align}
where $(i)$ follows by Jensen's inequality, $(ii)$ follows by substituting $z = (x' - x)/r$ with $\mc{Z}_r(x) := \{z: zr + x \in \Xset\}$ and $(iii)$ by substituting $\wt{x} = x + trz$ and noting that
\begin{equation*}
x \in \Xset ~~\textrm{and}~~ z \in \mc{Z}_r(x) \Longrightarrow x + trz \in \Xset ~~\textrm{and}~~ z \in \Reals^d.
\end{equation*}
Now, writing $\bigl(\nabla f(\wt{x}) ^T z\bigr)^2 = \bigl(\sum_{i = 1}^{d} z_{i} f^{(e_i)}(x) \bigr)^2$, expanding the square and integrating, we have that for any $\wt{x} \in \Xset$,
\begin{align*}
\int_{\Reals^d} \Bigl[\nabla f\bigl(\wt{x}\bigr)^T z\Bigr]^2 K\bigl(\|z\|\bigr) \,dz & = \sum_{i,j = 1}^{d} f^{(e_i)}(\wt{x}) f^{(e_j)}(\wt{x}) \int_{\Rd} z_iz_jK(\|z\|) \,dz \\
& = \sum_{i = 1}^{d} \bigl(f^{(e_i)}(\wt{x})\bigr)^2 \int_{\Rd} z_i^2 K\bigl(\|z\|\bigr) \,dz \\
& = \sigma_K \|\nabla f(\wt{x})\|^2
\end{align*}
and by plugging back into~\eqref{pf:first_order_graph_sobolev_seminorm_3}, we obtain the claimed result. 

\subsection{Stronger bounds under Lipschitz assumption}
Suppose $f$ satisfies $|f(x') - f(x)| \leq M \|x - x\|$ for all $x,x' \in \mc{X}$. Then we can strengthen the high probability bound in Lemma~\ref{lem:graph_sobolev_seminorm} from $1 - \delta$ to $1 - \delta^2/n$, at the cost of only a constant factor in the upper bound on $f^T \Lap_{n,r} f$.
\begin{proposition}
	\label{prop:graph_sobolev_seminorm_lipschitz}
	Let $r \geq C_0(\log n/n)^{1/d}$. For any $f$ such that $|f(x') - f(x)| \leq M \|x - x\|$ for all $x,x' \in \mc{X}$, with probability at least $1 - C\delta^2/n$ it holds that
	\begin{equation*}
	f^T \Lap_{n,r} f \leq \biggl(\frac{1}{\delta} + (1 + L_p)^2 \sigma_K\biggr) n^2 r^{d + 2} M^2
	\end{equation*}
\end{proposition}
\paragraph{Proof of Proposition~\ref{prop:graph_sobolev_seminorm_lipschitz}.}
We will prove Proposition~\ref{prop:graph_sobolev_seminorm_lipschitz} using Chebyshev's inequality, so the key step is to upper bound the variance of $f^{T}\Lap_{n,r}f$. Putting $\varDelta_{ij} := K(\|X_i - X_j\|/r) \cdot (f(X_i) - f(X_j))^2$, we can write the variance of $f^T \Lap_{n,r} f$ as a sum of covariances,
\begin{equation*}
\Var\bigl[f^T \Lap_{n,r} f\bigr] = \frac{1}{4} \sum_{i, j = 1}^{n} \sum_{\ell,m = 1}^{n} \Cov\bigl[\varDelta_{ij},\varDelta_{\ell m}\bigr].
\end{equation*}
Clearly $\Cov\bigl[\varDelta_{ij},\varDelta_{\ell m}\bigr]$ depends on the cardinality of $I := \{i,j,k,\ell\}$; we divide into cases, and upper bound the covariance in each case.
\begin{enumerate}
	\item[$\bigl|I\bigr| = 4$.] In this case $\varDelta_{ij}$ and $\varDelta_{\ell m}$ are independent, and $\Cov\bigl[\varDelta_{ij},\varDelta_{\ell m}\bigr] = 0$.
	\item[$\bigl|I\bigr| = 3$.] Taking $i = \ell$ without loss of generality, we have
	\begin{align*}
	\Cov\bigl[\varDelta_{ij},\varDelta_{i m}\bigr] & \leq \Ebb\bigl[\varDelta_{ij} \varDelta_{i m} \bigr] \\
	& = \int_{\Xset} \int_{\Xset} \int_{\Xset} \bigl(f(z) - f(x)\bigr)^2 \bigl(f(x') - f(x)\bigr)^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) K\biggl(\frac{\|z - x\|}{r}\biggr) p(z) p(x') p(x) \,dz \,dx' \,dx \\
	& \leq p_{\max}^3 M^4 r^4 \int_{\Xset} \int_{\Xset} \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr) K\biggl(\frac{\|z - x\|}{r}\biggr) \,dz \,dx' \,dx \\
	& \leq p_{\max}^3 M^4 r^{4 + 2d}.
	\end{align*}
	\item[$\bigl|I\bigr| = 2$.] Taking $i = \ell$ and $j = m$ without loss of generality, we have
	\begin{align*}
	\Var\bigl[\varDelta_{ij}\bigr] & \leq \Ebb\bigl[\varDelta_{ij}^2\bigr] \\
	& \leq \int_{\Xset} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^4 \biggl[K\biggl(\frac{\|x' - x\|}{r}\biggr)\biggr]^2 p(x') p(x) \,dx' \,dx \\
	& \leq p_{\max}^2 M^4 r^4 K(0) \int_{\Xset} \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr) \,dx' \,dx \\
	& \leq p_{\max}^2 M^4 r^{4 + d} K(0)
	\end{align*}
	\item[$\bigl|I\bigr| = 1$.] In this case $\varDelta_{i j} = \varDelta_{\ell m} = 0$.
\end{enumerate}
Therefore
\begin{equation*}
\Var\bigl[f^T \Lap_{n,r} f\bigr] \leq n^3 p_{\max}^3 M^4 r^{4 + 2d} + n^2 p_{\max}^2 M^4 r^{4 + d} K(0) \leq Cn^3r^{4 + 2d},
\end{equation*}
where the latter inequality follows since $nr^d \gg 1$. For any $\delta > 0$, it follows from Chebyshev's inequality that
\begin{equation*}
\Pbb\biggl(\Bigl|f^T \Lap_{n,r} f - \Ebb\bigl[f^T \Lap_{n,r} f\bigr]\Bigr| \geq \frac{M^2}{\delta}n^2 r^{d + 2}\biggr) \leq C\frac{\delta^2}{n}
\end{equation*}
and since we have already upper bounded $\Ebb\bigl[f^T \Lap_{n,r} f\bigr] \leq (1 + L_p)^2 \sigma_K M^2 n^2 r^{d + 2}$, the proposition follows. 

Note that the bound on $\Var[\varDelta_{i j}]$ follows as long as we can control $\|\nabla f\|_{\Leb^4(\Xset)}$; this implies the Lipschitz assumption---which gives us control of $\|\nabla f\|_{\Leb^{\infty}(\Xset)}$---can be weakened. However, the Sobolev assumption---which gives us control only over $\|\nabla f\|_{\Leb^2(\Xset)}$---will not do the job. 
\section{Bounds on neighborhood graph eigenvalues}
\label{sec:graph_eigenvalues}
In this section, we prove Lemma~\ref{lem:neighborhood_eigenvalue}, following the lead of~\citep{burago2014,trillos2019,calder2019}, who establish similar results with respect to a manifold without boundary. The bulk of our technical effort will stem from proving Theorem~\ref{thm:neighborhood_eigenvalue}, which gives estimates on eigenvalues of the graph Laplacian in terms of eigenvalues of the weighted Laplace-Beltrami operator $\Delta_P$. We recall $\Delta_P$ is defined as
\begin{equation*}
\Delta_Pf(x) := -\frac{1}{p} \dive\bigl(p^2\nabla f(x)\bigr)
\end{equation*}
To avoid confusion, in this section we write $\lambda_k(G_{n,r})$ for the $k$th smallest eigenvalue of the graph Laplacian matrix $\Lap_{n,r}$ and $\lambda_k(\Delta_P)$ for the $k$th smallest eigenvalue of $\Delta_P$ \footnote{Under the assumptions}. Some other notation: throughout this section, we will write $A, A_1,\ldots$ and $a,a_1,\ldots$ for constants which do not depend on $n$, and we let $\theta$ and $\wt{\delta}$ denote (small) positive numbers, which always satisfy the following restrictions.
\begin{enumerate}[label=(S\arabic*)]
	\item 
	\begin{equation*}
	\theta + \wt{\delta} \leq \frac{1}{4A_1}~~\textrm{and}~~C_0\biggl(\frac{\log n}{n}\biggr)^{1/d} \leq \wt{\delta} \leq r
	\end{equation*}
\end{enumerate}


\begin{theorem}
	\label{thm:neighborhood_eigenvalue}
	For any $\ell \in \mathbb{N}$ and $\theta$ and $\wt{\delta}$ such that
	\begin{equation}
	\label{eqn:neighborhood_eigenvalue_1}
	1 - A\Biggl(r \sqrt{\lambda_{\ell}(\Delta_P)} + \theta + \wt{\delta}\Biggr)\geq \frac{1}{2}
	\end{equation}
	with probability at least $1 - A_1n\exp(-a_1n\theta^2\wt{\delta}^{m})$, it holds that
	\begin{equation}
	\label{eqn:eigenvalue_bound}
	a \lambda_k(G_{n,r}) \leq nr^{d+2} \lambda_k(\Delta_P) \leq A \lambda_k(G_{n,r}),~~\textrm{for all $1 \leq k \leq \ell$}
	\end{equation}
\end{theorem}
We will return to proving Theorem~\ref{thm:neighborhood_eigenvalue}, but first we show that under our regularity conditions on $p$ and $\Xset$, it implies Lemma~\ref{lem:neighborhood_eigenvalue}. The link between the two is Weyl's Law.
\begin{proposition}[Weyl's Law]
	\label{prop:weyl}
	Suppose the density $p$ and the domain $\Xset$ satisfy~\ref{asmp:domain} and~\ref{asmp:bounded_lipschitz_density}. Then there exist constants $a_2$ and $A_2$ such that
	\begin{equation}
	\label{eqn:weyls_law}
	a_2k^{2/d} \leq \lambda_k(\Delta_P) \leq A_2k^{2/d}~~\textrm{for all $k \in \mathbb{N}, k > 1$}.
	\end{equation}
\end{proposition}
See Lemma 28 of~\citep{dunlop2020} for a proof that~\ref{asmp:domain} and~\ref{asmp:bounded_lipschitz_density} imply Weyl's Law.
\paragraph{Proof of Lemma~\ref{lem:neighborhood_eigenvalue}.}
Put
\begin{equation*}
\ell_{\star} = \floor{\biggl(\frac{\bigl(1/(2A_1) - (\theta + \wt{\delta})\bigr)}{rA_2^{1/2}}\biggr)^d},
\end{equation*}
and note that the condition $r < c_0$ guarantees $\ell_{\star} \geq 2$ when $c_0$ is chosen to be a sufficiently small constant. By Proposition~\ref{prop:weyl}, we have that
\begin{align*}
\sqrt{\lambda_{\ell_{\star}}(\Delta)} \leq \frac{1}{r}\biggl(\frac{1}{2A_1} - (\theta + \wt{\delta})\biggr) 
\end{align*}
so that condition~\eqref{eqn:neighborhood_eigenvalue_1} is satisfied. We will therefore assume the conclusions of Theorem~\ref{thm:neighborhood_eigenvalue}. Along with~\eqref{eqn:weyls_law}, this implies the following bounds on the graph Laplacian eigenvalues:
\begin{equation*}
\frac{a_2}{A_1} nr^{d + 2} k^{2/d} \leq \lambda_k(G_{n,r}) \leq \frac{A_2}{a_1} nr^{d + 2} k^{2/d}~~\textrm{for all $2 \leq k \leq \ell_{\star}$}.
\end{equation*}
It remains to bound $\lambda_k(G_{n,r})$ for those indices $k$ which are greater than $\ell_{\star}$. On the one hand, since the eigenvalues are sorted in ascending order, we can use the lower bound on $\lambda_{\ell_{\star}}(G_{n,r})$ that we have just derived:
\begin{align*}
\lambda_k(G_{n,r}) & \geq \lambda_{\ell_{\star}}(G_{n,r}) \\
& \geq \frac{a_2}{A_1}nr^{d + 2}\ell_{\star}^{2/d} \\
& = \frac{a_2}{A_1}nr^{d + 2} \floor{\biggl(\frac{\bigl(1/(2A_1) - (\theta + \wt{\delta})\bigr)}{rA_2^{1/2}}\biggr)^d}^2 \\
& \geq \frac{a_2}{32A_1^3 A_2} nr^{d}
\end{align*}
with the last line following since $\theta + \wt{\delta} \leq \frac{1}{4A_1}$. On the other hand, for any graph $G$ the maximum eigenvalue of the Laplacian is upper bounded by twice the maximum degree \citep{chung97}. Writing $D_{\max}(G_{n,r})$ for the maximum degree of $G_{n,r}$, we show in Lemma~\ref{lem:max_degree} that
\begin{equation*}
\lambda_k(G_{n,r}) \leq 2D_{\max}(G_{n,r}) \leq 4p_{\max}nr^d 
\end{equation*}
with probability at least $1 - 2n\exp\Bigl(-nr^dp_{\min}/(3K(0)^2)\Bigr)$. Thus we have established each of the inequalities in Lemma~\ref{lem:neighborhood_eigenvalue}.

\subsection{Proof of Theorem~\ref{thm:neighborhood_eigenvalue}}

In this section we prove Theorem~\ref{thm:neighborhood_eigenvalue}, following closely the approach of \citep{burago2014,trillos2019,calder2019}. As in these works, we relate $\lambda_k(\Delta_P)$ and $\lambda_k(G_{n,r})$ by means of the Dirichlet energies
\begin{equation*}
b_r(u) := \frac{1}{n^2 r^{d+ 2}}u^T \Lap_{n,r} u 
\end{equation*}
and
\begin{equation*}
D_2(f) :=
\begin{cases*}
\int_{\Xset} \|\nabla f(x)\|^2 p^2(x) \,dx~~ &\textrm{if $f \in H^1(\Xset)$} \\
\infty~~ & \textrm{otherwise,}
\end{cases*}
\end{equation*}
We recall that for a function $u: \{X_1,\ldots,X_n\} \to \Reals$, the empirical norm is defined as $\|u\|_n^2 := \frac{1}{n} \sum_{i = 1}^{n} (u(X_i))^2$, and we let the class $\Leb^2(P_n)$ consist of those $u \in \Reals^n$ for which $\|u\|_{n} < \infty$. For a function $f: \Xset \to \Reals$, the continuum limit of $\|f\|_n^2$ is simply
\begin{equation*}
\|f\|_{P}^2 := \int_{\Xset} \bigl|f(x)\bigr|^2 p(x) \,dx,
\end{equation*} 
and we let $\Leb^2(P)$ be the set of functions for which $\|f\|_P < \infty$.

Let us pause briefly to motivate the relevance of $b_r(u)$ and $D_2(f)$. Suppose one could show the following two results: 
\begin{enumerate}[(1)]
	\item an upper bound of $b_r(u)$ by $D_2\bigl(\mc{I}(u)\bigr)$ for an appropriate choice of interpolating map $\mc{I}: \Leb^2(P_n) \to \Leb^2(\mc{X})$, and vice versa upper bound $D_2(f)$ by $b_r(\mc{P}(f))$ for an appropriate choice of discretization map $\mc{P}: \Leb^2(\mc{X}) \to \Leb^2(P_n)$,
	\item that $\mc{I}$ and $\mc{P}$ were near-isometries, meaning $\|\mc{I}(u)\|_{P} \approx \|u\|_{n}$ and $\|\mc{P}(f)\|_{P} \approx \|f\|_{n}$.
\end{enumerate}
Then, by using the variational characterization of eigenvalues $\lambda_k(\Delta_P)$ and $\lambda_k(G_{n,r})$---i.e. the Courant-Fischer Theorem---one could obtain estimates on the error $\bigl|nr^{d + 2}\lambda_k(\Delta_P) - \lambda_k(G_{n,r})\bigr|$.

We will momentarily define maps $\wt{\mc{I}}$ and $\wt{\mc{P}}$ that satisfy both (1) and (2). These maps will be defined with respect to a particular probability measure $\wt{P}_n$, that with high probability is close in transportation distance to both $P_n$ and $P$. This estimate on the transportation distance---which we now give--- will be the workhorse that allows us to relate the random energy $b_r$ to the non-random $D_2$.

\paragraph{Transportation distance between $P_n$ and $P$.}
For a map $T$ defined on $\Xset$ and a measure $\mu$, let $T_{\sharp}\mu$ denote the push-forward of $\mu$ by $T$, i.e the measure for which
\begin{equation*}
\bigl(T_{\sharp}\mu\bigr)(U) := \mu\bigl(T^{-1}(U)\bigr)
\end{equation*}
for any Borel subset $U \subseteq \Xset$. Suppose $T_{\sharp}\mu = P_n$; then the map $T$ is referred to as transportation map between $\mu$ and $P_n$. The  $\infty$-transportation distance between $\mu$ and $P_n$ is then
\begin{equation*}
d_{\infty}(\mu,P_n) := \inf_{T: T_{\sharp} \mu = P_n} \biggl\{\sup_{x \in \Xset}~\bigl|T(x) - x\bigr|\biggr\}
\end{equation*}
\cite{calder2019} take $\Xset$ to be a smooth submanifold of $\Rd$ without boundary, i.e. they assume $\Xset$ satisfies~\ref{asmp:domain_manifold}. They exhibit an absolutely continuous measure $\wt{P}_n$ with density $\wt{p}_n$ which with high probability is close to $P_n$ in transportation distance, for which $\|p - \wt{p}_n\|_{\Leb^\infty}$ is also small. In Proposition~\ref{prop:optimal_transport}, we adapt this result to  the setting of full-dimensional manifolds with boundary.  
\begin{proposition}
	\label{prop:optimal_transport}
	Suppose $p$ satisfies~\ref{asmp:bounded_lipschitz_density} and $\Xset$ satisfies~\ref{asmp:domain}. Then with probability at least $1 - A_1 n \exp\bigl\{-a_1 n\theta^2\wt{\delta}^d\bigr\}$, the following statement holds: there exists a probability measure $\wt{P}_n$ with density $\wt{p}_n$ such that:
	\begin{equation*}
	d_{\infty}(\wt{P}_n, P_n) \leq A_1 \wt{\delta}
	\end{equation*}
	and
	\begin{equation*}
	\|\wt{p}_n - p\|_{\infty} \leq A_1\bigl(\wt{\delta} + \theta\bigr)
	\end{equation*}
\end{proposition}
For the rest of this section, we let $\wt{P}_n$ be a probability measure with density $\wt{p}_n$, that satisfies the conclusions of Proposition~\ref{prop:isometry}. Additionally we denote by $\wt{T}_n$ an \emph{optimal transport map} between $\wt{P}_n$ and $P_n$. Finally, we write $U_1,\ldots,U_n$ for the preimages of $X_1,\ldots,X_n$ under $\wt{T}_n$. 

\paragraph{Interpolation and discretization maps.}

The discretization map  $\wt{\mathcal{P}}: \Leb^2(\Xset) \to \Leb^2(P_n)$ is given by
\begin{equation*}
\bigl(\wt{\mathcal{P}}f\bigr)(X_i) := n \cdot \int_{U_i} f(x) \wt{p}_n(x) \,dx.
\end{equation*}
On the other hand, the interpolation map $\wt{\mc{I}}: \Leb^2(P_n) \to \Leb^2(\Xset)$ is defined as $\wt{\mc{I}}u := \Lambda_{r - 2\wt{\delta}}(\wt{\mc{P}}^{\star}u)$. Here, $\wt{\mc{P}}^{\star} = u \circ \wt{T}$ is the adjoint of $\wt{\mc{P}}$, i.e.
\begin{equation*}
\bigl(\wt{\mc{P}}^{\star}u\bigr)(x) = \sum_{j = 1}^{n} u(x_i) \1\{x \in U_i\} 
\end{equation*} 
and $\Lambda_r$ is a smoothening of this extension, formally
\begin{equation*}
\Lambda_r(f) := \frac{1}{r^d\tau(x)}\int_{\Xset} \eta_r(x',x) f(x') \,dx',~~ \eta_r(x',x) := \psi\biggl(\frac{\|x' - x\|}{r}\biggr)
\end{equation*}
where $\psi(t) := (1/\sigma_K)\int_{t}^{\infty} s K(s) \,ds$ and $\tau(x) := (1/r^d)\int_{\Xset} \eta_r(x',x) \,dx'$ is a normalizing constant.

Propositions~\ref{prop:dirichlet_energies} and~\ref{prop:isometry} establish our claims regarding $\wt{\mc{P}}$ and $\wt{\mc{I}}$: first, that they approximately preserve the Dirichlet energies $b_r$ and $D_2$, and second that they are near-isometries for functions $u \in \Leb^2(P_n)$ (or $f \in \Leb^2(P)$) of small Dirichlet energy $b_r(u)$ (or $D_2(f)$).

\begin{proposition}[\textbf{c.f. Proposition 4.1 of \citet{calder2019}}]
	\label{prop:dirichlet_energies}
	With probability at least $1 - A_1n\exp(-a_1n\theta^2\wt{\delta}^{m})$, we have the following.
	\begin{enumerate}[(1)]
		\item For any $u \in \Leb^2(P_n)$,
		\begin{equation*}
		\sigma_{K} D_2(\wt{\mc{I}}u) \leq \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \biggl(1 + A_3\frac{\wt{\delta}}{r}\biggr) b_r(u)
		\end{equation*}
		\item For any $f \in \Leb^2(\Xset)$,
		\begin{equation*}
		b_r(\wt{\mc{P}}f) \leq \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl(1 + L_p(2r + 4\wt{\delta})\Bigr) \cdot \Bigl(1 + A_4\frac{\wt{\delta}}{r}\Bigr) \sigma_{K} D_2(f).
		\end{equation*}
	\end{enumerate}
\end{proposition}

\begin{proposition}[\textbf{c.f. Proposition 4.2 of \citet{calder2019}}]
	\label{prop:isometry}
	With probability at least $1 - A_1n\exp(-a_1n\theta^2\wt{\delta}^{m})$, we have the following.
	\begin{enumerate}[(1)]
		\item For every $f \in \Leb^2(\Xset)$,
		\begin{equation*}
		\Bigl|\|f\|_{P}^2 - \|\wt{P}f\|_{n}^2\Bigr| \leq A_5 \wt{\delta} \|f\|_{P} \sqrt{D_2(f)} + \frac{A_1\bigl(\theta + \wt{\delta}\bigr)}{p_{\min}} \|f\|_{P}^2
		\end{equation*}
		\item For every $u \in \Leb^2(P_n)$,
		\begin{equation*}
		\Bigl|\|\wt{\mc{I}}u\|_{P}^2 - \|u\|_{n}^2\Bigr| \leq A_6 r \|u\|_{n} \sqrt{b_r(u)} + A_7\bigl(\theta + \wt{\delta}\bigr) \|u\|_{n}^2
		\end{equation*}
	\end{enumerate}
\end{proposition}

Given these propositions, the proof of Theorem~\ref{thm:neighborhood_eigenvalue} follows by the Courant-Fischer Theorem, as we now show.

\paragraph{Proof of Theorem~\ref{thm:neighborhood_eigenvalue}.}
We start with the upper bound, proceeding exactly as in Proposition 4.4 of \citep{burago2014}. Let $f_1,\ldots,f_{k}$ denote the first $k$ eigenfunctions of $\Delta_P$ and set $W := \mathrm{span}(f_1,\ldots,f_k)$, so that by the Courant-Fischer principle $D_2(f) \leq \lambda_k(\Delta_P) \|f\|_{P}^2$ for every $f \in W$. As a result, by Part (1) of Proposition~\ref{prop:isometry} we have that for any $f \in W$
\begin{equation*}
\bigl\|\wt{\mc{P}}f\bigr\|_{n}^2 \geq \biggl(1 - A_5\wt{\delta} \sqrt{\lambda_{k}(\Delta_P)} - \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr)\|f\|_{P}^2  \geq \frac{1}{2} \|f\|_{P}^2,
\end{equation*}
where the second inequality follows by assumption~\eqref{eqn:neighborhood_eigenvalue_1}.

Therefore $\wt{\mc{P}}$ is injective over $W$, and $\wt{\mc{P}}W$ has dimension $\ell$. This means we can invoke the Courant-Fischer Theorem, along with Proposition \ref{prop:dirichlet_energies}, and conclude that
\begin{align*}
\frac{\lambda_k(G_{n,r})}{nr^{d + 2}} & \leq \max_{\substack{u \in \wt{\mc{P}}W \\ u \neq 0} } \frac{b_r(u)}{\|u\|_{n}^2} \\
& = \max_{\substack{f \in W \\ f \neq 0} } \frac{b_r(\wt{\mc{P}}f)}{\bigl\|\wt{\mc{P}}f\bigr\|_{n}^2} \\
& \leq 2\Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl(1 + L_p(2r + 4\wt{\delta})\Bigr) \cdot \Bigl(1 + A_4\frac{\wt{\delta}}{r}\Bigr)\sigma_K \lambda_{k}(\Delta_P),
\end{align*}
establishing the lower bound in~\eqref{eqn:eigenvalue_bound} upon proper choice of $a$.

The upper bounds follows from essentially parallel reasoning. Recalling that $v_1,\ldots,v_k$ denote the first $k$ eigenvecotrs of $\Lap_{n,r}$, let $U := \mathrm{span}(v_1,\ldots,v_k)$, so that $nr^{d + 2} b_r(u) \leq \lambda_k(G_{n,r}) \|u\|_n^2$. By Proposition~\ref{prop:isometry}, Part (2), for every $u \in U$ it holds that
\begin{align*}
\bigl\|\wt{\mc{I}}u\bigr\|_{P}^2 & \geq \|u\|_n^2 - A_6 r \|u\|_n \sqrt{b_r(u)} - A_7\bigl(\theta + \wt{\delta}\bigr)\|u\|_n^2 \\
& \geq \|u\|_n^2 - A_6 r \|u\|_n^2 \sqrt{\frac{\lambda_{k}(G_{n,r})}{nr^{d + 2}}} - A_7\bigl(\theta + \wt{\delta}\bigr)\|u\|_n^2 \\
& \geq \|u\|_n^2 - A_6 r \|u\|_n^2 \sqrt{\frac{1}{a}\lambda_k(\Delta_P)} - A_7\bigl(\theta + \wt{\delta}\bigr)\|u\|_n^2 \\
& \geq \frac{1}{2}\|u\|_n^2
\end{align*}
where the second to last inequality follows from the lower bound $a \lambda_k(G_{n,r}) \leq nr^{d + 2}\lambda_k(\Delta_P)$ that we just derived, and the last inequality from assumption~\eqref{eqn:neighborhood_eigenvalue_1}.

Therefore $\wt{\mc{I}}$ is injective over $U$, $\wt{\mc{I}}U$ has dimension $k$, and by Proposition~\ref{prop:dirichlet_energies} we conclude that
\begin{align*}
\lambda_k(\Delta_P) & \leq \max_{u \in U} \frac{D_2(\wt{\mc{I}}u)}{\|u\|_P^2} \\
& \leq 80\frac{(1 + L_pr)}{\sigma_K}\biggl(1 + A_1(\theta + \wt{\delta})\biggr) \biggl(1 + A_3\Bigl(\theta + \frac{\wt{\delta}}{r}\Bigr)\biggr)\max_{u \in U} \frac{b_r(u)}{\|u\|_n^2} \\
& \leq 80\frac{(1 + L_pr)}{\sigma_K}\biggl(1 + A_1(\theta + \wt{\delta})\biggr) \biggl(1 + A_3\Bigl(\theta + \frac{\wt{\delta}}{r}\Bigr)\biggr) \frac{\lambda_k(G_{n,r})}{nr^{d + 2}}
\end{align*}
establishing the upper bound in~\eqref{eqn:eigenvalue_bound} upon proper choice of $A$.



\paragraph{Organization of this section.}
The rest of this section will be devoted to proving Propositions~\ref{prop:optimal_transport},~\ref{prop:dirichlet_energies} and~\ref{prop:isometry}. The proof of Proposition~\ref{prop:optimal_transport} is given in Subsection~\ref{subsec:proof_proposition_optimal_transport}. To prove the latter two propositions, it will help to introduce the intermediate energies
\begin{equation*}
\wt{E}_r(f,\eta,V) := \frac{1}{r^{d + 2}}\int_{V} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{r}\biggr) \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx
\end{equation*}
and
\begin{equation*}
{E}_r(f,\eta,V) := \frac{1}{r^{d + 2}}\int_{V} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx
\end{equation*}
where $\eta: [0,\infty) \to [0,\infty)$ is an arbitrary kernel, and $V \subseteq \Xset$ is a measurable set. We will abbreviate $\wt{E}_r(f,\eta,\Xset)$ as $\wt{E}_r(f,\eta)$ and $\wt{E}_r(f,K) = \wt{E}_r(f)$ (and likewise with $E_r$.)

In Subsection~\ref{subsec:integrals}, we establish relationships between the (non-random) functionals $E_r(f)$ and $D_2(f)$, as well as providing estimates on some assorted integrals. In Subsection~\ref{subsec:random_functionals}, we establish relationships between the stochastic functionals $\wt{E}_r(f)$ and $E_r(f)$,  between $\wt{E}_r\bigl(\wt{\mc{I}}(u)\bigr)$ and $b_r\bigl(u\bigr)$, and between $\wt{E}_r\bigl(f\bigr)$ and $b_r\bigl(\wt{\mc{P}}f\bigr)$. Finally, in Subsection~\ref{subsec:proof_of_prop_dirichlet_energies_and_isometry} we use these various relationships to prove Propositions~\ref{prop:dirichlet_energies} and~\ref{prop:isometry}.

\subsection{Proof of Proposition~\ref{prop:optimal_transport}}
\label{subsec:proof_proposition_optimal_transport}

We start by defining the density $\wt{p}_n$, which will be piecewise constant over a particular partition $\mc{Q}$ of $\Xset$. Specifically, for each $Q$ in $\mc{Q}$ and every $x \in Q$, we set
\begin{equation}
\label{pf:prop_optimal_transport_0}
\wt{p}_n(x) := \frac{P_n(Q)}{\vol(Q)}
\end{equation}
where $\vol(\cdot)$ denotes the Lebesgue measure. 

We now construct the partition $\mc{Q}$, in progressive degrees of generality on the domain $\Xset.$
\begin{itemize}
	\item In the special case of the unit cube $\Xset = (0,1)^d$, the partition will simply be the collection of cubes
	\begin{equation*}
	\set{Q_k: k \in [\wt{\delta}^{-1}]^d} 
	\end{equation*}
	where $Q_k = \wt{\delta}\Bigl([k_1 - 1,k_1] \otimes \cdots \otimes [k_d - 1,k_d]\Bigr)$ and we assume without loss of generality that $\wt{\delta}^{-1} \in \mathbb{N}$.
	\item If $\mc{\Xset}$ is an open, connected set with smooth boundary, then by Proposition 3.2 of \textcolor{red}{GarciaTrillos14}, there exist a finite number $N(\Xset) \in \mathbb{N}$ of disjoint polytopes which cover $\Xset$. Moreover, letting $U_j$ denote the intersection of the $j$th of these polytopes with $\wb{\Xset}$, this proposition establishes that for each $j$ there exists a bi-Lipschitz homeomorphism $\Phi_j: U_j \to [0,1]^d$. We take the collection of
	\begin{equation*}
	\mc{Q} = \biggl\{\Phi_j^{-1}(Q_k): j = 1,\ldots,N(\Xset)~~\textrm{and}~~k \in [\wt{\delta}^{-1}]^d\biggr\}
	\end{equation*}
	to be our partition. Denote by $L_{\Phi}$ the maximum of the bi-Lipschitz constants of $\Phi_1,\ldots,\Phi_{N(\Xset)}$.
	\item Finally, in the general case where $\Xset$ is an open, connected set with Lipschitz boundary, then there exists a bi-Lipschitz homeomorphism $\Psi$ between $\Xset$ and a smooth, open, connected set with Lipschitz boundary. Letting $\Phi_j$ and $\wt{Q}_{j,k}$ be as before, we take the collection
	\begin{equation*}
	\mc{Q} = \biggl\{\wt{Q}_{j,k} = \Bigl(\Psi^{-1} \circ \Phi_j^{-1}\Bigr)(Q_k): j = 1,\ldots,N(\Xset)~~\textrm{and}~~k \in [\wt{\delta}^{-1}]^d\biggr\}
	\end{equation*}
	to be our partition. Denote by $L_{\Psi}$ the bi-Lipschitz constant of $\Psi$.
\end{itemize}
Let us record a few facts which hold for all $\wt{Q}_{j,k} \in \mc{Q}$, which follow from the bi-Lipschitz properties of $\Phi_j$ and $\Psi$: first that
\begin{equation}
\label{pf:prop_optimal_transport_1}
\diam(\wt{Q}_{j,k}) \leq L_{\Psi} \L_{\Phi} \wt{\delta}
\end{equation}
and second that
\begin{equation}
\label{pf:prop_optimal_transport_2}
\vol(\wt{Q}_{j,k}) \geq \biggl(\frac{1}{L_{\Psi} L_{\Phi}}\biggr)^d \wt{\delta}^d.
\end{equation}
We now use these facts to show that $\wt{P}_n$ satisfies the claims of Proposition~\ref{prop:optimal_transport}. One on the one hand, by the construction~\eqref{pf:prop_optimal_transport_0} clearly there exists a transport map $\wt{T}: \Xset \to \mathbf{X}$ for which every $x \in \wt{Q}_{j,k}$ is moved to a design point $X \in {\bf X} \cap \wt{Q}_{j,k}$, and so by~\eqref{pf:prop_optimal_transport_1}
\begin{equation*}
\sup_{x \in \Xset} \abs{\wt{T}(x) - x} \leq  L_{\Psi} \L_{\Phi} \wt{\delta}.
\end{equation*}
On the other hand, applying the triangle inequality we have that for $x \in \wt{Q}_{j,k}$
\begin{align*}
|\wt{p}_n(x) - p(x)| \leq \biggl|\frac{P_n(\wt{Q}_{j,k}) - P(\wt{Q}_{j,k})}{\vol(\wt{Q}_{j,k})}\biggr| + \frac{1}{\vol(\wt{Q}_{j,k})} \int_{\wt{Q}_{j,k}} |p(x') - p(x)| \,dx 
\end{align*}
and using the Lipschitz property of $p$ we find that 
\begin{equation}
\label{pf:prop_optimal_transport_3}
\|\wt{p}_n - p\|_{\Leb^{\infty}} \leq \max_{j,k} \biggl|\frac{P_n(\wt{Q}_{j,k}) - P(\wt{Q}_{j,k})}{\vol(\wt{Q}_{j,k})}\biggr| + L_p L_{\Phi} L_{\Psi} \wt{\delta}
\end{equation}
From Hoeffding's inequality and a union bound, we obtain that 
\begin{align*}
\mathbb{P}\biggl( \bigl|P_n(\wt{Q}) - P(\wt{Q})\bigr| & \leq \theta P(\wt{Q}) \quad \forall \wt{Q} \in \mc{Q} \biggr) \geq 1 - 2 \sharp(\mc{Q}) \cdot \exp\biggl\{-\frac{\theta^2 n \min \{P(\wt{Q})\}}{3}\biggr\} \\
& \geq 1 - \frac{2 N(\Xset)}{\wt{\delta}^d} \cdot \exp\biggl\{-\frac{\theta^2 n p_{\min} \wt{\delta}^d }{3\bigl(L_{\Psi} L_{\Phi}\bigr)^d}\biggr\}.
\end{align*}
Noting that by assumption $P(\wt{Q}) \leq p_{\max} \vol(\wt{Q})$ and $\wt{\delta}^{-d} \leq n$, the claim follows upon plugging back into~\eqref{pf:prop_optimal_transport_3}, and setting
\begin{equation*}
a_1 := \frac{1}{3\bigl(L_{\Psi} L_{\Phi}\bigr)^d}~~\textrm{and}~~A_1 := \max \Bigl\{2N(\Xset),L_p L_{\Psi} L_{\Phi} \Bigr\}
\end{equation*}
in the statement of the proposition.


\subsection{Non-random functionals and integrals}
\label{subsec:integrals}

Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected} relates the non-local energy $E_r(f)$ to $D_2(f)$. It follows in large part from the proofs of Lemma 13 of \cite{trillos2019} and Lemma 6 of \cite{burago2014}, with minor modifications to account for the boundary of $\Xset$.
\begin{lemma}[c.f. Lemma 13 of \cite{trillos2019}, Lemma 6 of \cite{burago2014}]
	\label{lem:first_order_graph_sobolev_seminorm_expected}
	For any $f \in \Leb^2(\Xset)$,
	\begin{equation*}
	E_r(f) \leq (1 + L_pr)^2 \cdot \sigma_K D_2(f)
	\end{equation*}
\end{lemma}

In Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}, we establish the reverse of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected}. 
\begin{lemma}[c.f. Lemma 9 of \cite{trillos2019}, Lemma 5.5 of \cite{burago2014}]
	\label{lem:first_order_graph_sobolev_seminorm_expected_lb}
	For any $f \in \Leb^2(\Xset)$,
	\begin{equation*}
	\sigma_KD_2(\Lambda_rf) \leq \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K}E_r(f)
	\end{equation*}
\end{lemma}

To prove Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}, we require upper and lower bounds on $\tau(x)$, as well as an upper bound on the gradient of $\tau$. The lower bound here---$\tau(x) \geq 1/2$---is quite a bit a looser than in the case where $\Xset$ has no boundary---$\tau(x) \geq (1 + Cr^2)^{-1}$. The same is the case regarding the upper bound of the size of the gradient $\|\nabla \tau(x)\|$. However, the bounds as stated here will be sufficient for our purposes.
\begin{lemma}
	\label{lem:tau_bound}
	Suppose $r$ satisfies~\ref{asmp:r_small_3}. For all $x \in \Xset$,
	\begin{equation*}
	\frac{1}{2} \leq \tau(x) \leq 1,
	\end{equation*}
	and additionally $\|\nabla \tau(x)\| \leq \frac{1}{\sigma_K r}$.
\end{lemma}

Finally, to prove part (2) of Proposition~\ref{prop:isometry}, we require Lemma~\ref{lem:smoothening_error}, which gives an estimate on the error $\Lambda_h f - f$ in $\|\cdot\|_P^2$ norm.
\begin{lemma}[c.f Lemma 8 of \cite{trillos2019}, Lemma 5.4 of \cite{burago2014}]
	\label{lem:smoothening_error}
	For any $h > 0$, 
	\begin{equation}
	\label{eqn:smoothening_error_norm}
	\bigl\|\Lambda_hf\bigr\|_{P}^2 \leq \frac{2p_{\max}}{p_{\min}} \bigl\|f\bigr\|_{P}^2
	\end{equation}
	and
	\begin{equation}
	\label{eqn:smoothening_error_energy}
	\bigl\|\Lambda_hf - f\bigr\|_{P}^2 \leq \frac{2}{\sigma_Kp_{\min}} r E_r(f)
	\end{equation}
	for all $f \in \Leb^2(\Xset)$.
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected}.}
Suppose $f \in C^{\infty}(\wb{\Xset})$, which we may do without loss of generality due to the density of $C^{\infty}(\wb{\Xset})$ in  $\Leb^2(\Xset)$.

Taylor expanding $f(x')$ about $x' = x$, applying first Cauchy-Schwarz' and then Jensen's inequality, and then using the Lipschitz property of $p$, we have
\begin{align*}
E_r(f) & = \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} (f(x') - f(x))^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx \\
& = \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \biggl(\sum_{i = 1}^{d} (x' - x)^{e_i} \int_0^1 f^{(e_i)}\bigl(x + t(x' - x)\bigr) \,dt\biggr)^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx \\
& \leq \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \|x' - x\|^2 \sum_{i = 1}^{d}\biggl(\int_0^1 f^{(e_i)}\bigl(x + t(x' - x)\bigr) \,dt\biggr)^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx \\
& \leq \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \int_0^1   \|x' - x\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + t(x' - x)\bigr)\Bigr)^2\biggr\} K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x)   \,dt   \,dx' \,dx \\
& \leq \frac{(1 + L_pr)^2}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \int_0^1  \|x' - x\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + t(x' - x)\bigr)\Bigr)^2\biggr\} K\biggl(\frac{\|x' - x\|}{r}\biggr) p\bigl(x + t(x' - x)\bigr)^2   \,dt   \,dx' \,dx.
\end{align*} 
Then letting $z = (x' - x)/r$ and $\wt{x} = x + trz$, we obtain
\begin{align*}
E_r(f) & \leq (1 + L_pr)^2 \int_{\Xset} \int_{\mathcal{Z}_r(x)} \int_{0}^{1} \|z\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + trz\bigr)\Bigr)^2\biggr\} K\bigl(\|z\|\bigr) p(x + trz)^2 \,dt \,dz \,dx \\
& = (1 + L_pr)^2 \int_{B(0,1)} \int_{0}^{1}  \int_{\Xset_r(z)} \|z\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + trz\bigr)\Bigr)^2\biggr\} K\bigl(\|z\|\bigr) p(x + trz)^2 \,dx \,dt \,dz \\
& \leq (1 + L_pr)^2 \int_{B(0,1)} \int_{0}^{1} \|z\|^2 K\bigl(\|z\|\bigr) \biggl\{\int_{\wt{\Xset}_r(t,z)} \|\nabla f(\wt{x})\|^2 p(\wt{x})^2 \,d\wt{x}\biggr\} \,dt \,dz  \\
& \leq (1 + L_pr)^2 \int_{B(0,1)} \int_{0}^{1} \|z\|^2 K\bigl(\|z\|\bigr) \biggl\{\int_{\Xset} \|\nabla f(\wt{x})\|^2 p(\wt{x})^2 \,d\wt{x}\biggr\} \,dt \,dz \\
& = (1 + L_pr)^2 \int_{B(0,1)} \|z\|^2 K\bigl(\|z\|\bigr) \,dz 
\end{align*}
where $\mathcal{Z}_r(x) = B(0,1) \cap (\Xset - x)/r$, $\Xset_r(z) = \Xset \cap( \Xset - zr)$, $\wt{\Xset}_r(t,z) = \Xset_r(z) + trz$, and we note that since
\begin{align*}
\wt{x} \in \wt{\Xset}_r(t,z) & \Longrightarrow \wt{x} = x + trz ~~ \textrm{for some $x \in \Xset - zr$ and $t \in [0,1]$} \\
& \Longrightarrow \wt{x} \in \Xset,~~ \textrm{(since $x \in \Xset$, $x + rz \in \Xset$ and $\Xset$ is connected)}
\end{align*}
it must be that $\wt{X}_r(t,z) \subseteq \Xset$ for all $z \in B(0,1)$ and $t \in [0,1]$. 

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}.}
For any $a \in \Reals$, $\Lambda_rf$ satisfies the identity
\begin{equation*}
\Lambda_rf(x) = a + \frac{1}{r^d\tau(x)}\int_{\Xset} \eta_r(x',x)\bigl(f(x') - a\bigr)\,dx'
\end{equation*}
and by differentiating we obtain
\begin{equation*}
\bigl(\nabla \Lambda_rf\bigr)(x)= \frac{1}{r^d\tau(x)}\int_{\Xset} \bigl(\nabla \eta_r(x',\cdot)\bigr)(x)\bigl(f(x') - a\bigr)\,dx' + \nabla\bigl(\tau^{-1}\bigr)(x)\cdot \frac{1}{r^d}\int_{\Xset} \eta_r(x',x)\bigl(f(x') - a\bigr)\,dx'
\end{equation*} 
Plugging in $a = f(x)$, we get $\nabla\Lambda_rf(x) = J_1(x)/\tau(x) + J_2(x)$ for
\begin{equation*}
J_1(x) := \frac{1}{r^d}\int_{\Xset} \bigl(\nabla \eta_r(x',\cdot)\bigr)(x)\bigl(f(x') - f(x)\bigr)\,dx',~~ J_2(x) := \nabla\bigl(\tau^{-1}\bigr)(x)\cdot \frac{1}{r^d}\int_{\Xset} \eta_r(x',x)\bigl(f(x') - f(x)\bigr)\,dx'.
\end{equation*}
To upper bound $\bigl\|J_1(x)\bigr\|^2$, we first compute the gradient of $\eta_r(x',\cdot)$,
\begin{align*}
\bigl(\nabla\eta_r(x',\cdot)\bigr)(x) & = \frac{1}{r} \psi'\biggl(\frac{\|x'  - x\|}{r}\biggr) \frac{(x - x')}{\|x' - x\|} \\
& = \frac{1}{\sigma_Kr^2} K\biggl(\frac{\|x' - x\|}{r}\biggr) (x' - x),
\end{align*}
so that by the Cauchy-Schwarz inequality,
\begin{align*}
\bigl\|J_1(x)\bigr\|^2 & = \frac{1}{\sigma_K^2 r^{4 + 2d}} \Biggl[\int_{\Xset} \bigl(f(x') - f(x)\bigr)K\biggl(\frac{\|x' - x\|}{r}\biggr)(x' - x)\Biggr]^2 \,dx' \\
& \leq \frac{1}{\sigma_K^2r^{4 + 2d}} \biggl[\int_{\Xset}K\biggl(\frac{\|x' - x\|}{r}\biggr)\|x' - x\|^2\,dx'\biggr] \biggl[\int_{\Xset}K\biggl(\frac{\|x' - x\|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2\,dx'\biggr] \\
& \leq \frac{1}{\sigma_K r^{2 + d}}\int_{\Xset}K\biggl(\frac{\|x' - x\|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2\,dx'.
\end{align*}
To upper bound $\bigl\|J_2(x)\bigr\|^2$, we use the Cauchy-Schwarz inequality along with the observation $\eta_r(x',x) \leq \frac{1}{\sigma_K} K\bigl(\|x' - x\|/r\bigr)$ to deduce
\begin{align*}
\bigl\|J_2(x)\bigr\|^2 & \leq \Bigl\|\nabla\bigl(\tau^{-1}\bigr)(x)\Bigr\|^2\frac{1}{r^{2d}} \biggl[\int_{\Xset}\eta_r(x',x) \,dx'\biggr] \cdot \biggl[\int_{\Xset} \eta_r(x',x)\bigl(f(x') - f(x)\bigr)^2 \,dx' \biggr] \\
& = \Bigl\|\nabla\bigl(\tau^{-1}\bigr)(x)\Bigr\|^2\frac{\tau(x)}{r^d} \int_{\Xset} \eta_r(x',x)\bigl(f(x') - f(x)\bigr)^2 \,dx'\\ 
& \leq \Bigl\|\nabla\bigl(\tau^{-1}\bigr)(x)\Bigr\|^2\frac{\tau(x)}{\sigma_K r^d}\int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2 \,dx' \\
& \leq \frac{16}{\sigma_K^2r^{2 + d}} \biggl[\int_{\Xset} K\biggl(\frac{|x' - x|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2 \,dx' \biggr].
\end{align*}
where the last inequality follows by the estimates on $\tau$ and $\nabla \tau$ given in Lemma~\ref{lem:tau_bound}. Combining our bounds on $\bigl\|J_1(x)\bigr\|^2$ and $\bigl\|J_2(x)\bigr\|^2$ along with the lower bound on $\tau(x)$ in Lemma~\ref{lem:tau_bound} and integrating over $\Xset$, we have
\begin{align*}
\sigma_K D_2(\Lambda_r f) & = \sigma_K\int_{\Xset} \biggl\|\Bigl(\nabla \Lambda_rf)(x)\biggr\|^2 p^2(x) \,dx \\
& \leq 2 \sigma_K \int_{\Xset} \Biggl(\frac{\bigl\|J_1(x)\bigr\|^2}{\tau^2(x)} + \bigl\|J_2(x)\bigr\|^2\Biggr) p^2(x) \,dx \\
& \leq \frac{40}{\sigma_K r^{2 + d}} \int_{\Xset} \int_{\Xset} K\biggl(\frac{|x' - x|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2 p^2(x) \,dx' \,dx \\
& \leq \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} E_r(f),
\end{align*}
completing the proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}. 

\paragraph{Proof of Lemma~\ref{lem:tau_bound}.}
First prove the estimates of $\tau(x)$, and then upper bound on $\|\nabla\tau(x)\|$. Substituting $z = (x' - x)/r$ and using~\ref{asmp:r_small_3}, we have that
\begin{align*}
\tau(x) & = \frac{1}{r^d} \int_{\Xset} \psi\biggl(\frac{\|x' - x\|}{r}\biggr) \,dx' \\
& =  \int_{\mc{B}_x} \psi\bigl(\|z\|\bigr) \,dz \\
& \geq a_1(\Xset,d) \cdot \int_{B(0,1)} \psi(\|z\|) \,dz
\end{align*}
We will now show that $\int_{B(0,1)} \psi(\|z\|) \,dz = 1$, which will imply our estimate of $\tau(x)$. To see this identity, note that on the one hand, by converting to polar coordinates and integrating by parts we obtain
\begin{align*}
\int_{B(0,1)} \psi\bigl(\|z\|\bigr) \,dz = d \nu_d \int_{0}^{1} \psi(t) t^{d - 1} \,dt = \nu_d \int_{0}^{1} \psi'(t) t^{d} \,dt = \frac{\nu_d}{\sigma_K} \int_{0}^{1} t^{d + 1} K(t) \,dt;
\end{align*}
on the other hand, again converting to polar coordinate, we have
\begin{equation*}
\sigma_K = \frac{1}{d} \int_{\Reals^d} \|x\|^2 K(\|x\|) \,dx = \nu_d \int_{0}^{1}t^{d + 1} K(t) \,dt.
\end{equation*}

Finally, we upper bound $\|\nabla\tau(x)\|^2$. Exchanging derivative and integral, we have
\begin{align*}
\nabla\tau(x) = \frac{1}{r^d} \int_{\Xset} \bigl(\nabla \eta_r(x',\cdot)\bigr)(x) \,dx' = \frac{1}{\sigma_K r^{d + 2}} \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)(x' - x)\,dx',
\end{align*}
and by the Cauchy-Schwarz inequality,
\begin{equation*}
\|\nabla\tau(x)\|^2 \leq \frac{1}{\sigma_K^2 r^{2d + 4}} \biggl[\int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)\,dx'\biggr] \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)\|x' - x\|^2\,dx', \leq \frac{1}{\sigma_K r^{2}}
\end{equation*}
concluding the proof of Lemma~\ref{lem:tau_bound}. One note: while $\nabla\tau(x) = 0$ when $B(x,r) \in \Xset$, near the boundary the upper bound we derived by using Cauchy-Schwarz appears tight. 

\paragraph{Proof of Lemma~\ref{lem:smoothening_error}.}
By Jensen's inequality and Lemma~\ref{lem:tau_bound},
\begin{align*}
\Bigl[\Lambda_rf(x)\Bigr]^2 & \leq \frac{1}{r^d \tau(x)}\int_{\Xset} \eta_r(x',x) \bigl[f(x')\bigr]^2 \,dx' \\
& \leq \frac{2}{r^d p_{\min}}\int_{\Xset} \eta_r(x',x) \bigl[f(x')\bigr]^2 p(x') \,dx'
\end{align*}
Then integrating over $x$ and swapping the order of integraton, we have
\begin{align*}
\bigl\|\Lambda_rf\bigr\|_{P}^2 & \leq \frac{2}{r^d p_{\min}} \int_{\Xset} \int_{\Xset} \eta_r(x',x) \bigl[f(x')\bigr]^2 p(x') p(x) \,dx' \,dx \\ 
& \leq \frac{2p_{\max}}{p_{\min}} \int_{\Xset} \bigl[f(x')\bigr]^2 p(x') \,dx' \\
& = \frac{2p_{\max}}{p_{\min}} \|f\|_{P}^2.
\end{align*}

Noting that $\Lambda_ra = a$ for any $a \in \Reals$, by the Cauchy-Schwarz inequality we have that for all $x \in \Xset$,
\begin{align*}
\bigl|\Lambda_rf(x) - f(x)\bigr|^2 & = \biggl[\frac{1}{r^d\tau(x)} \int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr) \,dx'\biggr]^2 \\
& \leq \frac{1}{r^{2d} \tau^2(x)} \biggl[\int_{\Xset} \eta_r(x',x) \,dx'\biggr] \cdot \biggl[\int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr)^2 \,dx'\biggr] \\
& = \frac{1}{r^d \tau(x)} \int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr)^2 \,dx'. \\
& \leq \frac{1}{r^d \tau(x) p_{\min}} \int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr)^2 p(x') \,dx'.
\end{align*}
Then integrating over $\Xset$ with respect to $p$ yields~\eqref{eqn:smoothening_error_energy}.



\subsection{Random functionals}
\label{subsec:random_functionals}

We begin by relating $\wt{E}_r(f)$ and $E_r(f)$. Note that by assumption $A_2(\wt{\delta} + \theta)/p_{\min} \leq A_1(\wt{\delta} + \theta) \leq \frac{1}{2}$. Therefore, some standard calculations show that,
\begin{equation}
\label{eqn:calder19_1}
\bigl(1 - A_1(\theta + \wt{\delta})\bigr) E_r(f) \leq \wt{E}_r(f) \leq \bigl(1 + A_1(\theta + \wt{\delta})\bigr) E_r(f),
\end{equation}
as well as implying that the norms $\|f\|_{P}$ and $\|f\|_{n}$ satisfy
\begin{equation}
\label{eqn:calder19_2}
\bigl(1 - A_1(\theta + \wt{\delta})\bigr) \|f\|_{P}^2 \leq \|f\|_{\Leb^2(\wt{P}_n)}^2 \leq \bigl(1 + A_1(\theta + \wt{\delta})\bigr) \|f\|_{P}^2.
\end{equation}

Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized} relates the graph Sobolev semi-norm $b_r(\wt{\mc{P}}f)$ to the non-local energy $\wt{E}_r(f)$. It follows exactly from the derivations in the proof of Lemma 13 in \cite{trillos2019}; indeed, in our flat Euclidean setting, the proof is simplified, and we include this simpler proof for completeness purposes only.
\begin{lemma}[\textbf{c.f. Lemma 13 of \cite{trillos2019}, Lemma 4.3 of \cite{burago2014}}]
	\label{lem:first_order_graph_sobolev_seminorm_discretized}
	Suppose~\ref{asmp:kernel} is satisfied, and that the conclusions of Proposition~\ref{prop:optimal_transport} are met. Then
	\begin{equation*}
	b_r(\wt{\mc{P}}f) \leq \wt{E}_{r + 2\wt{\delta}}(f) + 2\frac{L_K\wt{\delta}}{K(1/2)r} \wt{E}_{2(r + 2\wt{\delta})}(f)
	\end{equation*}
	for any $f \in \Leb^2(\Xset)$.
\end{lemma}

In Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}, we establish the reverse of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized}. 
\begin{lemma}[c.f. Lemma 14 of \cite{trillos2019}]
	\label{lem:first_order_graph_sobolev_seminorm_discretized_lb}
	Suppose $\frac{\wt{\delta}}{r} \leq \min\bigl\{\frac{1}{4(2^{d + 2} - 1)}, \frac{K(1)}{2L_K}\bigr\}$. Then for any $u \in \Leb^2(P_n)$, 
	\begin{equation*}
	\wt{E}_{r - 2\wt{\delta}}\bigl(\wt{\mc{P}}^{\star}u\bigr) \leq \biggl(1 + A_3\frac{\wt{\delta}}{r}\biggr) b_{r}(u)
	\end{equation*}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized}.}
Recalling that $\bigl(\wt{P}f\bigr)(X_i) = n \cdot \int_{U_i} f(x) \wt{p}_n(x)$, the key point is that
\begin{equation*}
\biggl(\bigl(\wt{P}f\bigr)(X_i) - \bigl(\wt{P}f\bigr)(X_j)\biggr)^2 \leq n^2 \cdot \int_{U_i} \int_{U_j} \bigl(f(x') - f(x)\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx.
\end{equation*}
Additionally, the non-increasing and Lipschitz properties of $K$ imply that for any $x \in U_i$ and $x' \in U_j$, 
\begin{equation*}
K\biggl(\frac{\|X_i - X_j\|}{r}\biggr) \leq K\biggl(\frac{\bigl(\|x' - x\| - 2\wt{\delta}\bigr)_{+}}{r}\biggr) \leq K\biggl(\frac{\|x' - x\|}{r + 2\wt{\delta}}\biggr) + \frac{2L_K\wt{\delta}}{r}\1\Bigl\{\|x' - x\| \leq r + 2\wt{\delta}\Bigr\}
\end{equation*}
Using this along with the Lipschitz property of $K$, we get
\begin{align*}
b_r(f) & = \frac{1}{n^2r^{d + 2}} \sum_{i,j = 1}^n \Bigl(\bigl(\wt{P}f\bigr)(X_i) - \bigl(\wt{P}f\bigr)(X_j)\Bigr)^2 K\biggl(\frac{\|X_i - X_j\|}{r}\biggr) \\
& \leq \frac{1}{r^{d + 2}} \sum_{i,j = 1}^n \int_{U_i} \int_{U_j}  \bigl(f(x') - f(x)\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) K\biggl(\frac{\|X_i - X_j\|}{r}\biggr) \,dx' \,dx \\
& \leq \frac{1}{r^{d + 2}} \sum_{i,j = 1}^n \int_{U_i} \int_{U_j}  \bigl(f(x') - f(x)\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) \biggl[K\biggl(\frac{\|x' - x\|}{r + 2\wt{\delta}}\biggr) + \frac{2L_K\wt{\delta}}{r}\1\Bigl\{\|x' - x\| \leq r + 2\wt{\delta}\Bigr\}\biggr] \,dx' \,dx \\
& = \wt{E}_{r + 2\wt{\delta}}(f) + \frac{2L_K\wt{\delta}}{r}\wt{E}_{r + 2\wt{\delta}}(f; \1_{[0,1]})
\end{align*}
for $\1_{[0,1]}(t) = \1\{0 \leq t \leq 1\}$. But clearly $\wt{E}_{r + 2\wt\delta}(f; \1_{[0,1]}) \leq 1/(K(1/2))\wt{E}_{2r + 4\wt{\delta}}(f)$, and the Lemma is shown.

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}.}
For brevity, we write $\wt{r} := r - 2\wt{\delta}$. We begin by expanding the energy $\wt{E}_{r - 2\wt{\delta}}\bigl(\wt{\mc{P}}^{\star}u\bigr)$ as a double sum of double integrals,
\begin{align*}
\wt{E}_{\wt{r}}\bigl(\wt{\mc{P}}^{\star}u\bigr) & = \frac{1}{\wt{r}^{d + 2}} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \int_{U_i} \int_{U_j} \Bigl(u(X_i) - u(X_j)\Bigr)^2 K\biggl(\frac{\|x' - x\|}{\wt{r}}\biggr) \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx
\end{align*}
We next use the Lipschitz property of the kernel $K$---in particular that for $x \in U_i$ and $x' \in U_j$,
\begin{equation*}
K\biggl(\frac{\|x' - x\|}{\wt{r}}\biggr) \leq K\biggl(\frac{\|x_i - x_j\|}{r}\biggr) + \frac{2L_K\wt{\delta}}{\wt{r}} \cdot \1\biggl\{\frac{\|x' - x\|}{\wt{r}} \leq 1\biggr\}
\end{equation*}
---to conclude that
\begin{align}
\wt{E}_{\wt{r}}\bigl(\wt{\mc{P}}^{\star}u\bigr) & \leq \frac{1}{n^2\wt{r}^{d + 2}} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \Bigl(u(X_i) - u(X_j)\Bigr)^2 K\biggl(\frac{\|x_i - x_j\|}{r}\biggr) + \frac{2L_K\wt{\delta}}{\wt{r}}\wt{E}_{\wt{r}}(\wt{\mc{P}}^{\star}u,\1_{[0,1]}\bigr) \nonumber \\
& \leq \biggl(1 + 4\bigl(2^{d + 2} - 1\bigr)\frac{\wt{\delta}}{r}\biggr) b_r(u) + \frac{2L_K\wt{\delta}}{\wt{r}}\wt{E}_{\wt{r}}(\wt{\mc{P}}^{\star}u,\1_{[0,1]}\bigr) \nonumber \\
& \leq \biggl(1 + 4\bigl(2^{d + 2} - 1\bigr)\frac{\wt{\delta}}{r}\biggr) b_r(u) + \frac{2L_K\wt{\delta}}{K(1)\wt{r}} \wt{E}_{\wt{r}}(\wt{\mc{P}}^{\star}u\bigr) \nonumber 
\end{align}
or in other words
\begin{align*}
\wt{E}_{\wt{r}}\bigl(\wt{\mc{P}}^{\star}u\bigr) & \leq \biggl(1 - \frac{2L_K\wt{\delta}}{K(1)\wt{r}}\biggr)^{-1}\biggl(1 + 4\bigl(2^{d + 2} - 1\bigr)\frac{\wt{\delta}}{r}\biggr) b_r(u) \\
& \leq \biggl(1 + \frac{2L_K}{K(1)} + 8\bigl(2^{d + 2} - 1\bigr)\biggr) \frac{\wt{\delta}}{r}b_r(u)
\end{align*}
where the second inequality follows from the algebraic identities $(1 - t)^{-1} \leq (1 + 2t)$ for any $0 < t < 1/2$ and $(1 + s)(1 + t) < 1 + 2s + t$ for any $0 < t < 1$ and $s > 0$.


\subsection{Proof of Propositions~\ref{prop:dirichlet_energies} and~\ref{prop:isometry}}
\label{subsec:proof_of_prop_dirichlet_energies_and_isometry}

\paragraph{Proof of Proposition~\ref{prop:dirichlet_energies}.}
Part (1) of Proposition~\ref{prop:dirichlet_energies} follows from
\begin{align*}
\sigma_K D_2(\Lambda_{r - 2\wt{\delta}} \wt{\mc{P}}^{\star}u) & \overset{(i)}{\leq} \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} E_{r - 2\wt{\delta}}(\wt{\mc{P}}^{\star}u) \\
& \overset{(ii)}{\leq} \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \wt{E}_{r - 2\wt{\delta}}(\wt{\mc{P}}^{\star}u) \\
& \overset{(iii)}{\leq} \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \biggl(1 + A_3\frac{\wt{\delta}}{r}\biggr) b_r(u)
\end{align*}
where $(i)$ follows from Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}, $(ii)$ follows from~\eqref{eqn:calder19_1}, and $(iii)$ follows from~Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}.

Part (2) of Proposition~\ref{prop:dirichlet_energies} follows from
\begin{align*}
b_r(\wt{\mc{P}}f) & \overset{(iv)}{\leq} \wt{E}_{r + 2\wt{\delta}}(f) + \frac{2L_K}{K(1/2)}\frac{\wt{\delta}}{r}\wt{E}_{2(r + 2\wt{\delta})}(f)\\
& \overset{(v)}{\leq} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl({E}_{r + 2\wt{\delta}}(f) + \frac{2L_K}{K(1/2)}\cdot\frac{\wt{\delta}}{r}{E}_{2(r + 2\wt{\delta})}(f)\Bigr) \\
& \overset{(vi)}{\leq} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl(1 + L_p(2r + 4\wt{\delta})\Bigr) \cdot \Bigl(1 + \frac{2L_K}{K(1/2)}\frac{\wt{\delta}}{r}\Bigr) \cdot \sigma_{\eta} D_2(f)
\end{align*}
where $(iv)$ follows from Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized}, $(v)$ follows from~\eqref{eqn:calder19_1}, and $(vi)$ follows from Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected}. The proposition follows upon taking $A_4 := 2L_K/K(1/2)$. 

\paragraph{Proof of Proposition~\ref{prop:isometry}.}
\mbox{}\\
\mbox{}\\
\textit{Proof of (1).}
We begin by upper bounding $\bigl\|\wt{P}f\bigr\|_{n}$. By the Cauchy-Schwarz inequality and the bound on $\|\wt{p}_n - p\|_{\infty}$ in~\eqref{eqn:calder19_1},
\begin{align*}
\Bigl|\wt{P}f(X_i)\Bigr|^2 & = n^2 \Bigl|\int_{U_i} f(x) \wt{p}_n(x) \,dx\Bigr|^2 \\
& \leq n \int_{U_i} \bigl|f(x)\bigr|^2 \wt{p}_n(x) \,dx \\
& \leq n \cdot \biggl[\int_{U_i} \bigl|f(x)\bigr|^2 p(x) \,dx + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \int_{U_i} \bigl|f(x)\bigr|^2 p(x) \,dx\biggr]
\end{align*}
and summing over $i = 1,\ldots,n$, we obtain
\begin{equation}
\label{pf:prop_isometry_1}
\bigl\|\wt{P}f\bigr\|_{n}^2 \leq \biggl(1 + \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr) \bigl\|f\bigr\|_{P}^2.
\end{equation}
Now, noticing that $\bigl\|\wt{P}f\bigr\|_{n} = \bigl\|\wt{P}^{\star}\wt{P}f\bigr\|_{\Leb^2(\wt{P}_n)}$, we can use the upper bound~\eqref{pf:prop_isometry_1} to show that
\begin{align}
\Bigl|\bigl\|\wt{P}f\bigr\|_{n}^2 - \bigl\|f\bigr\|_{P}^2\Bigr| & \leq \Bigl|\bigl\|\wt{P}f\bigr\|_{n}^2 - \bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}^2\Bigr| + \Bigl|\bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}^2 - \bigl\|f\bigr\|_{P}^2\Bigr| \nonumber \\
& \overset{(i)}{\leq} \Bigl|\bigl\|\wt{P}f\bigr\|_{n}^2 - \bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}^2\Bigr|  + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \bigl\|f\bigr\|_{P}^2 \\
& \overset{(ii)}{\leq} \biggl(2 + \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr) \Bigl|\bigl\|\wt{P}f\bigr\|_{n} - \bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}\Bigr| \cdot \bigl\|f\bigr\|_{P} + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \bigl\|f\bigr\|_{P}^2 \nonumber \\
& \leq \biggl(2 + \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr) \bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)} \cdot \bigl\|f\bigr\|_{P} + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \bigl\|f\bigr\|_{P}^2 \label{pf:prop_isometry_2}
\end{align}
where $(i)$ follows from~\eqref{eqn:calder19_2} and $(ii)$ follows from~\eqref{pf:prop_isometry_1}. 

It remains to upper bound $\bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)}^2$. Noting that $\wt{P}^{\star}\wt{P}f$ is piecewise constant over the cells $U_i$, we have
\begin{equation*}
\bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)}^2 = \sum_{i = 1}^{n} \int_{U_i} \biggl(f(x) - n\cdot\int_{U_i} f(x') \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx.
\end{equation*}
We will now establish the following Poincare-type inequality, 
\begin{equation}
\label{pf:prop_isometry_3}
\int_{U_i} \biggl(f(x) - n\cdot\int_{U_i} f(x') \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx \leq \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d K(1/2)} \wt{\delta}^2 \wt{E}_{2\wt{\delta}}(f,U_i)
\end{equation}
which holds for all $i = 1,\ldots,n$. Summing up over $i$ implies
\begin{equation*}
\bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)}^2 \leq \frac{2^{d+2}}{a_1(\Xset,d)K(1/2)p_{\min}} \cdot \wt{E}_{2\wt{\delta}}(f) \leq \frac{2^{d+3}}{a_1(\Xset,d)K(1/2)p_{\min}} A_1(\Xset,d,K,p_{\min}) D_2(f)
\end{equation*}
with the latter inequality following just as in the proof of Proposition~\ref{prop:dirichlet_energies}; the Lemma then follows by plugging this inequality into~\eqref{pf:prop_isometry_2}.

Now it remains to show~\eqref{pf:prop_isometry_3}. The key point is that for any pair of points $x,x' \in \Xset$ such that $\|x - x'\| \leq r$  the triangle inequality
\begin{equation*}
\bigl|f(x') - f(x)\bigr|^2 \leq 2\bigl|f(x') - f(z)\bigr|^2 + 2\bigl|f(z) - f(x)\bigr|^2
\end{equation*}
holds for each $z \in B\bigl(x,\wt{\delta}\bigr) \cap B\bigl(x',\wt{\delta}\bigr) \cap \Xset =: W$, and we have that
\begin{align*}
\bigl|f(x') - f(x)\bigr|^2 & \leq \frac{2}{\vol(W)}\biggl[\int_{W} \bigl|f(x') - f(z)\bigr|^2 \,dz + \int_{W} \bigl|f(z) - f(x)\bigr|^2 \,dz\biggr] \\
& \leq \frac{2}{\vol(W)K(1/2)}\biggl[\int_{\Xset} K\biggl(\frac{x' - z}{2\wt{\delta}}\biggr)\bigl|f(x') - f(z)\bigr|^2 \,dz + \int_{\Xset} K\biggl(\frac{z - x}{2\wt{\delta}}\biggr)\bigl|f(z) - f(x)\bigr|^2 \,dz\biggr] \\
& \leq \frac{2^{d + 1}}{a_1(\Xset,d) \nu_d \wt{\delta}^d K(1/2)}\biggl[\int_{\Xset} K\biggl(\frac{x' - z}{2\wt{\delta}}\biggr)\bigl|f(x') - f(z)\bigr|^2 \,dz + \int_{\Xset} K\biggl(\frac{z - x}{2\wt{\delta}}\biggr)\bigl|f(z) - f(x)\bigr|^2 \,dz\biggr]
\end{align*}
where the last inequality follows from Assumption~\ref{asmp:r_small_1}. We use this along with the Cauchy-Schwarz inequality to prove~\eqref{pf:prop_isometry_3}:
\begin{align*}
\int_{U_i} & \biggl(f(x) - n\cdot\int_{U_i} f(x') \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx \\
& =\int_{U_i} \biggl(n\cdot\int_{U_i} \bigl(f(x) - f(x')\bigr) \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx \\
& \leq n \cdot \int_{U_i} \int_{U_i} \bigl(f(x) - f(x')\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx \\
& \leq \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d \wt{\delta}^d K(1/2)}n\int_{U_i} \int_{U_i} \int_{\Xset} K\biggl(\frac{x - z}{2\wt{\delta}}\biggr)\bigl|f(x) - f(z)\bigr|^2 \wt{\rho}_n(x') \wt{\rho}_n(x)  \,dx' \,dz \,dx \\
& \leq \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d \wt{\delta}^d K(1/2) p_{\min}} \int_{U_i} \int_{U_i} K\biggl(\frac{x - z}{2\wt{\delta}}\biggr)\bigl|f(x) - f(z)\bigr|^2 \wt{\rho}_n(z) \wt{\rho}_n(x)  \,dz \,dx \\
& =  \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d K(1/2) p_{\min}} \wt{\delta}^2 \wt{E}_{2\wt{\delta}}(f,U_i).
\end{align*}

\textit{Proof of (2).}
By the triangle inequality,
\begin{align}
\Bigl|\|\wt{\mc{I}}u\|_{P}^2 - \|u\|_{n}^2\Bigr| & \leq \Bigl|\|\wt{\mc{I}}u\|_{P}^2 - \|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2\Bigr| + \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 - \|u\|_{n}^2\Bigr| \nonumber \\
& \leq A_1(\theta + \wt{\delta}) \|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 + \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 - \|u\|_{n}^2\Bigr| \nonumber\\
& = A_1(\theta + \wt{\delta}) \|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 + \Bigl(\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} + \|u\|_{n}\Bigr) \cdot \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} - \|u\|_{n}\Bigr| \label{pf:prop_isometry_4}
\end{align}
To upper bound the second term in the above expression, we first note that~$\|u\|_{n} = \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)}$, and thus
\begin{align}
\Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} - \|u\|_{n} \Bigr| & = \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} - \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)}\Bigr| \nonumber \\
& \overset{(iii)}{\leq} \|\Lambda_{\wt{r}}\wt{\mc{P}}^{\star}u - \wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)} \nonumber \\
& \overset{(iv)}{\leq} r \sqrt{\frac{2}{\sigma_K p_{\min}} E_{\wt{r}}(\wt{\mc{P}}^{\star}u)} \nonumber \\
& \overset{(v)}{\leq} r \sqrt{\frac{2}{\sigma_K p_{\min}} \Bigl(1 + A_3\frac{\wt{\delta}}{r}\Bigr) b_r(u)} \label{pf:prop_isometry_5}
\end{align}
where $(iii)$ follows by the triangle inequality, $(iv)$ follows from Lemma~\ref{lem:smoothening_error}, and $(v)$ follows from~\eqref{eqn:calder19_1} and Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}. On the other hand, by~\eqref{eqn:calder19_2} and Lemma~\ref{lem:smoothening_error},
\begin{align*}
\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 & \leq \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \|\wt{\mc{I}}u\|_{P}^2 \\
& \leq \frac{2p_{\max}}{p_{\min}} \cdot \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \|\wt{\mc{P}}^{\star}u\|_{P}^2 \\
& \leq \frac{2p_{\max}}{p_{\min}} \cdot \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr)^2 \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)}^2 \\
& = \frac{2p_{\max}}{p_{\min}} \cdot \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr)^2 \|u\|_{n}^2
\end{align*}
and plugging this estimate along with~\eqref{pf:prop_isometry_5} back into~\eqref{pf:prop_isometry_4}, we obtain part (2) of Proposition~\ref{prop:isometry}.

\subsection{\textcolor{red}{(TODO)}: Move these.}

In the above we used the following:
\begin{enumerate}[label=(OT\arabic*)]
	\item
	\label{asmp:r_small_1} For all $x,x' \in \Xset$ such that $\|x - x'\| \leq r$, we have that for $W = B(x,r) \cap B(x',r) \cap \Xset$,
	\begin{equation*}
	\vol(W) \geq a_1(\Xset,d) \cdot \nu_d \biggl(\frac{r}{2}\biggr)^d
	\end{equation*}
	for some constant $a_1(\Xset,d)$. 
	\item 
	\label{asmp:r_small_3} Set $\mc{B}_x:= (\Xset - x)/r~ \cap~B(0,1)$. For any $x \in \Xset$ and any function $\psi: [0,\infty) \to [0,\infty)$,
	\begin{equation}
	\label{eqn:lb_expected_degree}
	\int_{\mc{B}_x} \psi(\|z\|) \,dz \geq a_1(\Xset,d) \cdot \int_{B(0,1)} \psi(\|z\|) \,dz.
	\end{equation}
\end{enumerate}

\section{Bounds on the empirical norm}
\label{sec:empirical_norm}

We use Lemma~\ref{lem:empirical_norm_sobolev} to lower bound $\norm{f_0}_n^2$ by (a constant times) the $\Leb^2(\Xset)$ norm of $f$.

\begin{lemma}
	\label{lem:empirical_norm_sobolev}
	Suppose $P$ satisfies~\ref{asmp:bounded_lipschitz_density}. If $f \in H^1(\Xset,M)$ is lower bounded in $\Leb^2(\Xset)$ norm,
	\begin{equation}
	\label{eqn:empirical_norm_sobolev_1}
	\norm{f}_{\Leb^2(\Xset)} \geq 
	\begin{cases*}
	\frac{C_6 M}{\delta} \cdot \max\Bigl\{n^{-1/2},n^{-1/d}\Bigr\},& ~~\textrm{if $2 \neq d$} \\
	\frac{C_6 M}{\delta} \cdot n^{-a/2},& ~~\textrm{if $2 = d$, for any $0 < a < 1$}
	\end{cases*}
	\end{equation}
	for some $\delta \leq 1$, then
	\begin{equation}
	\label{eqn:empirical_norm_sobolev}
	\norm{f}_n^2 \geq \delta \cdot \Ebb\Bigl[\norm{f}_n^2\Bigr] 
	\end{equation}
	with probability at least $1 - 5 \delta$.
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:empirical_norm_sobolev}.}
To establish~\eqref{eqn:empirical_norm_sobolev}, it is sufficient to show that
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot \left(\mathbb{E}\bigl[\norm{f}_n^2\bigr]\right)^2;
\end{equation*}
then \eqref{eqn:empirical_norm_sobolev} follows from the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). Since $p \leq p_{\max}$ is uniformly bounded, we can relate $\mathbb{E}\bigl[\norm{f}_n^4\bigr]$ to the $\Leb^4$ norm,
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] = \frac{(n-1)}{n}\left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + \frac{\mathbb{E}\Bigl[\bigl(f(x_1)\bigr)^4\Bigr]}{n} \leq \left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + p_{\max}^2\frac{\norm{f}_{\Leb^4}^4}{n}.
\end{equation*}
We will use a Sobolev inequality to relate $\norm{f}_{\Leb^4}$ to $\norm{f}_{H^1(\Xset)}$. The nature of this inequality depends on the relationship between $s$ and $d$ (see Theorem 6 in Section 5.6.3 of \cite{evans10} for a formal statement), so from this point on we divide our analysis into three cases: (i) the case where $2s > d$, (ii) the case where $2s < d$, and (iii) the borderline case $2s = d$.

\textit{Case 1: $2s > d$.}
When $2s > d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in C^{\gamma}(\overline{\Xset})$ for some $\gamma > 0$ which depends on $s$ and $d$, with the accompanying estimate
\begin{equation*}
\sup_{x \in \Xset} \abs{f(x)} \leq \norm{f}_{C^{\gamma}(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Therefore,
\begin{align*}
\norm{f}_{\Leb^4}^4 & = \int_{\Xset} [f(x)]^4 \,dx \\
& \leq \left(\sup_{x \in \Xset} \abs{f(x)}\right)^2 \cdot \int_{\Xset} [f(x)]^2 \,dx \\
& \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \norm{f}_{\Leb^2(\Xset)}^2.
\end{align*}
Since by assumption
\begin{equation*}
\norm{f}_{\Leb^2(\Xset)}^2 \geq c_1^2 \cdot b^2 \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2 \cdot \frac{1}{n},
\end{equation*}
we have
\begin{equation*}
p_{\max}^2\frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \frac{\norm{f}_{\Leb^2(\Xset)}^4}{n \norm{f}_{\Leb^2(\Xset)}^2} \leq c \frac{\norm{f}_{\Leb^2(\Xset)}^4}{c_1^2 b^2} \leq \frac{\Ebb\bigl[\norm{f}_n^2\bigr]}{b^2},
\end{equation*}
where the last inequality follows by taking $c_1$ sufficiently large.

\textit{Case 2: $2s < d$.}
When $2s < d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in \Leb^q(\Xset)$ for $q = 2d/(d - 2s)$, and moreover that
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Since $4 = 2\theta + (1 - \theta)q$ for $\theta = 2 - d/(2s)$, Lyapunov's inequality implies
\begin{equation*}
\norm{f}_{\Leb^4(\Xset)}^4 \leq \norm{f}_{\Leb^2}^{2\theta} \cdot \norm{f}_{\Leb^q(\Xset)}^{(1 - \theta)q} \leq c \norm{f}_{\Leb^2(\Xset)}^{4} \cdot \left(\frac{\norm{f}_{H^s(\Xset)}}{\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s}.
\end{equation*}
By assumption, $\norm{f}_{\Leb^2(\Xset)} \geq c_1 b \norm{f}_{H^s(\Xset)} n^{-s/d}$, and therefore
\begin{equation*}
p_{\max}^2 \frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c\norm{f}_{\Leb^2(\Xset)}^4 \left(\frac{\norm{f}_{H^s(\Xset)}}{n^{s/d}\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s} \leq \frac{c\norm{f}_{\Leb^2(\Xset)}^4}{c_1b^{d/s}} \leq \frac{\norm{f}_{\Leb^2(\Xset)}^4}{b^2}.
\end{equation*}
where the last inequality follows when $c_1$ is sufficiently large, and keeping in mind that $d/s > 2$ and $b \geq 1$. 

\textit{Case 3: $2s = d$.}
Assume $f$ satisfies~\eqref{eqn:paley_zygmund_1} for a given $0 < a < 1$. When $2s = d$, since $\Xset$ is a Lipschitz domain we have that $f \in L^q(\Xset)$ for any $q < \infty$, with the accompanying estimate
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
In particular the above holds for $q = 2/(1 - a)$ when $1/2 < a < 1$, and for any $q > 4$ when $0 < a < 1/2$. Using Lyapunov's inequality as in the previous case then implies the desired result.

\section{Proof of Main Results}
\label{sec:main_results}

\subsection{Proof of Theorem~\ref{thm:laplacian_smoothing_estimation1}}
\label{subsec:laplacian_smoothing_estimation1_pf}
Let $\mc{E}$ be the set of $\mathbf{X} = (X_1,\ldots,X_n)$ for which
\begin{enumerate}[(i)]
	\item
	\label{pf:laplacian_smoothing_estimation1_0}
	$f_0^T \Lap_{n,r} f_0 \leq \frac{p_{\max}^2 \sigma_K}{\delta} n^2 r^{d + 2}M^2$
	\item 
	\label{pf:laplacian_smoothing_estimation1_1}
	$c_3 \cdot \min\{k^{2/d}nr^{d+2},nr^d\} \leq \lambda_k(G_{n,r}) \leq C_3k^{2/d}nr^{d + 2} $ for all $2 \leq k \leq n$.
\end{enumerate}
We already know, from Lemma~\ref{lem:graph_sobolev_seminorm} and Corollary~\ref{cor:neighborhood_eigenvalue}, that for any $\delta > 0$ and $(\log(n)/n)^{1/d} \leq r \leq a_0$,
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_1.5}
\Pbb\bigl({\bf X} \in \mc{E}\bigr) \geq 1 - \delta - A_1n\exp\bigl(-a_1nr^d\bigr).
\end{equation}
We will now show that conditional on $\mathbf{X} \in \mc{E}$, the empirical mean square error is small with high probability. Having conditioned on $\mathbf{X}$, we can use Lemma~\ref{lem:ls_fixed_graph_estimation} to control the remaining randomness arising from the noisy response $Y$, which in this context implies
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_2}
\begin{aligned}
\Pbb\Biggl(& \frac{1}{n}\bigl\|\wt{f}_{\LS}(G_{n,r}) - f_0\bigr\|_2^2 \leq \frac{2\rho}{n} \bigl(f_0^T \Lap_{n,r} f_0\bigr) + \frac{10}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G_{n,r}) + 1\bigr)^2} \Bigg| \mathbf{X} = x \in \mc{E}\Biggr) \\
& \geq 1 - \exp\Biggl\{-\sum_{k = 1}^{n}\frac{1}{\bigl(\rho \lambda_k(G_{n,r}) + 1\bigr)^2}\Biggr\} \\
& \geq 1 - \exp\Biggl\{-\sum_{k = 1}^{n}\frac{1}{\bigl(C_3M^{-4/(2+d)}k^{2/d}n^{-2/(2+d)} + 1\bigr)^2}\Biggr\} \\
& \geq 1 - \exp\Bigl\{-c M^{d/(2d + 4)} n^{d/(2+d)}\Bigr\}
\end{aligned}
\end{equation}
where the second inequality follows from the upper bound in~\ref{pf:laplacian_smoothing_estimation1_1} and the choice of regularization parameter $\rho = M^{-4/(2+d)}(nr^{d+2})^{-1}n^{-2/(2+d)}$. 

It remains only to upper bound the (conditional on ${\bf X} = x$ for some $x \in \mc{E}$) squared bias and variance terms. A sufficient upper bound on the bias term comes immediately from the upper bound~\ref{pf:laplacian_smoothing_estimation1_0} and our choice of $\rho$,
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_2.5}
\frac{\rho}{n} \bigl(f_0^T \Lap_{{n,r}} f_0\bigr) \leq \frac{p_{\max}^2 \sigma_K}{\delta} M^{2/(2+d)} n^{-2/(2 + d)}.
\end{equation}

To upper bound the variance term, we replace the eigenvalues $\lambda_k(G_{n,r})$ by their lower bounds in~\ref{pf:laplacian_smoothing_estimation1_1} and plug in our choice of $\rho$ to obtain,
\begin{align}
\frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G_{n,r}) + 1\bigr)^2} & \leq \frac{C}{n}\sum_{k = 1}^{n} \biggl\{\frac{1}{(\rho k^{2/d} n r^{d + 2} + 1)^2} + \frac{1}{(\rho n r^{d} + 1)^2} \biggr\} \nonumber \\
& \leq \frac{C}{n}\sum_{k = 1}^{n} \biggl\{\frac{1}{\bigl(M^{-4/(2 + d)}n^{-2/(2+d)}k^{2/d} + 1\bigr)^2}\biggr\} + Cn^{4/(2 + d)}M^{8/(2+d)}r^{4} \nonumber \\
& \leq CM^{2d/(2 + d)} n^{-2/(2 + d)} + CM^{8/(2 + d)}n^{(2 - d)/(2 + d)} \sum_{k = k_{\star}}^{n} \Bigl\{\frac{1}{k^{4/d}}\Bigr\}  + CM^{8/(2 + d)}n^{4/(2 + d)}r^4 \label{pf:laplacian_smoothing_estimation1_2.25}
\end{align}
where $k_{\star} = M^{2d/(2 + d)}n^{d/(2 + d)}$.  

We now upper bound the 2nd and 3rd term on the right hand side of~\eqref{pf:laplacian_smoothing_estimation1_2.25}. To control the 3rd term, we use the upper bound $r \leq n^{-3/(4 + 2d)} M^{(d - 4)/(4 + 2d)}$ assumed in~\ref{asmp:ls_kernel_radius_estimation} to determine that
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_3}
M^{8/(2 + d)}n^{4/(2 + d)}r^4 \leq M^{2d/(2 + d)}n^{-2/(2 + d)} 
\end{equation}
To control the 2nd term on the right hand side of~\eqref{pf:laplacian_smoothing_estimation1_2.25}, we treat it as a Riemann sum evaluated at the right end points of $[k_{\star},k_{\star} + 1], [n - 1,n]$. We use the fact that such a Riemann sum upper bounds the integral of a montonically nonincreasing function to conclude that
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_4}
\begin{aligned}
\sum_{k = k_{\star}}^{n} \frac{1}{k^{4/d}} & \leq \int_{k_{\star}}^{n} \frac{1}{x^{4/d}} \,dx \\
& = - \frac{1}{(4/d - 1)} x^{-(4/d - 1)} \Big|_{k_{\star}}^{n} \\
& \leq \frac{1}{(4/d - 1)} k_{\star}^{-(4/d - 1)}
\end{aligned}
\end{equation}
where the equality in the above computation holds because $d < 4$. Recalling the definition of $k_{\star}$, plugging this back into~\eqref{pf:laplacian_smoothing_estimation1_3} implies a sufficient upper bound on the variance term. Along with~\eqref{pf:laplacian_smoothing_estimation1_1.5}-\eqref{pf:laplacian_smoothing_estimation1_2.5}, this establishes the claim.

\paragraph{Proving near-optimal bounds when $d = 4$.}
Note that nothing in the derivations of  ~\eqref{pf:laplacian_smoothing_estimation1_1.5}-\eqref{pf:laplacian_smoothing_estimation1_2.5} relied on $d < 4$, and so they all hold when $d = 4$. We now upper bound the 2nd and 3rd terms in~\eqref{pf:laplacian_smoothing_estimation1_2.5}. By setting $r = C_4(\log(n)/n)^{1/4}$ for some constant $C_4$, we have that
\begin{equation*}
M^{8/(2+d)}n^{4/(2+d)}r^4 = C_4^4 M^{8/(2+d)} \log(n) n^{-1/3},
\end{equation*}
taking care of the third term. On the other hand, we have that
\begin{equation*}
\sum_{k = k_{\star}}^{n} \frac{1}{k} \leq \log(n)
\end{equation*}
and so the second term is also upper bounded by $C M^{8/(2+d)} \log(n) n^{-1/3}$.

\subsection{Proof of Theorem~\ref{thm:laplacian_smoothing_testing}}
In this proof we will need to invoke Lemma~\ref{lem:empirical_norm_sobolev}, and it is in order to satisfy the conditions of this Lemma that we require $M \leq n^{(4 - d)/(4 + d)}$. More specifically, by~\eqref{eqn:laplacian_smoothing_testing} along with the restriction $M \leq n^{(4 - d)/(4 + d)}$,
\begin{equation*}
\norm{f}_{\Leb^2(\Xset)} \geq \frac{C}{\delta^2} M^{2d/(4 + d)} n^{-4/(4 + d)} \geq \frac{C}{\delta^2} M^2 n^{-2/d}
\end{equation*}
and~\eqref{eqn:empirical_norm_sobolev_1} is therefore satisfied when we choose $C \geq C_6$ in~\eqref{eqn:laplacian_smoothing_testing}.

We proceed to prove Theorem~\ref{thm:laplacian_smoothing_testing}. Throughout this proof we set $\delta = 1/b$. We will proceed, as in Subsection~\ref{subsec:laplacian_smoothing_estimation1_pf}, by conditioning on the random design $\mathbf{X}$. Let $\mc{E}(f_0)$ be the set of $\mathbf{X} \in \Xset^n$ such that~\eqref{eqn:graph_sobolev_seminorm},~\eqref{eqn:neighborhood_eigenvalue_2}, and~\eqref{eqn:empirical_norm_sobolev_1} hold: by Lemmas~\ref{lem:graph_sobolev_seminorm} and~\ref{lem:empirical_norm_sobolev}, and Corollary \ref{cor:neighborhood_eigenvalue},
\begin{equation*}
\Pbb\bigl(\mc{E}(f_0)\bigr) \geq 1 - 6\delta - A_1n\exp\bigl\{-a_1nr^d\bigr\}. 
\end{equation*}
For any $\mathbf{X} \in \mc{E}(f_0)$ the conditional variance of $T_{\LS}$ can be upper bounded using~\eqref{eqn:neighborhood_eigenvalue_2} and~\ref{asmp:ls_kernel_radius_testing},
\begin{align*}
\frac{1}{n} \biggl(\sum_{k = 1}^{n} \bigl[\lambda_k(G_{n,r}) + 1\bigr]^{-4}\biggr)^{1/2} & \leq \frac{1}{c_5^2 n} \biggl(M^{\frac{32}{8 + d}}r^8n^{\frac{20+d}{4+d}} + \sum_{k = 1}^{n} \bigl[n^{-\frac{4}{4 + d}}k^{\frac{2}{d}}M^{-\frac{8}{4 + d}} + 1\bigr]^{-4}\biggr)^{1/2} \\
& \leq \frac{1}{c_5^2 n} \biggl(M^{\frac{4d}{4 + d}}n^{\frac{2d}{4 + d}} + \sum_{k = 1}^{n} \bigl[n^{-\frac{4}{4 + d}}k^{\frac{2}{d}}M^{-\frac{8}{4 + d}} + 1\bigr]^{-4}\biggr)^{1/2} \\
& \leq \frac{1}{c_5^2 n} \biggl(3 M^{\frac{4d}{4 + d}}n^{2d/(4 + d)}\biggr)^{1/2} \\
& = \frac{\sqrt{3}}{c_5^2} M^{\frac{2d}{4 + d}} n^{-\frac{4}{4 + d}}.
\end{align*}
The last inequality in the above display similarly to~\eqref{pf:laplacian_smoothing_estimation1_4}. Setting $k_{\star} = n^{2d/(4+d)}M^{4d/(4 + d)}$, we derive that
\begin{equation}
\begin{aligned}
\label{pf:laplacian_smooting_testing1}
\sum_{k = 1}^{n} \bigl[n^{-\frac{4}{4 + d}}k^{\frac{2}{d}}M^{-\frac{8}{4 + d}} + 1\bigr]^{-4} & \leq k_{\star} + n^{\frac{16}{4 + d}} M^{\frac{32}{4 + d}}\sum_{k = k_{\star}}^{n} k^{-\frac{8}{d}}\\
& \leq k_{\star} + n^{\frac{16}{4 + d}} M^{\frac{32}{4 + d}} \int_{k_{\star}}^{n} x^{-\frac{8}{d}} \,dx \\
& \leq  k_{\star} + \frac{1}{(8/d - 1)}n^{\frac{16}{4 + d}} M^{\frac{32}{4 + d}} k_{\star}^{1 - 8/d} \\
& \leq 2k_{\star}.
\end{aligned}
\end{equation}
As a result, 
\begin{align*}
\frac{2\rho}{n} \bigl(f_0^T \Lap_{n,r} f_0) + \frac{4}{\delta n}\biggl(\sum_{k = 1}^{n} \bigl[\lambda_k(G_{n,r}) + 1\bigr]^{-4}\biggr)^{1/2} & \leq \frac{1}{\delta}\biggl(2C_1 + \frac{4\sqrt{3}}{c_5^2}\biggr)n^{-\frac{4}{4 + d}} M^{\frac{2d}{4 + d}} \\
& \leq \delta p_{\min} \norm{f}_{\Leb^2(\Xset)}^2 \leq \delta \Ebb\bigl[\norm{f}_n^2\bigr] \\
& \leq \norm{f}_n^2,
\end{align*}
where the second inequality follows upon choosing $C \geq p_{\min}^{-1}(2C_1 + 4\sqrt{3}/c_5^2)$ in~\eqref{eqn:laplacian_smoothing_testing}. We may therefore bound the Type II error of $\wt{\phi}_{\LS}$ by appealing to Lemma~\ref{lem:ls_fixed_graph_testing},
\begin{align*}
\Ebb_{f_0}\Bigl[1 - \wt{\phi}_{\LS}\Bigr] & \leq \Ebb_{f_0}\Bigl[1 - \wt{\phi}_{\LS}\big| \mathbf{X} \in \mc{E}(f_0)\Bigr] + 1 - \Pbb\Bigl(\mc{E}(f_0)\Bigr) \\
& \leq 4 \delta^2 + 8 \delta \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G_{n,r}) + 1)^4} \Biggr)^{-1/2} + 6\delta + A_1\exp\bigl\{-a_1nr^{d}\bigr\} \\
& \leq 4 \delta^2 + 16\delta n^{-d(4 + d)}M^{-2d/(4 + d)}  + 6\delta + A_1\exp\bigl\{-a_1nr^{d}\bigr\}
\end{align*}
where the last inequality follows from the reversing the inequalities in~\eqref{pf:laplacian_smooting_testing1}, with a factor of $1/2$ instead of $2$ in front. 

\subsection{Proof of~\eqref{eqn:laplacian_smoothing_testing_low_smoothness}}
When $\rho = 0$, the Laplacian smoother $\wh{f}_{\LS} = Y$, the test statistic $T_{\LS} = \frac{1}{n}\|Y\|_2^2$, and the threshold $\wh{t}_b = 1 + 2bn^{-1/2}$. The expectation of $T_{\LS}$ is 
\begin{equation*}
\Ebb\bigl[T_{\LS}\bigr] = \mathbb{E}\bigl[f_0^2(X)\bigr] + 1 \geq p_{\min} \norm{f_0}_{\Leb^2(\Xset)}^2 + 1
\end{equation*}
When $f_0 \in \Leb^4(\Xset,M)$, the variance can be upper bounded
\begin{equation*}
\Var\bigl[T_{\LS}\bigr] \leq \frac{1}{n}\Bigl(3 + p_{\max} M^4 + p_{\max}\norm{f_0}_{\Leb^2(\Xset)}^2\Bigr).
\end{equation*}
Now, let us assume that
\begin{equation*}
\norm{f_0}_{\Leb^2(X)}^2 \geq \frac{4b}{p_{\min}} n^{-1/2},
\end{equation*}
so that $E[T_{\LS}] - \wh{t}_b \geq E[T_{\LS}]/2$. Hence, by Chebyshev's inequality
\begin{align*}
\mathbb{P}_{f_0}\Bigl(T_{\LS} \leq \wh{t}_b \Bigr) & \leq 4 \frac{\Var_{f_0}\bigl[T_{\LS}\bigr]}{\mathbb{E}[f^2(X)]^2} \\
& \leq \frac{4}{n} \cdot \frac{ 3 + p_{\max}\bigl(M^4 + \|f_0\|_{\Leb^2(\Xset)}^2 \bigr)}{p_{\min}^2 \|f_0\|_{\Leb^2(\Xset)}^4} \\
& \leq \frac{1}{16b^2}\Bigl(3 + p_{\max}n^{-1/2} + p_{\max}M^4\Bigr).
\end{align*}
\subsection{Bounds on $\Leb^2(\Xset)$ error under Lipschitz assumption.}
Suppose $f_0$ has bounded Lipschitz constant. Let $V_1,\ldots,V_n$ denote the Voronoi tesselation of $\Xset$ with respect to $X_1,\ldots,X_n$. Extend $\wh{f}$ over $\Xset$ by taking it piecewise constant over the Voronoi cells, i.e.
\begin{equation*}
\wh{f}(x) := \sum_{i = 1}^{n} \cdot \wh{f}_i \1\{x \in V_i\}.
\end{equation*}
Note that we are abusing notation slightly by also using $\wh{f}$ to refer to this extension. 

In Proposition~\ref{prop:out_of_sample_error}, we establish that the out-of-sample error $\|\wh{f} - f_0\|_{\Leb^2(\Xset)}$ will not be too much larger than the in-sample error $\|\wh{f} - f_0\|_n$.
\begin{proposition}
	\label{prop:out_of_sample_error}
	Suppose $f_0$ satisfies $|f_0(x') - f_0(x)| \leq M \|x' - x\|$ for all $x',x \in \mc{X}$. Then, with probability at least $1 - \delta$ it holds that
	\begin{equation*}
	\|\wh{f} - f_0\|_{\Leb^2(\Xset)}^2 \leq C \log(1/\delta) \biggl(\log(n)\cdot \|\wh{f} - f_0\|_n^2 + M^2\Bigl(\frac{\log n}{n}\Bigr)^{2/d}\biggr).
	\end{equation*}
\end{proposition}
Note that $n^{-2/d} \ll n^{-2/(2 +d)}$. Therefore Proposition~\ref{prop:out_of_sample_error} together with Theorem~\ref{thm:laplacian_smoothing_estimation1} imply that with high probability, $\wh{f}$ achieves the nearly-optimal (up to a factor of $\log n$) estimation out-of-sample error---that is, $\|\wh{f} - f_0\|_{\Leb^2(\Xset)}^2 \leq C \log(n) M^{2d/(2 + d)}n^{-2/(2+d)}$---as long as $M \leq Cn^{1/d}$.
\paragraph{Proof of Proposition~\ref{prop:out_of_sample_error}.}
Suppose $x \in V_i$, so that we can upper bound the pointwise squared error $|\wh{f}(x) - f(x)|^2$ using the triangle inequality:
\begin{equation*}
\bigl(\wh{f}(x) - f_0(x)\bigr)^2 = \bigl(\wh{f}(X_i) - f_0(x)\bigr)^2 \leq 2\bigl(\wh{f}(X_i) - f_0(X_i)\bigr)^2 + 2\bigl(f_0(X_i) - f_0(x)\bigr)^2
\end{equation*}
Integrating both sides of the inequality, we have
\begin{align*}
\int_{\Xset} \bigl(\wh{f}(x) - f_0(x)\bigr)^2 \,dx & \leq 2  \sum_{i = 1}^{n} \int_{V_i} \Bigl(\wh{f}(X_i) - f_0(X_i)\Bigr)^2 \,dx + 2 \sum_{i = 1}^{n} \int_{V_i} \Bigl(f_0(X_i) - f_0(x)\Bigr)^2 \dx \\
& = 2 \sum_{i = 1}^{n} \vol(V_i) \Bigl(\wh{f}(X_i) - f_0(X_i)\Bigr)^2 + 2 \sum_{i = 1}^{n} \int_{V_i} \Bigl(f_0(X_i) - f_0(x)\Bigr)^2 \dx
\end{align*}
and so by invoking the Lipschitz property of $f_0$, we obtain
\begin{equation}
\label{pf:out_of_sample_error_0}
\|\wh{f} - f\|_{\Leb^2(\Xset)}^2 \leq 2 \sum_{i = 1}^{n} \vol(V_i) \Bigl(\wh{f}(X_i) - f_0(X_i)\Bigr)^2 + 2 \sum_{i = 1}^{n} \Bigl(\mathrm{diam}(V_i)\Bigr)^2
\end{equation}
where we write $\mathrm{diam}(V)$ for the diameter of a set $V$. 

Now we will use some results of \citep{chaudhuri2010} regarding uniform concentration of empirical counts. Letting
\begin{equation*}
\varepsilon_n := \biggl(\frac{2C_{o}\log(1/\delta)d\log n}{\nu_dp_{\min}a_0n}\biggr)^{1/d}
\end{equation*}
we have that for every $x \in \Xset$, $P(B(x,\varepsilon_n)) \geq 2C_{o}\log(1/\delta)d\frac{\log n}{n}$. Consequently, by Lemma~16 of \citep{chaudhuri2010} it holds that with probability at least $1 - \delta$,
\begin{equation}
\label{pf:out_of_sample_error_1}
\textrm{for all $x \in \mc{X}$},~~ B(x,\varepsilon) \cap \{X_1,\ldots,X_n\} \neq \emptyset.
\end{equation}
But if~\eqref{pf:out_of_sample_error_1} is true, it must also hold that for each $i = 1,\ldots,n$ and for every $x \in V_i$, the distance $\|x - X_i\| \leq \varepsilon_n$, and so
\begin{equation*}
\max_{i = 1,\ldots,n} \mathrm{diam}(V_i) \leq 2\varepsilon_n.
\end{equation*}
Plugging back in to~\eqref{pf:out_of_sample_error_0}, and using the upper bound volume $\vol(V_i) \leq \nu_d \bigl(\mathrm{diam}(V_i)\bigr)^d$, we obtain the desired upper bound on $\|\wh{f} - f\|_{\Leb^2(\Xset)}^2$.

\textcolor{red}{(TODO)}: Why do we have the lower bound on $P(B(x,\varepsilon_n))$?.


\section{Concentration Inequalities}
\label{sec:concentration}
\begin{lemma}
	\label{lem:chi_square_bound}
	Let $\xi_1,\ldots,\xi_N$ be independent $N(0,1)$ random variables, and let $U := \sum_{k = 1}^{N} a_k(\xi_k^2 - 1)$.  Then for any $t > 0$,
	\begin{equation*}
	\Pbb\Bigl[U \geq 2 \norm{a}_2 \sqrt{t} + 2 \norm{a}_{\infty}t\Bigr] \leq \exp(-t).
	\end{equation*}
	In particular if $a_k = 1$ for each $k = 1,\ldots,N$, then
	\begin{equation*}
	\Pbb\Bigl[U - N \geq 2\sqrt{N t} + 2t\Bigr] \leq \exp(-t).
	\end{equation*}
\end{lemma}

The proof of Lemma~\ref{lem:empirical_norm_sobolev} relies on (a variant of) the Paley-Zygmund Inequality.
\begin{lemma}
	\label{lem:paley_zygmund}
	Let $f$ satisfy the following moment inequality for some $b \geq 1$:
	\begin{equation}
	\label{eqn:paley_zygmund_1}
	\Ebb\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot\Bigl(\Ebb\bigl[\norm{f}_n^2\bigr]\Bigr)^2.
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:paley_zygmund_2}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}.
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. The Paley-Zygmund inequality says that for all $0 \leq \lambda \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund_pf1}
	\mathbb{P}(Z > \lambda \mathbb{E}(Z^p)) \geq \left[(1 - \lambda^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
	Applying~\eqref{eqn:paley_zygmund_pf1} with $Z = \norm{f}_n^2$, $p = 1$, $q = 2$ and $\lambda = \frac{1}{b}$, by assumption~\eqref{eqn:paley_zygmund_1} we have
	\begin{equation*}
	\mathbb{P}\Bigl(\norm{f}_n^2 > \frac{1}{b} \mathbb{E}[\norm{f}_n^2]\Bigr) \geq \Bigl(1 - \frac{1}{b}\Bigr)^2 \cdot  \frac{\bigl(\mathbb{E}[\norm{f}_n^2]\bigr)^2}{\mathbb{E}[\norm{f}_n^4]} \geq \frac{\Bigl(1 - \frac{2}{b}\Bigr)}{\Bigl(1 + \frac{1}{b^2}\Bigr)} \geq 1 - \frac{5}{b}.
	\end{equation*}
\end{proof}

Let $Z_1,\ldots,Z_n$ be independently distributed and bounded random variables, such that $\Ebb[Z_i] = \mu_i$. Let $S_n = Z_1 + \ldots + Z_n$ and $\mu = \mu_1 + \ldots + \mu_n$. The multiplicative form of Hoeffding's inequality gives sharp bounds when $\mu \ll 1$. 
\begin{lemma}[Hoeffding's Inequality, multiplicative form]
	\label{lem:hoeffding}
	Suppose $Z_i$ are independent random variables, which satisfy $Z_i \in [0,B]$ for $i = 1,\ldots,n$. For any $0 < \delta < 1$, it holds that
	\begin{equation*}
	\Pbb\biggl(\Bigl|S_n - \mu\Bigr| \geq \delta \mu\biggr) \leq 2\exp\biggl(-\frac{\delta^2\mu}{3B^2}\biggr)
	\end{equation*}
\end{lemma}
We use Lemma~\ref{lem:hoeffding}, along with properties of the kernel $K$ and density $p$, to upper bound the maximum degree in our neighborhood graph, which we denote by $D_{\max}(G_{n,r}) := \max_{i = 1,\ldots,n} D_{ii}$.
\begin{lemma}
	\label{lem:max_degree}
	Under the conditions of Lemma~\ref{lem:neighborhood_eigenvalue},
	\begin{equation*}
	D_{\max}(G_{n,r}) \leq 2p_{\max}nr^d,
	\end{equation*}
	with probability at least $1 - 2n\exp\Bigl(-nr^da_0p_{\min}/(3[K(0)]^2)\Bigr)$. 
\end{lemma}
\paragraph{Proof of Lemma~\ref{lem:max_degree}.}
Fix $x \in \Xset$, and set
\begin{equation*}
D_{n,r}(x) :=  \sum_{i = 1}^{n} K\biggl(\frac{\|X_i - x\|}{r}\biggr);
\end{equation*}
note that $D_{n,r}(X_i)$ is just the degree of $X_i$ in $G_{n,r}$. By Hoeffding's inequality
\begin{equation}
\label{pf:max_degree_1}
\Pbb\biggl(\Bigl|D_{n,r}(x) - \Ebb\bigl[D_{n,r}(x)\bigr]\Bigr| \geq \delta \Ebb\bigl[D_{n,r}(x)\bigr]\biggr) \leq 2\exp\biggl(-\frac{\delta^2\Ebb\bigl[D_{n,r}(x)\bigr]}{3[K(0)]^2}\biggr)
\end{equation}

Now we lower bound $\Ebb[D_{n,r}(x)]$ using the boundedness of the density $p$, and the fact that $\Xset$ has Lipschitz boundary:
\begin{align*}
\Ebb\bigl[D_{n,r}(x)\bigr] & = n \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x) \,dx \\
& \geq n p_{\min} \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr) \,dx \\
& =  nr^d p_{\min}\int_{\mc{B}_x} K\bigl(\|z\|\bigr) \,dz \\
& \geq nr^d a_0 p_{\min}
\end{align*}
with the final inequality following from~\eqref{eqn:lb_expected_degree}, and the normalization $\int_{\Rd} K(\|z\|) \,dz = 1$. Similar derivations yield the upper bound
\begin{equation*}
\Ebb\bigl[D_{n,r}(x)\bigr] \leq nr^{d} p_{\max}
\end{equation*} 
and plugging these bounds in to~\eqref{pf:max_degree_1}, we determine that
\begin{equation*}
\Pbb\biggl(D_{n,r}(x) \geq (1 + \delta) nr^d p_{\max}\biggr) \leq 2\exp\biggl(-\frac{\delta^2nr^da_0p_{\min}}{3[K(0)]^2}\biggr),
\end{equation*}
and applying a union bound, we get that
\begin{equation*}
\Pbb\biggl(\max_{i = 1,\ldots,n}D_{n,r}(X_i) \geq (1 + \delta) nr^d p_{\max}\biggr) \leq 2n\exp\biggl(-\frac{\delta^2nr^da_0p_{\min}}{3[K(0)]^2}\biggr);
\end{equation*}
taking $\delta = 1$ gives the claimed upper bound.

\clearpage

\bibliographystyle{plainnat}
\bibliography{../../../graph_testing_bibliography} 

\end{document}