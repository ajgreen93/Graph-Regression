\documentclass[twoside]{article}
\usepackage{aistats2021}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{tikz}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}
\newcommand{\Lap}{L}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Vset}{\mathcal{V}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

%%% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}


\begin{document}
	
\onecolumn
\aistatstitle{Supplementary materials for: \\
	Minimax Optimal Laplacian Smoothing}

\section{Organization of Supplement}

Our results all follow the same general two-part proof strategy, centered on conditioning. First, we establish (estimation or testing) error bounds which hold for any graph $G$; these bounds are stated with respect to functionals of the graph $G$, and allow us to upper bound the error of $\wh{f}_{\LS}$ and $T_{\LS}$ conditional on $\mathbf{X} = x$. Second, we analyze the behavior of these functionals with respect to the particular neighborhood graph $G_{n,r}$ and give high probability (upper or lower) bounds. It is in this second step where we invoke our various assumptions on the distribution $P$ and regression function $f_0$.

\section{Graph-dependent error bounds}
\label{sec:fixed_graph_error_bounds}

Suppose we observe a fixed graph $G = \bigl([n],W\bigr)$ with Laplacian $\Lap_G$ and responses
\begin{equation}
\label{eqn:fixed_graph_regression_model}
Y_i = f_{0,i} + \varepsilon_i
\end{equation}
where $f_0 = (f_{0,1},\ldots,f_{0,n}) \in \Reals^n$, and the noise variables $\varepsilon_i$ are independent $N(0,1)$. The Laplacian smoothing estimator of $f_0$ on $G$ is
\begin{equation}
\label{eqn:ls_G}
\wh{f}_{\LS}(G) := \argmin_{f \in \Reals^n} \biggl\{ \sum_{i = 1}^{n}(Y_i - f_i)^2 + \rho \cdot f^T \Lap_G^{s}  f \biggr\} = (\rho \Lap_G^s + \Id)^{-1}Y.
\end{equation}
The statistic
\begin{equation}
\label{eqn:ls_ts_G}
T_{\LS}(G) := \frac{1}{n} \Bigl\|\wh{f}_{\LS}(G)\Bigr\|_2^2 
\end{equation}
is used to distinguish
\begin{equation*}
\mathbf{H}_0: f_{0} = (0,...,0) ~~\textrm{versus}~~ \mathbf{H}_a: f_{0} \neq (0,...,0).
\end{equation*}
We note that~\eqref{eqn:laplacian_smoothing} and \eqref{eqn:laplacian_smoothing_test} specialize~\eqref{eqn:ls_G} and \eqref{eqn:ls_ts_G} to the case where $G = G_{n,r}$ and $s = 1$.

\subsection{Error bounds for linear smoothers}
Let $S \in \Reals^{n \times n}$ be a square, symmetric matrix, and let $\check{f} = SY$ be a linear estimator of $f_0$. Clearly \eqref{eqn:ls_G} can be written in this form, and Lemma~\ref{lem:linear_smoother_fixed_graph_estimation} is therefore useful in analyzing it. Let $\lambdavec(S) = (\lambda_1(S),\ldots,\lambda_n(S)) \in \Reals^n$ denote the eigenvalues of $S$ and let $v_k(S)$ denote the corresponding \emph{unit-norm} eigenvectors, so that $S = \sum_{k = 1}^{n} \lambda_k(S) \cdot v_k(S) v_k(S)^T$. Denote $Z_k = v_k(S)^T \varepsilon$, and observe that $Z = (Z_1,\ldots,Z_n) \sim N(0,\Id)$. 

\begin{lemma}
	\label{lem:linear_smoother_fixed_graph_estimation}
	Let $\check{f} = SY$ for a square, symmetric matrix, $S \in \Reals^{n \times n}$ satisfying $\lambda_n(S) \leq 1$. Then
	\begin{equation*}
	\Pbb_{f_0}\biggl(\frac{1}{n}\bigl\|\check{f} - f_0\bigr\|_2^2 \geq \frac{10}{n} \bigl\|\lambdavec(S)\bigr\|_2^2 + \frac{2}{n}\bigl\|(S - I)f_0\bigr\|_2^2\biggr) \leq 1 - \exp\Bigl(-\bigl\|\lambdavec(S)\bigr\|_2^2\Bigr)
	\end{equation*}
\end{lemma}

On the testing side, the functional $\norm{\check{f}}_2^2 = Y^T S^2 Y$ is a $U$-statistic of order $2$. The statistic $T_{\LS}(G)$ can be written in this form, and Lemma~\ref{lem:linear_smoother_fixed_graph_testing} is therefore useful in analyzing it.
\begin{lemma}
	\label{lem:linear_smoother_fixed_graph_testing}
	Let $T = Y^T S^2 Y$ for a square, symmetric matrix $S \in \Reals^{n \times n}$. Define the threshold $t_b$ to be 
	\begin{equation}
	t_{b} := \norm{\lambdavec(S)}_2^2 + 2b \norm{\lambdavec(S)}_4^2
	\end{equation}
	Suppose the eigenvalues $0 \leq \lambda_{1}(S) \leq \lambda_{n}(S) \leq 1$. Then,
	\begin{itemize}
		\item \textbf{Type I error.}
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_typeI}
		\Pbb_0\bigl(T > t_b\bigr) \leq \frac{1}{b^2}
		\end{equation}
		\item \textbf{Type II error.} Suppose further that
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_critical_radius}
		f_0^T S^2 f_0 \geq 4b \norm{\lambdavec(S)}_4^2.
		\end{equation}
		Then
		\begin{equation}
		\label{eqn:linear_smoother_fixed_graph_testing_typeII}
		\Pbb_{f_0}\bigl(T \leq t_b\bigr) \leq \frac{4}{b^2} + \frac{8}{b \norm{\lambdavec(S)}_4^{2}} 
		\end{equation}
	\end{itemize}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:linear_smoother_fixed_graph_estimation}.}
It holds that
\begin{align*}
\frac{1}{n}\bigl\|\check{f} - f_0\bigr\|_2^2 & \leq \frac{2}{n}\Bigl(\bigl\|\check{f} - \Ebb_{f_0}[\check{f}]\bigr\|_2^2 + \bigl\|\Ebb_{f_0}[\check{f}] - f_0\bigr\|_2^2\Bigr) \\ 
& = \frac{2}{n}\Bigl(\bigl\|S\varepsilon\bigr\|_2^2 + \bigl\|(S - I)f_0\bigr\|_2^2\Bigr)
\end{align*}
Writing $\norm{S\varepsilon}_2^2 = \sum_{k = 1}^{n} \lambda_k(S)^2 Z_k^2$, the claim follows from the result of \cite{laurent00} on concentration of $\chi^2$-random variables, which for completeness we restate in Lemma~\ref{lem:chi_square_bound}. To be explicit, taking $t = \norm{\lambdavec(S)}_2^2$ in Lemma~\ref{lem:chi_square_bound} completes the proof of Lemma~\ref{lem:linear_smoother_fixed_graph_estimation}. 

\paragraph{Proof of Lemma~\ref{lem:linear_smoother_fixed_graph_testing}.}
We compute the mean and variance of $T$ as a function of $f_0$, then apply Chebyshev's inequality.

\textit{Mean.} Writing $Y = f_0 + \varepsilon$, we make use of the eigendecomposition of $S$ to obtain
\begin{equation}
\label{pf:linear_smoother_fixed_graph_testing1}
\begin{aligned}
T & = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \varepsilon^T S^2 \varepsilon \\
& = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \sum_{k = 1}^{n}  \bigl(\lambda_k(S)\bigr)^2 (\varepsilon^T v_k(S))^2 \\
& = f_0^T S^2 f_0 + 2 f_0^T S^2 \varepsilon + \sum_{k = 1}^{n}  \bigl(\lambda_k(S)\bigr)^2 Z_k^2,
\end{aligned}
\end{equation}
implying
\begin{equation}
\label{pf:linear_smoother_fixed_graph_testing_mean}
\Ebb_{f_0}[T] = f_0^T S^2 f_0 + \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^2.
\end{equation}

\textit{Variance.} Starting from~\eqref{pf:linear_smoother_fixed_graph_testing1} and recalling the basic fact $\Var(Z_k^2) = 2$, we derive
\begin{equation}
\label{pf:linear_smoother_fixed_graph_testing_var}
\Var_{f_0}[T] \leq 8 f_0^T S^4 f_0 + 4 \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^4 \leq 8 f_0^T S^2 f_0 + 4 \sum_{k = 1}^{n} \bigl(\lambda_k(S)\bigr)^2
\end{equation}
where the second inequality follows since by assumption $\lambda_{n}(S) \leq 1$.

\textit{Bounding Type I and Type II error.} The bound~\eqref{eqn:linear_smoother_fixed_graph_testing_typeI} follows directly from Chebyshev's inequality, along with our above calculations on the mean and variance of $T$.

The bound~\eqref{eqn:linear_smoother_fixed_graph_testing_typeII} also follows from Chebyshev's inequality, as can be seen by the following manipulations,
\begin{equation*}
\begin{aligned}
\Pbb_{f_0}\bigl(T \leq t_b\bigr) & = \Pbb_{f_0}\bigl(T - \Ebb_{f_0}[T] \leq t_b - \Ebb_{f_0}[T]\bigr) \\
& \overset{(i)}{\leq} \Pbb_{f_0}\bigl(\abs{T - \Ebb_{f_0}[T]} \geq \abs{t_b - \Ebb_{f_0}[T]}\bigr) \\ 
& \overset{(ii)}{\leq} 4 \frac{\Var_{f_0}[T]}{(f_0^T S^2 f_0)^2} \\
& \overset{(iii)}{\leq} \frac{32}{f_0^T S^2 f_0} + \frac{4}{b^2} \\
& \overset{(iv)}{\leq} \frac{8}{b} \Biggl(\sum_{k = 1}^{n}\bigl(\lambda_k(S)\bigr)^4\Biggr)^{-1/2} + \frac{4}{b^2}
\end{aligned}
\end{equation*}
In the previous expression, $(i)$ and $(ii)$ follow since assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} and equation~\eqref{pf:linear_smoother_fixed_graph_testing_mean} together imply $\Ebb_{f_0}(T) - \frac{1}{2}f_0^T S^2f_0 \geq t_b$, $(iii)$ follows from assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} and the inequality~\eqref{pf:linear_smoother_fixed_graph_testing_var}, and $(iv)$ follows assumption~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius}. 

\subsection{Analysis of Laplacian Smoothing}
In Lemma~\ref{lem:ls_fixed_graph_estimation}, we upper bound the mean squared error of $\wh{f}_{\LS}(G)$.
\begin{lemma}
	\label{lem:ls_fixed_graph_estimation}
	For any $\rho,s > 0$,
	\begin{equation}
	\label{eqn:ls_fixed_graph_estimation_prob}
	\frac{1}{n}\bigl\|\wh{f}_{\LS}(G) - f_0\bigr\|_2^2 \leq \frac{2\rho}{n} \bigl(f_0^T \Lap_{G}^s f_0\bigr) + \frac{10}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G)^s + 1\bigr)^2}
	\end{equation}
	with probability at least $1 - \exp\Bigl(-\sum_{k = 1}^{n}\bigl(\rho \lambda_k(G)^s + 1\bigr)^{-2}\Bigr)$.
\end{lemma}

In Lemma~\ref{lem:ls_fixed_graph_testing}., we upper bound the testing error of a test based on $T_{\LS}(G)$.
\begin{lemma}
	\label{lem:ls_fixed_graph_testing}
	Define the threshold $\wh{t}_b$ to be
	\begin{equation*}
	\wh{t}_b := \frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k(G)^s + 1\bigr)^2} + \frac{2b}{n}\sqrt{\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k(G)^s + 1\bigr)^4}}
	\end{equation*}
	Then each of the following holds for any $\rho,s > 0$ and any $b \geq 1$.
	\begin{itemize}
		\item \textbf{Type I error.}
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_typeI}
		\Pbb_0\Bigl(T_{\LS}(G) > \wh{t}_b\Bigr) \leq \frac{1}{b^2}
		\end{equation}
		\item \textbf{Type II error.} If
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_critical_radius}
		\frac{1}{n}\norm{f_0}_2^2 \geq \frac{2 \rho}{n} \bigl(f_0^T \Lap_G^s f_0\bigr) + \frac{4b}{n} \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \Biggr)^{1/2}
		\end{equation}
		then
		\begin{equation}
		\label{eqn:ls_fixed_graph_testing_typeII}
		\Pbb_{f_0}\Bigl(T_{\LS}(G) \leq \wh{t}_b\Bigr) \leq \frac{4}{b^2} + \frac{8}{b} \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \Biggr)^{-1/2}
		\end{equation}
	\end{itemize}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:ls_fixed_graph_estimation}.}
Letting $\wh{S} = (\Id + \rho \Lap_G^s)^{-1}$, the estimator $\wh{f}_{\LS}(G) = \wh{S}Y$. The matrix $\wh{S} \in \Reals^{n \times n}$ is symmetric, and satisfies $\lambda_k(\wh{S}) \in (0,1)$ for all $k \in [n]$. The claim then follows from Lemma~\ref{lem:linear_smoother_fixed_graph_estimation} upon noting first that
\begin{equation*}
\bigl\|\lambdavec(\wh{S})\bigr\|_2^2 = \sum_{k = 1}^{n} \frac{1}{\bigl(1 + \rho \lambda_k(G)^s\bigr)^2}
\end{equation*} 
and second that
\begin{equation*}
\begin{aligned}
\bigl\|(\wh{S} - I)f_0\bigr\|_2^2 & = f_0^T \Lap_G^{s/2} \Lap_G^{-s/2}\bigl(\wh{S} - \Id\bigr) \Lap_G^{-s/2} \Lap_G^{s/2} f_0 \\
& \leq f_0^T \Lap_G^s f_0 \cdot \lambda_n\Bigl(\Lap_G^{-s/2}\bigl(\wh{S} - \Id\bigr)\Lap_G^{-s/2}\Bigr) \\
& = f_0^T \Lap_G^s f_0 \cdot \max_{k \in [n]} \biggl\{\frac{1}{\lambda_k(G)^s} \Bigl(1 - \frac{1}{\rho\lambda_k(G)^s + 1}\Bigr) \biggr\} \\
& = f_0^T \Lap_G^s f_0 \cdot \rho.
\end{aligned}
\end{equation*} 
where we write $\Lap_G^{-1}$ for the pseudoinverse of $\Lap_G$.

\paragraph{Proof of Lemma~\ref{lem:ls_fixed_graph_testing}.}
Let $\wh{S} := (\rho \Lap^s + \Id)^{-1}$. The matrix $\wh{S} \in \Reals^{n \times n}$ is symmetric and positive semidefinite, and our test statistic $T_{\LS}(G) = \frac{1}{n}Y^T \wh{S}^2 Y$. The desired result thus follows from Lemma~\ref{lem:linear_smoother_fixed_graph_testing}. To see that the conditions of this Lemma are satisfied, we first note that since
\begin{equation*}
\lambda_k(\wh{S}) = \frac{1}{(\rho\lambda_k(G)^s + 1)}
\end{equation*}
and $\rho, \lambda_k(G) > 0$, it is evident that $\lambda_{n}(\wh{S}) \leq 1$.  Then, by assumption~\eqref{eqn:ls_fixed_graph_testing_critical_radius}
\begin{equation*}
f_0^{T} \wh{S}^2 f_0 = \norm{f_0}_2^2 - f_0^T(I - \wh{S}^2)f_0 \geq 2 \rho (f_0^T \Lap^s f_0) + f_0^T(I - \wh{S}^2)f_0 + 4b \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \Biggr)^{-1/2},
\end{equation*}
and along with the following calculations,
\begin{equation*}
\begin{aligned}
f_0^T \Bigl(\Id - \wh{S}^2\Bigr) f_0  & = f_0^T L^{s/2} L^{-s/2}\Bigl(\Id - \wh{S}^2\Bigr) L^{-s/2} L^{s/2} f_0 \\ 
& \leq f_0^T L^{s} f_0 \cdot  \lambda_{\max}\biggl(L^{-s/2}\Bigl(\Id - \wh{S}^2\Bigr) L^{-s/2}\biggr) \\ 
& \overset{(i)}{=}  f_0^T L^{s} f_0 \cdot \max_{k} \biggl\{ \frac{1}{\lambda_k(G)^s} \Bigl(1 - \frac{1}{(\rho \lambda_k(G)^s + 1)^2}\Bigr) \biggr\} \\
& \overset{(ii)}{\leq} f_0^T L^{s} f_0 \cdot 2\rho,
\end{aligned}
\end{equation*}
we have that
\begin{equation*}
f_0^{T} \wh{S}^2 f_0 \geq 2b \biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G)^s + 1)^4} \biggr)^{-1/2}.
\end{equation*} 
In other words condition~\eqref{eqn:linear_smoother_fixed_graph_testing_critical_radius} in Lemma~\ref{lem:linear_smoother_fixed_graph_testing} is met, and applying that Lemma completes the proof.

(In the previous derivation: in $(i)$ the maximum is over all indices $k$ such that the eigenvalue $\lambda_k(G)$ is strictly positive; and $(ii)$ follows from the basic algebraic identity $1 - 1/(1 + \rho x)^2 \leq 2 \rho x$ for any $x, \rho > 0$.

\section{Neighborhood graph Sobolev semi-norm}
\label{sec:graph_sobolev_seminorm}
In this section, we prove Lemma~\ref{lem:graph_sobolev_seminorm}. To do so, we will show that for any $f \in H^1(\Xset)$,
\begin{equation*}
\Ebb\Bigl[f^T \Lap_{n,r} f\Bigr] \leq p_{\max}^2 \sigma_K n^2 r^{d + 2} |f|_{H^1(\Xset)}^2
\end{equation*}
whence the claim follows immediately by Markov's inequality (recall that $\Lap_{n,r}$ is positive semi-definite, and therefore $f^T \Lap_{n,r} f$ is a non-negative random variable).

Since
\begin{equation*}
f^T \Lap_{n,r} f = \frac{1}{2}\sum_{i, j = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr)^2 \mathbf{W}_{ij},
\end{equation*}
it follows that
\begin{equation}
\label{pf:first_order_graph_sobolev_seminorm_1}
\Ebb\Bigl[f^T \Lap_{n,r} f\Bigr] = \frac{n(n - 1)}{2} \Ebb\biggl[\Bigl(f(X') - f(X)\Bigr)^2 K\biggl(\frac{\|X' - X\|}{r}\biggr)\biggr]
\end{equation}
where $X$ and $X'$ are random variables independently drawn from $P$. 

For the remainder of this proof, we will assume that $f \in C^{\infty}(\Xset)$, which we may do without loss of generality because $C^{\infty}(\Xset)$ is dense in $H^1(\Xset)$ and the expectation on the right hand side of~\eqref{pf:first_order_graph_sobolev_seminorm_1} is continuous in $\Leb^2(\Xset)$. Obviously
\begin{equation}
\Ebb\biggl[\bigl(f(X') - f(X)\bigr)^2K\biggl(\frac{\|X' - X\|}{r}\biggr)\biggr] \leq p_{\max}^2 \int_{\Xset} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 K_r(x',x) \,dx' \,dx. \label{pf:first_order_graph_sobolev_seminorm_2}
\end{equation}
and it remains now to bound the integral. Taking a first-order Taylor expansion of $f$, we get
\begin{align}
\int_{\Xset} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 K_r(x',x) \,dx' \,dx & = \int_{\Xset} \int_{\Xset} \biggl[\int_{0}^{1} \nabla f\bigl(x + t(x' - x)\bigr)^T (x' - x)\,dt\biggr]^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) \,dx' \,dx \nonumber \\
& \overset{(i)}{\leq} \int_{\Xset} \int_{\Xset} \int_{0}^{1} \biggl[\nabla f\bigl(x + t(x' - x)\bigr)^T (x' - x)\biggr]^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) \,dt\,dx' \,dx \nonumber \\
& \overset{(ii)}{=} r^{d + 2} \int_{\Xset} \int_{\mc{Z}_r(x)} \int_{0}^{1} \biggl[\nabla f\bigl(x + trz\bigr)^T z\biggr]^2 K\bigl(\|z\|\bigr) \,dt\,dz \,dx \\
&  \overset{(iii)}{\leq} r^{d + 2} \int_{\Xset} \int_{\Reals^d} \int_{0}^{1} \Bigl[\nabla f\bigl(\wt{x}\bigr)^T z\Bigr]^2 K\bigl(\|z\|\bigr) \,dt \,dz \,d\wt{x} \nonumber \\
& = r^{d + 2} \int_{\Xset} \int_{\Reals^d} \Bigl[\nabla f\bigl(\wt{x}\bigr)^T z\Bigr]^2 K\bigl(\|z\|\bigr) \,dz \,d\wt{x} \label{pf:first_order_graph_sobolev_seminorm_3}
\end{align}
where $(i)$ follows by Jensen's inequality, $(ii)$ follows by substituting $z = (x' - x)/r$ with $\mc{Z}_r(x) := \{z: zr + x \in \Xset\}$ and $(iii)$ by substituting $\wt{x} = x + trz$ and noting that
\begin{equation*}
x \in \Xset ~~\textrm{and}~~ z \in \mc{Z}_r(x) \Longrightarrow x + trz \in \Xset ~~\textrm{and}~~ z \in \Reals^d.
\end{equation*}
Now, writing $\bigl(\nabla f(\wt{x}) ^T z\bigr)^2 = \bigl(\sum_{i = 1}^{d} z_{i} f^{(e_i)}(x) \bigr)^2$, expanding the square and integrating, we have that for any $\wt{x} \in \Xset$,
\begin{align*}
\int_{\Reals^d} \Bigl[\nabla f\bigl(\wt{x}\bigr)^T z\Bigr]^2 K\bigl(\|z\|\bigr) \,dz & = \sum_{i,j = 1}^{d} f^{(e_i)}(\wt{x}) f^{(e_j)}(\wt{x}) \int_{\Rd} z_iz_jK(\|z\|) \,dz \\
& = \sum_{i = 1}^{d} \bigl(f^{(e_i)}(\wt{x})\bigr)^2 \int_{\Rd} z_i^2 K\bigl(\|z\|\bigr) \,dz \\
& = \sigma_K \|\nabla f(\wt{x})\|^2
\end{align*}
and by plugging back into~\eqref{pf:first_order_graph_sobolev_seminorm_3}, we obtain the claimed result. 

\section{Bounds on neighborhood graph eigenvalues}
\label{sec:graph_eigenvalues}
In this section, we prove our claimed results regarding the eigenvalues $\lambda_k(G_{n,r})$. We begin with Corollary~\ref{cor:neighborhood_eigenvalue}, since its proof is straightforward, and then turn to the proof of Lemma~\ref{lem:neighborhood_eigenvalue}, which is more involved.

\subsection{Proof of Corollary~\ref{cor:neighborhood_eigenvalue}}
Put
\begin{equation*}
\ell_{\star} = \floor{\frac{\bigl(1/2 - A_1(\theta + \wt{\delta})\bigr)^d}{C_2^d A_1^d r^d}};
\end{equation*}
and note that for $\ell_{\star}$ the condition~\eqref{eqn:neighborhood_eigenvalue_1} is met, i.e.
\begin{equation*}
1 - A_1\Biggl(r \sqrt{\lambda_{\ell}(\Delta_P)} + (\theta + \wt{\delta})\Biggr)  \geq \frac{1}{2}.
\end{equation*}
We may therefore apply the conclusions of Lemma~\ref{lem:neighborhood_eigenvalue}, which along with~\eqref{eqn:weyls_law} imply that
\begin{equation*}
\frac{c_2}{A_1} nr^{d + 2} k^{2/d} \leq \lambda_k(G_{n,r}) \leq \frac{C_2}{a_1} nr^{d + 2} k^{2/d}~~\textrm{for all $2 \leq k \leq \ell_{\star}$}
\end{equation*}
On the other hand, if $k > \ell^{\star}$,...

\textcolor{red}{(TODO)}: Finish off the proof.

\subsection{Proof of Lemma~\ref{lem:neighborhood_eigenvalue}}

In this section we prove Lemma~\ref{lem:neighborhood_eigenvalue}, closely following the lead of~\citep{burago2014,trillos2019,calder2019}. As in these works, we relate $\lambda_k(\Delta_P)$ and $\lambda_k(G_{n,r})$ by means of the Sobolev semi-norms (Dirichlet energies)
\begin{equation*}
b_r(u) := \frac{1}{n^2 r^{d+ 2}}u^T \Lap_{n,r} u~~\textrm{for any $u \in \Leb^2(P_n)$}
\end{equation*}
and
\begin{equation*}
D_2(f) :=
\begin{cases*}
\int_{\Xset} \|\nabla f(x)\|^2 p^2(x) \,dx~~ &\textrm{if $f \in H^1(\Xset)$} \\
\infty~~ & \textrm{otherwise,}
\end{cases*}
\end{equation*}
Let us pause briefly to motivate the relevance of $b_r(u)$ and $D_2(f)$. Suppose one could upper bound $b_r(u)$ by $D_2\bigl(\mc{I}(u)\bigr)$ for an appropriate choice of interpolating map $\mc{I}: \Leb^2(P_n) \to \Leb^2(P)$, and vice versa upper bound $D_2(f)$ by $b_r(\mc{P}(f))$ for an appropriate choice of discretization map $\mc{P}: \Leb^2(P) \to \Leb^2(P_n)$. Suppose further that $\mc{I}$ and $\mc{P}$ were near-isometries, meaning $\|\mc{I}(u)\|_{\Leb^2(P)} \approx \|u\|_{\Leb^2(P_n)}$ and $\|\mc{P}(f)\|_{\Leb^2(P_n)} \approx \|f\|_{\Leb^2(P)}$. Then using the variational characterization of eigenvalues $\lambda_k(\Delta_P)$ and $\lambda_k(G_{n,r})$---i.e. the Courant-Fischer Theorem---one could obtain estimates on the error of $|\lambda_k(\Delta_P) - \lambda_k(G_{n,r})|$, and thereby establish Lemma~\ref{lem:neighborhood_eigenvalue}.

We will momentarily define the maps $\wt{\mc{I}}$ and $\wt{\mc{P}}$, and subsequently prove they satisfy the outlined desiderata. These maps will be defined with respect to a particular probability measure $\wt{P}_n$ that with high probability is close in transportation distance to both $P_n$ and $P$---this probabilistic estimate is the workhorse that allows us to relate the random energy $b_r$ to the non-random $D_2$.  We now review the notion of transportation distance, as well formalize the properties that $\wt{P}_n$ will satisfy.

\paragraph{Transportation distance between $P_n$ and $P$.}
For a map $T$ defined on $\Xset$ and a measure $\mu$, let $T_{\sharp}\mu$ denote the push-forward of $\mu$ by $T$, i.e the measure for which
\begin{equation*}
\bigl(T_{\sharp}\mu\bigr)(U) := \mu\bigl(T^{-1}(U)\bigr)
\end{equation*}
for any Borel subset $U \subseteq \Xset$. Suppose $T_{\sharp}\mu = P_n$; then the map $T$ is referred to as transportation map between $\mu$ and $P_n$. The  $\infty$-transportation distance between $\mu$ and $P_n$ is then
\begin{equation*}
d_{\infty}(\mu,P_n) := \inf_{T: T_{\sharp} \mu = P_n} \biggl\{\sup_{x \in \Xset}~\bigl|T(x) - x\bigr|\biggr\}
\end{equation*}
\cite{calder2019} consider $\Xset$ a smooth manifold without boundary, i.e. they assume $\Xset$ satisfies~\ref{asmp:domain_manifold}. They exhibit an absolutely continuous measure $\wt{P}_n$ with density $\wt{p}_n$ which with high probability is close to $P_n$ in transportation distance, for which $\|p - \wt{p}_n\|_{\Leb^\infty}$ is also small. In Proposition~\ref{prop:optimal_transport}, we adapt this result to  the setting of full-dimensional manifolds with boundary.  
\begin{proposition}[Restatement of Proposition~2.11 of \textcolor{red}{(Calder2019)}]
	\label{prop:optimal_transport}
	Suppose $p$ satisfies~\ref{asmp:bounded_lipschitz_density} and $\Xset$ satisfies~\ref{asmp:domain}. Then with probability at least $1 - A_2 n \exp\bigl\{-a_2 n\theta^2\wt{\delta}^d\bigr\}$, the following statement holds: there exists a probability measure $\wt{P}_n$ with density $\wt{p}_n$ such that:
	\begin{equation*}
	d_{\infty}(\wt{P}_n, P_n) \leq A_2 \wt{\delta}
	\end{equation*}
	and
	\begin{equation*}
	\|\wt{p}_n - p\|_{\infty} \leq A_2\bigl(\wt{\delta} + \theta\bigr)
	\end{equation*}
\end{proposition}
For the rest of this section, we let $\wt{P}_n$ be a probability measure with density $\wt{p}_n$, that satisfies the conclusions of Proposition~\ref{prop:isometry}. Additionally we denote by $\wt{T}_n$ an \emph{optimal transport map} between $\wt{P}_n$ and $P_n$. Finally, we write $U_1,\ldots,U_n$ for the preimages of $X_1,\ldots,X_n$ under $\wt{T}_n$. 

\paragraph{Interpolation and discretization maps.}

The discretization map  $\wt{\mathcal{P}}: \Leb^2(P) \to \Leb^2(P_n)$ is given by
\begin{equation*}
\bigl(\wt{\mathcal{P}}f\bigr)(X_i) := n \cdot \int_{U_i} f(x) \wt{p}_n(x) \,dx.
\end{equation*}
On the other hand, the interpolation map $\wt{\mc{I}}: \Leb^2(P_n) \to \Leb^2(\Xset)$ is defined as $\wt{\mc{I}}u := \Lambda_{r - 2\wt{\delta}}(\wt{\mc{P}}^{\star}u)$. Here, $\wt{\mc{P}}^{\star} = u \circ \wt{T}$ is the adjoint of $\wt{\mc{P}}$, i.e.
\begin{equation*}
\bigl(\wt{\mc{P}}^{\star}u\bigr)(x) = \sum_{j = 1}^{n} u(x_i) \1\{x \in U_i\} 
\end{equation*} 
and $\Lambda_r$ is a smoothening of this extension, formally
\begin{equation*}
\Lambda_r(f) := \frac{1}{r^d\tau(x)}\int_{\Xset} \eta_r(x',x) f(x') \,dx',~~ \eta_r(x',x) := \psi\biggl(\frac{\|x' - x\|}{r}\biggr)
\end{equation*}
where $\psi(t) := (1/\sigma_K)\int_{t}^{\infty} s K(s) \,ds$ and $\tau(x) := (1/r^d)\int_{\Xset} \eta_r(x',x) \,dx'$ is a normalizing constant.

Propositions~\ref{prop:dirichlet_energies} and~\ref{prop:isometry} establish our claims regarding $\wt{\mc{P}}$ and $\wt{\mc{I}}$: first, that they approximately preserve the Dirichlet energies $b_r$ and $D_2$, and second that they are near-isometries for functions $u \in \Leb^2(P_n)$ (or $f \in \Leb^2(P)$) of small Dirichlet energy $b_r(u)$ (or $D_2(f)$).

\begin{proposition}[\textbf{c.f. Proposition 4.1 of \citet{calder2019}}]
	\label{prop:dirichlet_energies}
	With probability at least $1 - A_2n\exp(-a_2n\theta^2\wt{\delta}^{m})$, we have the following.
	\begin{enumerate}[(1)]
		\item For any $u \in \Leb^2(P_n)$,
		\begin{equation*}
		\sigma_{K} D_2(\wt{\mc{I}}u) \leq \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \biggl(1 + A_3\frac{\wt{\delta}}{r}\biggr) b_r(u)
		\end{equation*}
		\item For any $f \in \Leb^2(\Xset)$,
		\begin{equation*}
		b_r(\wt{\mc{P}}f) \leq \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl(1 + L_p(2r + 4\wt{\delta})\Bigr) \cdot \Bigl(1 + A_4\frac{\wt{\delta}}{r}\Bigr) \sigma_{K} D_2(f).
		\end{equation*}
	\end{enumerate}
\end{proposition}

\begin{proposition}[\textbf{c.f. Proposition 4.2 of \citet{calder2019}}]
	\label{prop:isometry}
	With probability at least $1 - A_2n\exp(-a_2n\theta^2\wt{\delta}^{m})$, we have the following.
	\begin{enumerate}[(1)]
		\item For every $f \in \Leb^2(\Xset)$,
		\begin{equation*}
		\Bigl|\|f\|_{\Leb^2(P)}^2 - \|\wt{P}f\|_{\Leb^2(P_n)}^2\Bigr| \leq A_5 \wt{\delta} \|f\|_{\Leb^2(P)} \sqrt{D_2(f)} + \frac{A_1\bigl(\theta + \wt{\delta}\bigr)}{p_{\min}} \|f\|_{\Leb^2(P)}^2
		\end{equation*}
		\item For every $u \in \Leb^2(P_n)$,
		\begin{equation*}
		\Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(P)}^2 - \|u\|_{\Leb^2(P_n)}^2\Bigr| \leq C r \|u\|_{\Leb^2(P_n)} \sqrt{b_r(u)} + C\bigl(\theta + \wt{\delta}\bigr) \|u\|_{\Leb^2(P_n)}^2
		\end{equation*}
	\end{enumerate}
\end{proposition}

Given these propositions, the proof of Lemma~\ref{lem:neighborhood_eigenvalue} follows by the Courant-Fischer Theorem.

\paragraph{Proof of Lemma~\ref{lem:neighborhood_eigenvalue}.}
We start with the upper bound, proceeding exactly as in Proposition 4.4 of \citep{burago2014}. Let $W$ be the span of $f_1,\ldots,f_{k}$, the first $k$ eigenfunctions of $-\Delta_P$, so that by the Courant-Fischer principle $D_2(f) \leq \lambda_k(\Delta_P) \|f\|_{\Leb^2(P)}$ for every $f \in W$. As a result, by Part (1) of Proposition~\ref{prop:isometry} we have that for any $f \in W$
\begin{equation*}
\bigl\|\wt{\mc{P}}f\bigr\|_{\Leb^2(P_n)}^2 \geq \biggl(1 - A_5\wt{\delta} \sqrt{\lambda_{k}(\Delta_P)} - \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr)\|f\|_{\Leb^2(P)}^2  \geq \frac{1}{2} \|f\|_{\Leb^2(P)}^2,
\end{equation*}
where the second inequality follows by assumption~\eqref{eqn:neighborhood_eigenvalue_1}.

Therefore $\wt{\mc{P}}$ is injective over $W$, and $\wt{\mc{P}}W$ has dimension $\ell$. This means we can invoke the Courant-Fischer Theorem, along with Proposition \ref{prop:dirichlet_energies}, and conclude that
\begin{align*}
\lambda_k(G_{n,r}) & \leq \max_{\substack{u \in \wt{\mc{P}}W \\ u \neq 0} } \frac{b_r(u)}{\|u\|_{\Leb^2(P_n)}^2} \\
& = \max_{\substack{f \in W \\ f \neq 0} } \frac{b_r(\wt{\mc{P}}f)}{\bigl\|\wt{\mc{P}}f\bigr\|_{\Leb^2(P_n)}^2} \\
& \leq \frac{\Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl(1 + L_p(2r + 4\wt{\delta})\Bigr) \cdot \Bigl(1 + \frac{2L_K}{K(1/2)}\frac{\wt{\delta}}{r}\Bigr)}{1 - A_5\wt{\delta} \sqrt{\lambda_{\ell}(\Delta_P)} - 2\frac{A_1(\theta + \wt{\delta})}{p_{\min}}} \sigma_K \lambda_{\ell}(\Delta_P),
\end{align*}
establishing the upper bound in~\eqref{eqn:eigenvalue_bound} upon proper choice of $A$.

\paragraph{Organization of this section.}
The rest of this section will be devoted to proving Propositions~\ref{prop:optimal_transport},~\ref{prop:dirichlet_energies} and~\ref{prop:isometry}. The proof of Proposition~\ref{prop:optimal_transport} is given in Subsection~\ref{subsec:proof_proposition_optimal_transport}. To prove the latter two propositions, it will help to introduce the intermediate energies
\begin{equation*}
\wt{E}_r(f,\eta,V) := \frac{1}{r^{d + 2}}\int_{V} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{r}\biggr) \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx
\end{equation*}
and
\begin{equation*}
{E}_r(f,\eta,V) := \frac{1}{r^{d + 2}}\int_{V} \int_{\Xset} \bigl(f(x') - f(x)\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx
\end{equation*}
where $\eta: [0,\infty) \to [0,\infty)$ is an arbitrary kernel, and $V \subseteq \Xset$ is a measurable set. We will abbreviate $\wt{E}_r(f,\eta,\Xset)$ as $\wt{E}_r(f,\eta)$ and $\wt{E}_r(f,K) = \wt{E}_r(f)$ (and likewise with $E_r$.)

In Subsection~\ref{subsec:integrals}, we establish relationships between the (non-random) functionals $E_r(f)$ and $D_2(f)$, as well as providing estimates on some assorted integrals. In Subsection~\ref{subsec:random_functionals}, we establish relationships between the stochastic functionals $\wt{E}_r(f)$ and $E_r(f)$,  between $\wt{E}_r\bigl(\wt{\mc{I}}(u)\bigr)$ and $b_r\bigl(u\bigr)$, and between $\wt{E}_r\bigl(f\bigr)$ and $b_r\bigl(\wt{\mc{P}}f\bigr)$. Finally, in Subsection~\ref{subsec:proof_of_prop_dirichlet_energies_and_isometry} we use these various relationships to prove Propositions~\ref{prop:dirichlet_energies} and~\ref{prop:isometry}.

\subsection{Proof of Proposition~\ref{prop:optimal_transport}}
\label{subsec:proof_proposition_optimal_transport}

We start by defining the density $\wt{p}_n$, which will be piecewise constant over a particular partition $\mc{Q}$ of $\Xset$; specifically if $x \in Q$ for $Q \in \mc{Q}$, then
\begin{equation}
\label{pf:prop_optimal_transport_0}
\wt{p}_n(x) := \frac{P_n(Q)}{\vol(Q)}
\end{equation}
where $\vol(\cdot)$ denotes the full-dimensional Lebesgue measure. 

We now construct the partition $\mc{Q}$, in progressive degrees of generality on the domain $\Xset.$
\begin{itemize}
	\item In the special case of the unit cube $\Xset = (0,1)^d$, the partition will simply be the collection of cubes
	\begin{equation*}
	\set{Q_k: k \in [\wt{\delta}^{-1}]^d} 
	\end{equation*}
	where $Q_k = \wt{\delta}\Bigl([k_1 - 1,k_1] \otimes \cdots \otimes [k_d - 1,k_d]\Bigr)$ and we assume without loss of generality that $\wt{\delta}^{-1} \in \mathbb{N}$.
	\item If $\mc{\Xset}$ is an open, connected set with smooth boundary, then by Proposition 3.2 of \textcolor{red}{GarciaTrillos14}, there exist a finite number $N(\Xset) \in \mathbb{N}$ of disjoint polytopes which cover $\Xset$. Moreover, letting $U_j$ denote the intersection of the $j$th of these polytopes with $\wb{\Xset}$, this proposition establishes that for each $j$ there exists a bi-Lipschitz homeomorphism $\Phi_j: U_j \to [0,1]^d$. We take the collection of
	\begin{equation*}
	\mc{Q} = \biggl\{\Phi_j^{-1}(Q_k): j = 1,\ldots,N(\Xset)~~\textrm{and}~~k \in [\wt{\delta}^{-1}]^d\biggr\}
	\end{equation*}
	to be our partition. Denote by $L_{\Phi}$ the maximum of the bi-Lipschitz constants of $\Phi_1,\ldots,\Phi_{N(\Xset)}$.
	\item Finally, in the general case where $\Xset$ is an open, connected set with Lipschitz boundary, then there exists a bi-Lipschitz homeomorphism $\Psi$ between $\Xset$ and a smooth, open, connected set with Lipschitz boundary. Letting $\Phi_j$ and $\wt{Q}_{j,k}$ be as before, we take the collection
	\begin{equation*}
	\mc{Q} = \biggl\{\wt{Q}_{j,k} = \Bigl(\Psi^{-1} \circ \Phi_j^{-1}\Bigr)(Q_k): j = 1,\ldots,N(\Xset)~~\textrm{and}~~k \in [\wt{\delta}^{-1}]^d\biggr\}
	\end{equation*}
	to be our partition. Denote by $L_{\Psi}$ the bi-Lipschitz constant of $\Psi$.
\end{itemize}
Let us record a few facts which hold for all $\wt{Q}_{j,k} \in \mc{Q}$, which follow from the bi-Lipschitz properties of $\Phi_j$ and $\Psi$: first that
\begin{equation}
\label{pf:prop_optimal_transport_1}
\diam(\wt{Q}_{j,k}) \leq L_{\Psi} \L_{\Phi} \wt{\delta}
\end{equation}
and second that
\begin{equation}
\label{pf:prop_optimal_transport_2}
\vol(\wt{Q}_{j,k}) \geq \biggl(\frac{1}{L_{\Psi} L_{\Phi}}\biggr)^d \wt{\delta}^d.
\end{equation}
We now use these facts to show that $\wt{P}_n$ satisfies the claims of Proposition~\ref{prop:optimal_transport}. One on the one hand, by the construction~\eqref{pf:prop_optimal_transport_0} clearly there exists a transport map $\wt{T}: \Xset \to \mathbf{X}$ for which every $x \in \wt{Q}_{j,k}$ is moved to a design point $X \in {\bf X} \cap \wt{Q}_{j,k}$, and so by~\eqref{pf:prop_optimal_transport_1}
\begin{equation*}
\sup_{x \in \Xset} \abs{\wt{T}(x) - x} \leq  L_{\Psi} \L_{\Phi} \wt{\delta}.
\end{equation*}
On the other hand, applying the triangle inequality we have that for $x \in \wt{Q}_{j,k}$
\begin{align*}
|\wt{p}_n(x) - p(x)| \leq \biggl|\frac{P_n(\wt{Q}_{j,k}) - P(\wt{Q}_{j,k})}{\vol(\wt{Q}_{j,k})}\biggr| + \frac{1}{\vol(\wt{Q}_{j,k})} \int_{\wt{Q}_{j,k}} |p(x') - p(x)| \,dx 
\end{align*}
and using the Lipschitz property of $p$ we find that 
\begin{equation}
\label{pf:prop_optimal_transport_3}
\|\wt{p}_n - p\|_{\Leb^{\infty}} \leq \max_{j,k} \biggl|\frac{P_n(\wt{Q}_{j,k}) - P(\wt{Q}_{j,k})}{\vol(\wt{Q}_{j,k})}\biggr| + L_p L_{\Phi} L_{\Psi} \wt{\delta}
\end{equation}
From Hoeffding's inequality and a union bound, we obtain that 
\begin{align*}
\mathbb{P}\biggl( \bigl|P_n(\wt{Q}) - P(\wt{Q})\bigr| & \leq \theta P(\wt{Q}) \quad \forall \wt{Q} \in \mc{Q} \biggr) \geq 1 - 2 \sharp(\mc{Q}) \cdot \exp\biggl\{-\frac{\theta^2 n \min \{P(\wt{Q})\}}{3}\biggr\} \\
& \geq 1 - \frac{2 N(\Xset)}{\wt{\delta}^d} \cdot \exp\biggl\{-\frac{\theta^2 n p_{\min} \wt{\delta}^d }{3\bigl(L_{\Psi} L_{\Phi}\bigr)^d}\biggr\}.
\end{align*}
Noting that by assumption $P(\wt{Q}) \leq p_{\max} \vol(\wt{Q})$ and $\wt{\delta}^{-d} \leq n$, the claim follows upon plugging back into~\eqref{pf:prop_optimal_transport_3}, and setting
\begin{equation*}
a_1(d, \Xset) := \frac{1}{3\bigl(L_{\Psi} L_{\Phi}\bigr)^d}~~\textrm{and}~~A_1(d,\Xset) := \max \Bigl\{2N(\Xset),L_p L_{\Psi} L_{\Phi} \Bigr\}
\end{equation*}
in the statement of the proposition.


\subsection{Non-random functionals and integrals}
\label{subsec:integrals}

Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected} relates the non-local energy $E_r(f)$ to $D_2(f)$. It follows in large part from the proofs of Lemma 13 of \cite{trillos2019} and Lemma 6 of \cite{burago2014}, with minor modifications to account for the boundary of $\Xset$.
\begin{lemma}[c.f. Lemma 13 of \cite{trillos2019}, Lemma 6 of \cite{burago2014}]
	\label{lem:first_order_graph_sobolev_seminorm_expected}
	For any $f \in \Leb^2(\Xset)$,
	\begin{equation*}
	E_r(f) \leq (1 + L_pr)^2 \cdot \sigma_K D_2(f)
	\end{equation*}
\end{lemma}

In Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}, we establish the reverse of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected}. 
\begin{lemma}[c.f. Lemma 9 of \cite{trillos2019}, Lemma 5.5 of \cite{burago2014}]
	\label{lem:first_order_graph_sobolev_seminorm_expected_lb}
	For any $f \in \Leb^2(\Xset)$,
	\begin{equation*}
	\sigma_KD_2(\Lambda_rf) \leq \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K}E_r(f)
	\end{equation*}
\end{lemma}

To prove Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}, we require upper and lower bounds on $\tau(x)$, as well as an upper bound on the gradient of $\tau$. The lower bound here---$\tau(x) \geq 1/2$---is quite a bit a looser than in the case where $\Xset$ has no boundary---$\tau(x) \geq (1 + Cr^2)^{-1}$. The same is the case regarding the upper bound of the size of the gradient $\|\nabla \tau(x)\|$. However, the bounds as stated here will be sufficient for our purposes.
\begin{lemma}
	\label{lem:tau_bound}
	Suppose $r$ satisfies~\ref{asmp:r_small_3}. For all $x \in \Xset$,
	\begin{equation*}
	\frac{1}{2} \leq \tau(x) \leq 1,
	\end{equation*}
	and additionally $\|\nabla \tau(x)\| \leq \frac{1}{\sigma_K r}$.
\end{lemma}

Finally, to prove part (2) of Proposition~\ref{prop:isometry}, we require Lemma~\ref{lem:smoothening_error}, which gives an estimate on the error $\Lambda_h f - f$ in $\Leb^2(P)$ norm.
\begin{lemma}[c.f Lemma 8 of \cite{trillos2019}, Lemma 5.4 of \cite{burago2014}]
	\label{lem:smoothening_error}
	For any $h > 0$, 
	\begin{equation}
	\label{eqn:smoothening_error_norm}
	\bigl\|\Lambda_hf\bigr\|_{\Leb^2(P)}^2 \leq \frac{2p_{\max}}{p_{\min}} \bigl\|f\bigr\|_{\Leb^2(P)}^2
	\end{equation}
	and
	\begin{equation}
	\label{eqn:smoothening_error_energy}
	\bigl\|\Lambda_hf - f\bigr\|_{\Leb^2(P)}^2 \leq \frac{2}{\sigma_Kp_{\min}} r E_r(f)
	\end{equation}
	for all $f \in \Leb^2(\Xset)$.
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected}.}
Suppose $f \in C^{\infty}(\wb{\Xset})$, which we may do without loss of generality due to the density of $C^{\infty}(\wb{\Xset})$ in  $\Leb^2(\Xset)$.

Taylor expanding $f(x')$ about $x' = x$, applying first Cauchy-Schwarz' and then Jensen's inequality, and then using the Lipschitz property of $p$, we have
\begin{align*}
E_r(f) & = \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} (f(x') - f(x))^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx \\
& = \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \biggl(\sum_{i = 1}^{d} (x' - x)^{e_i} \int_0^1 f^{(e_i)}\bigl(x + t(x' - x)\bigr) \,dt\biggr)^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx \\
& \leq \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \|x' - x\|^2 \sum_{i = 1}^{d}\biggl(\int_0^1 f^{(e_i)}\bigl(x + t(x' - x)\bigr) \,dt\biggr)^2 K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x) \,dx' \,dx \\
& \leq \frac{1}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \int_0^1   \|x' - x\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + t(x' - x)\bigr)\Bigr)^2\biggr\} K\biggl(\frac{\|x' - x\|}{r}\biggr) p(x') p(x)   \,dt   \,dx' \,dx \\
& \leq \frac{(1 + L_pr)^2}{r^{d + 2}}\int_{\Xset} \int_{\Xset} \int_0^1  \|x' - x\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + t(x' - x)\bigr)\Bigr)^2\biggr\} K\biggl(\frac{\|x' - x\|}{r}\biggr) p\bigl(x + t(x' - x)\bigr)^2   \,dt   \,dx' \,dx.
\end{align*} 
Then letting $z = (x' - x)/r$ and $\wt{x} = x + trz$, we obtain
\begin{align*}
E_r(f) & \leq (1 + L_pr)^2 \int_{\Xset} \int_{\mathcal{Z}_r(x)} \int_{0}^{1} \|z\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + trz\bigr)\Bigr)^2\biggr\} K\bigl(\|z\|\bigr) p(x + trz)^2 \,dt \,dz \,dx \\
& = (1 + L_pr)^2 \int_{B(0,1)} \int_{0}^{1}  \int_{\Xset_r(z)} \|z\|^2 \biggl\{\sum_{i = 1}^{d} \Bigl(f^{(e_i)}\bigl(x + trz\bigr)\Bigr)^2\biggr\} K\bigl(\|z\|\bigr) p(x + trz)^2 \,dx \,dt \,dz \\
& \leq (1 + L_pr)^2 \int_{B(0,1)} \int_{0}^{1} \|z\|^2 K\bigl(\|z\|\bigr) \biggl\{\int_{\wt{\Xset}_r(t,z)} \|\nabla f(\wt{x})\|^2 p(\wt{x})^2 \,d\wt{x}\biggr\} \,dt \,dz  \\
& \leq (1 + L_pr)^2 \int_{B(0,1)} \int_{0}^{1} \|z\|^2 K\bigl(\|z\|\bigr) \biggl\{\int_{\Xset} \|\nabla f(\wt{x})\|^2 p(\wt{x})^2 \,d\wt{x}\biggr\} \,dt \,dz \\
& = (1 + L_pr)^2 \int_{B(0,1)} \|z\|^2 K\bigl(\|z\|\bigr) \,dz 
\end{align*}
where $\mathcal{Z}_r(x) = B(0,1) \cap (\Xset - x)/r$, $\Xset_r(z) = \Xset \cap( \Xset - zr)$, $\wt{\Xset}_r(t,z) = \Xset_r(z) + trz$, and we note that since
\begin{align*}
\wt{x} \in \wt{\Xset}_r(t,z) & \Longrightarrow \wt{x} = x + trz ~~ \textrm{for some $x \in \Xset - zr$ and $t \in [0,1]$} \\
& \Longrightarrow \wt{x} \in \Xset,~~ \textrm{(since $x \in \Xset$, $x + rz \in \Xset$ and $\Xset$ is connected)}
\end{align*}
it must be that $\wt{X}_r(t,z) \subseteq \Xset$ for all $z \in B(0,1)$ and $t \in [0,1]$. 

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}.}
For any $a \in \Reals$, $\Lambda_rf$ satisfies the identity
\begin{equation*}
\Lambda_rf(x) = a + \frac{1}{r^d\tau(x)}\int_{\Xset} \eta_r(x',x)\bigl(f(x') - a\bigr)\,dx'
\end{equation*}
and by differentiating we obtain
\begin{equation*}
\bigl(\nabla \Lambda_rf\bigr)(x)= \frac{1}{r^d\tau(x)}\int_{\Xset} \bigl(\nabla \eta_r(x',\cdot)\bigr)(x)\bigl(f(x') - a\bigr)\,dx' + \nabla\bigl(\tau^{-1}\bigr)(x)\cdot \frac{1}{r^d}\int_{\Xset} \eta_r(x',x)\bigl(f(x') - a\bigr)\,dx'
\end{equation*} 
Plugging in $a = f(x)$, we get $\nabla\Lambda_rf(x) = J_1(x)/\tau(x) + J_2(x)$ for
\begin{equation*}
J_1(x) := \frac{1}{r^d}\int_{\Xset} \bigl(\nabla \eta_r(x',\cdot)\bigr)(x)\bigl(f(x') - f(x)\bigr)\,dx',~~ J_2(x) := \nabla\bigl(\tau^{-1}\bigr)(x)\cdot \frac{1}{r^d}\int_{\Xset} \eta_r(x',x)\bigl(f(x') - f(x)\bigr)\,dx'.
\end{equation*}
To upper bound $\bigl\|J_1(x)\bigr\|^2$, we first compute the gradient of $\eta_r(x',\cdot)$,
\begin{align*}
\bigl(\nabla\eta_r(x',\cdot)\bigr)(x) & = \frac{1}{r} \psi'\biggl(\frac{\|x'  - x\|}{r}\biggr) \frac{(x - x')}{\|x' - x\|} \\
& = \frac{1}{\sigma_Kr^2} K\biggl(\frac{\|x' - x\|}{r}\biggr) (x' - x),
\end{align*}
so that by the Cauchy-Schwarz inequality,
\begin{align*}
\bigl\|J_1(x)\bigr\|^2 & = \frac{1}{\sigma_K^2 r^{4 + 2d}} \Biggl[\int_{\Xset} \bigl(f(x') - f(x)\bigr)K\biggl(\frac{\|x' - x\|}{r}\biggr)(x' - x)\Biggr]^2 \,dx' \\
& \leq \frac{1}{\sigma_K^2r^{4 + 2d}} \biggl[\int_{\Xset}K\biggl(\frac{\|x' - x\|}{r}\biggr)\|x' - x\|^2\,dx'\biggr] \biggl[\int_{\Xset}K\biggl(\frac{\|x' - x\|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2\,dx'\biggr] \\
& \leq \frac{1}{\sigma_K r^{2 + d}}\int_{\Xset}K\biggl(\frac{\|x' - x\|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2\,dx'.
\end{align*}
To upper bound $\bigl\|J_2(x)\bigr\|^2$, we use the Cauchy-Schwarz inequality along with the observation $\eta_r(x',x) \leq \frac{1}{\sigma_K} K\bigl(\|x' - x\|/r\bigr)$ to deduce
\begin{align*}
\bigl\|J_2(x)\bigr\|^2 & \leq \Bigl\|\nabla\bigl(\tau^{-1}\bigr)(x)\Bigr\|^2\frac{1}{r^{2d}} \biggl[\int_{\Xset}\eta_r(x',x) \,dx'\biggr] \cdot \biggl[\int_{\Xset} \eta_r(x',x)\bigl(f(x') - f(x)\bigr)^2 \,dx' \biggr] \\
& = \Bigl\|\nabla\bigl(\tau^{-1}\bigr)(x)\Bigr\|^2\frac{\tau(x)}{r^d} \int_{\Xset} \eta_r(x',x)\bigl(f(x') - f(x)\bigr)^2 \,dx'\\ 
& \leq \Bigl\|\nabla\bigl(\tau^{-1}\bigr)(x)\Bigr\|^2\frac{\tau(x)}{\sigma_K r^d}\int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2 \,dx' \\
& \leq \frac{16}{\sigma_K^2r^{2 + d}} \biggl[\int_{\Xset} K\biggl(\frac{|x' - x|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2 \,dx' \biggr].
\end{align*}
where the last inequality follows by the estimates on $\tau$ and $\nabla \tau$ given in Lemma~\ref{lem:tau_bound}. Combining our bounds on $\bigl\|J_1(x)\bigr\|^2$ and $\bigl\|J_2(x)\bigr\|^2$ along with the lower bound on $\tau(x)$ in Lemma~\ref{lem:tau_bound} and integrating over $\Xset$, we have
\begin{align*}
\sigma_K D_2(\Lambda_r f) & = \sigma_K\int_{\Xset} \biggl\|\Bigl(\nabla \Lambda_rf)(x)\biggr\|^2 p^2(x) \,dx \\
& \leq 2 \sigma_K \int_{\Xset} \Biggl(\frac{\bigl\|J_1(x)\bigr\|^2}{\tau^2(x)} + \bigl\|J_2(x)\bigr\|^2\Biggr) p^2(x) \,dx \\
& \leq \frac{40}{\sigma_K r^{2 + d}} \int_{\Xset} \int_{\Xset} K\biggl(\frac{|x' - x|}{r}\biggr)\bigl(f(x') - f(x)\bigr)^2 p^2(x) \,dx' \,dx \\
& \leq \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} E_r(f),
\end{align*}
completing the proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}. 

\paragraph{Proof of Lemma~\ref{lem:tau_bound}.}
First prove the estimates of $\tau(x)$, and then upper bound on $\|\nabla\tau(x)\|$. Substituting $z = (x' - x)/r$ and using~\ref{asmp:r_small_3}, we have that
\begin{align*}
\tau(x) & = \frac{1}{r^d} \int_{\Xset} \psi\biggl(\frac{\|x' - x\|}{r}\biggr) \,dx' \\
& =  \int_{\mc{B}_x} \psi\bigl(\|z\|\bigr) \,dz \\
& \geq a_1(\Xset,d) \cdot \int_{B(0,1)} \psi(\|z\|) \,dz
\end{align*}
We will now show that $\int_{B(0,1)} \psi(\|z\|) \,dz = 1$, which will imply our estimate of $\tau(x)$. To see this identity, note that on the one hand, by converting to polar coordinates and integrating by parts we obtain
\begin{align*}
\int_{B(0,1)} \psi\bigl(\|z\|\bigr) \,dz = d \nu_d \int_{0}^{1} \psi(t) t^{d - 1} \,dt = \nu_d \int_{0}^{1} \psi'(t) t^{d} \,dt = \frac{\nu_d}{\sigma_K} \int_{0}^{1} t^{d + 1} K(t) \,dt;
\end{align*}
on the other hand, again converting to polar coordinate, we have
\begin{equation*}
\sigma_K = \frac{1}{d} \int_{\Reals^d} \|x\|^2 K(\|x\|) \,dx = \nu_d \int_{0}^{1}t^{d + 1} K(t) \,dt.
\end{equation*}

Finally, we upper bound $\|\nabla\tau(x)\|^2$. Exchanging derivative and integral, we have
\begin{align*}
\nabla\tau(x) = \frac{1}{r^d} \int_{\Xset} \bigl(\nabla \eta_r(x',\cdot)\bigr)(x) \,dx' = \frac{1}{\sigma_K r^{d + 2}} \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)(x' - x)\,dx',
\end{align*}
and by the Cauchy-Schwarz inequality,
\begin{equation*}
\|\nabla\tau(x)\|^2 \leq \frac{1}{\sigma_K^2 r^{2d + 4}} \biggl[\int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)\,dx'\biggr] \int_{\Xset} K\biggl(\frac{\|x' - x\|}{r}\biggr)\|x' - x\|^2\,dx', \leq \frac{1}{\sigma_K r^{2}}
\end{equation*}
concluding the proof of Lemma~\ref{lem:tau_bound}. One note: while $\nabla\tau(x) = 0$ when $B(x,r) \in \Xset$, near the boundary the upper bound we derived by using Cauchy-Schwarz appears tight. 

\paragraph{Proof of Lemma~\ref{lem:smoothening_error}.}
By Jensen's inequality and Lemma~\ref{lem:tau_bound},
\begin{align*}
\Bigl[\Lambda_rf(x)\Bigr]^2 & \leq \frac{1}{r^d \tau(x)}\int_{\Xset} \eta_r(x',x) \bigl[f(x')\bigr]^2 \,dx' \\
& \leq \frac{2}{r^d p_{\min}}\int_{\Xset} \eta_r(x',x) \bigl[f(x')\bigr]^2 p(x') \,dx'
\end{align*}
Then integrating over $x$ and swapping the order of integraton, we have
\begin{align*}
\bigl\|\Lambda_rf\bigr\|_{\Leb^2(P)}^2 & \leq \frac{2}{r^d p_{\min}} \int_{\Xset} \int_{\Xset} \eta_r(x',x) \bigl[f(x')\bigr]^2 p(x') p(x) \,dx' \,dx \\ 
& \leq \frac{2p_{\max}}{p_{\min}} \int_{\Xset} \bigl[f(x')\bigr]^2 p(x') \,dx' \\
& = \frac{2p_{\max}}{p_{\min}} \|f\|_{\Leb^2(P)}^2.
\end{align*}

Noting that $\Lambda_ra = a$ for any $a \in \Reals$, by the Cauchy-Schwarz inequality we have that for all $x \in \Xset$,
\begin{align*}
\bigl|\Lambda_rf(x) - f(x)\bigr|^2 & = \biggl[\frac{1}{r^d\tau(x)} \int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr) \,dx'\biggr]^2 \\
& \leq \frac{1}{r^{2d} \tau^2(x)} \biggl[\int_{\Xset} \eta_r(x',x) \,dx'\biggr] \cdot \biggl[\int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr)^2 \,dx'\biggr] \\
& = \frac{1}{r^d \tau(x)} \int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr)^2 \,dx'. \\
& \leq \frac{1}{r^d \tau(x) p_{\min}} \int_{\Xset} \eta_r(x',x) \bigl(f(x') - f(x)\bigr)^2 p(x') \,dx'.
\end{align*}
Then integrating over $\Xset$ with respect to $p$ yields~\eqref{eqn:smoothening_error_energy}.



\subsection{Random functionals}
\label{subsec:random_functionals}

We begin by relating $\wt{E}_r(f)$ and $E_r(f)$. Note that by assumption $A_2(\wt{\delta} + \theta)/p_{\min} \leq A_1(\wt{\delta} + \theta) \leq \frac{1}{2}$. Therefore, some standard calculations show that,
\begin{equation}
\label{eqn:calder19_1}
\bigl(1 - A_1(\theta + \wt{\delta})\bigr) E_r(f) \leq \wt{E}_r(f) \leq \bigl(1 + A_1(\theta + \wt{\delta})\bigr) E_r(f),
\end{equation}
as well as implying that the norms $\|f\|_{\Leb^2(P)}$ and $\|f\|_{\Leb^2(P_n)}$ satisfy
\begin{equation}
\label{eqn:calder19_2}
\bigl(1 - A_1(\theta + \wt{\delta})\bigr) \|f\|_{\Leb^2(P)}^2 \leq \|f\|_{\Leb^2(\wt{P}_n)}^2 \leq \bigl(1 + A_1(\theta + \wt{\delta})\bigr) \|f\|_{\Leb^2(P)}^2.
\end{equation}

Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized} relates the graph Sobolev semi-norm $b_r(\wt{\mc{P}}f)$ to the non-local energy $\wt{E}_r(f)$. It follows exactly from the derivations in the proof of Lemma 13 in \cite{trillos2019}; indeed, in our flat Euclidean setting, the proof is simplified, and we include this simpler proof for completeness purposes only.
\begin{lemma}[\textbf{c.f. Lemma 13 of \cite{trillos2019}, Lemma 4.3 of \cite{burago2014}}]
	\label{lem:first_order_graph_sobolev_seminorm_discretized}
	Suppose~\ref{asmp:kernel} is satisfied, and that the conclusions of Proposition~\ref{prop:optimal_transport} are met. Then
	\begin{equation*}
	b_r(\wt{\mc{P}}f) \leq \wt{E}_{r + 2\wt{\delta}}(f) + 2\frac{L_K\wt{\delta}}{K(1/2)r} \wt{E}_{2(r + 2\wt{\delta})}(f)
	\end{equation*}
	for any $f \in \Leb^2(\Xset)$.
\end{lemma}

In Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}, we establish the reverse of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized}. 
\begin{lemma}[c.f. Lemma 14 of \cite{trillos2019}]
	\label{lem:first_order_graph_sobolev_seminorm_discretized_lb}
	Suppose $\frac{\wt{\delta}}{r} \leq \min\bigl\{\frac{1}{4(2^{d + 2} - 1)}, \frac{K(1)}{2L_K}\bigr\}$. Then for any $u \in \Leb^2(P_n)$, 
	\begin{equation*}
	\wt{E}_{r - 2\wt{\delta}}\bigl(\wt{\mc{P}}^{\star}u\bigr) \leq \biggl(1 + A_3\frac{\wt{\delta}}{r}\biggr) b_{r}(u)
	\end{equation*}
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized}.}
Recalling that $\bigl(\wt{P}f\bigr)(X_i) = n \cdot \int_{U_i} f(x) \wt{p}_n(x)$, the key point is that
\begin{equation*}
\biggl(\bigl(\wt{P}f\bigr)(X_i) - \bigl(\wt{P}f\bigr)(X_j)\biggr)^2 \leq n^2 \cdot \int_{U_i} \int_{U_j} \bigl(f(x') - f(x)\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx.
\end{equation*}
Additionally, the non-increasing and Lipschitz properties of $K$ imply that for any $x \in U_i$ and $x' \in U_j$, 
\begin{equation*}
K\biggl(\frac{\|X_i - X_j\|}{r}\biggr) \leq K\biggl(\frac{\bigl(\|x' - x\| - 2\wt{\delta}\bigr)_{+}}{r}\biggr) \leq K\biggl(\frac{\|x' - x\|}{r + 2\wt{\delta}}\biggr) + \frac{2L_K\wt{\delta}}{r}\1\Bigl\{\|x' - x\| \leq r + 2\wt{\delta}\Bigr\}
\end{equation*}
Using this along with the Lipschitz property of $K$, we get
\begin{align*}
b_r(f) & = \frac{1}{n^2r^{d + 2}} \sum_{i,j = 1}^n \Bigl(\bigl(\wt{P}f\bigr)(X_i) - \bigl(\wt{P}f\bigr)(X_j)\Bigr)^2 K\biggl(\frac{\|X_i - X_j\|}{r}\biggr) \\
& \leq \frac{1}{r^{d + 2}} \sum_{i,j = 1}^n \int_{U_i} \int_{U_j}  \bigl(f(x') - f(x)\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) K\biggl(\frac{\|X_i - X_j\|}{r}\biggr) \,dx' \,dx \\
& \leq \frac{1}{r^{d + 2}} \sum_{i,j = 1}^n \int_{U_i} \int_{U_j}  \bigl(f(x') - f(x)\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) \biggl[K\biggl(\frac{\|x' - x\|}{r + 2\wt{\delta}}\biggr) + \frac{2L_K\wt{\delta}}{r}\1\Bigl\{\|x' - x\| \leq r + 2\wt{\delta}\Bigr\}\biggr] \,dx' \,dx \\
& = \wt{E}_{r + 2\wt{\delta}}(f) + \frac{2L_K\wt{\delta}}{r}\wt{E}_{r + 2\wt{\delta}}(f; \1_{[0,1]})
\end{align*}
for $\1_{[0,1]}(t) = \1\{0 \leq t \leq 1\}$. But clearly $\wt{E}_{r + 2\wt\delta}(f; \1_{[0,1]}) \leq 1/(K(1/2))\wt{E}_{2r + 4\wt{\delta}}(f)$, and the Lemma is shown.

\paragraph{Proof of Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}.}
For brevity, we write $\wt{r} := r - 2\wt{\delta}$. We begin by expanding the energy $\wt{E}_{r - 2\wt{\delta}}\bigl(\wt{\mc{P}}^{\star}u\bigr)$ as a double sum of double integrals,
\begin{align*}
\wt{E}_{\wt{r}}\bigl(\wt{\mc{P}}^{\star}u\bigr) & = \frac{1}{\wt{r}^{d + 2}} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \int_{U_i} \int_{U_j} \Bigl(u(X_i) - u(X_j)\Bigr)^2 K\biggl(\frac{\|x' - x\|}{\wt{r}}\biggr) \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx
\end{align*}
We next use the Lipschitz property of the kernel $K$---in particular that for $x \in U_i$ and $x' \in U_j$,
\begin{equation*}
K\biggl(\frac{\|x' - x\|}{\wt{r}}\biggr) \leq K\biggl(\frac{\|x_i - x_j\|}{r}\biggr) + \frac{2L_K\wt{\delta}}{\wt{r}} \cdot \1\biggl\{\frac{\|x' - x\|}{\wt{r}} \leq 1\biggr\}
\end{equation*}
---to conclude that
\begin{align}
\wt{E}_{\wt{r}}\bigl(\wt{\mc{P}}^{\star}u\bigr) & \leq \frac{1}{n^2\wt{r}^{d + 2}} \sum_{i = 1}^{n} \sum_{j = 1}^{n} \Bigl(u(X_i) - u(X_j)\Bigr)^2 K\biggl(\frac{\|x_i - x_j\|}{r}\biggr) + \frac{2L_K\wt{\delta}}{\wt{r}}\wt{E}_{\wt{r}}(\wt{\mc{P}}^{\star}u,\1_{[0,1]}\bigr) \nonumber \\
& \leq \biggl(1 + 4\bigl(2^{d + 2} - 1\bigr)\frac{\wt{\delta}}{r}\biggr) b_r(u) + \frac{2L_K\wt{\delta}}{\wt{r}}\wt{E}_{\wt{r}}(\wt{\mc{P}}^{\star}u,\1_{[0,1]}\bigr) \nonumber \\
& \leq \biggl(1 + 4\bigl(2^{d + 2} - 1\bigr)\frac{\wt{\delta}}{r}\biggr) b_r(u) + \frac{2L_K\wt{\delta}}{K(1)\wt{r}} \wt{E}_{\wt{r}}(\wt{\mc{P}}^{\star}u\bigr) \nonumber 
\end{align}
or in other words
\begin{align*}
\wt{E}_{\wt{r}}\bigl(\wt{\mc{P}}^{\star}u\bigr) & \leq \biggl(1 - \frac{2L_K\wt{\delta}}{K(1)\wt{r}}\biggr)^{-1}\biggl(1 + 4\bigl(2^{d + 2} - 1\bigr)\frac{\wt{\delta}}{r}\biggr) b_r(u) \\
& \leq \biggl(1 + \frac{2L_K}{K(1)} + 8\bigl(2^{d + 2} - 1\bigr)\biggr) \frac{\wt{\delta}}{r}b_r(u)
\end{align*}
where the second inequality follows from the algebraic identities $(1 - t)^{-1} \leq (1 + 2t)$ for any $0 < t < 1/2$ and $(1 + s)(1 + t) < 1 + 2s + t$ for any $0 < t < 1$ and $s > 0$.


\subsection{Proof of Propositions~\ref{prop:dirichlet_energies} and~\ref{prop:isometry}}
\label{subsec:proof_of_prop_dirichlet_energies_and_isometry}

\paragraph{Proof of Proposition~\ref{prop:dirichlet_energies}.}
Part (1) of Proposition~\ref{prop:dirichlet_energies} follows from
\begin{align*}
\sigma_K D_2(\Lambda_{r - 2\wt{\delta}} \wt{\mc{P}}^{\star}u) & \overset{(i)}{\leq} \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} E_{r - 2\wt{\delta}}(\wt{\mc{P}}^{\star}u) \\
& \overset{(ii)}{\leq} \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \wt{E}_{r - 2\wt{\delta}}(\wt{\mc{P}}^{\star}u) \\
& \overset{(iii)}{\leq} \frac{40 \bigl(1 + L_p r\bigr)}{\sigma_K} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \biggl(1 + A_3\frac{\wt{\delta}}{r}\biggr) b_r(u)
\end{align*}
where $(i)$ follows from Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected_lb}, $(ii)$ follows from~\eqref{eqn:calder19_1}, and $(iii)$ follows from~Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}.

Part (2) of Proposition~\ref{prop:dirichlet_energies} follows from
\begin{align*}
b_r(\wt{\mc{P}}f) & \overset{(iv)}{\leq} \wt{E}_{r + 2\wt{\delta}}(f) + \frac{2L_K}{K(1/2)}\frac{\wt{\delta}}{r}\wt{E}_{2(r + 2\wt{\delta})}(f)\\
& \overset{(v)}{\leq} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl({E}_{r + 2\wt{\delta}}(f) + \frac{2L_K}{K(1/2)}\cdot\frac{\wt{\delta}}{r}{E}_{2(r + 2\wt{\delta})}(f)\Bigr) \\
& \overset{(vi)}{\leq} \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \cdot \Bigl(1 + L_p(2r + 4\wt{\delta})\Bigr) \cdot \Bigl(1 + \frac{2L_K}{K(1/2)}\frac{\wt{\delta}}{r}\Bigr) \cdot \sigma_{\eta} D_2(f)
\end{align*}
where $(iv)$ follows from Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized}, $(v)$ follows from~\eqref{eqn:calder19_1}, and $(vi)$ follows from Lemma~\ref{lem:first_order_graph_sobolev_seminorm_expected}. The proposition follows upon taking $A_4 := 2L_K/K(1/2)$. 

\paragraph{Proof of Proposition~\ref{prop:isometry}.}
\mbox{}\\
\mbox{}\\
\textit{Proof of (1).}
We begin by upper bounding $\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)}$. By the Cauchy-Schwarz inequality and the bound on $\|\wt{p}_n - p\|_{\infty}$ in~\eqref{eqn:calder19_1},
\begin{align*}
\Bigl|\wt{P}f(X_i)\Bigr|^2 & = n^2 \Bigl|\int_{U_i} f(x) \wt{p}_n(x) \,dx\Bigr|^2 \\
& \leq n \int_{U_i} \bigl|f(x)\bigr|^2 \wt{p}_n(x) \,dx \\
& \leq n \cdot \biggl[\int_{U_i} \bigl|f(x)\bigr|^2 p(x) \,dx + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \int_{U_i} \bigl|f(x)\bigr|^2 p(x) \,dx\biggr]
\end{align*}
and summing over $i = 1,\ldots,n$, we obtain
\begin{equation}
\label{pf:prop_isometry_1}
\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)}^2 \leq \biggl(1 + \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr) \bigl\|f\bigr\|_{\Leb^2(P)}^2.
\end{equation}
Now, noticing that $\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)} = \bigl\|\wt{P}^{\star}\wt{P}f\bigr\|_{\Leb^2(\wt{P}_n)}$, we can use the upper bound~\eqref{pf:prop_isometry_1} to show that
\begin{align}
\Bigl|\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)}^2 - \bigl\|f\bigr\|_{\Leb^2(P)}^2\Bigr| & \leq \Bigl|\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)}^2 - \bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}^2\Bigr| + \Bigl|\bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}^2 - \bigl\|f\bigr\|_{\Leb^2(P)}^2\Bigr| \nonumber \\
& \overset{(i)}{\leq} \Bigl|\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)}^2 - \bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}^2\Bigr|  + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \bigl\|f\bigr\|_{\Leb^2(P)}^2 \\
& \overset{(ii)}{\leq} \biggl(2 + \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr) \Bigl|\bigl\|\wt{P}f\bigr\|_{\Leb^2(P_n)} - \bigl\|f\bigr\|_{\Leb^2(\wt{P}_n)}\Bigr| \cdot \bigl\|f\bigr\|_{\Leb^2(P)} + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \bigl\|f\bigr\|_{\Leb^2(P)}^2 \nonumber \\
& \leq \biggl(2 + \frac{A_1(\theta + \wt{\delta})}{p_{\min}}\biggr) \bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)} \cdot \bigl\|f\bigr\|_{\Leb^2(P)} + \frac{A_1(\theta + \wt{\delta})}{p_{\min}} \bigl\|f\bigr\|_{\Leb^2(P)}^2 \label{pf:prop_isometry_2}
\end{align}
where $(i)$ follows from~\eqref{eqn:calder19_2} and $(ii)$ follows from~\eqref{pf:prop_isometry_1}. 

It remains to upper bound $\bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)}^2$. Noting that $\wt{P}^{\star}\wt{P}f$ is piecewise constant over the cells $U_i$, we have
\begin{equation*}
\bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)}^2 = \sum_{i = 1}^{n} \int_{U_i} \biggl(f(x) - n\cdot\int_{U_i} f(x') \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx.
\end{equation*}
We will now establish the following Poincare-type inequality, 
\begin{equation}
\label{pf:prop_isometry_3}
\int_{U_i} \biggl(f(x) - n\cdot\int_{U_i} f(x') \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx \leq \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d K(1/2)} \wt{\delta}^2 \wt{E}_{2\wt{\delta}}(f,U_i)
\end{equation}
which holds for all $i = 1,\ldots,n$. Summing up over $i$ implies
\begin{equation*}
\bigl\|\wt{P}^{\star}\wt{P}f - f\bigr\|_{\Leb^2(\wt{P}_n)}^2 \leq \frac{2^{d+2}}{a_1(\Xset,d)K(1/2)p_{\min}} \cdot \wt{E}_{2\wt{\delta}}(f) \leq \frac{2^{d+3}}{a_1(\Xset,d)K(1/2)p_{\min}} A_1(\Xset,d,K,p_{\min}) D_2(f)
\end{equation*}
with the latter inequality following just as in the proof of Proposition~\ref{prop:dirichlet_energies}; the Lemma then follows by plugging this inequality into~\eqref{pf:prop_isometry_2}.

Now it remains to show~\eqref{pf:prop_isometry_3}. The key point is that for any pair of points $x,x' \in \Xset$ such that $\|x - x'\| \leq r$  the triangle inequality
\begin{equation*}
\bigl|f(x') - f(x)\bigr|^2 \leq 2\bigl|f(x') - f(z)\bigr|^2 + 2\bigl|f(z) - f(x)\bigr|^2
\end{equation*}
holds for each $z \in B\bigl(x,\wt{\delta}\bigr) \cap B\bigl(x',\wt{\delta}\bigr) \cap \Xset =: W$, and we have that
\begin{align*}
\bigl|f(x') - f(x)\bigr|^2 & \leq \frac{2}{\vol(W)}\biggl[\int_{W} \bigl|f(x') - f(z)\bigr|^2 \,dz + \int_{W} \bigl|f(z) - f(x)\bigr|^2 \,dz\biggr] \\
& \leq \frac{2}{\vol(W)K(1/2)}\biggl[\int_{\Xset} K\biggl(\frac{x' - z}{2\wt{\delta}}\biggr)\bigl|f(x') - f(z)\bigr|^2 \,dz + \int_{\Xset} K\biggl(\frac{z - x}{2\wt{\delta}}\biggr)\bigl|f(z) - f(x)\bigr|^2 \,dz\biggr] \\
& \leq \frac{2^{d + 1}}{a_1(\Xset,d) \nu_d \wt{\delta}^d K(1/2)}\biggl[\int_{\Xset} K\biggl(\frac{x' - z}{2\wt{\delta}}\biggr)\bigl|f(x') - f(z)\bigr|^2 \,dz + \int_{\Xset} K\biggl(\frac{z - x}{2\wt{\delta}}\biggr)\bigl|f(z) - f(x)\bigr|^2 \,dz\biggr]
\end{align*}
where the last inequality follows from Assumption~\ref{asmp:r_small_1}. We use this along with the Cauchy-Schwarz inequality to prove~\eqref{pf:prop_isometry_3}:
\begin{align*}
\int_{U_i} & \biggl(f(x) - n\cdot\int_{U_i} f(x') \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx \\
& =\int_{U_i} \biggl(n\cdot\int_{U_i} \bigl(f(x) - f(x')\bigr) \wt{p}_n(x') \,dx'\biggr)^2 \wt{p}_n(x) \,dx \\
& \leq n \cdot \int_{U_i} \int_{U_i} \bigl(f(x) - f(x')\bigr)^2 \wt{p}_n(x') \wt{p}_n(x) \,dx' \,dx \\
& \leq \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d \wt{\delta}^d K(1/2)}n\int_{U_i} \int_{U_i} \int_{\Xset} K\biggl(\frac{x - z}{2\wt{\delta}}\biggr)\bigl|f(x) - f(z)\bigr|^2 \wt{\rho}_n(x') \wt{\rho}_n(x)  \,dx' \,dz \,dx \\
& \leq \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d \wt{\delta}^d K(1/2) p_{\min}} \int_{U_i} \int_{U_i} K\biggl(\frac{x - z}{2\wt{\delta}}\biggr)\bigl|f(x) - f(z)\bigr|^2 \wt{\rho}_n(z) \wt{\rho}_n(x)  \,dz \,dx \\
& =  \frac{2^{d + 2}}{a_1(\Xset,d) \nu_d K(1/2) p_{\min}} \wt{\delta}^2 \wt{E}_{2\wt{\delta}}(f,U_i).
\end{align*}

\textit{Proof of (2).}
By the triangle inequality,
\begin{align}
\Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(P)}^2 - \|u\|_{\Leb^2(P_n)}^2\Bigr| & \leq \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(P)}^2 - \|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2\Bigr| + \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 - \|u\|_{\Leb^2(P_n)}^2\Bigr| \nonumber \\
& \leq A_1(\theta + \wt{\delta}) \|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 + \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 - \|u\|_{\Leb^2(P_n)}^2\Bigr| \nonumber\\
& = A_1(\theta + \wt{\delta}) \|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 + \Bigl(\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} + \|u\|_{\Leb^2(P_n)}\Bigr) \cdot \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} - \|u\|_{\Leb^2(P_n)}\Bigr| \label{pf:prop_isometry_4}
\end{align}
To upper bound the second term in the above expression, we first note that~$\|u\|_{\Leb^2(P_n)} = \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)}$, and thus
\begin{align}
\Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} - \|u\|_{\Leb^2(P_n)} \Bigr| & = \Bigl|\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)} - \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)}\Bigr| \nonumber \\
& \overset{(iii)}{\leq} \|\Lambda_{\wt{r}}\wt{\mc{P}}^{\star}u - \wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)} \nonumber \\
& \overset{(iv)}{\leq} r \sqrt{\frac{2}{\sigma_K p_{\min}} E_{\wt{r}}(\wt{\mc{P}}^{\star}u)} \nonumber \\
& \overset{(v)}{\leq} r \sqrt{\frac{2}{\sigma_K p_{\min}} \Bigl(1 + A_3\frac{\wt{\delta}}{r}\Bigr) b_r(u)} \label{pf:prop_isometry_5}
\end{align}
where $(iii)$ follows by the triangle inequality, $(iv)$ follows from Lemma~\ref{lem:smoothening_error}, and $(v)$ follows from~\eqref{eqn:calder19_1} and Lemma~\ref{lem:first_order_graph_sobolev_seminorm_discretized_lb}. On the other hand, by~\eqref{eqn:calder19_2} and Lemma~\ref{lem:smoothening_error},
\begin{align*}
\|\wt{\mc{I}}u\|_{\Leb^2(\wt{P}_n)}^2 & \leq \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \|\wt{\mc{I}}u\|_{\Leb^2(P)}^2 \\
& \leq \frac{2p_{\max}}{p_{\min}} \cdot \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr) \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(P)}^2 \\
& \leq \frac{2p_{\max}}{p_{\min}} \cdot \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr)^2 \|\wt{\mc{P}}^{\star}u\|_{\Leb^2(\wt{P}_n)}^2 \\
& = \frac{2p_{\max}}{p_{\min}} \cdot \Bigl(1 + A_1(\theta + \wt{\delta})\Bigr)^2 \|u\|_{\Leb^2(P_n)}^2
\end{align*}
and plugging this estimate along with~\eqref{pf:prop_isometry_5} back into~\eqref{pf:prop_isometry_4}, we obtain part (2) of Proposition~\ref{prop:isometry}.

\subsection{\textcolor{red}{(TODO)}: Move these.}

In the above we used the following:
\begin{enumerate}[label=(OT\arabic*)]
	\item
	\label{asmp:r_small_1} For all $x,x' \in \Xset$ such that $\|x - x'\| \leq r$, we have that for $W = B(x,r) \cap B(x',r) \cap \Xset$,
	\begin{equation*}
	\vol(W) \geq a_1(\Xset,d) \cdot \nu_d \biggl(\frac{r}{2}\biggr)^d
	\end{equation*}
	for some constant $a_1(\Xset,d)$. 
	\item 
	\label{asmp:r_small_3} For any $x \in \Xset$, letting $\mc{B}_x:= (\Xset - x)/r~ \cap~B(0,1)$, we have that $\int_{\mc{B}_x} \psi(\|z\|) \,dz \geq a_1(\Xset,d) \cdot \int_{B(0,1)} \psi(\|z\|) \,dz$.
\end{enumerate}

\section{Bounds on the empirical norm}
\label{sec:empirical_norm}

We use Lemma~\ref{lem:empirical_norm_sobolev} to lower bound $\norm{f_0}_n^2$ by (a constant times) the $\Leb^2(\Xset)$ norm of $f$.

\begin{lemma}
	\label{lem:empirical_norm_sobolev}
	Suppose $P$ satisfies~\ref{asmp:bounded_lipschitz_density}. If $f \in H^1(\Xset,M)$ is lower bounded in $\Leb^2(\Xset)$ norm,
	\begin{equation}
	\label{eqn:empirical_norm_sobolev_1}
	\norm{f}_{\Leb^2(\Xset)} \geq 
	\begin{cases*}
	\frac{C_6 M}{\delta} \cdot \max\Bigl\{n^{-1/2},n^{-1/d}\Bigr\},& ~~\textrm{if $2 \neq d$} \\
	\frac{C_6 M}{\delta} \cdot n^{-a/2},& ~~\textrm{if $2 = d$, for any $0 < a < 1$}
	\end{cases*}
	\end{equation}
	for some $\delta \leq 1$, then
	\begin{equation}
	\label{eqn:empirical_norm_sobolev}
	\norm{f}_n^2 \geq \delta \cdot \Ebb\Bigl[\norm{f}_n^2\Bigr] 
	\end{equation}
	with probability at least $1 - 5 \delta$.
\end{lemma}

\paragraph{Proof of Lemma~\ref{lem:empirical_norm_sobolev}.}
To prove~\eqref{eqn:empirical_norm_sobolev} we will show
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot \left(\mathbb{E}\bigl[\norm{f}_n^2\bigr]\right)^2
\end{equation*}
whence the claim follows from the Paley-Zygmund inequality (Lemma~\ref{lem:paley_zygmund}). Since $p \leq p_{\max}$ is uniformly bounded, we can relate $\mathbb{E}\bigl[\norm{f}_n^4\bigr]$ to the $\Leb^4$ norm,
\begin{equation*}
\mathbb{E}\bigl[\norm{f}_n^4\bigr] = \frac{(n-1)}{n}\left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + \frac{\mathbb{E}\Bigl[\bigl(f(x_1)\bigr)^4\Bigr]}{n} \leq \left(\mathbb{E}\Bigl[\norm{f}_n^2\Bigr]\right)^2 + p_{\max}^2\frac{\norm{f}_{\Leb^4}^4}{n}.
\end{equation*}
We will use a Sobolev inequality to relate $\norm{f}_{\Leb^4}$ to $\norm{f}_{H^1(\Xset)}$. The nature of this inequality depends on the relationship between $s$ and $d$ (see Theorem 6 in Section 5.6.3 of \cite{evans10} for a formal statement), so from this point on we divide our analysis into three cases: (i) the case where $2s > d$, (ii) the case where $2s < d$, and (iii) the borderline case $2s = d$.

\textit{Case 1: $2s > d$.}
When $2s > d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in C^{\gamma}(\overline{\Xset})$ for some $\gamma > 0$ which depends on $s$ and $d$, with the accompanying estimate
\begin{equation*}
\sup_{x \in \Xset} \abs{f(x)} \leq \norm{f}_{C^{\gamma}(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Therefore,
\begin{align*}
\norm{f}_{\Leb^4}^4 & = \int_{\Xset} [f(x)]^4 \,dx \\
& \leq \left(\sup_{x \in \Xset} \abs{f(x)}\right)^2 \cdot \int_{\Xset} [f(x)]^2 \,dx \\
& \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \norm{f}_{\Leb^2(\Xset)}^2.
\end{align*}
Since by assumption
\begin{equation*}
\norm{f}_{\Leb^2(\Xset)}^2 \geq c_1^2 \cdot b^2 \cdot \norm{f}_{W_d^{s,2}(\Xset)}^2 \cdot \frac{1}{n},
\end{equation*}
we have
\begin{equation*}
p_{\max}^2\frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c \norm{f}_{H^s(\Xset)}^2 \cdot \frac{\norm{f}_{\Leb^2(\Xset)}^4}{n \norm{f}_{\Leb^2(\Xset)}^2} \leq c \frac{\norm{f}_{\Leb^2(\Xset)}^4}{c_1^2 b^2} \leq \frac{\Ebb\bigl[\norm{f}_n^2\bigr]}{b^2},
\end{equation*}
where the last inequality follows by taking $c_1$ sufficiently large.

\textit{Case 2: $2s < d$.}
When $2s < d$, since $\Xset$ is a Lipschitz domain the Sobolev inequality establishes that $f \in \Leb^q(\Xset)$ for $q = 2d/(d - 2s)$, and moreover that
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
Since $4 = 2\theta + (1 - \theta)q$ for $\theta = 2 - d/(2s)$, Lyapunov's inequality implies
\begin{equation*}
\norm{f}_{\Leb^4(\Xset)}^4 \leq \norm{f}_{\Leb^2}^{2\theta} \cdot \norm{f}_{\Leb^q(\Xset)}^{(1 - \theta)q} \leq c \norm{f}_{\Leb^2(\Xset)}^{4} \cdot \left(\frac{\norm{f}_{H^s(\Xset)}}{\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s}.
\end{equation*}
By assumption, $\norm{f}_{\Leb^2(\Xset)} \geq c_1 b \norm{f}_{H^s(\Xset)} n^{-s/d}$, and therefore
\begin{equation*}
p_{\max}^2 \frac{\norm{f}_{\Leb^4(\Xset)}^4}{n} \leq c\norm{f}_{\Leb^2(\Xset)}^4 \left(\frac{\norm{f}_{H^s(\Xset)}}{n^{s/d}\norm{f}_{\Leb^2(\Xset)}}\right)^{d/s} \leq \frac{c\norm{f}_{\Leb^2(\Xset)}^4}{c_1b^{d/s}} \leq \frac{\norm{f}_{\Leb^2(\Xset)}^4}{b^2}.
\end{equation*}
where the last inequality follows when $c_1$ is sufficiently large, and keeping in mind that $d/s > 2$ and $b \geq 1$. 

\textit{Case 3: $2s = d$.}
Assume $f$ satisfies~\eqref{eqn:paley_zygmund_1} for a given $0 < a < 1$. When $2s = d$, since $\Xset$ is a Lipschitz domain we have that $f \in L^q(\Xset)$ for any $q < \infty$, with the accompanying estimate
\begin{equation*}
\norm{f}_{\Leb^q(\Xset)} \leq c \norm{f}_{H^s(\Xset)}.
\end{equation*}
In particular the above holds for $q = 2/(1 - a)$ when $1/2 < a < 1$, and for any $q > 4$ when $0 < a < 1/2$. Using Lyapunov's inequality as in the previous case then implies the desired result.

\paragraph{Proof of Lemma~\ref{lem:empirical_norm_sobolev}.}

\textcolor{red}{(TODO)}

\section{Proof of theorems}

\subsection{Proof of Theorem~\ref{thm:laplacian_smoothing_estimation1}}
\label{subsec:laplacian_smoothing_estimation1_pf}
Let $\mc{E}$ be the set of $\mathbf{X} = (X_1,\ldots,X_n)$ for which
\begin{enumerate}[(i)]
	\item
	\label{pf:laplacian_smoothing_estimation1_0}
	$f_0^T \Lap_{n,r} f_0 \leq \frac{p_{\max}^2 \sigma_K}{\delta} n^2 r^{d + 2}M^2$
	\item 
	\label{pf:laplacian_smoothing_estimation1_1}
	$c_3 \cdot \min\{k^{2/d}nr^{d+2},nr^d\} \leq \lambda_k(G_{n,r}) \leq C_3k^{2/d}nr^{d + 2} $ for all $2 \leq k \leq n$.
\end{enumerate}
We already know, from Lemma~\ref{lem:graph_sobolev_seminorm} and Corollary~\ref{cor:neighborhood_eigenvalue}, that for any $\delta > 0$ and $(\log(n)/n)^{1/d} \leq r \leq a_0$,
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_1.5}
\Pbb\bigl({\bf X} \in \mc{E}\bigr) \geq 1 - \delta - A_1n\exp\bigl(-a_1nr^d\bigr).
\end{equation}
We will now show that conditional on $\mathbf{X} \in \mc{E}$, the empirical mean square error is small with high probability. Having conditioned on $\mathbf{X}$, we can use Lemma~\ref{lem:ls_fixed_graph_estimation} to control the remaining randomness arising from the noisy response $Y$, which in this context implies
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_2}
\begin{aligned}
\Pbb\Biggl(& \frac{1}{n}\bigl\|\wt{f}_{\LS}(G_{n,r}) - f_0\bigr\|_2^2 \leq \frac{2\rho}{n} \bigl(f_0^T \Lap_{n,r} f_0\bigr) + \frac{10}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G_{n,r}) + 1\bigr)^2} \Bigg| \mathbf{X} = x \in \mc{E}\Biggr) \\
& \geq 1 - \exp\Biggl\{-\sum_{k = 1}^{n}\frac{1}{\bigl(\rho \lambda_k(G_{n,r}) + 1\bigr)^2}\Biggr\} \\
& \geq 1 - \exp\Biggl\{-\sum_{k = 1}^{n}\frac{1}{\bigl(C_3M^{-4/(2+d)}k^{2/d}n^{-2/(2+d)} + 1\bigr)^2}\Biggr\} \\
& \geq 1 - \exp\Bigl\{-c M^{d/(2d + 4)} n^{d/(2+d)}\Bigr\}
\end{aligned}
\end{equation}
where the second inequality follows from the upper bound in~\ref{pf:laplacian_smoothing_estimation1_1} and the choice of regularization parameter $\rho = M^{-4/(2+d)}(nr^{d+2})^{-1}n^{-2/(2+d)}$. 

It remains only to upper bound the (conditional on ${\bf X} = x$ for some $x \in \mc{E}$) squared bias and variance terms. A sufficient upper bound on the bias term comes immediately from the upper bound~\ref{pf:laplacian_smoothing_estimation1_0} and our choice of $\rho$,
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_2.5}
\frac{\rho}{n} \bigl(f_0^T \Lap_{{n,r}} f_0\bigr) \leq \frac{p_{\max}^2 \sigma_K}{\delta} M^{2/(2+d)} n^{-2/(2 + d)}.
\end{equation}

To upper bound the variance term, we replace the eigenvalues $\lambda_k(G_{n,r})$ by their lower bounds in~\ref{pf:laplacian_smoothing_estimation1_1} and plug in our choice of $\rho$ to obtain,
\begin{align}
\frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G_{n,r}) + 1\bigr)^2} & \leq \frac{C}{n}\sum_{k = 1}^{n} \biggl\{\frac{1}{(\rho k^{2/d} n r^{d + 2} + 1)^2} + \frac{1}{(\rho n r^{d} + 1)^2} \biggr\} \nonumber \\
& \leq \frac{C}{n}\sum_{k = 1}^{n} \biggl\{\frac{1}{\bigl(M^{-4/(2 + d)}n^{-2/(2+d)}k^{2/d} + 1\bigr)^2}\biggr\} + Cn^{4/(2 + d)}M^{8/(2+d)}r^{4} \nonumber \\
& \leq CM^{2d/(2 + d)} n^{-2/(2 + d)} + CM^{8/(2 + d)}n^{(2 - d)/(2 + d)} \sum_{k = k_{\star}}^{n} \Bigl\{\frac{1}{k^{4/d}}\Bigr\}  + CM^{8/(2 + d)}n^{4/(2 + d)}r^4 \label{pf:laplacian_smoothing_estimation1_2.25}
\end{align}
where $k_{\star} = M^{2d/(2 + d)}n^{d/(2 + d)}$.  

We now upper bound the 2nd and 3rd term on the right hand side of~\eqref{pf:laplacian_smoothing_estimation1_2.25}. To control the 3rd term, we use the upper bound $r \leq n^{-3/(4 + 2d)} M^{(d - 4)/(4 + 2d)}$ assumed in~\ref{asmp:ls_kernel_radius_estimation} to determine that
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_3}
M^{8/(2 + d)}n^{4/(2 + d)}r^4 \leq M^{2d/(2 + d)}n^{-2/(2 + d)} 
\end{equation}
To control the 2nd term on the right hand side of~\eqref{pf:laplacian_smoothing_estimation1_2.25}, we treat it as a Riemann sum evaluated at the right end points of $[k_{\star},k_{\star} + 1], [n - 1,n]$. We use the fact that such a Riemann sum upper bounds the integral of a montonically nonincreasing function to conclude that
\begin{equation}
\label{pf:laplacian_smoothing_estimation1_4}
\begin{aligned}
\sum_{k = k_{\star}}^{n} \frac{1}{k^{4/d}} & \leq \int_{k_{\star}}^{n} \frac{1}{x^{4/d}} \,dx \\
& = - \frac{1}{(4/d - 1)} x^{-(4/d - 1)} \Big|_{k_{\star}}^{n} \\
& \leq \frac{1}{(4/d - 1)} k_{\star}^{-(4/d - 1)}
\end{aligned}
\end{equation}
where the equality in the above computation holds because $d < 4$. Recalling the definition of $k_{\star}$, plugging this back into~\eqref{pf:laplacian_smoothing_estimation1_3} implies a sufficient upper bound on the variance term. Along with~\eqref{pf:laplacian_smoothing_estimation1_1.5}-\eqref{pf:laplacian_smoothing_estimation1_2.5}, this establishes the claim.

\paragraph{Proving near-optimal bounds when $d = 4$.}
Note that nothing in the derivations of  ~\eqref{pf:laplacian_smoothing_estimation1_1.5}-\eqref{pf:laplacian_smoothing_estimation1_2.5} relied on $d < 4$, and so they all hold when $d = 4$. We now upper bound the 2nd and 3rd terms in~\eqref{pf:laplacian_smoothing_estimation1_2.5}. By setting $r = C_4(\log(n)/n)^{1/4}$ for some constant $C_4$, we have that
\begin{equation*}
M^{8/(2+d)}n^{4/(2+d)}r^4 = C_4^4 M^{8/(2+d)} \log(n) n^{-1/3},
\end{equation*}
taking care of the third term. On the other hand, we have that
\begin{equation*}
\sum_{k = k_{\star}}^{n} \frac{1}{k} \leq \log(n)
\end{equation*}
and so the second term is also upper bounded by $C M^{8/(2+d)} \log(n) n^{-1/3}$.

\subsection{Proof of Theorem~\ref{thm:laplacian_smoothing_testing}}
In this proof we will need to invoke Lemma~\ref{lem:empirical_norm_sobolev}, and it is in order to satisfy the conditions of this Lemma that we require $M \leq n^{(4 - d)/(4 + d)}$. More specifically, by~\eqref{eqn:laplacian_smoothing_testing} along with the restriction $M \leq n^{(4 - d)/(4 + d)}$,
\begin{equation*}
\norm{f}_{\Leb^2(\Xset)} \geq \frac{C}{\delta^2} M^{2d/(4 + d)} n^{-4/(4 + d)} \geq \frac{C}{\delta^2} M^2 n^{-2/d}
\end{equation*}
and~\eqref{eqn:empirical_norm_sobolev_1} is therefore satisfied when we choose $C \geq C_6$ in~\eqref{eqn:laplacian_smoothing_testing}.

We proceed to prove Theorem~\ref{thm:laplacian_smoothing_testing}. Throughout this proof we set $\delta = 1/b$. We will proceed, as in Subsection~\ref{subsec:laplacian_smoothing_estimation1_pf}, by conditioning on the random design $\mathbf{X}$. Let $\mc{E}(f_0)$ be the set of $\mathbf{X} \in \Xset^n$ such that~\eqref{eqn:graph_sobolev_seminorm},~\eqref{eqn:neighborhood_eigenvalue_2}, and~\eqref{eqn:empirical_norm_sobolev_1} hold: by Lemmas~\ref{lem:graph_sobolev_seminorm} and~\ref{lem:empirical_norm_sobolev}, and Corollary \ref{cor:neighborhood_eigenvalue},
\begin{equation*}
\Pbb\bigl(\mc{E}(f_0)\bigr) \geq 1 - 6\delta - A_1n\exp\bigl\{-a_1nr^d\bigr\}. 
\end{equation*}
For any $\mathbf{X} \in \mc{E}(f_0)$ the conditional variance of $T_{\LS}$ can be upper bounded using~\eqref{eqn:neighborhood_eigenvalue_2} and~\ref{asmp:ls_kernel_radius_testing},
\begin{align*}
\frac{1}{n} \biggl(\sum_{k = 1}^{n} \bigl[\lambda_k(G_{n,r}) + 1\bigr]^{-4}\biggr)^{1/2} & \leq \frac{1}{c_5^2 n} \biggl(M^{\frac{32}{8 + d}}r^8n^{\frac{20+d}{4+d}} + \sum_{k = 1}^{n} \bigl[n^{-\frac{4}{4 + d}}k^{\frac{2}{d}}M^{-\frac{8}{4 + d}} + 1\bigr]^{-4}\biggr)^{1/2} \\
& \leq \frac{1}{c_5^2 n} \biggl(M^{\frac{4d}{4 + d}}n^{\frac{2d}{4 + d}} + \sum_{k = 1}^{n} \bigl[n^{-\frac{4}{4 + d}}k^{\frac{2}{d}}M^{-\frac{8}{4 + d}} + 1\bigr]^{-4}\biggr)^{1/2} \\
& \leq \frac{1}{c_5^2 n} \biggl(3 M^{\frac{4d}{4 + d}}n^{2d/(4 + d)}\biggr)^{1/2} \\
& = \frac{\sqrt{3}}{c_5^2} M^{\frac{2d}{4 + d}} n^{-\frac{4}{4 + d}}.
\end{align*}
The last inequality in the above display similarly to~\eqref{pf:laplacian_smoothing_estimation1_4}. Setting $k_{\star} = n^{2d/(4+d)}M^{4d/(4 + d)}$, we derive that
\begin{equation}
\begin{aligned}
\label{pf:laplacian_smooting_testing1}
\sum_{k = 1}^{n} \bigl[n^{-\frac{4}{4 + d}}k^{\frac{2}{d}}M^{-\frac{8}{4 + d}} + 1\bigr]^{-4} & \leq k_{\star} + n^{\frac{16}{4 + d}} M^{\frac{32}{4 + d}}\sum_{k = k_{\star}}^{n} k^{-\frac{8}{d}}\\
& \leq k_{\star} + n^{\frac{16}{4 + d}} M^{\frac{32}{4 + d}} \int_{k_{\star}}^{n} x^{-\frac{8}{d}} \,dx \\
& \leq  k_{\star} + \frac{1}{(8/d - 1)}n^{\frac{16}{4 + d}} M^{\frac{32}{4 + d}} k_{\star}^{1 - 8/d} \\
& \leq 2k_{\star}.
\end{aligned}
\end{equation}
As a result, 
\begin{align*}
\frac{2\rho}{n} \bigl(f_0^T \Lap_{n,r} f_0) + \frac{4}{\delta n}\biggl(\sum_{k = 1}^{n} \bigl[\lambda_k(G_{n,r}) + 1\bigr]^{-4}\biggr)^{1/2} & \leq \frac{1}{\delta}\biggl(2C_1 + \frac{4\sqrt{3}}{c_5^2}\biggr)n^{-\frac{4}{4 + d}} M^{\frac{2d}{4 + d}} \\
& \leq \delta p_{\min} \norm{f}_{\Leb^2(\Xset)}^2 \leq \delta \Ebb\bigl[\norm{f}_n^2\bigr] \\
& \leq \norm{f}_n^2,
\end{align*}
where the second inequality follows upon choosing $C \geq p_{\min}^{-1}(2C_1 + 4\sqrt{3}/c_5^2)$ in~\eqref{eqn:laplacian_smoothing_testing}. We may therefore bound the Type II error of $\wt{\phi}_{\LS}$ by appealing to Lemma~\ref{lem:ls_fixed_graph_testing},
\begin{align*}
\Ebb_{f_0}\Bigl[1 - \wt{\phi}_{\LS}\Bigr] & \leq \Ebb_{f_0}\Bigl[1 - \wt{\phi}_{\LS}\big| \mathbf{X} \in \mc{E}(f_0)\Bigr] + 1 - \Pbb\Bigl(\mc{E}(f_0)\Bigr) \\
& \leq 4 \delta^2 + 8 \delta \Biggl(\sum_{k = 1}^{n} \frac{1}{(\rho\lambda_k(G_{n,r}) + 1)^4} \Biggr)^{-1/2} + 6\delta + A_1\exp\bigl\{-a_1nr^{d}\bigr\} \\
& \leq 4 \delta^2 + 16\delta n^{-d(4 + d)}M^{-2d/(4 + d)}  + 6\delta + A_1\exp\bigl\{-a_1nr^{d}\bigr\}
\end{align*}
where the last inequality follows from the reversing the inequalities in~\eqref{pf:laplacian_smooting_testing1}, with a factor of $1/2$ instead of $2$ in front. 

\subsection{Proof of~\eqref{eqn:laplacian_smoothing_testing_low_smoothness}}
When $\rho = 0$, the Laplacian smoother $\wh{f}_{\LS} = Y$, the test statistic $T_{\LS} = \frac{1}{n}\|Y\|_2^2$, and the threshold $\wh{t}_b = 1 + 2bn^{-1/2}$. The expectation of $T_{\LS}$ is 
\begin{equation*}
\Ebb\bigl[T_{\LS}\bigr] = \mathbb{E}\bigl[f_0^2(X)\bigr] + 1 \geq p_{\min} \norm{f_0}_{\Leb^2(\Xset)}^2 + 1
\end{equation*}
When $f_0 \in \Leb^4(\Xset,M)$, the variance can be upper bounded
\begin{equation*}
\Var\bigl[T_{\LS}\bigr] \leq \frac{1}{n}\Bigl(3 + p_{\max} M^4 + p_{\max}\norm{f_0}_{\Leb^2(\Xset)}^2\Bigr).
\end{equation*}
Now, let us assume that
\begin{equation*}
\norm{f_0}_{\Leb^2(X)}^2 \geq \frac{4b}{p_{\min}} n^{-1/2},
\end{equation*}
so that $E[T_{\LS}] - \wh{t}_b \geq E[T_{\LS}]/2$. Hence, by Chebyshev's inequality
\begin{align*}
\mathbb{P}_{f_0}\Bigl(T_{\LS} \leq \wh{t}_b \Bigr) & \leq 4 \frac{\Var_{f_0}\bigl[T_{\LS}\bigr]}{\mathbb{E}[f^2(X)]^2} \\
& \leq \frac{4}{n} \cdot \frac{ 3 + p_{\max}\bigl(M^4 + \|f_0\|_{\Leb^2(\Xset)}^2 \bigr)}{p_{\min}^2 \|f_0\|_{\Leb^2(\Xset)}^4} \\
& \leq \frac{1}{16b^2}\Bigl(3 + p_{\max}n^{-1/2} + p_{\max}M^4\Bigr).
\end{align*}

\section{Concentration Inequalities}
\begin{lemma}
	\label{lem:chi_square_bound}
	Let $\xi_1,\ldots,\xi_N$ be independent $N(0,1)$ random variables, and let $U := \sum_{k = 1}^{N} a_k(\xi_k^2 - 1)$.  Then for any $t > 0$,
	\begin{equation*}
	\Pbb\Bigl[U \geq 2 \norm{a}_2 \sqrt{t} + 2 \norm{a}_{\infty}t\Bigr] \leq \exp(-t).
	\end{equation*}
	In particular if $a_k = 1$ for each $k = 1,\ldots,N$, then
	\begin{equation*}
	\Pbb\Bigl[U - N \geq 2\sqrt{N t} + 2t\Bigr] \leq \exp(-t).
	\end{equation*}
\end{lemma}

The proof of Lemma~\ref{lem:empirical_norm_sobolev} relies on (a variant of) the Paley-Zygmund Inequality.
\begin{lemma}
	\label{lem:paley_zygmund}
	Let $f$ satisfy the following moment inequality for some $b \geq 1$:
	\begin{equation}
	\label{eqn:paley_zygmund_1}
	\Ebb\bigl[\norm{f}_n^4\bigr] \leq \left(1 + \frac{1}{b^2}\right)\cdot\Bigl(\Ebb\bigl[\norm{f}_n^2\bigr]\Bigr)^2.
	\end{equation}
	Then,
	\begin{equation}
	\label{eqn:paley_zygmund_2}
	\mathbb{P}\left[\norm{f}_n^2 \geq \frac{1}{b} \Ebb\bigl[\norm{f}_n^2\bigr]\right] \geq 1 - \frac{5}{b}.
	\end{equation}
\end{lemma}
\begin{proof}
	Let $Z$ be a non-negative random variable such that $\mathbb{E}(Z^q) < \infty$. The Paley-Zygmund inequality says that for all $0 \leq \lambda \leq 1$,
	\begin{equation}
	\label{eqn:paley_zygmund_pf1}
	\mathbb{P}(Z > \lambda \mathbb{E}(Z^p)) \geq \left[(1 - \lambda^p) \frac{\mathbb{E}(Z^p)}{(\mathbb{E}(Z^q))^{p/q}}\right]^{\frac{q}{q - p}}
	\end{equation}
	Applying~\eqref{eqn:paley_zygmund_pf1} with $Z = \norm{f}_n^2$, $p = 1$, $q = 2$ and $\lambda = \frac{1}{b}$, by assumption~\eqref{eqn:paley_zygmund_1} we have
	\begin{equation*}
	\mathbb{P}\Bigl(\norm{f}_n^2 > \frac{1}{b} \mathbb{E}[\norm{f}_n^2]\Bigr) \geq \Bigl(1 - \frac{1}{b}\Bigr)^2 \cdot  \frac{\bigl(\mathbb{E}[\norm{f}_n^2]\bigr)^2}{\mathbb{E}[\norm{f}_n^4]} \geq \frac{\Bigl(1 - \frac{2}{b}\Bigr)}{\Bigl(1 + \frac{1}{b^2}\Bigr)} \geq 1 - \frac{5}{b}.
	\end{equation*}
\end{proof}

\end{document}