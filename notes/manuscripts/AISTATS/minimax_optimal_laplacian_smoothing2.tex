\documentclass[twoside]{article}
\usepackage{aistats2021}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{tikz}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}
\newcommand{\Lap}{L}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Vset}{\mathcal{V}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

%%% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}


\begin{document}
	
\twocolumn[

	\aistatstitle{Minimax optimal Laplacian smoothing}
	
	\aistatsauthor{Alden Green, Sivaraman Balakrishnan, and Ryan J. Tibshirani}

	\aistatsaddress{Carnegie Mellon University}]
	

\begin{abstract}
	\textcolor{red}{(TODO)}:
\end{abstract}

\section{Introduction}

In the random design nonparametric regression problem, we observe data $(X_1,Y_1),\ldots,(X_n,Y_n)$, where $X_1,\ldots,X_n$ are independent samples from a distribution $P$ supported on a domain $\Xset \subset \Reals^d$, and 
\begin{equation}
\label{eqn:random_design_regression}
Y_i = f_0(X_i) + \varepsilon_i
\end{equation}
with $\varepsilon_i \sim N(0,1)$ independent Gaussian noise. We will assume the following throughout:
\begin{enumerate}[label=(P\arabic*)]
	\item
	\label{asmp:bounded_lipschitz_density} 
	$P$ admits a density $p$ bounded away from $0$ and $\infty$, i.e.
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty,~~\textrm{for all $x \in \Xset$.}
	\end{equation*}
	Additionally, $p$ is Lipschitz, with Lipschitz constant $L_p$.
\end{enumerate}
Our goal is to perform statistical inference on the unknown regression function $f_0: \Xset \to \Reals$, by which we mean either (a) \emph{estimating} $f_0$ by $\wh{f}$, an estimator constructed from the data $(X_1,Y_1),\ldots,(X_n,Y_n)$ or (b) simply \emph{testing} whether $f_0 = 0$, i.e whether there is any signal present. 

In graph-based nonparametric regression, we perform the above inferential tasks by first building a neighborhood graph $G_{n,r}$ which captures the geometry of $P$ and $\mc{X}$ in an appropriate sense, and then constructing an estimate $\wh{f}$ which is smooth with respect to the graph $G_{n,r}$. The neighborhood graph $G_{n,r} = ([n],{\bf W})$ is a weighted, undirected graph on vertices $[n] = \{1,...,n\}$, which we associate with the samples $\{X_1,\ldots,X_n\}$. The $n \times n$ weight matrix ${\bf W} = ({\bf W}_{ij})_{ij}$ encodes proximity between pairs of samples; for a kernel function $K: [0,\infty) \to \Reals$ and connectivity radius $r > 0$, the entries $\mathbf{W}_{ij}$ are given by
\begin{equation*}
\label{eqn:neighborhood_graph}
{\bf W}_{ij} = K\Biggl(\frac{\norm{X_i - X_j}}{r}\Biggr).
\end{equation*}
where $\norm{\cdot} = \norm{\cdot}_{\Rd}$ is the Euclidean norm. Then the degree matrix ${\bf D}$ is the $n \times n$ diagonal matrix with entries ${\bf D}_{ii} = \sum_{j = 1}^{n}{\bf W}_{ij}$, and the graph Laplacian can be written as
\begin{equation}
\label{eqn:graph_Laplacian}
\Lap_{n,r} = \bf{D} - \bf{W}
\end{equation}

The Laplacian smoothing estimator $\wh{f}_{\LS}$ \citep{smola2003} is a penalized least squares estimator, given by
\begin{equation}
\label{eqn:laplacian_smoothing}
\wh{f}_{\LS} := \min_{f \in \Reals^n} \biggl\{\sum_{i = 1}^{n}(Y_i - f_i)^2 + \rho \cdot f^T \Lap_{n,r} f \biggr\}
\end{equation}
where $\rho > 0$ acts a tuning parameter on the penalty $f^T \Lap_{{n,r}} f$.
Assuming~\eqref{eqn:laplacian_smoothing} is a reasonable estimator of $f_0$, the squared empirical norm
\begin{equation}
T_{\LS} = \frac{1}{n}\bigl\|\wt{f}_{\LS}\bigr\|_2^2 \label{eqn:laplacian_smoothing_test}
\end{equation}
is in turn a reasonable statistic to assess whether or not $f_0 = 0$. 

Laplacian smoothing has many advantages compared to other nonparametric regression methods. For instance:
\begin{itemize}
	\item It is fast, easy, and stable to compute. Letting ${\bf Y} = (Y_1,\ldots,Y_n) \in \Reals^n$, it is not hard to see that the minimizer $\wt{f}_{\LS} = (\rho \Lap_{n,r} + I)^{-1}{\bf Y}$ can be computed by solving a linear equation; in practice, one typically chooses $r$ to be small, with the result being that the matrix $\Lap_{n,r}$ is sparse. There exist known fast solvers of exactly this system.
	\item Is is generalizable to non-standard data---for instance, text or images, or really any data modality on which it is possible to define a kernel.
	\item It is easily adaptable to the semi-supervised learning setting. 
\end{itemize}
\textcolor{red}{(TODO)}: Perhaps additional motivation is needed?

\paragraph{Related work.} For this reason a considerable body of work has emerged analyzing the properties of Laplacian smoothing, as well as of graph-based methods more generally. Roughly speaking this work can divided into two categories.

Works in the first category view the responses $Y = \theta + \varepsilon$ as noisy samples of a regression function $\theta \in \Reals^n$ defined on the vertices of a \emph{fixed graph} $G$. and bound the statistical error as a function of $\theta$ and $G$. (In the estimation literature, see for instance \citep{wang2016, sadhanala16,sadhanala17,kirichenko2017,kirichenko2018}; in the testing literature, see \citep{sharpnack2013,sharpnack2013b,sharpnack2015}.) Note that in our setting, the graph $G_{n,r}$ and the evaluations $(f_0(X_1),\ldots,f_0(X_n))$ are random rather than fixed, and our assumptions are placed only on the underlying objects $P$ and $f_0$ from which these random object arise. As such, both our analysis and conclusions are naturally quite different.

Works in the second category prove the consistency of neighborhood graph Laplacian smoothing and other neighborhood graph-based methods in the large-sample limit. They do so by establishing the convergence of the graph Laplacian $\Lap_{{n,r}}$ to the continuum Laplace operator $\Delta_P: C^2(\Xset) \to \Leb^2(\Xset)$,
\begin{equation}
\label{eqn:laplace_operator}
\Delta_Pg := -\frac{1}{p} \dive(p^2 \nabla g)
\end{equation}
as $n \to \infty$ and $r \to 0$. This is meant in (at least) three senses: (1) \emph{pointwise convergence}, meaning that $\Lap_{n,r}f \to \Delta_Pf$ in an appropriate norm \citep{belkin03,belkin05,lafon04,hein05,singer06,gine06}; (2) spectral convergence, meaning the eigenvalues $\lambda_k(\Lap_{n,r}) \to \lambda_k(\Delta_P)$, where the latter are the Dirichlet eigenvalues of $\Delta_P$ \citep{belkin07,burago2014,trillos2018,trillos2019,calder2019}.\footnote{\label{footnote:dirichlet_eigenvalues} Formally, Dirichlet eigenvalues are solutions to the problem
	$$
	\Delta_Pg = \lambda g,\\
	\restr{g}{\partial \Xset} = 0
	$$}, and (3) convergence of norms, meaning $f^T \Lap_{n,r} f \to \langle f,\Delta_Pf \rangle_{\Leb^2(P)}$ \citep{bousquet03,hein06,zhou11}. While these results imply \emph{consistency} of many different graph Laplacian based procedures, the statistical \emph{optimality} of Laplacian smoothing over neighborhood graphs is still not well understood.

\paragraph{Our contributions.} Our three main contributions lie in this latter direction, and they can all be summarized as follows: Laplacian smoothing methods are minimax optimal over first order Sobolev spaces. In more detail, we will show that when $f_0$ belongs to the first order Sobolev ball $H^1(\Xset;M)$:
\begin{itemize}
	\item \textbf{Minimax optimal estimation.} With high probability, $\frac{1}{n}\bigl\|\wt{f}_{\LS} - f_0\bigr\|_{2}^2 \lesssim M^{d/(2+d)}n^{-2/(2 + d)}$.
	\item \textbf{Minimax optimal testing.}
	A level-$\alpha$ test constructed using $T_{\LS}$ has non-trivial power whenever $\norm{f_0}_{\Leb^2(\Xset)}^2 \gtrsim M^{2d/(4 + d)}n^{-4/(4 + d)}$. 
	\item \textbf{Manifold adaptivity.}
	If $\mc{X} \subset \Rd$ is a submanifold of dimension $m < d$, both of the aforementioned rates hold with $d$ replaced by $m$.
\end{itemize}

\paragraph{Organization.} We now describe the structure of the rest of this paper, which builds to the aforementioned minimax results.
\begin{itemize}
	\item In Section~\ref{sec:minimax_optimal_regression_sobolev_spaces}, for completeness we review some standard results regarding minimax nonparametric regression over Sobolev spaces.
	\item In Section~\ref{sec:graph_sobolev_classes}, we introduce the graph Sobolev class $H^1(G_{n,r})$, which we view as a graph-based analog to the continuum Sobolev class $H^1(\Xset)$. We show that key properties of $H^1(\Xset)$ used to derive minimax rates are shared by $H^1(G_{n,r})$.
	\item In Section~\ref{sec:minimax_optimal_laplacian_smoothing}, we leverage the results of Section~\ref{sec:graph_sobolev_classes} to derive tight upper bounds on the error of Laplacian smoothing methods.
	\item In Section~\ref{sec:simulations} we provide some experiments supporting our theory, before concluding in Section~\ref{sec:discussion}.
\end{itemize}

\textbf{Notation.}

We use $A,A_1, \ldots$ and $C,C_1, \ldots$ to refer to constants which do not depend on $n$, $r$, or $f_0$; likewise with $a, a_1,\ldots$ and $c, c_1,\ldots$.

\textcolor{red}{(TODO)}: Complete this.

\section{Minimax-optimal regression over Sobolev spaces}
\label{sec:minimax_optimal_regression_sobolev_spaces}

Before we get to our main results, we review minimax optimal estimation and testing rates over Sobolev classes. We also single out a notable method which achieves these rates: \emph{smoothing splines}. As we will see, Laplacian smoothing can be seen as a graph-based---i.e. discrete---counterpart to the ``continuous-time'' approach of smoothing splines. 

\paragraph{Sobolev spaces, and the Sobolev embedding theorem.} We start by briefly introducing Sobolev spaces, emphasizing only the material relevant to our statistical context. 

Roughly speaking, (Hilbert-)Sobolev spaces contain functions $f \in \Leb^2(\Xset)$ with derivatives which themselves belong to $\Leb^2(\Xset)$. More formally, for a given integer $s$, the Sobolev space $H^{s}(\Xset)$ consists of all functions $f \in \Leb^2(\Xset)$ such that for each multiindex $\alpha = (\alpha_1,\ldots,\alpha_d) \in \mathbb{N}^d$ satisfying $\abs{\alpha} := \sum_{i = 1}^{d} \alpha_i \leq s$, the weak derivative $D^{\alpha}f$ exists and belongs to $\Leb^2(\Xset)$. For such functions, the Sobolev seminorm $\seminorm{f}_{H^{s}(\Xset)}$ and norm $\norm{f}_{H^{s}(\Xset)}$ are given by 
\begin{equation*}
\seminorm{f}_{H^s(\Xset)}^2 = \sum_{\abs{\alpha} = s}\int_{\mathcal{X}} \Bigl|\bigl(D^{\alpha}f\bigr)(x)\Bigr|^2 \,dx, ~~ \norm{f}_{H^{s}(\Xset)}^2 = \sum_{k = 0}^{s} \seminorm{f}_{H^k(\Xset)}^2
\end{equation*}
and for a given $M > 0$, the order $s$ Sobolev ball is $H^s(\Xset, M) = \set{f: \norm{f}_{H^s(\Xset)} \leq M}$. The graph Sobolev seminorm $f^T \Lap_{n,r} f$ should be viewed as a random, discrete approximation to the continuum first-order Sobolev seminorm $|f|_{H^1(\Xset)}^2$, and so in our analysis of Laplacian smoothing we will mainly focus on the first-order Sobolev space $H^1(\Xset)$ hereafter.

Much of statistical analysis of nonparametric regression over Sobolev spaces relies on the Sobolev embedding theorem, which establishes that functions in Sobolev spaces also satisfy other, (even more) classical notions of smoothness. Concretely, the Sobolev space $H^1(\Xset)$ will continuously embed either into a Holder space $C^{0,q}(\Xset)$ or an $\Leb^p$ space, depending on the dimension $d$.
\begin{itemize}
	\item When $d = 1$, any function $f \in H^1(\Xset)$ satisfies $\|f\|_{H^1(\Xset)} \leq C\|f\|_{C^{0,q}(\Xset)}$ for $q = 1/2.$
	\item When $d > 2$, any function $f \in H^1(\Xset)$ satisfies $\|f\|_{H^1(\Xset)} \leq C\|f\|_{\Leb^{p}(\Xset)}$ for $p = 2d/(d - 2)$. 
\end{itemize}
In the above, the exponents $p$ and $q$ cannot be improved upon. 

% AJG commented this out.
% When $d = 1$, the embedding of $H^1(\Xset)$ into $C^{0,q}(\Xset)$ facilitates a general analysis of (penalized) least estimators over $H^1(\Xset)$, using arguments from empirical process theory. However when $d \geq 2$, such arguments fall apart: for example, good bounds on the metric entropy of $H^1(\Xset)$ are not known in this setting. More troubling still, least squares estimators over $H^1(\Xset)$ are not even well-posed when $d \geq 2$.

% For these reasons, analysis of least squares estimators for the nonparametric regression problem over Sobolev spaces typically assume $d = 1$---or more generally that $f_0 \in H^s(\Xset)$ for some $2s > d$. However, the Laplacian smoother $\wh{f}_{\LS}$ is a special least-squares estimator---it has a nice closed-form solution amenable to a bias-variance decomposition. We therefore do not need to rely on empirical process theory, and so we will be able to analyze $\wh{f}_{\LS}$ and $T_{\LS}$ beyond the univariate setting.

As we will see, the Sobolev embedding theorem has rather grave consequences for least-squares estimators over $H^1(\Xset)$ when $d > 1$, rendering such estimators ill-posed. However, the Laplacian smoother $\wh{f}_{\LS}$ is a special least-squares estimator. It has a nice closed-form solution amenable to a bias-variance decomposition, and so we will be able to analyze $\wh{f}_{\LS}$ and $T_{\LS}$ beyond the univariate setting.

\paragraph{Connecting Laplacian smoothing to smoothing splines.} 
To better motivate Laplacian smoothing as a tool for nonparametric regression, we connect it to one of the most classical methods of nonparametric regression: smoothing splines.

Smoothing splines are specially designed for regression over Sobolev spaces, as they are penalized least squares estimators where the penalty is itself a Sobolev semi-norm. Mathematically, the (first-order) smoothing spline estimator $\wh{f}_{\SM}$ is defined as the solution to the following optimization problem,
\begin{equation}
\label{eqn:smoothing_spline}
\wt{f}_{\SM} = \argmin_{f} \biggl\{\sum_{i = 1}^{n} \bigl(Y_i - f(X_i)\bigr)^2 + \rho \cdot \seminorm{f}_{H^{1}(\Xset)}^2\biggr\}.
\end{equation}
In~\eqref{eqn:smoothing_spline}, the minimum can either be taken over all $f \in H^1(\Xset)$ or over all $f \in C^1(\Xset)$; we shall refer to the former as smoothing splines, and the latter as \emph{Lipschitz} smoothing splines.

By viewing $\Delta_P$ as the continuum limit of~$\Lap_{n,r}$, one can tie the penalty used by the smoothing spline estimator $\wt{f}_{\SM}$ to that used by the Laplacian smoothing estimator $\wh{f}_{\LS}$. Suppose for the moment that $P$ is the uniform distribution over $\Xset$. Using the definition~\eqref{eqn:laplace_operator}, integrating by parts, and invoking the aforementioned convergence properties of $\Lap_{n,r}$, we have that for any $f \in C^{2}(\Xset)$ which satisfies Dirichlet boundary conditions (see footnote~\ref{footnote:dirichlet_eigenvalues}),
\begin{equation}
\label{eqn:seminorm_convergence}
\abs{f}_{H^1(\Xset)}^2 = \int_{\Xset} \bigl(\Delta_P f\bigr)(x) \cdot f(x) \,dx = \lim_{n \to \infty} \frac{1}{n^{2}r^{(d + 2)}}f^T \Lap_{n,r} f.
\end{equation}

\eqref{eqn:seminorm_convergence} establishes asymptotic equality (up to a scaling factor) between the penalties in~\eqref{eqn:smoothing_spline} and~\eqref{eqn:laplacian_smoothing}, when the distribution $P$ is uniform. But it also sheds light on an additional feature of Laplacian smoothing, relevant when $P$ is non-uniform: the penalty $f^T \Lap_{n,r} f$ automatically adapts to the design density $p$, in a way that the penalty $|f|_{H^1(\Xset)}^2$ does not. This automatic adaptivity to $P$ turns out to be a formal advantage of Laplacian smoothing relative to smoothing splines, as we will see in Section~\ref{sec:minimax_optimal_laplacian_smoothing}.

\paragraph{Minimax-optimality of smoothing splines.} 
One reason all of this is intriguing is that, as mentioned previously, the smoothing spline estimator $\wt{f}_{\SM}$---and the resultant test statistic $T_{\SM} := \|\wt{f}_{\SM}\|_n^2$--- have strong statistical properties. To be precise, they achieve the well-known minimax rates over Sobolev spaces.

Under~\ref{asmp:domain}, the minimax estimation rate over Sobolev balls is (see e.g. \citep{tsybakov2008_book})
\begin{equation}
\label{eqn:sobolev_space_estimation_minimax_rate}
\inf_{\wh{f}} \sup_{f_0 \in H^1(\Xset, M)} \Ebb\Bigl[\norm{\wh{f} - f_0}_{L^2(\Xset)}^2\Bigr] \asymp M^{2d/(2 + d)}n^{-2/(2 + d)}
\end{equation}
In the goodness-of-fit testing problem, we ask for a test function which can distinguish between the null and alternative hypotheses
\begin{equation}
\mathbf{H}_0: f_0 = 0, ~~\textrm{versus}~~ \mathbf{H}_a: f_0 \in \mc{F} \setminus \{0\}
\end{equation} 
where $\mc{F}$ is some class of functions. The minimax critical radius is the smallest value of $\epsilon$ such that some level-${\alpha}$ test $\phi$ has power at least $1 - \alpha$ over all $\mc{F}_{\epsilon} := \mc{F} \cap \{f: \|f\|_{\Leb^2(\Xset)} \geq \epsilon\}$. Testing whether a regression function $f_0$ is equal to $0$ is an easier problem than estimating $f_0$, and so the minimax testing critical radius over $H^1(\Xset,M)$ is much smaller than the minimax estimation rate:
\begin{equation}
\label{eqn:sobolev_space_testing_critical_radius}
\epsilon^2\Bigl(H^1(\Xset,M)\Bigr):= \inf\Biggl\{\epsilon^2: \inf_{\phi} \biggl[\Ebb_0[\phi] +  \sup_{f_0 \in H_{\epsilon}^1(\Xset,M)} \Ebb_{f_0}[1 - \phi]\biggr] \leq 2\alpha \Biggr\} \asymp M^{2d/(4 + d)}n^{-4/(4 + d)}~~\textrm{for $1 \leq d < 4$;}
\end{equation}
see \cite{ingster09} for a proof of this statement, and \cite{ariascastro2018} for a more extended discussion of the minimax paradigm in nonparametric testing. When $d \geq 4$ we no longer have a continuous embedding of $H^1(\Xset)$ into $\Leb^4(\Xset)$; the functions in $H^1(\Xset)$ are very irregular, and the minimax rates in this regime are unknown. 

The estimator $\wt{f}_{\SM}$ and the level-$\alpha$ test $\wt{\phi}_{\SM} := \1\{T_{\SM} \geq \wt{t}_{\alpha}\}$ are known to be minimax rate-optimal---meaning they achieve the rates in~\eqref{eqn:sobolev_space_estimation_minimax_rate} and~\eqref{eqn:sobolev_space_testing_critical_radius}, respectively---when $d = 1$ (see e.g. \cite{vandergeer2000} for the estimation result, and \cite{liu2019} for the testing result). When $d > 1$, as a consequence of the Sobolev Embedding Theorem\footnote{Or to be more precise, as a consequence of the \emph{tightness} of the Sobolev Embedding Theorem.} the variational problem~\eqref{eqn:smoothing_spline} is not well-posed: as illustrated in e.g. \cite{green93}, the criterion is minimized by interpolating the responses $\mathbf{Y}$, and the resulting estimator $\wt{f}_{\SM}$ will be inconsistent. By replacing the domain of minimization $H^1(\Xset)$ by $C^1(\Xset)$, we can force the solution to~\eqref{eqn:smoothing_spline} to be less wild, and \cite{birge1993} show that the resulting Lipschitz smoothing spline is not merely consistent but in fact minimax optimal when $d = 2$.\footnote{At least in the estimation problem. We could not find either a positive or negative result for the testing problem.} However when $d > 2$, the Holder space $C^1(\Xset)$ is itself too large, and even the Lipschitz smoothing spline has risk provably larger than optimal \cite{birge1993}. Previewing things a bit, we will see in Section~\ref{sec:minimax_optimal_laplacian_smoothing} that a similar phenomenon occurs for the Laplacian smoother $\wh{f}$, but interestingly the breakdown point will come when $d > 4$, rather than $d > 1$ or $d > 2$.

Finally, it is well known that if $\Xset$ satisfies the manifold hypothesis, meaning informally that it has intrinsic dimension $m < d$ (see~\ref{asmp:domain_manifold} in Section~\ref{sec:graph_sobolev_classes} for a formal statement), both~\eqref{eqn:sobolev_space_estimation_minimax_rate} and~\eqref{eqn:sobolev_space_testing_critical_radius} continue to hold, but with the ambient dimension $d$ replaced by the intrinsic dimension $m$ \citep{bickel2007,ingster2000}. Encouragingly, the smoothing spline estimator $\wt{f}_{\SM}$ and test $\wt{\phi}_{\SM}$ achieve these faster rates, with one caveat: in order for smoothing splines to properly leverage the manifold hypothesis, the domain $\Xset$ must be known.

\paragraph{Consequences?} Intuitively, the connections between Laplacian smoothing and smoothing splines give us reason to hope that the strong statistical properties of the latter might be shared by the former. Unfortunately, it is not so easy to mathematically tie $\wh{f}_{\LS}$ to $\wt{f}_{\SM}$: for example, note that the convergence in~\eqref{eqn:seminorm_convergence} assumes $f \in C^2(\Xset)$ whereas the domain of minimization in~\eqref{eqn:smoothing_spline} is either $H^1(\Xset)$ or $C^1(\Xset)$. Instead, we will take a different approach and analyze $\wh{f}_{\LS}$ by leveraging properties of the graph Sobolev class $H^1(G_{n,r})$, which we now introduce.

\section{Graph Sobolev classes}
\label{sec:graph_sobolev_classes}

For $M > 0$, define the (first-order) graph Sobolev ball to be
\begin{equation}
\label{eqn:graph_sobolev_ball}
H^1(G_{n,r},M) = \Bigl\{f \in \Reals^n: f^T \Lap_{n,r} f \leq M^2\Bigr\}
\end{equation}
where we recall that the \emph{graph Sobolev seminorm} $f^T \Lap_{{n,r}} f$ is precisely the penalty term in~\eqref{eqn:laplacian_smoothing}. 

To understand the role graph Sobolev balls play in our analysis of Laplacian smoothing, let us return for a moment to the continuous-time setting. Upper bounds on the error of smoothing splines rest on two crucial facts, both of which involve the continuum Laplace operator $\Delta_P$. The first is the \textit{a priori} assumption that the Sobolev semi-norm $|f_0|_{H^1(\Xset)}^2 = \langle f_0, \Delta_P f_0 \rangle_{\Leb^2(P)} \leq M^2$.\footnote{The equality holding when $P$ is uniform and  for functions $f_0 \in C^2(\Xset)$ which satisfy Dirichlet conditions.} The second is an upper bound on the metric entropy of $H^1(\Xset,M)$, characterized by the eigenvalue decay $\lambda_k(\Delta_P) \asymp k^{2/d}$ (i.e. Weyl's Law). In this section we will show that both these statements have analogues in the graph setting.
\begin{itemize}
	\item For functions $f$ belonging to $H^1(\Xset,M)$, the evaluations $f \in (f(X_1),\ldots,f(X_n)) \in \Reals^n$ are upper bounded,
	\begin{equation}
	\label{eqn:graph_sobolev_seminorm_vague}
	f^T \Lap_{n,r} f \leq C M^2(n^{2}r^{(d + 2)}),
	\end{equation}
	\item For a range $1 \leq k \leq k_{\max}(n)$, the eigenvalues $\lambda_k(G_{n,r})$ satisfy
	\begin{equation}
	\label{eqn:neighborhood_graph_eigenvalue_vague}
	ck^{2/d} \leq \frac{1}{nr^{d + 2}} \lambda_k(G_{n,r})  \leq Ck^{2/d}.
	\end{equation}
\end{itemize}
Both statements will hold with high probability, and when we assume $\Xset$ is a manifold of intrinsic dimension $m$, they will hold with $d$ replaced by $m$.  These results elucidate why Laplacian smoothing ``works'' as well as its continuous-time counterpart ---in the sense of being statistically rate-optimal over (continuum) Sobolev classes.

\paragraph{Graph Sobolev semi-norm.}
We begin by upper bounding $f^T \Lap_{n,r} f$ under the assumption $f_0 \in H^1(\Xset,M)$. We will assume the kernel $K$ satisfies condition~\ref{asmp:kernel}.
\begin{enumerate}[label=(K\arabic*)]
	\item
	\label{asmp:kernel}
	$K:[0,\infty) \to [0,\infty)$ is a non-increasing function supported on $[0,1]$, its restriction to $[0,1]$ is Lipschitz, and $K(1) > 0$. Additionally, it is normalized so that
	\begin{equation*}
	\int_{\Reals^d} K\bigl(\norm{z}\bigr) \,dz = 1.
	\end{equation*}
	Let $\sigma_K := \frac{1}{d} \int_{\Rd} \|x\|^2 K(\|x\|) \,dx$.
\end{enumerate}
This is a relatively mild assumption: the choice of kernel is under the user's control, and moreover~\ref{asmp:kernel} covers many (though certainly not all) common kernel choices. In Lemma~\ref{lem:graph_sobolev_seminorm}, we prove that if the density $p$ is merely upper bounded then~\eqref{eqn:graph_sobolev_seminorm_vague} holds with respect to the graph Sobolev semi-norm.
\begin{lemma}
	\label{lem:graph_sobolev_seminorm}
	Assume~\ref{asmp:kernel} and additionally that $p(x) \leq p_{\max}$ for all $x \in \Xset$. Then, for any $f \in H^1(\Xset)$, 
	\begin{equation}
	\label{eqn:graph_sobolev_seminorm}
	f^T \Lap_{n,r} f \leq \frac{p_{\max}^2 \sigma_K}{\delta} n^2 r^{d + 2} |f|_{H^1(X)}^2
	\end{equation}
	with probability at least $1 - \delta$.
\end{lemma}
A few remarks:
\begin{itemize}
	\item 
	The inequality~\eqref{eqn:graph_sobolev_seminorm} resembles~\eqref{eqn:seminorm_convergence}. However, as we pointed out previously, to derive~\eqref{eqn:seminorm_convergence} one needs $f \in C^{2}(\Xset)$ as opposed to $f \in H^1(\Xset)$, and imposing this extra degree of regularity is unsuitable for our purposes. We use a more careful analysis to prove Lemma~\ref{lem:graph_sobolev_seminorm}, but note well that~\eqref{eqn:graph_sobolev_seminorm} is only an upper bound rather than a statement about consistency. This is the price we pay for insisting that $f$ have only one derivative; fortunately an upper bound will be sufficient for our purposes in Section~\ref{sec:minimax_optimal_laplacian_smoothing}.
	\item We note also that~\eqref{eqn:graph_sobolev_seminorm}---which holds with probability $1 - \delta$---depends proportionally on $1/\delta$, as opposed to $\log(1/\delta)$. The reason: the Sobolev assumption $f \in H^1(\Xset)$ only grants us control over the second moment of $f$ and its first derivative, and so we are afforded only the relatively weak concentration implied by Markov's inequality. If we assume $f$ and its first derivative are more uniformly bounded--for instance if $f \in C^1(\Xset)$---we can use Hoeffding's inequality to get sharper upper bounds.
\end{itemize}
 
\paragraph{Graph Laplacian eigenvalues.} Lemma~\ref{lem:neighborhood_eigenvalue} establishes upper and lower bounds on the graph Laplacian eigenvalues $\lambda_{k}(G_{n,r})$ in terms of the continuum Laplace eigenvalues $\lambda_k(\Delta_P)$. These bounds hold with high probability under appropriate regularity conditions on the domain $\Xset$, and for a range of radii $r$.
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{1}
	\item
	\label{asmp:domain}
	The domain $\Xset$ is an open, connected set with Lipschitz boundary.
\end{enumerate}
\begin{enumerate}[label=(R\arabic*)]
	\item
	\label{asmp:radius1}
	The radius $r$ satisfies $(\log n/n)^{1/d} \leq r \leq c_1$.
\end{enumerate}
\begin{lemma}
	\label{lem:neighborhood_eigenvalue}
	Suppose the density $p$, the domain $\Xset$, the kernel $K$ and the radius $r$ satisfy~\ref{asmp:bounded_lipschitz_density}, \ref{asmp:domain}, \ref{asmp:kernel},and~\ref{asmp:radius1} respectively. The following statement holds with probability at least $1 - A_1n\exp(-a_1nr^{d})$: for any $\ell \in \mathbb{N}$ such that
	\begin{equation}
	\label{eqn:neighborhood_eigenvalue_1}
	1 - A_1 r \sqrt{\lambda_{\ell}(\Delta_P)} \geq \frac{1}{2}
	\end{equation}
	we have that
	\begin{equation}
	\label{eqn:eigenvalue_bound}
	a_1 \lambda_k(G_{n,r}) \leq nr^{d+2} \lambda_k(\Delta_P) \leq A_1 \lambda_k(G_{n,r}),~~\textrm{for all $1 \leq k \leq \ell$}
	\end{equation}
\end{lemma}

The scaling~\eqref{eqn:neighborhood_graph_eigenvalue_vague} then follows by Weyl's Law (see Lemma 28 of~\citep{dunlop2020} for a proof that~\ref{asmp:bounded_lipschitz_density} and~\ref{asmp:domain} are sufficient for Weyl's Law to hold.).
\begin{corollary}
	\label{cor:neighborhood_eigenvalue}
	Suppose the density $p$ and the domain $\Xset$ satisfy~\ref{asmp:bounded_lipschitz_density} and~\ref{asmp:domain}; then there exist constants $a_2$ and $A_2$ such that
	\begin{equation}
	\label{eqn:weyls_law}
	a_2k^{2/d} \leq \lambda_k(\Delta_P) \leq A_2k^{2/d}~~\textrm{for all $k \in \mathbb{N}, k > 1$}.
	\end{equation}
	Therefore under the conditions of Lemma~\ref{lem:neighborhood_eigenvalue}, we have that
	\begin{equation}
	\label{eqn:neighborhood_eigenvalue_2}
	a_3 \cdot \min\Bigl\{nr^{d + 2}k^{2/d},nr^d\Bigr\} \leq \lambda_k(G_{n,r}) \leq A_3 \cdot \min\Bigl\{nr^{d + 2}k^{2/d}, nr^d\Bigr\}~~\textrm{for all $2 \leq k \leq n$}
	\end{equation}
	with probability at least $1 - A_1n\exp(-a_1nr^d)$.
\end{corollary}

We see that the inequalities in~\eqref{eqn:neighborhood_eigenvalue_2} match the desired scaling~\eqref{eqn:neighborhood_graph_eigenvalue_vague} for all $2 \leq k \leq (1/r)^d$. Indeed, if we could choose $r = n^{-1/d}$, we would get the desired scaling for all $2 \leq k \leq n$, but unfortunately choosing $r$ to be so small would violate~\ref{asmp:radius1}. Nevertheless, by choosing $r$ to be relatively small while still being in accordance with~\ref{asmp:radius1}, we can guarantee that only some of the largest eigenvalues $\lambda_k(G_{n,r})$ will fail to satisfy~\eqref{eqn:neighborhood_graph_eigenvalue_vague}. Fortunately, the behavior of these large eigenvalues will not meaningfully contribute to the error of $\wh{f}_{\LS}$ or $T_{\LS}$, as we shall see.

\paragraph{Manifold hypothesis.}

As mentioned in the introduction, an attractive quality of the neighborhood graph $G_{n,r}$ is that it captures the geometry of $P$ and $\Xset$. We now review two manifestations of this phenomenon, from \cite{trillos2019} and \cite{calder2019}: both the graph Sobolev semi-norm and graph Laplacian eigenvalues automatically adapt to the intrinsic dimension of $\Xset$. Formally, we make the following assumption on $\Xset$ (in order to directly use the results of~\cite{trillos2019} and~\cite{calder2019}).
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:domain_manifold}
	The measure $P$ is supported on a compact, connected, smooth submanifold $\Xset$ of $\Reals^d$; the domain $\Xset$ is of fixed dimension $1 \leq m \leq d$ and without boundary. Moreover, $P$ has a density $p$ with respect to the volume form of $\Xset$ which is bounded away from $0$ and $\infty$,
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty
	\end{equation*}
	for all $x \in \Xset$, and the density $p$ is Lipschitz with Lipschitz constant $L_p$.
\end{enumerate}
\begin{enumerate}[label=(R\arabic*)]
	\setcounter{enumi}{1}
	\item 
	\label{asmp:radius2}
	The radius $r$ satisfies $(\log n/n)^{1/m} \leq r \leq c_2$.
\end{enumerate}
\begin{proposition}[Lemma 5 of~\cite{trillos2019}]
	\label{prop:manifold_graph_sobolev_seminorm}
	Assume~\ref{asmp:kernel},~\ref{asmp:domain_manifold} and~\ref{asmp:radius2}. Then for any $f \in H^1(\Xset)$,
	\begin{equation*}
	f^T \Lap_{n,r} f \leq \frac{(1 + L_pr)^2}{\delta}n^{2}r^{m + 2} \sigma_K |f|_{H^1(\Xset)}^2
	\end{equation*}
	with probability at least $1 - \delta$.
\end{proposition}

\begin{proposition}[Restatement of Theorem 4 of~\cite{trillos2019}]
	\label{prop:manifold_neighborhood_eigenvalue}
	Assume~\ref{asmp:kernel}, ~\ref{asmp:domain_manifold}, and~\ref{asmp:radius2}. It holds that
	\begin{equation}
	\label{eqn:manifold_neighborhood_eigenvalue}
	a_5 \cdot \min\Bigl\{nr^{m + 2}k^{2/m},nr^d\Bigr\} \leq \lambda_k(G_{n,r}) \leq A_5 \cdot \min\Bigl\{nr^{m + 2}k^{2/m}, nr^m\Bigr\}~~\textrm{for all $2 \leq k \leq n$}	
	\end{equation}
	with probability at least $1 - A_4n\exp(-a_4nr^m)$.
\end{proposition}
Note that~\ref{asmp:domain_manifold} assumes $\Xset$ is without boundary, and so Propositions~\ref{prop:manifold_graph_sobolev_seminorm} and~\ref{prop:manifold_neighborhood_eigenvalue} do not imply Lemmas~\ref{lem:graph_sobolev_seminorm} or~\ref{lem:neighborhood_eigenvalue}.

\section{Minimax optimality of Laplacian smoothing}
\label{sec:minimax_optimal_laplacian_smoothing}

We now leverage the results of Section~\ref{sec:graph_sobolev_classes} to formalize the main conclusions of this work: that Laplacian smoothing methods are minimax rate-optimal. 

For all results in this section, we will assume that the density~$p$ satisfies~\ref{asmp:bounded_lipschitz_density} and the kernel $K$ satisfies~\ref{asmp:kernel}. Additionally, within our theorem statements we will use $c$ and $C$ to denote constants which possibly differ from theorem to theorem, and which may depend on $K$, $d$, $p$, and $\Xset$ but do not depend on $f_0$, the radius of the Sobolev ball $M$, or the sample size $n$. 

\paragraph{Estimation error of Laplacian smoothing.} 
Suppose the domain $\Xset$ satisfies~\ref{asmp:domain}. Under appropriate conditions, the Laplacian smoothing estimator $\wh{f}_{\LS}$ achieves, up to constants, the minimax risk $n^{-2/(2 + d)}$ over all $f_0 \in H^1(\Xset,M)$. To prove this, we will invoke Lemma~\ref{lem:graph_sobolev_seminorm} and Corollary~\ref{cor:neighborhood_eigenvalue} to suitably upper bound the squared-bias and variance of $\wt{f}_{\LS}$, respectively. In order to do so, we must restrict the range of the neighborhood graph radius $r$. 
\begin{enumerate}[label=(R\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:ls_kernel_radius_estimation}
	The neighborhood graph radius $r$ satisfies
	\begin{equation*}
	\biggl(\frac{\log n}{n}\biggr)^{1/d} \leq r \leq n^{-3/(4 + 2d)} M^{(d - 4)/(4 + 2d)}.
	\end{equation*}
\end{enumerate}
\begin{theorem}
	\label{thm:laplacian_smoothing_estimation1}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}, with $d < 4$ and $f_0 \in H^1(\Xset,M)$. Suppose that that the neighborhood graph $G_{n,r}$ is computed with a radius $r$ which satisfies~\ref{asmp:ls_kernel_radius_estimation},  and the Laplacian smoothing estimator $\wh{f}_{\LS}$ with $\rho = M^{-4/(2 + d)} (nr^{d + 2})^{-1} n^{-2/(2 + d)}$. It holds that
	\begin{equation*}
	\Bigl\|\wh{f}_{\LS} - f_0\Bigr\|_n^2 \leq \frac{C}{\delta} M^{d/(2 + d)} n^{-2/(2 + d)}
	\end{equation*}
	with probability at least $1 - \delta -  A_1n\exp(-a_1nr^d) - \exp(-c M^{d/(2d + 4)} n^{d/(2+d)})$.
\end{theorem}
To summarize: when $d = 1,2$ or $3$, the Laplacian smoothing estimator $\wh{f}_{\LS}$ has in-sample mean squared error within a constant factor of the minimax risk, except on a set of small probability. Some remarks:
\begin{itemize}
	\item When $d = 4$, our analysis results in an upper bound on the mean squared error within a $\log n$ factor of the minimax risk, but when $d \geq 5$ our analysis yields upper bounds that are much worse than the minimax rate. This mirrors the conclusions of~\cite{sadhanala16}, who investigate estimation rates of Laplacian smoothing over the $d$-dimensional grid graph. In our setting, when $d = 4$ we would set $r \asymp (\log n/n)^{1/4}$---slightly outside the scaling~\ref{asmp:ls_kernel_radius_estimation}---to get the near-optimal rate.
	\item The range of radii $r$ imposed by~\ref{asmp:ls_kernel_radius_estimation} is  compatible with practice, where by far the most common choice of radius is the connectivity threshold $r \asymp (\log(n)/n)^{1/d}$, chosen to make the graph $G_{n,r}$ as sparse as possible while still being connected. 
	\item The tuning parameter $\rho$ is chosen to balance the (conditional on ${\bf X}$) squared-bias and variance, i.e. chosen as large as possible while still with high probability satisfying
	\begin{equation}
	\label{eqn:laplacian_smoothing_bias_variance}
	\frac{\rho}{n} f_0^T \Lap_{{n,r}} f_0 \lesssim \frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_{k}(G_{n,r}) + 1\bigr)^2}.
	\end{equation}
	To balance bias and variance as in~\eqref{eqn:laplacian_smoothing_bias_variance} we require control on all the graph eigenvalues $\lambda_1(G_{n,r}),\ldots,\lambda_n(G_{n,r})$. As mentioned in our commentary after Lemma~\ref{lem:neighborhood_eigenvalue}, when $r$ is small enough the large eigenvalues $\lambda_k(G_{n,r})$ contribute negligibly to the variance term. The upper bound in~\ref{asmp:radius1} is the largest value of $r$ such that this statement is still true.
	\item The Laplacian smoothing estimator $\wh{f}_{\LS}$ is defined only at the design points $X_1,\ldots,X_n$ and we therefore measure error using the in-sample mean squared error $\norm{\cdot}_n^2$. Of course the in-sample error is a random variable, and our bound is thus a bound in high probability. That being said, it is possible to smoothly extend $\wh{f}_{\LS}$ to be defined on all of $\Xset$, or indeed all of $\Reals^d$---for instance, using the \emph{Nystrom extension}---and evaluate the error of the extension in $\Leb^2(\Xset)$ norm. Assuming that the extension of our estimators and the regression function $f_0$ are suitably smooth, tools from empirical process theory will guarantee that the $\Leb^2(\Xset)$ error is not too much greater than the in-sample error, but we do not further pursue the details here.
\end{itemize}

\paragraph{Testing error of Laplacian smoothing.}
Let us define a test using the statistic $T_{\LS}$. For $b \geq 1$, define the threshold $\wh{t}_b$ to be
\begin{equation*}
\wh{t}_{b} := \frac{1}{n}\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k(G_{n,r}) + 1\bigr)^2} + \frac{2b}{n}\sqrt{\sum_{k = 1}^{n} \frac{1}{\bigl(\rho \lambda_k(G_{n,r}) + 1\bigr)^4}},
\end{equation*}
and the Laplacian smoothing test $\wh{\phi}_{\LS} := \1\bigl\{T_{\LS} \leq \wh{t}_b\bigr\}$. (Clearly the test $\wh{\phi}_{\LS}$ depends on $b$, but we suppress this notationally.) The number $b \geq 1$ is selected by the user, with the choice made based on the tolerated level of Type I and Type II error. In fact, the threshold $\wh{t}_b$ is precisely the right choice to control the Type I error of $\wh{\phi}_{\LS}$; as we show in Lemma~\ref{lem:ls_fixed_graph_testing} in the supplementary material,
\begin{equation*}
\Ebb_0\Bigl[\wh{\phi}_{\LS}\Bigr] \leq \frac{1}{b^2}.
\end{equation*}

Theorem~\ref{thm:laplacian_smoothing_testing} establishes that this test also has small Type II error, uniformly over all $f_0$ separated from $0$ by at least the critical radius $\epsilon\bigl(H^1(\Xset,M)\bigr)$ given in~\eqref{eqn:sobolev_space_testing_critical_radius}. For this to hold, we will require a tight range of scalings for $r$.
\begin{enumerate}[label=(R\arabic*)]
	\setcounter{enumi}{3}
	\item 
	\label{asmp:ls_kernel_radius_testing}
	The neighborhood graph radius $r$ satisfies
	\begin{equation*}
	\biggl(\frac{\log n}{n}\biggr)^{1/d} \leq r \leq M^{(16 - d)/(8 + 2d)}n^{(d - 20)/(32 + 8d)}.
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_smoothing_testing}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}, with $d < 4$ and $f_0 \in H^1(\Xset,M)$ for $M \leq n^{(4 - d)/(4 + d)}$. Suppose that the neighborhood graph $G_{n,r}$ is computed with radius $r$ which satisfies~\ref{asmp:ls_kernel_radius_testing}, and the Laplacian smoothing test $\wh{\phi}_{\LS}$ with $\rho = (nr^{d + 2})^{-1} n^{-4/(4 + d)} M^{-8/(4 + d)}$, and $b \geq 1$. For all $f_0$ such that
	\begin{equation}
	\label{eqn:laplacian_smoothing_testing}
	\bigl\|f_0\bigr\|_{\Leb^2(\Xset)}^2 \geq C b^2 M^{2d/(4 + d)} n^{-4/(4 + d)}
	\end{equation} 
	the Type II error is upper bounded:
	\begin{equation*}
	\Ebb_{f_0}\Bigl[1 - \wh{\phi}_{\LS} \Bigr] \leq C\biggl(\frac{1}{b}\Bigl[1 + n^{-d(4 + d)}M^{-2d/(4 + d)}\Bigr] + A_1n\exp\bigl(-a_1nr^d\bigr)\biggr).
	\end{equation*}
\end{theorem}

A few remarks:
\begin{itemize}
	\item As mentioned in Section~\ref{sec:minimax_optimal_regression_sobolev_spaces}, when $d \geq 4$ the Sobolev balls $H^1(\Xset,M)$ include quite irregular functions $f \not\in \Leb^4(\Xset)$. Proving tight lower bounds over such classes is non-trivial, and to the best of our knowledge such an analysis remains outstanding. On the other hand, if we explicitly assume that $f \in \Leb^4(\Xset,M)$, then \cite{guerre02} show that the testing problem is characterized by the dimension-free lower bound $\epsilon^{2}(\Leb^4(\Xset,M)) \gtrsim n^{-1/2}$. Moreover, by training $\wh{f}_{\LS}$ to the interpolation limit, i.e. setting $\rho = 0$, the test $\wh{\phi}_{\LS}$ achieves (up to constants) this lower bound. That is, 
	\begin{equation}
	\label{eqn:laplacian_smoothing_testing_low_smoothness}
	E_{f_0}\Bigl[1 - \wh{\phi}_{\LS}\Bigr] \leq \frac{C(1 + M^4)}{b^2}
	\end{equation}
	for any $f_0 \in \Leb^4(\Xset,M)$ for which $\|f_0\|_{\Leb^2(\Xset)}^2 \geq C b^2n^{-1/2}$. 
	\item To compute the threshold $\wh{t}_b$, one must know each of the eigenvalues $\lambda_1(G_{n,r}),\ldots,\lambda_n(G_{n,r})$. Computing all $n$ of these eigenvalues is far more expensive than computing $T_{\LS}$. That being said, in practice we would not recommend using $\wh{t}_b$ anyway: the threshold is justified by asymptotic theory, but in practice $n$ is always finite, and using such thresholds can result in substantial loss of efficiency. We would instead make the standard recommendation to calibrate via permutation. 
\end{itemize}
It is worth circling back to our comparison of Laplacian smoothing methods to smoothing splines. While we motivated Laplacian smoothing methods in part by suggesting they might replicate the statistical properties of smoothing splines, we have in fact managed to show something stronger. Theorems~\ref{thm:laplacian_smoothing_estimation1} and~\ref{thm:laplacian_smoothing_testing} imply that Laplacian smoothing methods are minimax optimal for both estimation and testing in dimensions $d = 1,2$ and $3$, and are nearly minimax-optimal for the estimation problem when $d = 4$. On the other hand, as covered in Section~\ref{sec:minimax_optimal_regression_sobolev_spaces} smoothing spline methods are optimal only when $d = 1$ or, for Lipschitz smoothing splines, $d = 2$. Thus, we see a somewhat surprising gap emerge between the optimality properties of the discrete-time (Laplacian smoothing) and continuous-time (smoothing spline) approaches, in favor of the former.

\paragraph{Manifold adaptivity.} 

In Section~\ref{sec:graph_sobolev_classes}, we reviewed that when $\Xset$ is a submanifold of $\Rd$, the graph Laplacian automatically adapts to the geometry---and in particular, the intrinsic dimension $m$---of $\Xset$. This translates into improved upper bounds on the estimation and testing error of Laplacian smoothing; the upper bounds now depend on $m$ instead of $d$.

\begin{theorem}
	\label{thm:laplacian_smoothing_manifold}
	Let $(X_1,Y_1),\ldots,(X_n,Y_n)$ be random samples drawn according to the model~\eqref{eqn:random_design_regression}. Suppose that $\Xset$ satisfies~\ref{asmp:domain_manifold} for $m < 4$, and that $f_0 \in H^1(\Xset,M)$. For appropriate choices of tuning parameters $r$, $\rho$, and $b \geq 1$, each of the following statements holds.
	\begin{itemize}
		\item The Laplacian smoothing estimator satisfies
		\begin{equation*}
		\frac{1}{n}\bigl\|\wh{f}_{\LS} - f_0\bigr\|_2^2 \leq \frac{C}{\delta}M^{m/(2 + m)}n^{-2/(2 + m)}
		\end{equation*}
		with probability at least $1 - \delta - A_4n\exp(-a_4nr^m) - \exp(-c M^{m/(2m + 4)} n^{m/(2+m)})$.
		\item Suppose $M \leq n^{(4 - m)/(4 + m)}$. For all $f_0$ such that
		\begin{equation*}
		\bigl\|f_0\bigr\|_{\Leb^2(\Xset)}^2 \geq C b^2 M^{2m/(4 + m)} n^{-4/(4 + m)}
		\end{equation*} 
		the type II error of $\wh{\phi}$ is upper bounded:
		\begin{equation*}
		\Ebb_{f_0}\Bigl[1 - \wh{\phi}_{\LS} \Bigr] \leq C\biggl(\frac{1}{b}\Bigl[1 + n^{-m(4 + m)}M^{-2m/(4 + m)}\Bigr] + A_4n\exp\bigl(-a_4nr^m\bigr)\biggr).
		\end{equation*}
	\end{itemize}
\end{theorem}
In our supplementary material, we specify the right choices of $\rho$, as well as the permissible ranges of $r$. For now, we merely point out that they will depend only on the intrinsic dimension $m$, and not on any other properties of the manifold $\Xset$. In all other respects, $\wh{f}_{\LS}$ and $T_{\LS}$ are computed without any knowledge of $\Xset$. By contrast, the penalty in~\eqref{eqn:smoothing_spline} must be specially tailored to $\Xset$ in order for $\wt{f}_{\SM}$ and $T_{\SM}$ to achieve the minimax estimation and testing rates. This reveals another advantage of Laplacian smoothing as opposed to smoothing splines.

\section{Simulations}
\label{sec:simulations}

\textcolor{red}{(TODO)}

\section{Discussion}
\label{sec:discussion}

\textcolor{red}{(TODO)}

A few discussion items:
\begin{itemize}
	\item \textbf{Higher-order smoothness classes.} Laplacian smoothing can also be adapted to take advantage of additional assumed regularity on $f_0$, i.e. $f_0 \in H^s(\mc{\Xset})$ for $s > 1$. In the very special case of $s = 2$ and $d = 2$, we can show that this adapted estimator achieves the sharper minimax rates over these classes, but the general story--for all combinations of $s$ and $d$---remains beyond our reach.
	\item \textbf{Possible extensions}. Methodologically: kNN graphs and different normalizations of the Laplacian. Theoretically: relaxing~\ref{asmp:bounded_lipschitz_density}. 
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{../../../graph_testing_bibliography} 

\end{document}