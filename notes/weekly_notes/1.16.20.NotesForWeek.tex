\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
	\title{Notes for Week of 1/9/20 - 1/16/20}
	\author{Alden Green}
	\date{\today}
	\maketitle
	
	Let $X = \{x_1,
	\ldots, x_n\}$ be a sample drawn i.i.d. from a distribution $P$ on $\Rd$,
	with density~$p$.  For a radius $r > 0$, we define $G_{n,r}=(V,E)$ to be the
	\emph{$r$-neighborhood graph} of $X$, an unweighted, undirected graph with
	vertices $V=X$, and an edge $(x_i,x_j) \in E$ if and only if $K_r(x_i,x_j) = \norm{x_i -x_j} \leq r$, where $\norm{\cdot}$ is the Euclidean norm. We denote by $A \in
	\Reals^{n \times n}$ the adjacency matrix, with entries $A_{uv} = 1$ if
	$(u,v) \in E$ and $0$ otherwise.  We also denote by $D$ the diagonal degree
	matrix, with entries $D_{uu} := \sum_{v \in V} A_{uv}$. The graph Laplacian is $L = D - A$, and we write its spectral decomposition as $L = V S V^T$. 
	
	Suppose in addition to the random design points $X = \set{x_1,\ldots,x_n} \sim P$, we observe responses
	\begin{equation}
	\label{eqn:regression_known_variance}
	y_i = f(x_i) + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
	\end{equation} 
	To test whether $f = f_0$, we propose the following \emph{eigenvector projection} test statistic:
	\begin{equation}
	\label{eqn:graph_spectral_projections}
	T_{\mathrm{spec}} := \frac{1}{n} \sum_{k = 0}^{\kappa} \left(\sum_{i = 1}^{n} v_{k,i} y_i\right)^2
	\end{equation}
	where $v_k$ is the $k$th eigenvector of $L$ (ordered according to eigenvalues $s_1 \leq s_2 \leq \ldots \leq s_n$).
	
	The eigenvector projection test is minimax optimal over the balls in higher order Holder spaces $C_d^s(\Xset;L)$.
	
	\begin{theorem}
		\label{thm:higher_order_holder_testing_rate}
		Let $b \geq 1$ be a fixed constant, and let $d$ and $s$ be positive integers such that $d < 4s$. Suppose that $P$ is an absolutely continuous probability measure over $\mathcal{X} = [0,1]^d$ with density function $p \in C^{s-1}(\mathcal{X};R)$ bounded above and below by constants, i.e
		\begin{equation*}
		0 < p_{\min} < p(x) < p_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
		\end{equation*}
		Then the following statement holds: if the test $\phi_{\spec} = \1\{T_{\spec} \geq \tau \}$ is performed with parameter choices 
		\begin{equation*}
		n^{-1/(2(s - 1) + d)} \leq r(n) \leq n^{-4/((4s + d)(2+d))}, ~\kappa = n^{2d/(4s + d)}, ~\tau = \frac{\kappa}{n} + b\sqrt{\frac{2\kappa}{n^2}}
		\end{equation*}
		then there exists constants $c_1,c_2$ which may depend on $d,R,$ and $s$ but are independent of the sample size $n$ such that for every $\epsilon \geq 0$ satisfying
		\begin{equation}
		\label{eqn:higher_order_holder_testing_rate}
		\epsilon^2 \geq c_1 \cdot b^2 \cdot n^{-4s/(4s + d)}
		\end{equation}
		the worst-case risk is upper bounded
		\begin{equation}
		\label{eqn:higher_order_holder_testing_rate_1}
		\mathcal{R}_{\epsilon}(\phi_{\mathrm{spec}}; C_d^{s}(\mathcal{X};R)) \leq \frac{c_2}{b}.
		\end{equation}
	\end{theorem}

\section{Proof}
Let $G = (V,E)$ be a graph over vertices $V = \set{v_1,\ldots,v_n}$, and let $\beta = (\beta_1,\ldots,\beta_n) \in \Reals^n$ be a signal over the vertices $V$. We observe responses $Y = (y_1,\ldots,y_n)$ according to the model
\begin{equation*}
y_i = \beta_i + \varepsilon_i, ~~ \varepsilon_i \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1)
\end{equation*}
Letting $L = VSV^T$ be the spectral decomposition of the Laplacian $L$ of $G$, our graph spectral test statistic is 
\begin{equation*}
T_{\spec} = \frac{1}{n}\sum_{k = 1}^{\kappa} \left(\sum_{i = 1}^{n} v_i y_i\right)^2
\end{equation*}
where $\kappa$ is a tuning parameter. The resulting test we will use is
\begin{equation*}
\phi_{\spec} = \1\{T_{\spec} \geq \frac{\kappa}{n} + t(b)\},~~\textrm{where}~ t(b) = b\sqrt{\frac{2\kappa}{n^2}}~~\textrm{for $b \geq 1$.}
\end{equation*}

Let $S_s(\beta;G)$ be a measure of smoothness the signal $\beta$ displays over the graph $G$, given by
\begin{equation*}
S_s(\beta;G) := \beta^T L^s \beta
\end{equation*}
In Lemma~\ref{lem:higher_order_fixed_graph_testing}, we upper bound the Type I and Type II error of the test $\phi_{\spec}$. Our bound on the Type II error will be stated as a function of $S_2(\beta;G)$ as well as the $\kappa$th eigenvalue $s_{\kappa}$.

\begin{lemma}
	\label{lem:higher_order_fixed_graph_testing}
	Let $1 \leq \kappa \leq n$ be an integer.
	\begin{enumerate}
		\item \textbf{Type I error:} Under the null hypothesis $\beta = \beta_0 = 0$, the Type I error of $\phi_{\spec}$ is upper bounded
		\begin{equation}
		\label{eqn:higher_order_graph_spectral_type_I_error}
		\mathbb{E}_{\beta_0}(\phi_{\spec}) \leq \frac{1}{b^2}.
		\end{equation}
		\item \textbf{Type II error:} For any $b$ and $\beta$ such that
		\begin{equation}
		\label{eqn:higher_order_fixed_graph_testing_critical_radius}
		\frac{1}{n} \sum_{i = 1}^{n} \beta_i^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_s(\beta;G)}{ns_{\kappa}^s}
		\end{equation}
		the Type II error of $\phi_{\spec}$ is upper bounded,
		\begin{equation}
		\label{eqn:higher_order_graph_spectral_type_II_error}
		\mathbb{E}_{\beta}(1 - \phi_{\spec}) \leq \frac{3}{b}.
		\end{equation}
	\end{enumerate}
\end{lemma}

To prove Lemma~\ref{lem:higher_order_fixed_graph_testing} we will first compute (bounds on) the expectation and variance of the test statistic $T_{\mathrm{spec}}$, and then use Chebyshev's inequality to show \eqref{eqn:higher_order_graph_spectral_type_I_error} and \eqref{eqn:higher_order_graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Mean of $T_{\mathrm{spec}}$:} Using the notation $\dotp{v}{w} = \sum_{i = 1}^{n} v_iw_i$, we have
\begin{align*}
\Ebb(T_{\spec}) & = \frac{1}{n}\left(\sum_{k = 1}^{\kappa} \dotp{\beta}{v_k}^2 + \Ebb\bigl( \dotp{\varepsilon}{v_k}^2 + 2 \dotp{\varepsilon}{v_k} \dotp{\beta}{v_k}\bigr)\right) \\
& = \frac{\kappa}{n} + \frac{1}{n}\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2.
\end{align*}
When $\beta = 0$, this equals $\kappa/n$. Otherwise, we have the following lower bound:
\begin{align*}
\sum_{k = 1}^{\kappa}\dotp{\beta}{v_k}^2 & = \norm{\beta}_2^2 - \sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 \\
& \geq \norm{\beta}_2^2 - \frac{1}{s_{\kappa}^s}\sum_{k = \kappa + 1}^{n} \dotp{\beta}{v_k}^2 s_k^s \\
& \geq \norm{\beta}_2^2 - \frac{S_s(\beta;G)}{s_{\kappa}^s},
\end{align*}
and therefore $\Ebb(T_{\spec}) \geq \kappa/n + n^{-1}(\norm{\beta}_2^2 - S_s(\beta;G)/s_{\kappa}^s)$. 

\vspace{.2 in}

\textit{Variance of $T_{\mathrm{spec}}$:}
We write $T_{\mathrm{spec}} = n^{-1} y^T V_{\kappa} V_{\kappa}^T y$ where $V_{\kappa}$ is the $n \times \kappa$ matrix with eigenvectors $v_1,\ldots,v_{\kappa}$ as columns. Consequently,
\begin{align}
\Var(T_{\spec}) & = \frac{1}{n^2} \Var(y^T V_{\kappa} V_{\kappa}^T y) \\
& = \frac{1}{n^2} \Var((\beta + \varepsilon)^T V_{\kappa} V_{\kappa}^T (\beta + \varepsilon)) \\
& = \frac{1}{n^2} \Var(2 \beta^T V_{\kappa} V_{\kappa}^T \varepsilon + \varepsilon^T V_{\kappa} V_{\kappa}^T \varepsilon) \\
& \leq \frac{1}{n^2}(4 \beta^T V_{\kappa} V_{\kappa}^T \beta + 2\kappa)
\end{align}
where the last inequality follows from standard properties of the Gaussian distribution. We now move on to showing the desired inequalities \eqref{eqn:higher_order_graph_spectral_type_I_error} and \eqref{eqn:higher_order_graph_spectral_type_II_error}.

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:higher_order_graph_spectral_type_I_error}:} By Chebyshev's inequality,
\begin{align*}
\Pbb_{\beta = 0}\bigl(T_{\spec} \geq \frac{\kappa}{n} + t(b)\bigr)
& \leq \Pbb_{\beta = 0}\bigl(\abs{T_{\spec} - \frac{\kappa}{n}} \geq t(b)\bigr) \\
& \leq \frac{\Var_{\beta = 0}(T_{\spec})}{t(b)^2} = \frac{1}{b^2}.
\end{align*}

\vspace{.2 in}

\textit{Proof of~\eqref{eqn:higher_order_graph_spectral_type_II_error}:} For simplicity, we introduce the notation
\begin{equation*}
\Delta = \frac{\norm{\beta}_2^2}{n} - \frac{S_s(\beta;G)}{ns_{\kappa}^s}.
\end{equation*}
Assumption~\eqref{eqn:higher_order_fixed_graph_testing_critical_radius} implies $\Delta \geq 2 t(b)$. Then another application of Chebyshev's inequality gives us
\begin{align*}
\Pbb_{\beta}\bigl(T_{\spec} \leq \frac{\kappa}{n} + t(b)\bigr) & = \Pbb_{\beta}\bigl(T_{\spec} - \Ebb_{\beta}(T_{\spec}) \leq t(b) - \Delta \bigr) \\
& \leq \Pbb_{\beta}\bigl(\abs{T_{\spec} - \Ebb_{\beta}(T_{\spec})} \leq \Delta - t(b) \bigr) \tag{since $\Delta \geq t(b)$}	\\
& \leq \frac{\Var_{\beta}(T_{\spec})}{(\Delta - t(b))^2} \\
& \leq 4\frac{\Var_{\beta}(T_{\spec})}{\Delta^2} \tag{since $\Delta \geq 2t(b)$} \\
& \leq 4\frac{2\kappa/n^2 + 4\beta^T V_{\kappa} V_{\kappa}^T \beta /n^2}{\Delta^2}.
\end{align*}
We handle each summand in the numerator separately. For the first term, since $\Delta \geq 2 t(b)$, we have
\begin{equation}
\label{eqn:higher_order_spectral_type_II_error_pf1}
\frac{2\kappa}{n^2\Delta^2} \leq \frac{1}{2b^2}.
\end{equation}

For the second term, noting that $\Delta = \beta^T V_{\kappa} V_{\kappa}^T \beta/n$, we have
\begin{align}
\frac{\beta^T V_{\kappa} V_{\kappa}^T \beta/n^2}{\Delta^2} & = \frac{1}{n\Delta} \nonumber \\
& \leq \frac{1}{2nt(b)} \nonumber \\
& = \frac{1}{2b\sqrt{2\kappa}}, \label{eqn:higher_order_spectral_type_II_error_pf2}
\end{align}
and combining~\eqref{eqn:higher_order_spectral_type_II_error_pf1} and~\eqref{eqn:higher_order_spectral_type_II_error_pf2} yields~\eqref{eqn:higher_order_graph_spectral_type_II_error}.


\subsubsection{Step 2: Bounding neighborhood graph functionals}

To make use of Lemma~\ref{lem:higher_order_fixed_graph_testing} we will need to show that when $r$ and $\kappa$ are appropriately tuned and $\norm{f}_{\Leb^2(\mathcal{X})}$ is sufficiently large, the inequality~\eqref{eqn:higher_order_fixed_graph_testing_critical_radius} holds with respect to $G = G_{n,r}$ and $\beta = (f(x_1),\ldots,f(x_n))$. In particular, we will show that for some constants $c_1,c_2,c_3,c_4$ which may depend on $L$, $d$ and $s$ but do not depend on $n$, $f$ or $b$, the following statements:
\begin{enumerate}[label=(E\arabic*)]
	\item 
	\label{event:higher_order_discrete_sobolev_norm}
	\textbf{Graph Sobolev norm:} For $f \in C_d^{s}(\mathcal{X};R)$, $p \in C_d^{s - 1}(\Xset;R)$, and $1 \geq r(n) \geq n^{-1/(2(s - 1) + d)}$,
	\begin{equation}
	\label{eqn:higher_order_continuous_to_discrete_sobolev_norm}
	S_s(f;G_{n,r}) \leq c_1 \cdot b \cdot n^{s + 1} r^{s(d + 2)} 
	\end{equation}
	\item 
	\label{event:higher_order_eigenvalue_tail_decay}
	\textbf{Eigenvalue tail bound:} For any $a >0$ and $(\log n/n)^{1/d} n^{a} \leq r \leq n^{-4/((2+d)(4s + d))}$,and for $\kappa = n^{2d/(4s + d)}$, 
	\begin{equation}
	\label{eqn:higher_order_eigenvalue_tail_bound}
	s_{\kappa} \geq c_2 \cdot n r^{d + 2} \kappa^{2/d}
	\end{equation}
	\item 
	\label{event:higher_order_l2_norm}
	\textbf{Empirical norm of $f$:} When $f \in C^{s}(\mathcal{X};R)$ and $\norm{f}_{\Leb^2} \geq c_3 \cdot b \cdot n^{-4s/(4s + d)}$,
	\begin{equation}
	\label{eqn:higher_order_l2_to_empirical_norm}
	\norm{f}_n^2 \geq \frac{1}{b} \cdot \norm{f}_{\Leb^2}^2
	\end{equation}
\end{enumerate} 

each hold with probability at least $1 - c_4/b$ for sufficiently large $n$.

\paragraph{Proof of~\eqref{eqn:higher_order_continuous_to_discrete_sobolev_norm}:}

We will take $s$ to be even, as the proof when $s$ is odd follows essentially the same steps. To simplify exposition, we introduce the iterated difference operator, defined recursively as
\begin{equation*}
D_{jk}f(x) = (D_{k}f(x_j) - D_{k}f(x))\frac{K_r(x_j,x)}{r^d}~~\textrm{for $j \in [n], k \in [n]^q$}, \quad D_jf(x) = (f(x_j) - f(x))\frac{K_r(x_j,x)}{r^d}
\end{equation*}

Now when $s$ is even, letting $q = s/2$ we have the decomposition
\begin{equation}
\label{eqn:higher_order_continuous_to_discrete_sobolev_norm_pf1}
f^T L^s f =  \sum_{i = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} r^{ds} D_kf(x_i) D_{\ell}f(x_i)
\end{equation}
For given index vectors $k,\ell$ and index $i$, let $I = \abs{k \cup \ell \cup i}$ be the total number of unique indices. We separate our analysis into cases based on the magnitude of $I$, specifically whether $I < s + 1$ or $I = s + 1$, and show that
\begin{equation}
\label{eqn:expected_difference_operators_1}
\Ebb(D_kf(x_i) D_\ell f(x_i)) =
\begin{cases*}
O(r^{2s}), & ~~\textrm{if $I = s + 1$} \\
O(r^{2} r^{d(I - (2q + 1))}), & ~~\textrm{otherwise}~ 
\end{cases*}
\end{equation}
uniformly over $f \in C^{s}(L)$. 
Before proving~\eqref{eqn:expected_difference_operators_1}, we verify that~\eqref{eqn:higher_order_continuous_to_discrete_sobolev_norm} is directly implied by~\eqref{eqn:expected_difference_operators_1}. In the sum on the right hand side of~\eqref{eqn:higher_order_continuous_to_discrete_sobolev_norm_pf1}, there are $O(n^{I})$ terms with exactly $I$ distinct indices. When $I < s + 1$, the total contribution of such terms to the sum is $O(n^{I}r^{d(I - 1) + 2})$. Since $r(n) \geq n^{-1/d}$, this increases with $I$. Taking $I = s$ to be the largest integer less than $s + 1$, the contribution of these terms to the sum is therefore $O(n^sr^{d(s - 1) + 2})$ which in light of the restriction $r \geq n^{-1/(2(s - 1) + d)}$ is $O(n^{s+1}r^{s(d +2)})$. On the other hand when $I = s + 1$, by~~\eqref{eqn:expected_difference_operators_1} we immediately have that the total contribution of these terms is $O(n^{s + 1}r^{2(s + d)})$. Therefore,
\begin{equation*}
\Ebb(f^T L^s f) = O(n^{s+1}r^{s(d+2)})
\end{equation*}
uniformly over $f \in C^s(L)$, and~\eqref{eqn:higher_order_continuous_to_discrete_sobolev_norm} by Markov's inequality.

Now we prove~\eqref{eqn:expected_difference_operators_1}. Since $f \in C_d^s(R) \subseteq C_d^1(R)$, using a first-order Taylor expansion of $f(x)$ we can show that for all index vectors $k,\ell \in [n]^q$ and indices $i \in [n]$, the product of iterated difference operators $D_kf(x_i) D_{\ell}f(x_i)$ satisfies
\begin{equation*}
\abs{D_kf(x_i) D_{\ell}f(x_i)} \leq 4^{q} R^2 r^{2 - 2dq}
\end{equation*}
Moreover $D_kf(x_i) D_{\ell}f(x_i)$ will equal zero if there exists $x_j, j \in k \cup \ell \cup i$ such that
\begin{equation*}
\norm{x_j - x_h} > r,~~\textrm{for all $h \neq j \in k \cup \ell \cup i$}
\end{equation*}
Therefore $D_kf(x_i) D_{\ell}f(x_i)$ is nonzero with probability $O(r^{d(\abs{k \cup \ell \cup i} - 1)})$, which along with the boundedness of $D_kf(x_i) D_{\ell}f(x_i)$ implies the second upper bound in~\eqref{eqn:expected_difference_operators_1}.

To show the first upper bound in~\eqref{eqn:expected_difference_operators_1}, we apply the law of iterated expectation, 
\begin{align}
\Ebb\left[D_kf(x_i)D_{\ell}f(x_i)\right] & = \Ebb\left[\Ebb\left(D_kf(x)|x_i = x\right) \Ebb\left(D_{\ell}f(x)|x_i = x\right)\right] \nonumber \\
& = \Ebb\left[\Ebb\left(D_kf(x)|x_i = x\right)^2\right]. \label{eqn:expected_difference_operators_pf1}
\end{align}
By Lemma~\ref{lem:leading_term}, we have that $\Ebb(D_kf(x)) = O(r^s)$ for all values of $x$, implying the desired result.

\paragraph{Proof of~\eqref{eqn:higher_order_eigenvalue_tail_bound}:}

We prove~\eqref{eqn:higher_order_eigenvalue_tail_bound} by comparing $G_{n,r}$ to the tensor product of a $d$-dimensional lattice and a complete graph. The latter is a highly structured graph with known eigenvalues, which as we will see are sufficiently lower bounded for our purposes.

Let $\wt{r} = r/(3(2\sqrt{d} + 1)), M = (1/\wt{r})^d, N = n\wt{r}^d$. Assume without loss of generality that $M$ and $N$ are integers. Additionally, for $t = n^{1/d}$ and $m = M^{1/d}$ let 
\begin{equation*}
\overline{X} = \set{\frac{1}{t}(k_1,\ldots,k_d): k \in [t]^d},~~ \overline{Z} = \set{\frac{1}{m}(j_1,\ldots,j_d): j \in [m]^d}.
\end{equation*}
For a given $\overline{z}_j \in \overline{Z}$, we write $Q(z_j) = m^{-1}[j_1 - 1,j_1] \times \cdots \times m^{-1}[j_d - 1,j_d]$ for the cube of side length $1/m$ with $z_j$ at one corner. 

Consider the graph $H = (\overline{X}, E_H)$, where $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$ if
\begin{equation*}
\textrm{there exists}~\ol{z}_i, \ol{z}_j \in \ol{Z}~\textrm{such that}~\ol{x}_k \in Q(\ol{z}_i),~ \ol{x}_\ell \in Q(\ol{z}_j),~\textrm{and}~\norm{i - j}_1 \leq 1.
\end{equation*}
On the one hand $H \cong \ol{G}^M_d \otimes K_N$ where $\ol{G}^M_d$ is the $d$-dimensional lattice on $M$ nodes, and $K_N$ is the complete graph on $N$ nodes. On the other hand, we now show that with high probability $G_{n,r} \succeq H$. If $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$, then there exist $\ol{z}_i, \ol{z}_j$ such that
\begin{equation*}
\norm{\ol{x}_k - \ol{x}_{\ell}}_2 \leq m^{-1} + \norm{\ol{x}_k - \ol{z}_{i}}_2 + \norm{\ol{x}_{\ell} - \ol{z}_{j}}_2 \leq \wt{r}(1 + 2\sqrt{d}) = r/3.
\end{equation*}

Assuming~\eqref{eqn:transport_distance} holds, if $(\ol{x}_k, \ol{x}_{\ell}) \in E_H$, then for sufficiently large $n$
\begin{equation*}
\norm{\pi(\ol{x}_k) - \pi(\ol{x}_\ell)}_2 \leq 2 c \left(\frac{\log n}{n}\right)^{1/d} + \frac{r}{3} \leq r,
\end{equation*}
implying that $(\pi(\ol{x}_k), \pi(\ol{x}_{\ell})) \in E$. Therefore, $G_{n,r} \succeq \ol{G}^M_d \otimes K_N$ whenever~\eqref{eqn:transport_distance} holds.

The eigenvalues of lattices and complete graphs are known to satisfy, respectively
\begin{equation*}
\lambda_k(\ol{G}^{M}_d) \geq \frac{k^{2/d}}{M^{2/d}}~\textrm{for $k = 0,\ldots,M - 1$},~~ \textrm{and}~\lambda_{j}(K_N) \geq N\1\{j > 0\}~\textrm{for $j = 0,\ldots,N-1$.}
\end{equation*}
and by standard facts regarding the eigenvalues of tensor product graphs, we have that the spectrum $\Lambda(H)$ satisfies
\begin{equation*}
\Lambda(H) = \set{N\lambda_k(\ol{G}^{M}_d) + M\lambda_j(K_N): \textrm{for $k = 0,\ldots,M - 1$ and $j = 0,\ldots,N-1$}}
\end{equation*}
For all $j = 1,\ldots,N-1$, we have that $M\lambda_j(K_N) = MN = n$. Therefore,
\begin{align*}
\lambda_{\kappa}(H) & \geq \{n \wedge N\lambda_{\kappa}(\ol{G}^{M}_d)\} \\
& \geq \{n \wedge n\wt{r}^d\frac{\kappa^{2/d}}{M^{2/d}}\} \\
& \geq \{n \wedge (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d}\} \\
& \geq (3\sqrt{d} + 3)^{-(2+d)}nr^{d+2}\kappa^{2/d},
\end{align*}
where the last inequality can be verified by a quick calculation in light of $\kappa = n^{2d/(4s + d)}$ and $r \leq n^{-4/((2+d)(4s + d))}$. Since we've already shown that $\lambda_{\kappa}(G_{n,r}) \geq \lambda_{\kappa}(H)$ when~\eqref{eqn:transport_distance} is satisfied, which happens probability $1 - o(n^{-1})$, this completes the proof of~\eqref{eqn:higher_order_eigenvalue_tail_bound}.

\paragraph{Proof of~\eqref{eqn:higher_order_l2_to_empirical_norm}:}

Let $Z = \frac{1}{n}\sum_{i = 1}^{n} f^2(x_i)$. We upper bound $\Ebb[Z^2]$,
\begin{align*}
\Ebb[Z^2] & \leq \Ebb(f^2(x_1))^2 + \frac{1}{n}\Ebb(f^4(x_1)) \\
& \leq \Ebb(f^2(x_1))^2 + \frac{R^2}{n}\Ebb(f^2(x_1)) \\
& \overset{(i)}{\leq} \Ebb(f^2(x_1))^2\left(1 + \frac{R^2}{n/\Ebb(f^2(x_1))}\right) \\
& \overset{(ii)}{\leq} \Ebb(f^2(x_1))^2\left(1 + \frac{R^2}{c_3^2 b^2 n^{d/(4s + d)}}\right)
\end{align*}
where $(i)$ follows since $f \in C_d^s(\Xset;R)$ implies $\abs{f(x)} \leq R$, and $(ii)$ follows by assumption. The statement then follows by the Paley-Zygmund inequqality.

\subsubsection{Step 3: Conclusion}

We note that for all possible values of $X \in \Xset^n$, under the null hypothesis $f = f_0 = 0$ and therefore $\beta = (f(x_1),\ldots,f(x_n)) = 0$ as well. Therefore by~\eqref{eqn:higher_order_graph_spectral_type_I_error}, we have the following bound on Type I error:
\begin{equation}
\Ebb_{f_0}(\phi_{\mathrm{spec}}) = \mathbb{E}(\mathbb{E}_{\beta = 0}(\phi_{\spec}) | X) \leq \frac{1}{b^2}.
\end{equation}

Now, we bound Type II error under the assumption $f \in C_d^s(\mathcal{X};R), p \in C_d^{s - 1}(\Xset;R)$ uniformly bounded away from $0$ and $\infty$ over $\Xset$, and 
\begin{equation}
\label{eqn:higher_order_critical_radius_1}
\norm{f}_{\Leb^2}^2 \geq \epsilon^2 = c_3^2 \cdot b^2 \cdot n^{-4s/(4s + d)}.
\end{equation}
Choosing $n^{-1/(2(s - 1) + d)}\leq r(n) \leq n^{-4/((2+d)(4s + d))}$, we may therefore apply our conclusions in Step 2; namely, that for every possible choice of $f$ there exists a good set $\mathcal{E}_f \subseteq \Xset^n$ with $\Pbb(\mathcal{E}_f) \geq 1 - c_4/b$ such that each of \eqref{eqn:higher_order_continuous_to_discrete_sobolev_norm}, \eqref{eqn:higher_order_eigenvalue_tail_bound}, and \eqref{eqn:higher_order_l2_to_empirical_norm} hold for all $X \subseteq \mathcal{E}_f$. Choosing $\kappa = n^{2d/(4s + d)}$ to balance the squared bias and variance terms on the right hand side of~\eqref{eqn:higher_order_fixed_graph_testing_critical_radius}, we have that for all $X \subseteq \mathcal{E}_f$
\begin{align*}
2b\sqrt{\frac{2\kappa}{n^2}} + \frac{S_s(\beta;G_{n,r})}{ns_{\kappa}} & \leq 2bn^{-4s/(4s+d)} + c\cdot b \cdot \frac{n^{s} r^{s(d + 2)}}{s_{\kappa}^s} \tag{by \eqref{eqn:higher_order_continuous_to_discrete_sobolev_norm}} \\
& \leq 2bn^{-4s/(4s+d)} + c\cdot b \cdot\frac{1}{n^{4s/(4s+d)}} \tag{by \eqref{eqn:higher_order_eigenvalue_tail_bound}} \\
& \leq \frac{1}{b}\norm{f}_{\Leb^2} \\
& \leq \frac{1}{n}\sum_{i = 1} \beta_i^2. \tag{by \eqref{eqn:higher_order_l2_to_empirical_norm}}
\end{align*}
where the last two inequalities follow for a suitably large choice of $c_3$ in~\eqref{eqn:higher_order_critical_radius_1}.
We conclude that for all $X \subseteq \mathcal{E}_f$, the inequality \eqref{eqn:higher_order_fixed_graph_testing_critical_radius} is satisfied with respect to $\beta = (f(x_1),\ldots,f(x_n))$ and $G = G_{n,r}$. As a result the worst-case Type II error is bounded
\begin{equation*}
\sup_{\substack{f \in C_d^{s}(\mathcal{X;R}), \\ \norm{f}_{\Leb^2} \geq \epsilon}}\mathbb{E}_{f}(1 - \phi_{\spec}) \leq \sup_{\substack{f \in C_d^{s}(\mathcal{X;R}), \\ \norm{f}_{\Leb^2} \geq \epsilon}} \mathbb{E}\bigl[\mathbb{E}_{\beta}(1 - \phi_{\spec}|X \in \mathcal{E}_f)\bigr] + \frac{c_4}{b} \leq \frac{3 + c_4}{b},
\end{equation*}
completing the proof of Theorem~\ref{thm:higher_order_holder_testing_rate}.

\section{Supporting Results}

\begin{lemma}
	\label{lem:leading_term}
	Let $s$ be a positive integer, and $R \geq 0$. Suppose $f \in C^s(R)$, $p \in C^{s-1}(R)$, $k \in [n]^q$ for some $q \geq 1$, and that $K_r$ is a $2$nd-order kernel. Then if $2q \leq s - 1$,
	\begin{equation}
	\label{eqn:leading_term}
	\Ebb(D_kf(x)) = \sum_{\ell = 2q}^{s - 1} O(r^{\ell}) \cdot f_{\ell}(x) + O(r^s),
	\end{equation}
	for some $f_{\ell} \in C^{s - \ell}(L)$. If $2q \geq s$, $\Ebb(D_kf(x)) = O(r^s)$.  All $O(\cdot)$ terms may depend $L$ and $s$, but do not depend on $f$ or $x$.
\end{lemma}
\begin{proof}
	We will prove Lemma~\ref{lem:leading_term} in the case where $d = 1$. When $d \geq 2$, using multivariate Taylor expansions we find the same result, but with more notational overhead. 
	
	We will prove by induction on $q$ in the case where $d = 1$. The When $q = 1$ and $k \in [n]$, by taking Taylor expansions of $f$ and $p$, we obtain
	\begin{align}
	\nonumber 
	\Ebb(D_kf(x)) & = \sum_{\ell = 1}^{s - 1} f^{(\ell)}(x) \int (z - x)^{\ell} K_r(z,x) p(z) dz + O(r^s) \\ 
	& = \sum_{\ell = 1}^{s - 1} \sum_{a = 0}^{s - 1} f^{(\ell)}(x) p^{(a)}(x) \underbrace{\int(z - x)^{\ell + a} K_r(z,x) dz}_{:=I_{t + a}} + O(r^s) \label{eqn:leading_term_pf1}
	\end{align}
	Since $K$ is a $2$nd-order kernel, $I_{1} = 0$ and $I_t = O(r^{t})$ for $t \geq 2$.  Additionally, when $\ell + a \leq s - 1$, we have that $f^{(\ell)}p^{(a)} \in C^{\min\{s - \ell, s - 1 - a\}} \subseteq C^{s - (\ell + a)}$, and $\abs{f^{(\ell)}p^{(a)}} \leq L^2$ for any $\ell$ and $a$. We can therefore simplify~\eqref{eqn:leading_term_pf1} by combining all terms where $\ell + a = t$, obtaining
	\begin{equation}
	\label{eqn:leading_term_pf2}
	\Ebb(D_kf(x)) = \sum_{t = 1}^{s - 1} f_t(x) I_t + O(r^s) = \sum_{t = 2}^{s - 1} f_t(x) I_t + O(r^s),
	\end{equation}
	which establishes~\eqref{eqn:leading_term} in the base case.
	
	Now, we assume \eqref{eqn:leading_term} holds for all $k \in [n]^q$, and prove the desired estimate on $\Ebb(D_{kj}f(x))$ for each $j \in [n]$. By the law of iterated expectation and~\eqref{eqn:leading_term_pf2},
	\begin{align*}
	\Ebb(D_{kj}f(x)) & = \Ebb(D_k(\Ebb(D_jf))(x)) \\
	& = \Ebb\left(D_k\left(\sum_{t = 2}^{s - 1} I_t f_t + O(r^s)\right)(x)\right) \\
	& = \sum_{t = 2}^{s - 1} I_t \cdot \Ebb(D_kf_t(x)) + O(r^s)
	\end{align*}
	where the second equality follows from the linearity and boundedness of $f \mapsto \Ebb(D_kf)$. We now apply the inductive hypothesis to $\Ebb(D_kf_t(x))$. If $2(q + 1) \geq s$, note that since $f_t \in C^{s - t}(L)$ for $t \geq 2$, we have by hypothesis $\Ebb(D_kf_t(x)) = O(r^{s - t})$. As a result
	\begin{equation*}
	\sum_{t = 2}^{s - 1} I_t \cdot \Ebb(D_kf_t(x)) = \sum_{t = 2}^{s - 1} I_t \cdot O(r^{s - t}) = O(r^s)
	\end{equation*} 
	Otherwise $2(q + 1) \leq s - 1$. For each $t = 2,\ldots, s - 1$, if additionally  $2q \leq s - t - 1$, then by hypothesis $\Ebb(D_kf_t(x)) = \sum_{\ell = 2q}^{s - t - 1} O(r^{\ell}) \cdot g_{\ell}(x) + O(r^{s - t})$ for some $g_{\ell} \in C^{s - t - \ell}(L)$, and otherwise $\Ebb(D_kf_t(x)) = O(r^{s - t})$. Therefore,
	\begin{align*}
	\sum_{t = 2}^{s - 1} I_t \cdot \Ebb(D_kf_t(x)) & = \sum_{t = 2}^{s - 1 - 2q} I_t \cdot \left\{\sum_{\ell = 2q}^{s - t - 1} O(r^{\ell}) \cdot g_{\ell}(x) + O(r^{s - t}) \right\} + \sum_{t = s - 1 - 2q}^{s - 1}I_{t} \cdot O(r^{s - t}) \\
	& = \sum_{t = 2}^{s - 1 - 2q} I_t \cdot \left\{\sum_{\ell = 2q}^{s - t - 1} O(r^{\ell}) \cdot g_{\ell}(x)\right\}  + O(r^s) \\
	& = \sum_{\ell = 2q}^{s - 3} \sum_{t = 2}^{s - \ell - 1} I_t \cdot O(r^{\ell}) \cdot g_{\ell}(x) + O(r^s).
	\end{align*}
	Noting that $g_{\ell} \in C^{s - (t + \ell)}(L)$ for some $\ell + t = 2(q + 1),\ldots,s - 1$, and $I_t \cdot O(r^{\ell}) = O(r^{t + \ell})$, we can rewrite the final equation as a sum over $\ell + t = 2(q + 1),\ldots,s - 1$, which proves~\eqref{eqn:leading_term}.
\end{proof}

\begin{theorem}[Theorem 1 of ``On the rate of Convergence of Empirical Measures in $\infty$-Transportation Distance'']
	Let $X = \set{x_1,\ldots,x_n} \sim P$, where $P$ is a distribution on $\Xset = [0,1]^d$ with density $p$ satisfying
	\begin{equation*}
	0 < p_{\min} < p(x) < p_{\max} < \infty, \quad \textrm{for all $x \in \mathcal{X}$.}
	\end{equation*} 
	With probability at least $1 - o(n^{-1})$, there exists a bijection between grid points and data points $\pi: \overline{X} \to X$ such that
	\begin{equation}
	\label{eqn:transport_distance}
	\max_{k \in [t]^d} \abs{\overline{x}_k - \pi(\overline{x}_k)} \leq c \left(\frac{\log n}{n}\right)^{1/d}
	\end{equation}
\end{theorem}


\end{document}
