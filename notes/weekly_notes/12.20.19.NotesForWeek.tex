\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{\mathcal{L}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\Unq}{\mathrm{Unq}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Notes for Week 12/7/19 - 12/29/19}
\author{Alden Green}
\date{\today}
\maketitle

We observe random design and responses $(x_i,y_i)$ according to the following regression model: $x_1,\ldots,x_n$ are drawn i.i.d from distribution $P$ with density $p$ supported on $\mathcal{X} \subset \Reals^d$, and
\begin{equation*}
y_i = f(x_i) + \varepsilon_i,~ \varepsilon_i \overset{i.i.d}{\sim} \mathcal{N}(0,1).
\end{equation*}
Suppose we form the undirected, weighted graph $G_{n,r} = (X,W)$ with edge weights $W = (W_{ij})$ formed according to the kernel function $K$ as follows:
\begin{equation*}
W_{ij} = K_r(x_i,x_j) := \frac{1}{r^d}K(\norm{x_i - x_j}^2)
\end{equation*}
Let $L_n$ be the graph Laplacian operator associated with $G_{n,r}$, defined by the action
\begin{equation*}
L_nf(x) := \frac{1}{nr^2} \sum_{i = 1}^{n}(f(x_i) - f(x))K_r(x_i,x).
\end{equation*}
The graph Laplacian $L_n$ induces a class of roughness functionals $R_s(f)$, defined by
\begin{equation*}
R_{s,n}(f) = R_s(f;G_{n,r}) = \frac{1}{n} f^T L_n^s f
\end{equation*} 
We refer to $R_{s,n}(f)$ as the order-$s$ roughness functional. As part of our graph testing work, we have shown that if $f \neq 0$, then for any design points $X$ and resulting neighborhood graph $G$ with Laplacian $L$ such that
\begin{equation}
\label{eqn:bias_variance}
\norm{f}_n^2 \geq 2b\sqrt{\frac{2\kappa}{n^2}} + \frac{R_{s}(f;G) }{\lambda_{\kappa}^s}
\end{equation}
the graph spectral test $\phi_{\spec}$ makes a Type II error with probability at most $3/b$. Suppose
\begin{equation}
\label{eqn:graph_functionals}
\norm{f}_n^2 \geq \frac{1}{2}\norm{f}_2^2, ~R_s(f;G_{n,r}) \leq ||f||_{W_{d}^{s}(\Xset)}^2 ~~\textrm{and}~~ \lambda_{\kappa} \leq \kappa^{2/d};
\end{equation}
then it can be verified that choosing $\kappa = n^{-2d/(4s + d)}$, the equation~\eqref{eqn:bias_variance} is satisfied whenever $\norm{f}_2^2 \geq n^{-4s/(4s + d)}$. It is therefore sufficient to show that~\eqref{eqn:graph_functionals} holds with high probability over the random design points $X$.

In this week's notes, we will focus our attention on upper bounding the roughness functional $R_{n,s}(f)$. Our main results should take the following form: when (a) $f \in C^{s}(L)$ for a fixed constant $L$ (or more ideally $f \in \mathcal{W}^{s}(L)$), (b) the neighborhood graph radius $r = r(n)$ is properly chosen as a function of $n$, and (c) $K$ is an appropriately chosen kernel, the roughness functional satisfies
\begin{equation}
\label{eqn:roughness_functional_bound}
\mathbb{E}\left[R_{s,n}(f)\right] = O\left(1\right)
\end{equation}
uniformly over all $f \in C^{s}(1)$. 
These notes detail our current progress in showing such a result for different values of $s$. We showed the following useful representation of $R_{s,n}(f)$ in the \emph{8.7.19} Notes. For $k = (k_1,\ldots,k_n) \in [n]^s$, recursively defined the graph difference operator $D_k$ by 
\begin{equation*}
D_{k_1}f(x) = (f(x_{k_1}) - f(x))K_r(x_{k_1},x),~~ D_kf(x) = \bigl(D_{k_1}(D_{(k_2,\ldots,k_n)}f\bigr)(x)
\end{equation*}
Then when $s$ is even, letting $q = s/2$ we have
\begin{equation}
\label{eqn:roughness_functional_representation_even}
R_{s,n}(f) = \frac{1}{n}\sum_{i = 1}^{n} \left(\frac{1}{(nr^2)^q}\sum_{k \in [n]^q} D_kf(x_i)\right)^2
\end{equation}
and when $s$ is odd, letting $q = (s - 1)/2$ we have
\begin{equation}
\label{eqn:roughness_functional_representation_odd}
R_{s,n}(f) =  \frac{1}{2n^2r^2}\sum_{i,j = 1}^{n}\left(\frac{1}{(nr^2)^q}\sum_{k \in [n]^q}\bigl(D_kf(x_i) - D_kf(x_j)\bigr)\right)^2K_r(x_i,x_j)
\end{equation}

\section{Bounding the Roughness Functional in Expectation}

For an integer $\ell > 0$, we will say $K$ is a $2$nd-order kernel if it is bounded and compactly supported on $B(0,1)$ and additionally
\begin{equation*}
\int K(z) = 1 ~~\textrm{and}~~\int z K(z) = 0
\end{equation*}
That $K$ be a $2$nd-order kernel is crucial for $L_n$ to act as a $2$nd-order differential operator, and for $R_{s,n}$ to scale at the proper rate.

\begin{lemma}
	\label{lem:roughness_functional_expectation}
	Suppose $f \in C^{s}(L)$ for some positive integer $s$ and $L > 0$, and further suppose $p \in C^{\ell}(L)$ for $\ell = \floor{s}$ the largest integer strictly less than $s$. For any $2$nd-order kernel $K$ and any $1 \geq r(n) \geq n^{-1/(2(s - 1)+d)}$, we have that
	\begin{equation*}
	\Ebb(R_{s,n}(f)) = O(1),
	\end{equation*} 
	uniformly over all $f \in C^{s}(L)$.
\end{lemma}

Note that Lemma~\ref{lem:roughness_functional_expectation} implies a high-probability bound on $R_{s,n}(f)$ by Markov's inequality. To prove Lemma~\ref{lem:roughness_functional_expectation}, we first break the sum~\eqref{eqn:roughness_functional_representation_even} (or ~\eqref{eqn:roughness_functional_representation_odd}) into cases according to the number of unique indices, then bound the expectation case by case. The following Lemma will be the workhorse which supplies a sufficient bound in each case.
\begin{lemma}
	\label{lem:expected_difference_operators}
	Let $q = s/2$ when $s$ is even, and $q = (s - 1)/2$ when $s$ is odd. Under the same conditions as Lemma~\ref{lem:roughness_functional_expectation}, for any indices $k = (k_1,\ldots,k_q)$ and $\ell = (\ell_1,\ldots,\ell_q)$, we have
	\begin{equation}
	\label{eqn:expected_difference_operators_1}
	\Ebb(D_kf(x_i) D_\ell f(x_i)) =
	\begin{cases*}
	O(r^{2s}), & ~~\textrm{if all indices are distinct} \\
	O(r^{2} r^{d(\abs{k \cup \ell \cup i} - (2q + 1))}), & ~~\textrm{otherwise}~ 
	\end{cases*}
	\end{equation}
	Additionally, we have
	\begin{equation}
	\label{eqn:expected_difference_operators_2}
	\Ebb(d_iD_kf(x_j) d_iD_\ell f(x_j)) =
	\begin{cases*}
	O(r^{2s}), & ~~\textrm{if all indices are distinct} \\
	O(r^{2} r^{d(\abs{k \cup \ell \cup i \cup j} - (2q + 2))}), & ~~\textrm{otherwise}~ 
	\end{cases*}
	\end{equation}
\end{lemma}

The minimum scaling condition $r(n) \geq n^{-1/(2(s - 1) + d)}$ is needed in order to ensure the diagonal terms of the sums in~\eqref{eqn:roughness_functional_representation_even} and~\eqref{eqn:roughness_functional_representation_odd} (where not all indices are distinct) do not dominate the off-diagonal terms (where all indices are distinct).

\section{Proofs}

\subsection{Proof of Lemma~\ref{lem:roughness_functional_expectation}}
We will take $s$ to be even, as the proof when $s$ is odd follows essentially the same steps. Now when $s$ is even, we have the decomposition
\begin{equation*}
R_{s,n}(f) = \frac{1}{n^{s+1}r^{2s}} \sum_{i = 1}^{n} \sum_{k \in [n]^q} \sum_{\ell \in [n]^q} D_kf(x_i) D_{\ell}f(x_i)
\end{equation*}
For given index vectors $k,\ell$ and index $i$, let $I = \abs{k \cup \ell \cup i}$ be the total number of unique indices. We separate our analysis into cases based on the magnitude of $I$. 

When $I < s + 1$, there is at least one repeated index, and by Lemma~\ref{lem:expected_difference_operators} we have that
\begin{equation*}
\Ebb(D_kf(x_i)D_{\ell}f(x_i)) = O(r^2r^{d(I - (s + 1))}).
\end{equation*}
There are $O(n^{I})$ terms in $R_{s,n}(f)$ with exactly $I$ distinct indices. Therefore, the total contribution of such terms to the sum is $O(r^{-2(s - 1)}r^{d(I - (s + 1))}n^{I - (s + 1)}r)$. Since $r(n) \geq n^{-1/d}$, this increases with $I$. Taking $I = s$ to be as large as possible such that there is at least one repeated index, the contribution of these terms to the sum is $O(r^{-(2(s - 1) + d)}n^{-1}) = O(1)$, with the equality following by the restriction $r \geq n^{-1/(2(s - 1) + d)}$. 

When $I = s + 1$, every index in $\ell,k$ and $i$ is unique. Therefore by Lemma~\ref{lem:roughness_functional_expectation},  
\begin{equation*}
\Ebb(D_kf(x_i)D_{\ell}f(x_i)) = O(r^{2s}).
\end{equation*}
Since there are $O(n^{s+1})$ such terms, we have that the total contribution of these terms is $O(n^{s + 1}r^{2s}n^{-(s + 1)}r^{-2s}) = O(1)$.

\section{Proof of Lemma~\ref{lem:expected_difference_operators}}

\subsection{Proof of~\eqref{eqn:expected_difference_operators_1}}

Let $k,\ell \in [n]^q$ be index vectors. We first prove the desired bound in the case when some indices are repeated, and then the desired bound in the case when all indices are distinct.

\paragraph{Repeated indices.}

Since $f \in C^1(L)$, for all index vectors $k,\ell \in [n]^q$ and indices $i \in [n]$, the product of iterated difference operators $D_kf(x_i) D_{\ell}f(x_i)$ satisfies
\begin{equation*}
\abs{D_kf(x_i) D_{\ell}f(x_i)} \leq 4^{q} L^2 r^{2 - ds}
\end{equation*}
Moreover $D_kf(x_i) D_{\ell}f(x_i)$ will equal zero if there exists $x_j, j \in k \cup \ell \cup i$ such that
\begin{equation*}
\norm{x_j - x_h} > r,~~\textrm{for all $h \in k \cup \ell \cup i$}
\end{equation*}
Therefore $D_kf(x_i) D_{\ell}f(x_i)$ is nonzero with probability $O(r^{d(\abs{k \cup \ell \cup i} - 1)})$, which along with the boundedness of $D_kf(x_i) D_{\ell}f(x_i)$ implies the claimed result.

\paragraph{All indices distinct.}

By the law of iterated expectation, we have
\begin{align}
\Ebb\left[D_kf(x_i)D_{\ell}f(x_i)\right] & = \Ebb\left[\Ebb\left(D_kf(x)|x_i = x\right) \Ebb\left(D_{\ell}f(x)|x_i = x\right)\right] \nonumber \\
& = \Ebb\left[\Ebb\left(D_kf(x)|x_i = x\right)^2\right] \label{eqn:expected_difference_operators_pf1}
\end{align}
The result then follows from Lemma~\ref{lem:leading_term}.

\subsection{Proof of~\eqref{eqn:expected_difference_operators_2}}

The proof of~\eqref{eqn:expected_difference_operators_2} when some index is repeated is essentially the same, and we do not reproduce it.

When all indices $\ell,k,i$ and $j$ are distinct, by Lemma~\ref{lem:leading_term} there exists some $f_{s-1} \in C^1(L)$ such that
\begin{align*}
\Ebb(d_iD_kf(x_j)d_iD_{\ell}f(x_j)K_r(x_i,x_j)) & = \Ebb\biggl(\Bigl(d_i\bigl(O(r^{s-1})\cdot f_{s - 1} + O(r^s)\bigr)\Bigr)^2K_r(x_i,x_j)\biggr) \\
& = O(r^{2s - 2})\Ebb\Bigl(\bigl(d_if_{s-1}(x_j))\bigr)^2K_{r}(x_i,x_j)\Bigr) + O(r^{2s - 1})\Ebb\bigl(D_if_{s - 1}(x_j)\bigr)+ O(r^{2s})
\end{align*}
where the second equality follows from the linearity of the difference operator. Then since $f_{s-1} \in C^1(L)$, we have $\Ebb(D_if_{s-1}(x_j)) = O(r)$, and
\begin{equation*}
\Ebb\Bigl(\bigl(d_if_{s-1}(x_j))\bigr)^2K_{r}(x_i,x_j)\Bigr) = \Ebb\Bigl(\bigl(O(r)\bigr)^2K_{r}(x_i,x_j)\Bigr) = O(r^2),
\end{equation*}
establishing $\Ebb(d_iD_kf(x_j)d_iD_{\ell}f(x_j)K_r(x_i,x_j)) = O(r^{2s})$ as claimed.
\section{Auxiliary Results.}
\begin{lemma}
	\label{lem:difference_operator_taylor}
	When $f \in C^s(L)$, for any $k \in [n]$ 
	\begin{equation*}
	D_kf(x) = \left(\sum_{r = 1}^{s - 1} (x_k - x)^r f^{(r)}(x) + O((x_k - x)^{s})\right) \cdot K_r(x_k,x)
	\end{equation*}
\end{lemma}

\begin{lemma}
	\label{lem:expected_kernel}
	When $K$ is a bounded and compactly supported kernel, then for any $s$ and for all $x \in \Rd$
	\begin{equation*}
	\Ebb(\abs{K(x_i,x)}^s) \leq CLr^{(1 - s)d}
	\end{equation*}
\end{lemma}

\begin{lemma}
	\label{lem:leading_term}
	Suppose $f \in C^s(L)$, $p \in C^{s-1}(L)$, $k \in [n]^q$ for some $q \geq 1$, and that $K_r$ is a $2$nd-order kernel. Then if $2q \leq s - 1$,
	\begin{equation}
	\label{eqn:leading_term}
	\Ebb(D_kf(x)) = \sum_{\ell = 2q}^{s - 1} O(r^{\ell}) \cdot f_{\ell}(x) + O(r^s),
	\end{equation}
	for some $f_{\ell} \in C^{s - \ell}(L)$. If $2q \geq s$, $\Ebb(D_kf(x)) = O(r^s)$.  All $O(\cdot)$ terms may depend $L$ and $s$, but do not depend on $f$ or $x$.
\end{lemma}
\begin{proof}
	We will prove Lemma~\ref{lem:leading_term} in the case where $d = 1$. When $d \geq 2$, using multivariate Taylor expansions we find the same result, but with more notational overhead. 
	
	We will prove by induction on $q$ in the case where $d = 1$. The When $q = 1$ and $k \in [n]$, by taking Taylor expansions of $f$ and $p$, we obtain
	\begin{align}
	\nonumber 
	\Ebb(D_kf(x)) & = \sum_{\ell = 1}^{s - 1} f^{(\ell)}(x) \int (z - x)^{\ell} K_r(z,x) p(z) dz + O(r^s) \\ 
	& = \sum_{\ell = 1}^{s - 1} \sum_{a = 0}^{s - 1} f^{(\ell)}(x) p^{(a)}(x) \underbrace{\int(z - x)^{\ell + a} K_r(z,x) dz}_{:=I_{t + a}} + O(r^s) \label{eqn:leading_term_pf1}
	\end{align}
	Since $K$ is a $2$nd-order kernel, $I_{1} = 0$ and $I_t = O(r^{t})$ for $t \geq 2$.  Additionally, when $\ell + a \leq s - 1$, we have that $f^{(\ell)}p^{(a)} \in C^{\min\{s - \ell, s - 1 - a\}} \subseteq C^{s - (\ell + a)}$, and $\abs{f^{(\ell)}p^{(a)}} \leq L^2$ for any $\ell$ and $a$. We can therefore simplify~\eqref{eqn:leading_term_pf1} by combining all terms where $\ell + a = t$, obtaining
	\begin{equation}
	\label{eqn:leading_term_pf2}
	\Ebb(D_kf(x)) = \sum_{t = 1}^{s - 1} f_t(x) I_t + O(r^s) = \sum_{t = 2}^{s - 1} f_t(x) I_t + O(r^s),
	\end{equation}
	which establishes~\eqref{eqn:leading_term} in the base case.
	
	Now, we assume \eqref{eqn:leading_term} holds for all $k \in [n]^q$, and prove the desired estimate on $\Ebb(D_{kj}f(x))$ for each $j \in [n]$. By the law of iterated expectation and~\eqref{eqn:leading_term_pf2},
	\begin{align*}
	\Ebb(D_{kj}f(x)) & = \Ebb(D_k(\Ebb(D_jf))(x)) \\
	& = \Ebb\left(D_k\left(\sum_{t = 2}^{s - 1} I_t f_t + O(r^s)\right)(x)\right) \\
	& = \sum_{t = 2}^{s - 1} I_t \cdot \Ebb(D_kf_t(x)) + O(r^s)
	\end{align*}
	where the second equality follows from the linearity and boundedness of $f \mapsto \Ebb(D_kf)$. We now apply the inductive hypothesis to $\Ebb(D_kf_t(x))$. If $2(q + 1) \geq s$, note that since $f_t \in C^{s - t}(L)$ for $t \geq 2$, we have by hypothesis $\Ebb(D_kf_t(x)) = O(r^{s - t})$. As a result
	\begin{equation*}
	\sum_{t = 2}^{s - 1} I_t \cdot \Ebb(D_kf_t(x)) = \sum_{t = 2}^{s - 1} I_t \cdot O(r^{s - t}) = O(r^s)
	\end{equation*} 
	Otherwise $2(q + 1) \leq s - 1$. For each $t = 2,\ldots, s - 1$, if additionally  $2q \leq s - t - 1$, then by hypothesis $\Ebb(D_kf_t(x)) = \sum_{\ell = 2q}^{s - t - 1} O(r^{\ell}) \cdot g_{\ell}(x) + O(r^{s - t})$ for some $g_{\ell} \in C^{s - t - \ell}(L)$, and otherwise $\Ebb(D_kf_t(x)) = O(r^{s - t})$. Therefore,
	\begin{align*}
	\sum_{t = 2}^{s - 1} I_t \cdot \Ebb(D_kf_t(x)) & = \sum_{t = 2}^{s - 1 - 2q} I_t \cdot \left\{\sum_{\ell = 2q}^{s - t - 1} O(r^{\ell}) \cdot g_{\ell}(x) + O(r^{s - t}) \right\} + \sum_{t = s - 1 - 2q}^{s - 1}I_{t} \cdot O(r^{s - t}) \\
	& = \sum_{t = 2}^{s - 1 - 2q} I_t \cdot \left\{\sum_{\ell = 2q}^{s - t - 1} O(r^{\ell}) \cdot g_{\ell}(x)\right\}  + O(r^s) \\
	& = \sum_{\ell = 2q}^{s - 3} \sum_{t = 2}^{s - \ell - 1} I_t \cdot O(r^{\ell}) \cdot g_{\ell}(x) + O(r^s).
	\end{align*}
	Noting that $g_{\ell} \in C^{s - (t + \ell)}(L)$ for some $\ell + t = 2(q + 1),\ldots,s - 1$, and $I_t \cdot O(r^{\ell}) = O(r^{t + \ell})$, we can rewrite the final equation as a sum over $\ell + t = 2(q + 1),\ldots,s - 1$, which proves~\eqref{eqn:leading_term}.
\end{proof}

\section{Old Work}

\subsection{Bounding the 2nd-Order Roughness Functional}
The (first and) second order roughness functionals are somewhat special. These roughness functionals do not involve taking compositions of the difference operator $D_k$, and as a result, we get the desired bounds in expectation even when $r(n)$ is small. 
\begin{lemma}
	\label{lem:2nd_order_roughness_functional}
	Fix $L > 0$, and form the neighborhood graph $G_{n,r}$ using kernel $K(z) = \1\{z^2 \leq r^2\}$. Suppose $f \in C^2(L)$ and $p \in C^1(L)$. Then for any $r(n) \geq n^{-1/(2+d)}$ and , we have
	\begin{equation*}
	\Ebb(R_{2,n}(f)) = O(L^2)
	\end{equation*}
\end{lemma}

\subsection{Bounding the 3rd-Order Roughness Functional}

For the order-$s$ roughness functionals when $s \geq 3$, the graph Laplacian is forced to approximate higher order derivatives by composing difference operators-- that is, taking differences of differences, etc. In for the roughness functional to continue scaling at the desired rate, we will need $r(n)$ to be larger, and we cannot use the uniform kernel $K$ any longer. In the particular case $s = 3$, we will require a kernel $K$ which is bounded and compactly supported on $B(0,1)$, and additionally satisfies
\begin{equation*}
\int z^j K(z) = 0~~\textrm{for $j = 1,2$.}
\end{equation*}
We then have the following result.
\begin{lemma}
	\label{lem:3rd_order_roughness_functional}
	Suppose $f \in C^{3}(L)$ for some $L > 0$, and further suppose $p \in C^{2}(L)$. For any kernel $K$ which satisfies the previous conditions, and any $r(n) \geq n^{-1/(4+d)}$, we have that
	\begin{equation*}
	\Ebb(R_{s,n}(f)) = O(1)
	\end{equation*} 
\end{lemma}

\subsection{Proof of Lemma~\ref{lem:2nd_order_roughness_functional}}
We prove Lemma~\ref{lem:2nd_order_roughness_functional} in the case when $d = 1$.

\textcolor{red}{TODO:} Extend the proof to all values of $d$. It's not hard, just requires you to replace derivatives by partial derivatives.

Separating the squared sum in~\eqref{eqn:roughness_functional_representation_even} into diagonal and off-diagonal terms, we obtain
\begin{align*}
R_{2,n}(f) & = \frac{1}{n}\sum_{i = 1}^{n}\left(\frac{1}{nr^2}\sum_{j = 1}^{n} D_jf(x_i)\right)^2 \\
& = \frac{1}{n^3r^4}\sum_{i = 1}^{n} \sum_{j = 1}^{n}\underbrace{(D_jf(x_i))^2}_{V_1} + \frac{1}{n^3r^4}\sum_{i = 1}^{n} \sum_{j = 1} \sum_{k \neq j}\underbrace{(D_jf(x_i))(D_kf(x_i))}_{U_1}
\end{align*}

The expectation of $U_1$ will be of the right order of magnitude thanks to a cancellation of the first-order term in a Taylor expansion. Noting that the kernel $K$ satisfies
\begin{equation*}
\int z K(z) \,dz = 0
\end{equation*}
we therefore derive that for any $x \in \mathcal{X}$,
\begin{align*}
\mathbb{E}(D_jf(x)) & = \frac{1}{r}\int ((y - x)f'(x) + O(L(y-x)^2) K(y - x) p(y) \,dy \tag{by $f \in C^2(L)$} \\
& = \frac{1}{r}\int(y - x)K(y - x)f'(x)(p(x) + O(L(y - x))) \,dy + O(Lr^2)  \tag{by $p \in C^1(L)$}\\
& = \underbrace{\frac{f'(x) p(x)}{r} \int z K(z) \,dz}_{= 0} + O(Lr^2) = O(Lr^2).
\end{align*}
As a result, we have
\begin{equation*}
\frac{1}{n^3r^4}\sum_{i = 1}^{n} \sum_{j = 1} \sum_{k \neq j} \Ebb\left[(D_jf(x_i))(D_kf(x_i))\right] = \frac{1}{n^3r^4}\sum_{i = 1}^{n} \sum_{j = 1} \sum_{k \neq j} \Ebb\left[ \Ebb(D_kf(x)\mid x_i = x)^2\right] = O(L^2).
\end{equation*}

The expectation of the diagonal terms may be much larger, but on the other hand there are many fewer of them. Specifically, we compute
\begin{align*}
\Ebb\bigl((D_kf(x_i))^2\bigr) & = \Ebb\biggl(O\left(L^2(x_k - x_i)^2\right)\cdot K_r(x_k,x)^2\biggr) \\
& = O(L^2r^2)\cdot\Ebb(K_r(x_k,x_i)^2) = O(L^2 r)
\end{align*}
and therefore
\begin{equation*}
\frac{1}{n^3r^4}\sum_{i = 1}^{n} \sum_{j = 1}^{n} \Ebb\bigl((D_jf(x_i))^2\bigr) = \frac{1}{n} O(L^2 r^{-3}) = O(L^2)
\end{equation*}
where the second equality follows since by assumption when $d = 1$, $n^{-1} = O(r^{3})$. 

\subsection{Proof of Lemma~\ref{lem:3rd_order_roughness_functional}}
We prove Lemma~\ref{lem:3rd_order_roughness_functional} when $d = 1$ and $r(n) \geq n^{-1/5}$.

\textcolor{red}{TODO:} Prove for all dimensions $d$, by replacing derivatives with partial derivatives.

From~\eqref{eqn:roughness_functional_representation_odd}, we have
\begin{align*}
R_{3,n}(f) & = \frac{1}{2n^2r^2}\sum_{i,j = 1}^{n}\left(\frac{1}{(nr^2)}\sum_{k \in n}\bigl(D_kf(x_i) - D_kf(x_j)\bigr)\right)^2K_r(x_i,x_j) \\
& = \frac{1}{2n^4r^6}\sum_{i,j = 1}^{n} \sum_{k,\ell = 1}^{n} D_{i\ell}f(x_j) \cdot d_i(D_kf(x_j)).
\end{align*}
We separate our analysis into cases based on the distinct indices in the previous summation.

\paragraph{Case 1: $i = k = \ell$.}
In this case, we can rewrite the summand as
\begin{align*}
D_iif(x_j) d_iD_if(x_j) & = (d_if(x_j))^2 K_r(x_i,x_j)^3 \\
& \leq 4L^2r^2 \abs{K_r(x_i,x_j)^3}
\end{align*}
Therefore,
\begin{align*}
\Ebb(D_iif(x_j) d_iD_if(x_j)) & \leq 4L^2 r^2 \Ebb\bigl(\abs{K_r(x_i,x_j)^3}\bigr)\\
& \leq 4CL^3, \tag{Lemma~\ref{lem:expected_kernel}}
\end{align*}
and as a result
\begin{equation*}
\frac{1}{2n^4r^6}\sum_{i,j = 1}^{n} \Ebb(D_{ii}f(x_j) \cdot d_i(D_if(x_j))) \leq \frac{2CL^3}{n^2r^6} \leq \frac{2CL^3}{n^{4/5}}.
\end{equation*}

\paragraph{Case 2: $i = \ell \neq k$.}
In this case, we can rewrite the summand as $-(D_if(x_j))(d_iD_kf(x_j))K_r(x_i,x_j)$. We have that
\begin{equation*}
\abs{D_if(x_j)} \leq 2LrK(x_i,x_j),~~\textrm{and}~ \abs{d_iD_kf(x_j)} \leq 2Lr \bigl(\abs{K_r(x_k,x_i)} + \abs{K_r(x_{\ell},x_i)}\bigr)
\end{equation*}
Therefore, we can upper bound
\begin{equation*}
\abs{D_if(x_j)d_iD_kf(x_j)K_r(x_i,x_j)} \leq 4Lr^2K(x_i,x_j)^2\bigl(\abs{K_r(x_k,x_i)} + \abs{K_r(x_{\ell},x_i)}\bigr)
\end{equation*}
and the resultant expectation satisfies
\begin{equation*}
\Ebb\bigl(\abs{D_if(x_j)d_iD_kf(x_j)K_r(x_i,x_j)}\bigr) \leq 8CL^4r.
\end{equation*}
Since there are order $n^3$ terms in the sum for which $i = k \neq \ell$, we have
\begin{equation*}
\frac{1}{2n^4r^4}\sum_{\ell\neq k = i \neq j\neq }^{n} \Ebb(D_{ii}f(x_j) \cdot d_i(D_{\ell}f(x_j))) \leq \frac{8 C L^3}{r^5n} \leq 8 C L^3.
\end{equation*}
Note that by symmetry, the same analysis applies to the case $i = \ell \neq k$.
\paragraph{Case 3: $i \neq k = \ell$.}
In this case, we can rewrite the summand as $(d_{i}D_kf(x_j))^2K_r(x_i,x_j)$. We bound the order of magnitude of the composed difference operators using Taylor expansion:
\begin{align*}
\abs{d_{i}D_kf(x_j)} & = \abs{(f(x_k) - f(x_i))K_r(x_k,x_i) - (f(x_k) - f(x_j))K_r(x_k,x_j)} \\
& \leq \abs{(f(x_k) - f(x_i))}\abs{K_r(x_k,x_i)} + \abs{(f(x_k) - f(x_j))}\abs{K_r(x_k,x_ij)} \\
& \leq 2L\left(\abs{x_k - x_j} \abs{K_r(x_k,x_i)} + \abs{x_k - x_i} \abs{K_r(x_k,x_i)}\right) \\
& \leq 2Lr \bigl(\abs{K_r(x_k,x_i)} + \abs{K_r(x_{\ell},x_i)}\bigr)
\end{align*}
Taking expectation with respect to $x_k$, we have
\begin{align*}
\Ebb(\bigl(d_{i}D_kf(x_j)\bigr)^2|x_i,x_j) & \leq 8L^2r^2\left(\int K_r(x,x_i)^2 p(x)\,dx + \int K_r(x,x_j)^2 p(x)\,dx\right) \\
& \leq 16L^3r^{2} \int \left(\frac{1}{r}K(z/r)\right)^2 \,dz \tag{$p \in C^2(L)$} \\
& \leq 16L^3r \int \bigl(K(t)\bigr)^2 \,dt \tag{change of variables} \\
& \leq 16CL^3r. \tag{$K$ compactly supported and bounded}
\end{align*}
As a result, 
\begin{align*}
\Ebb((d_{i}D_kf(x_j))^2K_r(x_i,x_j)) & = \Ebb(\Ebb((d_{i}D_kf(x_j))^2|x_i,x_j)K_r(x_i,x_j)) \\
& \leq 16CL^3r\Ebb(\abs{K_r(x_i,x_j)}) \\
& \leq 16CL^3r.
\end{align*}
Since there are not quite $n^3$ terms in the sum with $i \neq k = \ell$, and since $r \geq n^{-1/5}$, 
\begin{equation*}
\frac{1}{2n^4r^6}\sum_{i \neq j \neq k = \ell \neq i}^{n} \Ebb(D_{ik}f(x_j) \cdot d_i(D_kf(x_j))) \leq \frac{8 C L^3}{r^5n} \leq 8 C L^3.
\end{equation*}

\paragraph{Case 4: $i,k,\ell$ all distinct.}
We rewrite the summand as $d_i(D_{\ell}f(x_j)) d_i(D_{k}f(x_j))K_r(x_i,x_j)$. Noting that
\begin{equation*}
d_i(D_{k}f(x_j)) = [f(x_k) - f(x_i)]K_r(x_k,x_i) -  [f(x_k) - f(x_j)]K_r(x_k,x_j)
\end{equation*}
by Lemma~\ref{lem:leading_term} we have that
\begin{equation*}
\abs{\Ebb(d_i(D_{k}f(x_j))|x_i,x_j)} \leq 2Lr^3.
\end{equation*}
Therefore,
\begin{align*}
\abs{\Ebb(d_i(D_{\ell}f(x_j)) d_i(D_{k}f(x_j))K_r(x_i,x_j))} & \leq \Ebb\biggl(\abs{\Ebb(d_k(D_{\ell}f(x_j))|x_i,x_j)\cdot\Ebb(d_k(D_{\ell}f(x_j))|x_i,x_j)\cdot K_r(x_k,x_j)}\biggr) \\
& \leq 4L^2r^6 \Ebb(\abs{K_r(x_k,x_j)}) \leq 4CL^2r^6.
\end{align*}
Since there are order $n^4$ terms in the summation for which $i,j,k$ and $\ell$ are all distinct, we have that
\begin{equation*}
\frac{1}{2n^4r^6} \sum_{i \neq j \neq k \neq \ell}^{n} \Ebb(D_{ik}f(x_j) d_iD_{\ell}f(x_j) K_r(x_i,x_j)) \leq 4C.
\end{equation*}
The above four cases cover all terms in the summation, and so we have proved Lemma~\ref{lem:3rd_order_roughness_functional}.



\end{document}