\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue
}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{fullpage}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

% Make a widecheck symbol (thanks, Stack Exchange!)
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
% widecheck made

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}
\newcommand{\Lap}{L}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Vset}{\mathcal{V}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Note on: Out-of-Sample Error for Laplacian Eigenmaps}
\author{Alden Green}
\date{\today}
\maketitle

The graph Laplacian eigenmaps estimator $\wh{\theta}_{\LE}$ is a least-squares estimator, where a response vector $Y = (Y_1,\ldots,Y_n)$ is projected onto the span of the first $K$ eigenvectors of a graph Laplacian matrix. It is defined only in-sample, whereas out-of-sample error is more interesting in many applications. There is no commonly agreed upon way in the literature to extend $\wh{\theta}_{\LE}$, or indeed any type of Laplacian estimator. In this note, I discuss three methods for doing so: least-squares regression of $\wh{\theta}_{\LE}$ onto an a priori defined set of features, Nadaraya-Watson kernel smoothing, treating $\wh{\theta}_{\LE}$ as the response, and Nystr\"{o}m extension of Laplacian eigenvectors. These methods are roughly ordered from least to most preferable, but also (as these things go) easiest to hardest to analyze. I begin in Section~\ref{sec:least_squares_arbitrary_design_and_features} by reviewing known bounds on the out-of-sample error of a (generic) least-squares estimator; these bounds will be useful for the analysis of both the first and third methods.

\section{Out-of-sample error for least-squares estimators: arbitrary design and features}
\label{sec:least_squares_arbitrary_design_and_features}
This section largely summarizes results from a paper by Daniel Hsu on random design regression. We start with some general setup that will apply to everything that follows.

Suppose we observe $(X_1,\wh{\theta}_1),\ldots,(X_n,\wh{\theta}_n)$, where $X_1,\ldots,X_n \in \Reals^d$ and $\wh{\theta}_{1:n} = (\wh{\theta}_1,\ldots,\wh{\theta}_n) \in \Reals^n$. Our goal is to use this observed data to construct an estimator $\wh{f}$ which is on average close to $f^{\ast}: \Omega \to \Reals$ on the domain $\Omega$ (for instance, $\Omega = [0,1]^d$.) Specifically, we will consider the following least-squares estimator,
\begin{equation*}
\wh{\beta} = \argmin \| \wh{\theta}_{1:n} - \Phi_n^{\top} \beta \|_n^2 = (\Phi_n^\top \Phi_n)^{-1}\Phi_n^{\top} \wh{\theta}_{1:n},~~ \wh{f} = \sum_{k = 1}^{\kappa} \wh{\beta}_k \varphi_k
\end{equation*}
where $\varphi_1,\ldots,\varphi_{K}$ are each elements of $\Leb^2(\Omega)$, and $\Phi_n \in \Reals^{n \times K}$ has entries $(\Phi_{n})_{ik} = \varphi_k(X_i)$. If $\wh{\theta}_{1:n}$ lies in the span of the columns of $\Phi_n$, then $\wh{f}$ is a interpolant of $\wh{\theta}_{1:n}$; this applies to our third method, Nystr\"{o}m extension of Laplacian eigenvectors, but not to our first method. We will measure error in the $\Leb^2(P)$ norm $\norm{\wh{f} - f^{\ast}}_P^2 = \int_{\Omega} (\wh{f}(x) - f^{\ast}(x))^2 \,dP(x)$, where $P$ is a probability measure supported on domain $\Omega$. For this section, we assume no relationship between $P$ and $X_1,\ldots,X_n$.

We begin by decomposing this norm into several modular terms, which capture the different types of error intrinsic to the problem. To this end, let
\begin{align*}
\wt{\beta} & = \argmin_{\beta \in \Reals^K} \|f^{\ast}_{1:n} - \Phi_n^{\top} \beta \|_n^2 = (\Phi_n^\top \Phi_n)^{-1}\Phi_n^{\top} {f}^{\ast}_{1:n},&& \wt{f}_K = \sum_{k = 1}^{K} \wt{\beta}_k \varphi_k \\ 
\beta^{\star} & = \argmin_{\beta \in \Reals^K} \|f^{\ast} - \sum_{k = 1}^{K} \beta_k \varphi_k \|_P^2,&& f_K^{\ast} = \sum_{k = 1}^{K} \beta_k^{\ast} \varphi_k
\end{align*}
and let $f_{\perp}^{\ast} = f^{\ast} - f_{K}^{\ast}$. We decompose $\norm{\wh{f} - f^{\ast}}_P^2$ as follows:
\begin{equation*}
\norm{\wh{f} - f^{\ast}}_P^2 \leq 3\Bigl(\underbrace{\norm{\wh{f} - \wt{f}_K}_P^2}_{\textrm{estimation error}} + \underbrace{\norm{\wt{f}_K - f_K^{\ast}}_P^2}_{\textrm{sampling error}} + \underbrace{\norm{f_K^{\ast} - f^{\ast}}_P^2}_{\textrm{truncation error}}\Bigr)
\end{equation*}
This leaves us with three separate kinds of error, and we now upper bound the first two.

\paragraph{Notation.} Let $e_{1:n} = \wh{\theta}_{1:n} - {f}^{\ast}_{1:n}$. Let~$\Sigma \in \Reals^{K \times K}$ be the second moment matrix of $\varphi_1,\ldots,\varphi_K$ with $(k,\ell)$-th entry $\Sigma_{k\ell} = \Ebb_{X \sim P}\bigl[\varphi_k(X) \varphi_{\ell}(X)\bigr]$. Likewise let $\wh{\Sigma} = \Phi_n^{\top} \Phi_n/n$ be the empirical second moment matrix. For a matrix $A \in \Reals^{K \times K}$, let $\|A\|$ denote the operator norm of $A$.  For a probability distribution $Q$ on $\Omega$, let $d_{\min}(Q) = \inf_{x \in \Omega} \int K(\|z - x\|/r) \,dQ(z)$ be the minimum degree of $Q$.

\paragraph{Estimation error.}
Noting that
\begin{equation*}
~~\wh{\beta} - \wt{\beta} = \frac{1}{n}\wh{\Sigma}^{-1} \Phi_n^{\top} e_{1:n},
\end{equation*}
the estimation error satisfies the inequality,
\begin{align*}
\norm{\wh{f} - \wt{f}_K}_P^2 & = (\wh{\beta} - \wt{\beta})^{\top} \Sigma (\wh{\beta} - \wt{\beta}) \\
& = \frac{1}{n^2} e_{1:n}^{\top} \Phi_n \wh{\Sigma}^{-1} \Sigma \wh{\Sigma}^{-1} \Phi_n^{\top} e_{1:n} \\
& \leq \|\Sigma^{1/2} \wh{\Sigma}^{-1} \Sigma^{1/2}\| \cdot \|e_{1:n}\|_n^2.
\end{align*}
with the inequality following since $\|AB\| = \|BA\|$. 

\paragraph{Sampling error.}
Multiplying $\wt{\beta} - \beta^{\star}$ on the left by $\Sigma^{1/2}$, we have
\begin{equation*}
\Sigma^{1/2}(\wt{\beta} - \beta^{\ast}) =  \Sigma^{1/2} \wh{\Sigma}^{-1} \bigl( \frac{1}{n} \Phi_n^{\top} f_{1:n}^{\ast} - \frac{1}{n} \Phi_n^{\top} \Phi_n \beta^{\ast} \bigr) = \Sigma^{1/2} \wh{\Sigma}^{-1} \frac{1}{n} \Phi_n^{\top}  \bigl(\underbrace{f_{1:n}^{\ast} - \Phi_n \beta^{\ast}}_{ = (f_{\perp}^{\ast})_{1:n}} \bigr) = \frac{1}{n}\Sigma^{1/2} \wh{\Sigma}^{-1} \Phi_n^{\top} (f_{\perp}^{\ast})_{1:n}.
\end{equation*}
Therefore
\begin{equation*}
\|\wt{f}_K - f_K^{\ast}\|_P^2  = \|\wt{\beta} - \beta^{\ast}\|_{\Sigma}^2 \leq \|\Sigma^{1/2} \wh{\Sigma}^{-1} \Sigma^{1/2}\| \cdot \Bigl\|f_{\perp}^{\ast}\|_n^2
\end{equation*}

The results of this section are summarized by Lemma~\ref{lem:l2p_error_fixed_design}. 
\begin{lemma}
	\label{lem:l2p_error_fixed_design}
	Suppose $\wh{\Sigma} \succeq 0$. Then the $\Leb^2(P)$ error $\|\wh{f} - f^{\ast}\|_P^2$ is upper bounded as follows:
	\begin{equation}
	\label{eqn:l2p_error_fixed_design}
	\|\wh{f} - f^{\ast}\|_P^2 \leq 3\Bigl(\|\Sigma^{1/2} \wh{\Sigma}^{-1} \Sigma^{1/2}\| \cdot \|\wh{\theta}_{1:n} - {f}^{\ast}_{1:n}\|_n^2 + \|\Sigma^{1/2} \wh{\Sigma}^{-1} \Sigma^{1/2}\| \cdot \|f_{\perp}^{\ast}\|_n^2 + \|f_{\perp}^{\ast}\|_P^2\Bigr).
	\end{equation}
	If $\wh{\Sigma} = I$ then
	\begin{equation}
	\label{eqn:l2p_error_fixed_design_identity}
	\|\wh{f} - f^{\ast}\|_P^2 \leq 3\Bigl(\|\Sigma\| \cdot \|\wh{\theta}_{1:n} - {f}^{\ast}_{1:n}\|_n^2 + \|\Sigma\| \cdot \|f_{\perp}^{\ast}\|_n^2 + \|f_{\perp}^{\ast}\|_P^2\Bigr).
	\end{equation}
\end{lemma}
(Basically) no assumptions so far have been made on $X_1,\ldots,X_n$ or $\varphi_1,\ldots,\varphi_K$. In the standard setup $X_1,\ldots,X_n$ are randomly sampled from $P$, and $\varphi_1,\ldots,\varphi_K$ are a priori determined. Then matrix Bernstein's inequality gives an upper bound on $\|\Sigma^{1/2} \wh{\Sigma}^{-1} \Sigma^{1/2}\|$, standard Bernstein's inequality gives an upper bound on $\|f_{\perp}^{\ast}\|_n^2 - \|f_{\perp}^{\ast}\|_P^2$, and we obtain from~\eqref{eqn:l2p_error_fixed_design} an upper bound that depends on $\|e_{1:n}\|_n^2$ and $\|f_{\perp}^{\ast}\|_P^2$.  

If $\varphi_1,\ldots,\varphi_{K}$ are chosen in a data-dependent manner, then the analysis is plainly quite different. Each of $\|\Sigma\|$, $\|f_{\perp}^{\ast}\|_n^2$ and $\|f_{\perp}^{\ast}\|_P^2$ are random variables which depend on $\varphi_1,\ldots,\varphi_K$. We also do not have concentration of $\|f_{\perp}^{\ast}\|_n^2$ around $\|f_{\perp}^{\ast}\|_P^2$. 

\section{Kernel smoothing}
Suppose we observe $(X_1,\wh{\theta}_1),\ldots,(X_n,\wh{\theta}_n)$, where $X_1,\ldots,X_n \in \Reals^d$ are sampled i.i.d from an unknown distribution $P$ on domain $\Omega$, and $\wh{\theta}_{1:n} = (\wh{\theta}_1,\ldots,\wh{\theta}_n) \in \Reals^n$. Our goal is to construct an estimator $\wh{f}$ such that the error $\|\wh{f} - f^{\ast}\|_P^2$ is as small as possible relative to $\|\wh{\theta}_{1:n} - f^{\ast}_{1:n}\|_n^2$.

For a kernel function $\eta(\cdot): [0,\infty) \to (-\infty,+\infty)$, bandwidth $\varepsilon > 0$, and a distribution $Q$, the \emph{Nadaraya-Watson kernel smoother} $T_{\varepsilon,Q}$ is given by
\begin{equation*}
\bigl(T_{\varepsilon,Q}f)(x) := \frac{1}{d_Q(x)} \int_{\Omega} f(z)\eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dQ(z)
\end{equation*}
where $d_Q(x) = \int_{\Omega} \eta\bigl(\|z - x\|/\varepsilon\bigr) \,dQ(z)$. We denote $T_{\varepsilon,P_n}$ by $T_{\varepsilon,n}$, and $d_{P_n}$ by $d_n$.

The estimated function $\wh{f}$ we will consider is a kernel smoother passed over $\wh{\theta}$,
\begin{equation}
\label{eqn:kernel_smoother_laplacian_eigenmaps}
\wh{f}(x) := T_{\varepsilon,n}\wh{\theta}_{1:n}(x).
\end{equation}
In order to relate the out-of-sample error $\|\wh{f} - f^{\ast}\|_P$ to the in-sample error $\|\wh{\theta}_{1:n} - f^{\ast}_{1:n}\|_n$, we will assume the following.
\begin{enumerate}[label=(A\arabic*)]
	\item
	\label{asmp:kernel}
	The kernel function $\eta$ is compactly supported on a subset of $[0,1]$. Additionally, $\eta$ is Lipschitz on its support. Finally, for convenience we will assume that $\eta$ is normalized, 
	\begin{equation*}
	\int_{0}^{\infty} \eta(s) \,ds = 1.
	\end{equation*}
	\item 
	\label{asmp:domain} There exists $\alpha > 0$ such that for every $x \in \mc{X}$, $d_{\nu}(x) \geq \alpha \cdot \varepsilon^d$.
	\item 
	\label{asmp:density}The distribution $P$ admits a density $p$ with respect to Lebesgue measure. The density $p \in C^1(\mc{X})$, and is bounded away from $0$ and $\infty$ for all $x \in \mc{X}$,
	\begin{equation*}
	0 < p_{\min} \leq p(x) < p_{\max} < \infty.
	\end{equation*}
\end{enumerate}
Under Assumption~\ref{asmp:kernel}, we automatically have that $d_\nu(x) = \varepsilon^d$ for all $x \in \mc{X}_{\varepsilon}$. Assumption~\ref{asmp:domain} is thus a condition on the boundary $\partial \Omega$, and is implied for all sufficiently small $\varepsilon$ if the boundary is Lipschitz. Denoting $K(\cdot) = (\eta(\cdot))^2$, we note that Assumption~\ref{asmp:kernel} implies that $\eta$ is square-integrable, $\int_{0}^{\infty} K(s) \,ds < \infty$.
\begin{lemma}
	\label{lem:kernel_smoothing_laplacian_eigenmaps}
	Suppose $f^{\ast} \in H^1(\mc{X})$ and $p \in C^1(\mc{X})$. Then with probability at least $1 - \delta - C\varepsilon^d\exp\{-Cn\varepsilon^d\}$, it holds that
	\begin{equation*}
	\|\wh{f} - f^{\ast}\|_P^2 \leq C\biggl(\|\wh{\theta}_{1:n} - f^{\ast}_{1:n}\|_n^2 + \frac{1}{\delta} \cdot \frac{\varepsilon^2}{n\varepsilon^d} |f|_{H^1(\mc{X})}^2 + \frac{1}{\delta}\|T_{\varepsilon,P}f^{\ast} - f^{\ast}\|_P^2\biggr)
	\end{equation*}
\end{lemma}
Notice that the variance term in the above is smaller than the typical variance term for kernel smoothing of noisy data (on the order of $1/(n\varepsilon^d)$) by a factor of $\varepsilon^2$. The bias term, on the other hand, is standard (at least when $f^{\ast} \in C_0^s(\Omega)$). In particular the following estimate of the bias can be obtained. 
\begin{lemma}
	\label{lem:kernel_smoothing_bias}
	Suppose $f^{\ast} \in H_0^{s}(\Omega)$, $p \in C^{r}(\Omega)$ for $r = s - 1$, and that $\eta$ is an order-$s$ kernel. It follows that
	\begin{equation*}
	\|T_{\varepsilon,P}f^{\ast} - f^{\ast}\|_P^2 \leq C \varepsilon^{2s}.
	\end{equation*}
\end{lemma}
For the proof of this claim see Section~\ref{sec:kernel_smoothing_bias_pf}.

Thus balancing bias and variance, using a smaller bandwidth ($\varepsilon \asymp n^{-1/(2r + d)}$) than is typical for kernel smoothing with noisy data, leads to an asymptotically negligible error ($n^{-2s/(2r + d)}$) compared to the minimax rate for estimating Sobolev functions. 

\begin{proof}[Proof of Lemma~\ref{lem:kernel_smoothing_laplacian_eigenmaps}]
	By the triangle inequality, 
	\begin{equation*}
	\|\wh{f} - f^{\ast}\|_P \leq \|\wh{f} - T_{\varepsilon,n}f^{\ast}_{1:n}\|_P + \|T_{\varepsilon,n}f^{\ast}_{1:n} - f^{\ast}\|_P.
	\end{equation*}
	We analyze each of the two terms in the above expression separately. We will show that
	\begin{equation}
	\label{pf:kernel_smoothing_laplacian_eigenmaps_0}
	\|\wh{f} - T_{\varepsilon,n}f^{\ast}_{1:n}\|_P \leq \frac{1}{\alpha}\bigl(1 + o_P(1)\bigr) \cdot \|\wh{\theta}_{1:n} - f^{\ast}_{1:n}\|_n
	\end{equation}
	and 
	\begin{equation}
	\label{pf:kernel_smoothing_laplacian_eigenmaps_0.5}
	\|T_{\varepsilon,n}f^{\ast}_{1:n} - f^{\ast}\|_P^2 \leq \frac{C}{\delta}\frac{\varepsilon^2}{n\varepsilon^d} |f|_{H^1(\mc{X})}^2 + \frac{1}{\delta}\|T_{\varepsilon,P}f^{\ast} - f^{\ast}\|_P^2,
	\end{equation}
	the latter inequality holding with probability $1 - \delta$.
	
	\textit{Proof of \eqref{pf:kernel_smoothing_laplacian_eigenmaps_0}.} We will begin by assuming that $\eta$ is positive, $\eta(z) \geq 0$ for all $z$, and return to prove the claim in its full generality at the end.
	
	The left hand side of~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_0} is equal to $\|T_{\varepsilon,n}(\wh{\theta}_{1:n} - f^{\ast}_{1:n})\|_P$, and~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_0} is implied by showing that the operator $T_{\varepsilon,n}: L^2(X) \to L^2(\Omega)$ is (up to a constant) a contraction. We will show that for any $u \in L^2(X)$,
	\begin{equation}
	\label{pf:kernel_smoothing_laplacian_eigenmaps_1}
	\|T_{\varepsilon,n}u\|_P^2 \leq \frac{1}{\alpha}\bigl(1 + o_P(1)\bigr) \|u\|_n^2.
	\end{equation}
	We begin with a pointwise estimate, which follows from Jensen's inequality:
	\begin{equation*}
	\Bigl(\bigl(T_{n,\varepsilon}u\bigr)(x)\Bigr)^2 \leq \frac{1}{d_n(x) n} \sum_{i = 1}^{n} \bigl(u(X_i)\bigr)^2 \eta\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr).
	\end{equation*}
	(Note that it is here that we have used the positivity of $\eta(\cdot)$.) Integrating with respect to $P$ and exchanging sum with integral yields an upper bound on the $L^2(P)$ norm of $T_{n,\varepsilon}u$,
	\begin{equation}
	\label{pf:kernel_smoothing_laplacian_eigenmaps_2}
	\|T_{n,\varepsilon}u\|_P^2 \leq \sum_{i = 1}^{n} u_i^2 \Bigl(\int_{\Omega} \frac{\eta(\|X_i - x\|/\varepsilon)}{n d_n(x)} p(x) \,dx \Bigr)
	\end{equation}
	The degree estimate $d_n(x)$ is quite close to $p(x)$. As we show in Lemma~\ref{lem:measure_of_ball_estimate}, if $\varepsilon = \omega((\log n/n)^{1/d})$ then the empirical measure $d_n(x)$ concentrates around $d_P(x)$ uniformly over $x \in \Omega$, 
	\begin{equation*}
	\sup_{x \in \Omega}|d_n(x) - d_P(x)| = o_P(\varepsilon^d).
	\end{equation*}
	Moreover, it follows from the Lipschitz continuity of $p$ that for any $x \in \Omega$,
	\begin{equation*}
	d_P(x) = \int_{\Omega} \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) p(x') \,dx' = p(x) d_{\nu}(x) + O(\varepsilon^{d + 1}),
	\end{equation*}
	and thus by the triangle inequality $\sup_{x \in \Omega}|d_n(x) - d_\nu(x) p(x)| = o_P(\varepsilon^d)$.
	
	As pointed out previously, if $x' \in \Omega_{\varepsilon}$ then $d_{\nu}(x') = \varepsilon^d$. On the other hand by assumption~\ref{asmp:domain} for any $x' \in \Omega, d_{\nu}(x') \geq \varepsilon^d/\alpha$. Thus 
	\begin{equation*}
	\int_{\Omega} \frac{\eta(\|x' - x\|/\varepsilon)}{d_n(x')} p(x') \,dx' = \int_{\Omega} \frac{\eta(\|x' - x\|/\varepsilon)}{d_{\nu}(x') - o_P(\varepsilon^d)} \,dx' \leq 
	\begin{dcases*}
	\frac{1}{1 - o_P(1)}, & \textrm{if $x \in \Omega_{2\varepsilon}$} \\
	\frac{1}{\alpha} \cdot \frac{1}{1 - o_P(1)}, & \textrm{otherwise.}
	\end{dcases*}
	\end{equation*}
	Plugging back in to~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_2}, we deduce that
	\begin{equation*}
	\|T_{n,\varepsilon}u\|_P^2 \leq \sum_{i = 1}^{n} u_i^2 \biggl(\1\Bigl\{x \in \Omega_{2\varepsilon}\Bigr\} \frac{1}{1 - o_P(1)} + \1\Bigl\{x \in (\partial\Omega)_{2\varepsilon}\Bigr\} \frac{1}{\alpha} \frac{1}{1 - o_P(1)}\biggr).
	\end{equation*}
	This establishes~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_1}, and in turn~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_0}.
	
	Now, suppose $\eta(\cdot)$ can take on negative values. In this case, we have $\eta(\cdot) = \mathrm{sgn}(\eta(\cdot))K^{1/2}(\cdot)$. Let $d_n(x;K^{1/2})$ be the empirical degree with respect to kernel function $K^{1/2}$. Then
	\begin{align*}
	T_{n,\varepsilon}(u)(x) & = \frac{1}{d_n(x)n} \sum_{i = 1}^{n} \bigl(u(X_i)\bigr) \eta\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr) \\
	& = \frac{d_n(x;K^{1/2})}{d_n(x)}\frac{1}{d_n(x;K^{1/2})n} \sum_{i = 1}^{n} \bigl(u(X_i)\bigr) \mathrm{sgn}\biggl(\eta\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr)\biggr) K^{1/2}\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr).
	\end{align*}
	Taking the square and applying Jensen's inequality, we deduce that
	\begin{align*}
	\bigl(T_{n,\varepsilon}u(x)\bigr)^2 & \leq \biggl(\frac{d_n(x;K^{1/2})}{d_n(x)}\biggr)^2 \cdot \frac{1}{d_n(x;K^{1/2})n}\sum_{i = 1}^{n} \bigl(u(X_i)\bigr)^2 K^{1/2}\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr) \\
	& \leq \frac{(1 + o_P(1))\|\eta\|_{\infty}^2}{\alpha^2} \cdot \frac{1}{d_n(x;K^{1/2})n}\sum_{i = 1}^{n} \bigl(u(X_i)\bigr)^2 K^{1/2}\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr),
	\end{align*}
	with the latter inequality following by Lemma~\ref{lem:measure_of_ball_estimate} and assumption~\ref{asmp:kernel}. From here the proof proceeds along similar lines to the case where $\eta$ is non-negative.
	
	\textit{Proof of~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_0.5}.}  
	We will begin by introducing some notation. For a function $u \in L^2(\Omega)$, let
	\begin{equation*}
	L_{\varepsilon,n}u(x) := T_{\varepsilon,n}u(x) - u(x) = \frac{1}{d_n(x)n} \sum_{i = 1}^{n} \bigl(u(x) - u(X_i)\bigr) \eta\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr)
	\end{equation*}
	and let
	\begin{equation*}
	\wt{L}_{\varepsilon,n}u(x) := \frac{d_n(x)}{d_P(x)}L_{\varepsilon,n}u(x) = \frac{1}{d_P(x)n} \sum_{i = 1}^{n} \bigl(u(x) - u(X_i)\bigr) \eta\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr).
	\end{equation*}
	We have already shown that $d_n(x)/d_P(x) = (1 + o_P(1))$, and so we will focus our analysis on upper bounding $\Ebb[\|\wt{L}_{\varepsilon,n}f^{\ast}\|_P^2]$. We begin by providing expressions for the pointwise mean and variance of $\wt{L}_{\varepsilon,n}f^{\ast}(x)$:
	\begin{equation*}
	\Ebb\bigl[\wt{L}_{\varepsilon,n}f^{\ast}(x)\bigr] = T_{P,\varepsilon}f^{\ast}(x) - f^{\ast}(x)
	\end{equation*}
	and
	\begin{align*}
	\mathbb{V}\bigl[\wt{L}_{\varepsilon,n}f^{\ast}(x)\bigr] & = \frac{1}{(d_P(x))^2 n} \mathbb{V}\bigl[\bigl(f^{\ast}(X) - f^{\ast}(x)\bigr) \eta\bigl(\|X - x\|/\varepsilon\bigr)\bigr] \\
	& \leq \frac{1}{p_{\min}^2 \alpha^2 \varepsilon^{2d}} \mathbb{E}\bigl[\bigl(f^{\ast}(X) - f^{\ast}(x)\bigr)^2 K\bigl(\|X - x\|/\varepsilon\bigr)\bigr] \\
	& = \frac{1}{p_{\min}^2 \alpha^2 \varepsilon^{2d}} \int_{\Omega} \bigl(f^{\ast}(x') - f^{\ast}(x)\bigr)^2 K(\|x' - x\|/\varepsilon) p(x) \,dx.
	\end{align*}
	Then, integrating with respect to $P$ over $\Omega$, we obtain
	\begin{align*}
	\Ebb\bigl[\|\wt{L}_{\varepsilon,n}f^{\ast}\|_P^2\bigr]  & \leq \|T_{P,\varepsilon}f^{\star} - f^{\star}\|_{P}^2 + \frac{1}{p_{\min}^2 \alpha^2 \varepsilon^{2d}} E_{\varepsilon}(f^{\ast}) \\
	& \leq \|T_{P,\varepsilon}f^{\star} - f^{\star}\|_{P}^2 + \frac{C\varepsilon^2}{p_{\min}^2 \alpha^2 \varepsilon^{d}} |f|_{H^1(\Omega)}^2
	\end{align*}
	where $E_{\varepsilon}(f^{\ast}) := \int_{\Omega} \int_{\Omega} (f^{\ast}(x') - f^{\ast}(x))^2 K(\|x' - x\|/\varepsilon) p(x') p(x) ,\dx' \,dx$, and we have shown in our AISTATS paper that $E_{\varepsilon}(f^{\ast}) \leq C \varepsilon^{d + 2} |f|_{H^1(\Omega)}^2$, justifying the second inequality. Consequently, 
	\begin{align*}
	\|T_{n,\varepsilon}f^{\ast}_{1:n} - f^{\ast}\|_P^2 \leq (1 + o_P(1))^2 \|\wt{L}_{\varepsilon,n}f^{\ast}\|_P^2 & \leq \frac{\bigl(1 + o_P(1)\bigr)^2}{\delta} \mathbb{E}\bigl[\|\wt{L}_{\varepsilon,n}f^{\ast}\|_P^2\bigr] \\ 
	& \leq \frac{\bigl(1 + o_P(1)\bigr)^2}{\delta}\biggl(\|T_{P,\varepsilon}f^{\star} - f^{\star}\|_{P}^2 + \frac{C\varepsilon^2}{p_{\min}^2 \alpha^2 \varepsilon^{d}} |f|_{H^1(\Omega)}^2\biggr)
	\end{align*}
	with probability at least $1 - \delta$, establishing~\eqref{pf:kernel_smoothing_laplacian_eigenmaps_0.5}.
\end{proof}

Recall the notation $d_P(x) = \int \eta(\|z - x\|/\varepsilon) \,dP(z)$, $d_n(x) = d_{P_n}(x)$. Recall also that $K(\cdot) = |\eta(\cdot)|^2$. 
\begin{lemma}
	\label{lem:measure_of_ball_estimate}
	Let $\eta$ be a kernel compactly supported and Lipschitz on $[0,1]$. There exists a constant $C$ that does not depend on $\varepsilon$ or $\eta$, such that
	\begin{equation*}
	\sup_{x \in \Omega}|d_n(x) - d_P(x)| \leq C \sqrt{\frac{\varepsilon^d}{n}},
	\end{equation*}
	and
	\begin{equation*}
	\sup_{x \in \Omega}|d_n(x;K^{1/2}) - d_P(x;K^{1/2})| \leq C \sqrt{\frac{\varepsilon^d}{n}},
	\end{equation*}
	with probability at least $1 - \textcolor{red}{(?)}\exp\{-Cn\varepsilon^d\}$. 
\end{lemma}

\begin{proof}
	Since $\eta$ is compactly supported on $[0,1]$, a standard computation implies that since for any $x \in \mc{X}$,
	\begin{equation*}
	\Var\bigl[d_n(x)\bigl] = \frac{1}{n} \Var[\eta(\|X - x\|/\varepsilon)] \leq \frac{1}{n}\|\eta\|_{\infty}^2 \nu_d \varepsilon^d,
	\end{equation*}
	where we note that $\|\eta\|_{\infty} < \infty$ follows from the assumption that $\eta$ is Lipschitz. It follows from Bernstein's inequality that 
	\begin{equation*}
	\Pbb\biggl(|d_n(x) - d_P(x)| \leq C \sqrt{\frac{\varepsilon^d}{n}} \biggr) \leq 2\exp\biggl\{-\frac{t^2 \varepsilon^d}{C(\varepsilon^d + \delta \sqrt{\varepsilon^d/n})}\biggr\}.
	\end{equation*}
	for $C = \|\eta\|_{\infty} \nu_d$. \textcolor{red}{(...)}
\end{proof}

\section{Nystr\"{o}m extension of Laplacian eigenvectors}
The Nystr\"{o}m extension $\varphi_k$ of an eigenvector $v_k$ of the graph Laplacian $L$ is
\begin{equation}
\label{eqn:nystrom}
\varphi_k(x) := \frac{1}{d_n(x) - \lambda_k} \sum_{i = 1}^{n} v_{k,i} K\Bigl(\frac{\|X_i - x\|}{r}\Bigr)
\end{equation}
where $d_n(x) := \sum_{i = 1}^{n} K(\|X_i - x\|/r)$, and we assume $d_n(x) > \lambda_k$ for all $x \in \Omega$ and $k = 1,\ldots,K$. There are certain nice properties of the Nystr\"{o}m extension. It is an interpolant---that is, for any $\theta \in \mathrm{span}\{v_1,\ldots,v_K\}$ the function $f \in \Leb^2(\Omega)$ will agree with $\theta$ at sample design points---and it requires no information on $\Omega$ to be well-defined. However it is also tricky to analyze, because it involves taking $\theta \in \Leb^2(X)$, transforming it into eigenspace, and then applying a \emph{different} smoother to each eigenvector. To build some intuition, we will warm up by analyzing piecewise constant extension, which is much easier to tackle theoretically, if suboptimal for $f^{\ast} \in H^s(\mc{X})$ when $s > 1$.

\subsection{Piecewise constant extension}
Both $\|\Sigma\|$ and $\|f_{\perp}^{\ast}\|_P^2$ are easy to analyze when $\varphi_1,\ldots,\varphi_k$ are taken to be piecewise constant extensions of eigenvectors $v_k$ of the Laplacian matrix $L$.
Let $\{V_1,\ldots,V_n\}$ partition the domain $\Omega$ into $n$ cells, each of probability mass $\Pbb(V_j) = 1/n$, such that $x_j \in V_j$. The piecewise constant extension operator $P^{\star}: L^2(X)\to L^2(\Omega)$ is
\begin{equation}
\label{eqn:piecewise_extension}
\bigl(P^{\star}\theta\bigr)(x) = \sum_{j = 1}^{n} \theta_j \1\{x \in V_j\},
\end{equation}
and we take $\varphi_k = P^{\star}v_k$ for $k = 1,\ldots,K$.

It is clear that $\dotp{{\varphi}_k}{{\varphi}_{\ell}}_{P} = \dotp{v_k}{v_{\ell}}_{n} = \delta_{ij}$, so that $\Sigma = I$ and $\|\Sigma\| = 1$. Moreover, we have that $f_K^{\ast} = P^{\ast}\wt{f}_K$, so that
\begin{equation*}
\|P^{\star}f_{1:n}^{\ast} - {f}_K^{\ast}\|_P = \|f^{\ast} - \wt{f}_K\|_n = \|f^{\ast} - f_K^{\ast}\|_n;
\end{equation*}
the term $\|f^{\ast} - \wt{f}_K\|_n$ is an in-sample bias term that we have already analyzed. It remains to upper bound $\|P^{\star}f^{\ast} - f^{\ast}\|_P$, which we can do using crude but sufficient techniques when $f^{\ast} \in C^1(\Omega)$. 

\subsection{Nystr\"{o}m extension}

The analysis of the Nystr\"{o}m extension seems challenging. At present I have suitable bounds on zero out of three of the functionals in question ($\|\Sigma\|$, $\|f_{\perp}^{\ast}\|_n^2$ and $\|f_{\perp}^{\ast}\|_P^2$). 

\paragraph{Energy of population-level covariance $(\|\Sigma\|)$.}
The following representation of $\|\Sigma\|$ seems useful:
\begin{equation}
\label{eqn:covariance_energy}
\|\Sigma\| = \sup_{\substack{f \in \mathrm{span}\{\varphi_1,\ldots,\varphi_K\} \\ \|f\|_n = 1}} \|f\|_P.
\end{equation}
The equivalence~\eqref{eqn:covariance_energy} suggests a useful intermediary step in upper bound $\|\Sigma\|$ is an upper bound on $\|\varphi_k\|_P^2$. To do so, it is useful express the Nystrom extension of $v_k$ in terms of a kernel smoother. Recall that $T_{r,n}$ is the kernel smoother,
\begin{equation*}
\bigl(T_{r,n}\theta\bigr)(x) := \frac{1}{d_n(x)} \sum_{i = 1}^{n} \theta_i K\biggl(\frac{\|X_i - x\|}{r}\biggr).
\end{equation*}
Then constructing $\varphi_k$ is equivalent to first passing a kernel smoother over $v_k$, then applying a multiplication operator, i.e
\begin{equation}
\label{eqn:nystrom_extension_kernel_smoother_representation}
\varphi_k(x) = \frac{d_n(x)}{d_n(x) - \lambda_k} \bigl(T_{r,n}v_k\bigr)(x).
\end{equation}  
The operator $T_{r,n}$ is a contraction, in the sense that $\|T_{r,n}\theta\|_P \leq C \|\theta\|_n$, as the following derivations make clear:
\begin{align*}
\|T_{r,n}\theta\|_P^2 & = \int \Biggl(\frac{1}{d_n(x)} \sum_{i = 1}^{n} \theta_i K\biggl(\frac{\|X_i - x\|}{r}\biggr)\Biggr)^2 \,dP(x) \\
& \leq \sum_{i = 1}^{n} \theta_i^2 \int \frac{1}{d_n(x)}  K\biggl(\frac{\|X_i - x\|}{r}\biggr) \,dP(x) \\
& \leq \|\theta\|_n^2 \cdot \frac{d_{\max}(P)}{d_{\min}(P_n)}.
\end{align*}
(One can do better than this, actually, but this suffices for our purposes).

Therefore from~\eqref{eqn:nystrom_extension_kernel_smoother_representation}, and Holder's inequality, we have that
\begin{equation}
\label{eqn:nystrom_extension_l2p_norm}
\|\varphi_k\|_P \leq \sup_{x \in \Omega} \biggl\{\frac{d_n(x)}{d_n(x) - \lambda_k} \biggr\} \cdot  \|T_rv_k\|_P \leq \frac{d_{\min}(P_n)}{d_{\min}(P_n) - \lambda_k/n} \cdot \sqrt{ \frac{d_{\max}(P)}{d_{\min}(P_n)}}.
\end{equation}
For \textcolor{red}{typical} choices of $K$ and $r$, we have that $d_{\min}(P_n) \gg \lambda_K/n$, and therefore the upper bound~\eqref{eqn:nystrom_extension_l2p_norm} is $O(1)$, which suffices for our purposes.

Unfortunately, notwithstanding my prior claim, I don't yet see the right way to leverage this into a bound on $\|\Sigma\|$. For a given $\theta = \sum_{k = 1}^{K} \beta_k v_k$, and the resulting extrapolant $f = \sum_{k = 1}^{K} \beta_k \varphi_k(x)$, we can write $f$ in terms of $T_r\theta$ and a remainder term as follows,
\begin{equation*}
f(x) = \bigl(T_r\theta\bigr)(x) + \sum_{k = 1}^{K} \beta_k \frac{\lambda_k}{d_n(x) - \lambda_k} \bigl(T_rv_k\bigr)(x).
\end{equation*}
However, my best analysis of the remainder term, which I state here, is doomed by accumulation of error:
\begin{align*}
\biggl\|\sum_{k = 1}^{K} \beta_k \frac{\lambda_k}{d_n(x) - \lambda_k} \bigl(T_rv_k\bigr)(x)\biggr\|_P^2 & \overset{(i)}{\leq} K \cdot \sum_{k = 1}^{K} \beta_k^2  \biggl(\frac{\lambda_k/n}{d_{\min}(P_n) - \lambda_k/n}\biggr)^2 \cdot  \|T_rv_k\|_P^2 \\
& \overset{(ii)}{\leq} K \biggl(\frac{\lambda_K/n}{d_{\min}(P_n) - \lambda_K/n}\biggr)^2 \cdot \frac{d_{\max}(P)}{d_{\min}(P_n)} \\
& \asymp \frac{K \lambda_K^2}{n^2d_{\min}(P_n)}.
\end{align*}
To see that this bound is insufficient: recall the optimal choice $K_{\star} \asymp n^{d/(d + 2s)}$, that the eigenvalue $\lambda_K \asymp nr^{d + 2}K^{2/d}$ and that $d_{\min}(P_n) \asymp r^d$. Thus the bound reduces to $n^{\gamma}$ with exponent $\gamma = 3d/(2s + d) - 4/d$, which is greater than $0$ for most values of $d$. Moreover, the looseness in inequality $(ii)$ did not kill us---replacing $\lambda_K$ by $\lambda_2$ on the right hand side of $(ii)$ still results in a bound growing with $n$. On the other hand, the inequality $(i)$ was rather gross---it is tight only when $\varphi_k \propto \varphi_{\ell}$ for all $k,\ell = 1,\ldots,K$, and it seems likely that we can do better.

\paragraph{Distance between $f^{\ast}$ and subspace spanned by $\varphi_1,\ldots,\varphi_K$ $(f_{\perp}^{\ast})$.}

It seems like a useful starting place would be to take $f^{\ast} \propto \psi_k$, where $\psi_k$ is the $k$th eigenvector of the continuum weighted Laplace-Beltrami operator, and show that this is close to $\mathrm{span}\{\varphi_1,\ldots,\varphi_K\}$. I do not know an upper bound on the distance between $\psi_k$ and $\mathrm{span}\{\varphi_1,\ldots,\varphi_K\}$, but I do know such a bound for a different extension of $v_1,\ldots,v_K$. 

\paragraph{$(\|f_{\perp}^{\ast}\|_n)$.}

This one I frankly have no thoughts on how to analyze. I don't even understand it all that well, as evidenced by the fact that I can't name it.


\clearpage

\section{Prediction error for least-squares estimators: random design, fixed basis.}
In this section, we will assume that $X_1,\ldots,X_n$ are independent samples from a distribution $P$ supported over $\Omega \subseteq \Rd$, and that $\phi_1,\ldots,\phi_k$ are deterministic functions which in particular do not depend on $X_1,\ldots,X_n$.

The prediction error of $\wh{f}$ is $\mathbb{E}_{(X,Y)}[(Y - \wh{f}(X))^2]$, where $X \sim P$ and $\mathbb{E}[Y|X] = f^{\ast}(X)$ are independent of $X_1,\ldots,X_n$, and we additionally assume $Y - \mathbb{E}[Y|X]$ is independent of $X$ (i.e. homoskedastic noise). The prediction error can be decomposed into the sum of the $L^2(P)$ error and an irreducible error,
\begin{equation}
\mathbb{E}_{(X,Y)}[(Y - \wh{f}(X))^2] = \|\wh{f} - f^{\ast}\|_P^2 + \mathbb{E}_{(X,Y)}[(Y - f^{\ast}(X))^2],
\end{equation}
justifying our interest in the $\|\cdot\|_P^2$ error.

We will apply Lemma~\ref{lem:l2p_error_fixed_design}, and then use our assumptions on the design points and basis functions to control the right hand side of~\eqref{eqn:l2p_error_fixed_design}. In particular, we have the following. 
\begin{lemma}
	content...
\end{lemma}

\section{Miscellaneous}
I put things here which do not belong in any other section.

\subsection{Proof of Lemma~\ref{lem:kernel_smoothing_bias}, $d = 1$ and $p$ uniform}
\label{subsec:kernel_smoothing_bias_pf_1d_uniform_p}
Without loss of generality take the domain $\Omega = (0,1)$. Recall that we are interested in upper bounding
\begin{equation*}
\|T_{P,\varepsilon}u - u\|_{P}^2 = \int_{\Omega} \bigl(L_{\varepsilon,P}u(x)\bigr)^2 \,dP(x) \leq C\varepsilon^{2k}
\end{equation*}
where $L_{P,\varepsilon}$ is a non-local approximation to the continuum weighted Laplacian, given by
\begin{equation*}
L_{P,\varepsilon}u(x) := \frac{1}{d_P(x)}\int_{\Omega} \bigl(u(z) - u(x)\bigr) \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dP(z).
\end{equation*}
Let $\Omega_{\varepsilon} := \{x: B(x,\varepsilon) \in \Omega\}$ and let $\partial_{\varepsilon}\Omega := \Omega \setminus \Omega_{\varepsilon}$. Clearly
\begin{equation*}
\|L_{P,\varepsilon}u\|_{P}^2 = \underbrace{\int_{\Omega_{\varepsilon}} \bigl(L_{\varepsilon,P}u(x)\bigr)^2 \,dP(x)}_{:=I_1} + \underbrace{\int_{\partial_{\varepsilon}\Omega} \bigl(L_{\varepsilon,P}u(x)\bigr)^2 \,dP(x)}_{:=I_2}
\end{equation*}
and we now separately upper bound $I_1$ and $I_2$.

\paragraph{Upper bound on $I_1$.}
We begin with a pointwise analysis of $L_{p,\varepsilon}u(x)$ at a given $x \in \Omega_{\varepsilon}$. We may assume without loss of generality that $u \in C_0^{\infty}(\Omega)$, since $C_0^{\infty}(\Omega)$ is dense in $H_0^k(\Omega)$ and $\|T_{P,\varepsilon}u - u\|_P^2$ is continuous with respect to $\|\cdot\|_{H_0^k}$. Since $u$ is smooth, for every $x,z \in \Omega$, we have the $k$th-order Taylor expansion
\begin{equation*}
u(z) - u(x) = \sum_{j = 1}^{k - 1} (z - x)^{j} u^{(j)}(x)  + \frac{(z - x)^k}{(k - 1)!}\int_{0}^{1}(1 - t)^{k - 1}u^{(k)}(x + t(z - x))\,dt.
\end{equation*}
As a result,
\begin{equation}
\label{pf:kernel_smoothing_bias_1d_1}
\begin{aligned}
L_{P,\varepsilon}u(x) = \frac{1}{d_P(x)} & \Biggl\{\sum_{j = 1}^{k - 1} u^{(j)}(x) \int_{\Omega}(z - x)^{j}\eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr)\,dz + \\ 
& \frac{1}{(k - 1)!} \int_{\Omega} \int_{0}^{1} (1 - t)^k (z - x)^k u^{(k)}(x + t(z - x)) \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dt \,dz \Biggr\}
\end{aligned}
\end{equation} 

Now, since $x \in \Omega_{\varepsilon}$ then all $z \in B(x,\varepsilon)$ also belong to $\Omega$. For one thing, this implies
\begin{equation*}
d_P(x) = \int_{\Omega} \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dz = \varepsilon \cdot \int_{B(0,1)}  \eta(v) \,dv = \epsilon.
\end{equation*}
Additionally, we have that all $j = 1,\ldots,k - 1$, 
\begin{equation*}
\int_{\Omega}(z - x)^{k}\eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dz = \varepsilon^j \cdot \int_{B(0,1)} v^j \eta(v) \,dv = 0,
\end{equation*}
where the second equality follows because $\eta$ is a $k$th-order kernel. Consequently $L_{P,\varepsilon}$ depends only on the $k$th derivative of $u$: changing variables to $v = (z - x)/\varepsilon$, we have
\begin{align*}
L_{P,\varepsilon}u(x) & = \frac{1}{\varepsilon} \cdot \frac{1}{(k - 1)!} \int_{0}^{1} \int_{\Omega} (1 - t)^k (z - x)^k u^{(k)}(x + t(z - x)) \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dz \,dv \\
& = \frac{\varepsilon^{k}}{(k - 1)!} \int_{0}^{1} \int_{B(0,1)} (1 - t)^k v^k u^{(k)}(x + t\varepsilon v) \eta(|v|) \,dv \,dt
\end{align*} 
and as a result
\begin{equation*}
\Bigl(L_{P,\varepsilon}u(x)\Bigr)^2 = \frac{\varepsilon^{2k}}{(k!)^2} {\sigma_\eta}^2 \bigl(u^{(k)}(x)\bigr)^2 + \frac{\varepsilon^{2k}}{((k - 1)!)^2}A_1(x)A_{2}(x),
\end{equation*}
where $\sigma_\eta = \int v^k \eta(|v|) \,dv$ and 
\begin{align*}
A_1(x) & := \int_{0}^{1} \int_{B(0,1)} (1 - t)^k v^k \bigl(u^{(k)}(x + t\varepsilon v) - u^{(k)}(x)\bigr) \eta(|v|) \,dv \,dt \\
A_2(x) & := \int_{0}^{1} \int_{B(0,1)} (1 - t)^k v^k \bigl(u^{(k)}(x + t\varepsilon v) + u^{(k)}(x)\bigr) \eta(|v|) \,dv \,dt.
\end{align*}
Integrating over all $x \in \Omega_{\varepsilon}$, we have that
\begin{align*}
I_1 = \int_{\Omega_{\varepsilon}} \bigl(L_{P,\varepsilon}u(x)\bigr)^2 \,dx & = \frac{\varepsilon^{2k}}{(k!)^2} \sigma_\eta^2 \int_{\Omega_{\varepsilon}} \bigl(u^{(k)}(x)\bigr)^2 \,dx + \frac{\varepsilon^{2k}}{((k - 1)!)^2} \int_{\Omega_{\varepsilon}}A_1(x) A_2(x) \,dx \\
& \leq \frac{\varepsilon^{2k}}{(k!)^2} \Bigl(\sigma_\eta^2 \|u^{(k)}\|_{P}^2 + k^2 \|A_1\|_P \|A_2\|_P\Bigr).
\end{align*}
It remains to show that $\|A_1\|_P\|A_2\|_P = o(1)$.

\textcolor{red}{(TODO): Follow through on this calculation, referring to pages 192 - 195 of Tsybakov's book.}

\paragraph{Upper bound on $I_2$.}
We will prove the upper bound
\begin{equation}
\label{pf:kernel_smoothing_bias_1d_2}
\int_{0}^{\varepsilon} \bigl|L_{P,\varepsilon}u(x)\bigr|^2 \,dx \leq C \varepsilon^{2k + 1}\|u^{(k)}\|_P^2,
\end{equation}
with the bound on $\int_{1 - \varepsilon}^{1} \bigl|L_{P,\varepsilon}u\bigr|^2$ following in a completely analogous fashion.

Unlike the previous case, when $x$ is near the boundary of $\Omega$ we cannot use the higher-order property of $\eta$ to cancel the order-$1$ up through order-$(k - 1)$ derivatives of $u$. Instead, we will use the zero-trace property to show that $u$ itself must be quite small. Taking a Taylor expansion of $u$ around $0$, we have that for any $x \in \Omega$,
\begin{align}
u(x) & = u(0) + \sum_{j = 1}^{k - 1} x^j u^{(j)}(0) + \frac{1}{(k - 1)!} \int_{0}^{x}u^{(k)}(s)(x - s)^{k - 1}\,ds \nonumber \\
& = \frac{1}{(k - 1)!} \int_{0}^{x}u^{(k)}(s)(x - s)^{k - 1}\,ds. \label{pf:kernel_smoothing_bias_1d_3}
\end{align}
with the second inequality following since $u \in C_c^{\infty}(\Omega)$ implies $u(0) = 0$ as well as $u^{(j)}(0) = 0$ for $j = 1,\ldots,k - 1$. To make use of this upper bound on $u(x)$, we upper bound $|L_{P,\varepsilon}u(x)|^2$ using the Cauchy-Schwarz inequality,
\begin{align*}
|L_{P,\varepsilon}u(x)|^2 & = \Biggl(\frac{1}{d_P(x)} \int_{\Omega} \bigl(u(x) - u(z)\bigr) \eta\biggl(\frac{|x - z|}{\varepsilon}\biggr) \,dz\Biggr)^2 \\
& \leq \frac{1}{d_P(x)}\biggl(\int_{\Omega} K^{1/2}\biggl(\frac{x - z}{\varepsilon}\biggr) \,dz\biggr) \cdot \frac{1}{d_P(x)}\int_{\Omega} \bigl(u(x) - u(z)\bigr)^2 K^{1/2}\biggl(\frac{|x - z|}{\varepsilon}\biggr) \,dz \\
& \leq 2\frac{\|\eta\|_{\infty}}{\alpha^2\varepsilon} \cdot \int_{\Omega} \bigl(u(x)^2 + u(z)^2\bigr) K^{1/2}\biggl(\frac{|x - z|}{\varepsilon}\biggr) \,dz,
\end{align*}
where in the final inequality we have used the estimates $d_P(x;K^{1/2}) \leq \|\eta\|_{\infty} \varepsilon$ and $d_P(x) \geq (\varepsilon\alpha)$. 

We use these pointwise estimates to upper bound~\eqref{pf:kernel_smoothing_bias_1d_2}. Since $\eta$ is compactly supported, if $x \in (0,\varepsilon]$ and $\eta(|x - z|/\varepsilon) > 0$ then $z \in (0,2\varepsilon]$. As a result,
\begin{align*}
\int_{0}^{\varepsilon} |L_{P,\varepsilon}u(x)|^2 \,dP(x) & \leq 2\frac{\|\eta\|_{\infty}}{\alpha^2\varepsilon} \int_{0}^{\varepsilon}  \int_{\Omega} \bigl(u(x)^2 + u(z)^2\bigr) K^{1/2}\biggl(\frac{|x - z|}{\varepsilon}\biggr) \,dz \,dx \\
& \leq 2\frac{\|\eta\|_{\infty}}{\alpha^2\varepsilon} \int_{0}^{2\varepsilon}  \int_{0}^{2\varepsilon} \bigl(u(x)^2 + u(z)^2\bigr) K^{1/2}\biggl(\frac{|x - z|}{\varepsilon}\biggr) \,dz \,dx \\
& \leq 4\frac{\|\eta\|_{\infty}^2}{\alpha^2} \int_{0}^{2\varepsilon} (u(x))^2 \,dx,
\end{align*}
where the last inequality follows by Fubini's Theorem.

It remains to upper bound the $\Leb^2$ norm of $u$ over $(0,2\varepsilon]$. Here we use the bound~\eqref{pf:kernel_smoothing_bias_1d_3} and the Cauchy-Schwarz inequality to conclude that 
\begin{equation*}
\int_{0}^{2\varepsilon} \bigl(u(x)\bigr)^2 \,dx \leq \int_{0}^{2\varepsilon} \biggl(\int_{0}^{x} u^{(k)}(s)(x - s)^{k - 1} \,ds\biggr)^2 \,dx \leq \varepsilon^{2k - 1} \int_{0}^{2\varepsilon} \int_{0}^{x} \bigl(u^{(k)}(s)\bigr)^2 \,ds \,dx. 
\end{equation*}
Finally, switching the order of integration and substituting $v = s/(2\varepsilon)$ gives
\begin{align*}
\int_{0}^{2\varepsilon} \int_{0}^{x} \bigl(u^{(k)}(s)\bigr)^2 \,ds \,dx = \int_{0}^{2\varepsilon} \int_{x}^{2\varepsilon} \bigl(u^{(k)}(s)\bigr)^2 \,dx \,ds \leq 2\varepsilon \int_{0}^{2\varepsilon} \bigl(u^{(k)}(s)\bigr)^2 \,ds = 4\varepsilon^2 \|u^{(k)}\|_P^2,
\end{align*}
establishing~\eqref{pf:kernel_smoothing_bias_1d_2} upon choosing $C = \frac{16\|\eta\|_{\infty}^2}{\alpha^2}$ in the statement of the inequality.

\subsection{Proof of Lemma~\ref{lem:kernel_smoothing_bias}, $d = 1$ and $p \in C^k$.}
\label{subsec:kernel_smoothing_bias_pf_1d_smooth_p}
As in Section~\ref{subsec:kernel_smoothing_bias_pf_1d_uniform_p}, we take $\Omega = (0,1)$, let
\begin{equation*}
\|L_{P,\varepsilon}u\|_{P}^2 = \underbrace{\int_{\Omega_{\varepsilon}} \bigl(L_{\varepsilon,P}u(x)\bigr)^2 \,dP(x)}_{:=I_1} + \underbrace{\int_{\partial_{\varepsilon}\Omega} \bigl(L_{\varepsilon,P}u(x)\bigr)^2 \,dP(x)}_{:=I_2}.
\end{equation*}
We upper bound $I_2$ immediately, using~\eqref{pf:kernel_smoothing_bias_1d_2}, the equivalent upper bound when $p$ is uniform, to determine that
\begin{equation*}
I_2 \leq p_{\max}\int_{\partial_{\varepsilon}\Omega} |L_{P,\varepsilon}u(x)|^2 \,dx \leq C\varepsilon^{2k + 1}\|u^{(k)}\|_P^2.
\end{equation*}
It therefore remains only to upper bound $I_1$. 

\paragraph{Upper bound on $I_1$.}
The $k$th order Taylor expansion of $u$ is
\begin{equation}
\label{pf:kernel_smoothing_bias_1d_smooth_p_1}
u(z) - u(x) = \sum_{j = 1}^{k - 1}\frac{1}{j!} u^{(j)}(x) (z - x)^j + (z - x)^k r_z^{(k)}(x;u).
\end{equation}
The $k-1$st order Taylor expansion of $p$ is
\begin{equation}
\label{pf:kernel_smoothing_bias_1d_smooth_p_2}
p(z) = \sum_{j = 0}^{k - 2}\frac{1}{j!} p^{(j)}(x) (z - x)^j + (z - x)^{k - 1} r_z^{(k - 1)}(x;p).
\end{equation}
Note that we adopt the convention that if $i > j$, then $\sum_{i}^{j} a_i = 0$, to cover the case when $k = 1$. Also, for a given $j \in \mathbb{N}$ and $j$ times differentiable function $f$, we have written $r_z^{(j)}(x;f)$ for the $j$th order remainder term,
\begin{equation*}
r_z^{(j)}(x;f) = \frac{1}{j!}\int_{0}^{1} (1 - t)^{j - 1} f^{(j)}(x + t(z - x)) \,dt.
\end{equation*}
Now, we plug the expressions~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_1} and~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_2} in to the definition of $L_{P,\varepsilon}u(x)$ as follows (let $\ell = k - 1$ for simplicity):
\begin{align}
d_P(x) \cdot L_{P,\varepsilon}u(x) & = \int_{\Omega} (u(z) - u(x)) \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \,dz \nonumber \\
& = \sum_{j = 1}^{\ell} u^{(j)}(x) \int_{\Omega} (z - x)^j \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \,dz + \int_{\Omega} (z - x)^{k} r_z^{(k)}(x;u) \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \,dx \nonumber \\
& = \sum_{j_1 = 1}^{\ell} \sum_{j_2 = 0}^{\ell - 1} \frac{u^{(j_1)}(x) p^{(j_2)}(x)}{j_1!j_2!} \int_{\Omega} (z - x)^{j_1 + j_2} \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \,dz + \sum_{j = 1}^{\ell} u^{(j)}(x) \int_{\Omega} (z - x)^{j + \ell} r_z^{(\ell)}(x;p) \,dz \nonumber \\
& ~~ + \int_{\Omega} (z - x)^{k} r_z^{(k)}(x;u) \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \nonumber \\
& =: T_1(x) + T_2(x) + T_3(x). \label{pf:kernel_smoothing_bias_1d_smooth_p_3}
\end{align}

By Assumptions~\ref{asmp:domain} and~\ref{asmp:density}, we have that $|d_P(x)| \geq p_{\min} \alpha\varepsilon$ for all $x \in \Omega$. It will therefore suffice to show the following upper bounds
\begin{equation}
\label{pf:kernel_smoothing_bias_1d_smooth_p_4}
\|T_i\|_{\Omega_{\varepsilon}}^2 \leq C_i \varepsilon^{2(k + 1)} \|u\|_{H^{k}(P)}^2
\end{equation}
for $i = 1,2,3$, since along with~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_3} this will imply
\begin{equation*}
I_1 \leq \frac{3p_{\max}}{\alpha^2 p_{\min}^2 \varepsilon^2} \sum_{i = 1}^{3} C_i \varepsilon^{2(k + 1)} \|u\|_{H^{k}(P)}^2 = C \varepsilon^{2k} \|u\|_{H^k(P)}^2.
\end{equation*}
We now establish~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_4} for each of $i = 1,2,3$.

\textit{Upper bound on $\|T_1\|_{\Omega_{\varepsilon}}^2$.}
We introduce some notation to keep our formulae as readable as possible. Let $[k] = \{1,\ldots,k\}$ and let $\mathbf{J} := \bigl\{(j_1,j_2): j_1 \in [\ell], j_2 \in [\ell - 1] \cup \{0\}, j_1 + j_2 \geq k\bigr\}$. Note that if $j_1 \in [\ell]$ and $j_2 \in [\ell - 1] \cup \{0\}$ do not satisfy $(j_1,j_2) \in \mathbf{J}$, then the order-$s$ property of $\eta$ combined with the fact $B(x,\varepsilon) \in \Omega$ implies that
\begin{equation*}
\int_{\Omega} (z - x)^{j_1 + j_2} \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \,dz = 0.
\end{equation*}
On the other hand if $(j_1,j_2) \in \mathbf{J}$ then by a change of variables
\begin{equation*}
\int_{\Omega} (z - x)^{j_1 + j_2} \eta\biggl(\frac{z - x}{\varepsilon}\biggr) p(z) \,dz = \varepsilon^{j_1 + j_2} \int_{B(0,1)} v^{j_1 + j_2 + 1} \eta(|v|) \,dv =: \varepsilon^{j_1 + j_2 + 1} \sigma_\eta^{(j_1 + j_2)},
\end{equation*}
with Assumption~\ref{asmp:kernel} guaranteeing that $\sigma_\eta^{(j)} < \infty$ for all $j \in \mathbb{N}$. Therefore,
\begin{equation*}
T_1(x) = \sum_{(j_1,j_2) \in \mathbf{J}} \frac{\varepsilon^{j_1 + j_2 + 1}}{j_1!j_2!} u^{(j_1)}(x) p^{(j_2)}(x) .
\end{equation*}
Next, we note that by H\"{o}lder's inequality, $\|u^{(j_1)} p^{(j_2)} \|_{\Omega_{\varepsilon}}^2 \leq  \|p^{(j_2)}\|_{L^{\infty}(\Omega_{\varepsilon})}^2 \|u^{(j_1)}\|_{\Omega_{\varepsilon}}^2 \leq \|u\|_{H^k(\Omega)}^2$. As a result,
\begin{equation*}
\|T_1\|_{\Omega_{\varepsilon}}^2 \leq |\mathbf{J}| \cdot \sum_{(j_1,j_2) \in \mathbf{J}} \frac{\varepsilon^{2(j_1 + j_2 + 1)}}{(j_1!j_2!)^2} \|p^{(j_1)}  u^{(j_2)}\|_{\Omega_{\varepsilon}}^2 \leq \frac{1}{(k - 1)!} \varepsilon^{2(k + 1)} \|u\|_{H^k(\Omega)}^2,
\end{equation*}
and thus~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_3} follows with $C_1 = 1/(k - 1)!$. 

\textit{Upper bound on $\|T_2\|_{\Omega_{\varepsilon}}^2$.}
Note that if $k = 1$ then $T_2(x) = 0$, so that we need only cover the case where $k > 1$. In this case, observe that since $p \in C^{\ell}(P;1)$, it follows that $|r_z^{(\ell)}(x;p)| \leq 1/\ell!$ for all $z \in B(x,\varepsilon)$, and thus by H\"{o}lder's inequality,
\begin{equation*}
\biggl|\int_{\Omega} (z - x)^{j + \ell} r_z^{(\ell)}(x;p) \eta\biggl(\frac{z - x}{\varepsilon}\biggr) \,dz\biggr| \leq \varepsilon^{j + \ell}\int_{\Omega} \biggl| \eta\biggl(\frac{z - x}{\varepsilon}\biggr) \biggr| \,dz \leq \|\eta\|_{\infty} \nu_1 \varepsilon^{j + \ell + 1} 
\end{equation*}
for any $j \in \mathbb{N}$. An upper bound on $|T_2(x)|$ follows immediately,
\begin{equation*}
|T_2(x)| \leq \sum_{j = 1}^{\ell} \biggl|\frac{u^{(j)}(x)}{j!}\biggr| \biggl|\int_{\Omega} (z - x)^{j + \ell} r_z^{(\ell)}(x;p) \eta\biggl(\frac{z - x}{\varepsilon}\biggr) \,dz\biggr| \leq \|\eta\|_{\infty} \nu_1 \varepsilon^{k + 1} \sum_{j = 1}^{\ell} \frac{|u^{(j)}(x)|}{j!\ell!}.
\end{equation*}
and calculations similar to above imply that
\begin{equation*}
\|T_2\|_{\Omega_{\varepsilon}}^2 \leq \biggl(\frac{\|\eta\|_{\infty} \nu_1}{(\ell - 1)!}\biggr)^2 \varepsilon^{2(k + 1)} \|u\|_{H^k(\Omega)}^2,
\end{equation*}
and thus~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_3} follows with $C_2 = \|\eta\|_{\infty} \nu_1/(\ell - 1)!$. 

\textit{Upper bound on $\|T_3\|_{\Omega_{\varepsilon}}^2$.}
We begin by upper bounding $|T_3(x)|$ for any $x \in \Omega_{\varepsilon}$. Using the change of variables $v = (z - x)/\varepsilon$, we determine that
\begin{equation*}
|T_3(x)| \leq \int_{\Omega} \biggl| (z - x)^{k} r_z^{(k)}(x;u) \eta\biggl(\frac{z - x}{\varepsilon}\biggr)  p(z) \biggr|\,dz  \leq \varepsilon^{k + 1} \|\eta\|_{\infty} p_{\max} \int_{B(0,1)} \Bigl|r_{x + \varepsilon v}^{(k)}(x;u)\Bigr| \,dv.
\end{equation*}
From here, using the Cauchy-Schwarz inequality twice, we obtain the following upper bound on $\|T_3\|_{\Omega_{\varepsilon}}^2$,
\begin{align*}
\|T_3\|_{\Omega_{\varepsilon}}^2 & \leq (\|\eta\|_{\infty} p_{\max})^2 \varepsilon^{2(k + 1)} \int_{\Omega_{\varepsilon}}\biggl(\int_{B(0,1)} \Bigl|r_{x + \varepsilon v}^{(k)}(x;u)\Bigr| \,dv\biggr)^2 \,dx \\
& \leq (\|\eta\|_{\infty} p_{\max} \nu_1)^2 \varepsilon^{2(k + 1)} \int_{\Omega_{\varepsilon}} \int_{B(0,1)}\Bigl|r_{x + \varepsilon v}^{(k)}(x;u)\Bigr|^2 \,dv \,dx \\
& \leq \biggl(\frac{\|\eta\|_{\infty} p_{\max} \nu_1}{(k - 1)!}\biggr)^2 \varepsilon^{2(k + 1)} \int_{\Omega_{\varepsilon}} \int_{B(0,1)} \int_{0}^{1} \bigl|u^{(k)}\bigl(x + t\varepsilon v\bigr)\bigr|^2 \,dv \,dx.
\end{align*}
Finally, exchanging the order of integration gives
\begin{equation*}
\|T_3\|_{\Omega_{\varepsilon}}^2 \leq \biggl(\frac{\|\eta\|_{\infty} p_{\max}}{(k - 1)!}\biggr)^2 \nu_1 \varepsilon^{2(k + 1)} \|u\|_{H^k(\Omega)}^2,
\end{equation*}
and thus~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_3} follows with $C_3 = (\|\eta\|_{\infty} p_{\max}/(k - 1)!)^2 \nu_1$. 

\textcolor{red}{(TODO)}
\begin{itemize}
	\item \textcolor{red}{Equation~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_3} is not well formatted}.
	\item \textcolor{red}{Need to define the norm used on the left hand side of  Equation~\eqref{pf:kernel_smoothing_bias_1d_smooth_p_4}}.
\end{itemize}

\section{Proof of Lemma~\ref{lem:kernel_smoothing_bias}}
\label{sec:kernel_smoothing_bias_pf}
Letting $\Omega_{\varepsilon} = \{x: B(x,\varepsilon) \subset \Omega\}$ and $\partial_{\varepsilon}\Omega = \Omega \setminus \Omega_{\varepsilon}$, we have that 
\begin{equation*}
\|L_{P,\varepsilon}u\|_P^2 = \underbrace{\int_{\Omega_{\varepsilon}} \bigl(L_{P,\varepsilon}u(x)\bigr)^2 \,dP(x)}_{:=I_1} + \underbrace{\int_{\partial_{\varepsilon}\Omega} \bigl(L_{\varepsilon,P}u(x)\bigr)^2 \,dP(x)}_{:=I_2}
\end{equation*}
We will separately upper bound $I_1$---using the higher order property of $\eta$---and $I_2$---using the zero-trace property of $u$ and its $1$st up through $(j - 1)$st order derivatives. 

\subsection{Upper bound on $I_1$.}
\paragraph{Taylor expansions, and relevant facts.}
Suppose $x \in \mc{X}_{\varepsilon}$ and $x' \in B(x,\varepsilon)$. We take a Taylor expansion of $u(x')$ around $x' = x$,
\begin{equation*}
u(x') - u(x) = \sum_{j = 1}^{s - 1} \frac{1}{j!}\bigl(d_x^{j}u\bigr)(x' - x) + r_{x'}^{s}(x;u)
\end{equation*}
We likewise take an order $k$ Taylor expansion of $p(x')$ around $x'  = x$ (where $k = 1$ if $s = 1$, and otherwise $k = s - 1$,)
\begin{equation*}
p(x') = p(x) + \sum_{j = 1}^{k - 1}\frac{1}{j!}\bigl(d_x^{j}p\bigr)(x' - x) + r_{x'}^{k}(x;p).
\end{equation*}
Here for a given $z \in \Rd$ and $j$-times differentiable function $f: \mc{X} \to \Reals$, we use the notation $\bigl(d_x^jf\bigr)(z) := \sum_{\abs{\alpha} = j} D^{\alpha}f(x) z^{\alpha}$. The remainder term $r_{x'}$ is given by
\begin{equation*}
r_{x'}^j(x;f) = \frac{1}{(j - 1)!} \int_{0}^{1}(1 - t)^{j - 1} \bigl(d_{x + t(x' - x)}^{j}f\bigr)(x' - x) \,dt,
\end{equation*}
where we note that $x + t(x' - x) \in B(x,\varepsilon) \subseteq \mc{X}$. Finally, we have adopted the convention that $\sum_{j = 1}^{0} a_j = 0$.

We now recall some standard facts regarding Taylor expansions. First, remember that that $\bigl(d_x^{j}f\bigr)(z)$ is a degree-$j$ polynomial---and thus a $j$-homogeneous function---in $z$. On the other hand, for any degree-$j$ polynomial $q_j(z)$ and order-$s$ kernel $\eta$, if $j < s$ by converting to polar coordinates we can verify that
\begin{equation*}
\int_{B(0,1)} q_j(z) \eta(\|z\|) \,dz = 0.
\end{equation*}
We now give estimates on the remainder term in both sup-norm and $L^2(\mc{X}_{\varepsilon})$ norm, each of which hold for any $z \in B(0,1)$. In sup-norm, we have that 
\begin{equation*}
\sup_{x \in \mc{X}_{\varepsilon}}|r_{x + \varepsilon z}^j(x;f)| \leq C \varepsilon^j \|f\|_{C^j(\mc{X})},
\end{equation*}
whereas in $L^2(\mc{X}_{\varepsilon})$ norm we have,
\begin{equation}
\label{eqn:sobolev_remainder_term}
\int_{\mc{X}_{\varepsilon}} \bigl|r_{x + t \varepsilon z}^j(x;f)\bigr|^2 \,dx \leq \varepsilon^{2j} \int_{\mc{X}_{\varepsilon}} \int_{0}^{1} |d_{x + t\varepsilon z}^jf(z)|^2 \,dt \,dx \leq \varepsilon^{2j} \|d^jf\|_{\Leb^2(\mc{X})}^2.
\end{equation}

\paragraph{Upper bound on $I_1$.}
Taking a Taylor expansion of $u(x')$ around $x' = x$, we see that
\begin{equation*}
d_P(x) \cdot L_{P,\varepsilon}u(x) = \sum_{j = 1}^{s - 1} \int \bigl(d_x^ju\bigr)(x' - x) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') + \int r_{x'}^j(x;u) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x')
\end{equation*}
Next we take a Taylor expansion of $p(x')$ around $x' = x$ and represent $L_{P,\varepsilon}u(x)$ as the sum of three terms,
\begin{equation}
\label{pf:kernel_smoothing_bias_1}
\begin{aligned}
d_P(x) \cdot L_{P,\varepsilon}u(x) & = \sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{k - 1} \int \bigl(d_x^{j_1}u\bigr)(x' - x) \bigl(d_x^{j_2}p\bigr)(x' - x) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \quad + \\
& \quad \sum_{j = 1}^{s - 1} \int \bigl(d_x^ju\bigr)(x' - x)  r_{x'}^{k}(x;p) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \quad  + \\
& \quad \int r_{x'}^j(x;u) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \\
& := J_1(x) + J_2(x) + J_3(x).
\end{aligned}
\end{equation}
Note immediately that by Assumption~\ref{asmp:density}, we have that $\inf_{x \in \mc{X}_{\varepsilon}}|d_{P}(x)| \geq p_{\min} \varepsilon^d.$ We will additionally show that for each of $m = 1,2,3$, 
\begin{equation*}
\int_{\Omega_{\varepsilon}} |J_m(x)|^2 \,dP(x) \leq C \varepsilon^{2(s + d)} \|u\|_{H^s(\mc{X})}^2,
\end{equation*}
for a constant $C$ that does not depend on $u$. Plugging these bounds back into~\eqref{pf:kernel_smoothing_bias_1} will imply that
\begin{equation*}
I_1 \leq C \varepsilon^{2s} \|u\|_{H^s(\mc{X})}^2.
\end{equation*}

\underline{\textit{Upper bound on $J_1(x)$}}.
Recalling that $d_x^jf(\cdot)$ is a $j$-homogeneous function, we have that
\begin{align*}
J_1(x) & = \sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{k - 1} \int_{\mc{X}} \bigl(d_x^{j_1}u\bigr)(x' - x) \cdot \bigl(d_x^{j_2}p\bigr)(x' - x) \cdot \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \\
& = \varepsilon^d \sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{k - 1} \int_{B(0,1)} \bigl(d_{x }^{j_1}u\bigr)(\varepsilon z) \cdot \bigl(d_{x}^{j_2}p\bigr)(\varepsilon z) \cdot \eta\bigl(\|z\|\bigr) \,dz \\
& = \varepsilon^d \sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{k - 1} \varepsilon^{j_1 + j_2} \int_{B(0,1)} \bigl(d_{x}^{j_1}u\bigr)(z) \cdot \bigl(d_{x}^{j_2}p\bigr)(z) \cdot \eta\bigl(\|z\|\bigr) \,dz.
\end{align*}
Noting $d_{x}^{j_1}u \cdot d_x^{j_2}p$ is a degree $j_1 + j_2$ polynomial, we note that for all terms in the above sum such that $j_1 + j_2 < s$,
\begin{equation*}
\int_{B(0,1)} \bigl(d_{x}^{j_1}u\bigr)(z) \cdot \bigl(d_{x}^{j_2}p\bigr)(z) \cdot \eta\bigl(\|z\|\bigr) \,dx' = 0.
\end{equation*}
On the other hand if $j_1 + j_2 \geq s$, we have that
\begin{align*}
\biggl|\int_{B(0,1)} \bigl(d_{x}^{j_1}u\bigr)(z) \cdot \bigl(d_{x}^{j_2}p\bigr)(z) \cdot \eta\bigl(\|z\|\bigr) \,dz\biggr| \leq \|d^jp\|_{L^{\infty}} \int_{B(0,1)} \Bigl|\bigl(d_x^{j}u\bigr)(z)\Bigr| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz.  
\end{align*}
Consequently, applying these bounds for each $x \in \mc{X}_{\varepsilon}$ and integrating, we obtain
\begin{align*}
\int |J_1(x)|^2 \,dP(x) & \leq C \varepsilon^{2(s + d)} \|p\|_{C^k(\mc{X})} \sum_{j = 1}^{s - 1}  \int_{\mc{X}_{\varepsilon}} \biggl(\int_{B(0,1)} \Bigl|\bigl(d_x^{j}u\bigr)(z)\Bigr| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz\biggr)^2 \,dx \\
& \leq C \varepsilon^{2(s + d)} \|p\|_{C^k(\mc{X})} \sum_{j = 1}^{s - 1} \|d_ju\|_{L^2(\mc{X}_{\varepsilon})}^2 \\
& \leq  C \varepsilon^{2(s + d)} \|p\|_{C^k(\mc{X})} \|u\|_{H^s(\mc{X})}^2,
\end{align*}
where we show the middle inequality in~\eqref{pf:kernel_smoothing_bias_3}, below.


\underline{\textit{Upper bound on $J_2(x)$}}.
Recalling that $|r_{x + z\varepsilon}^{k}(x; p)| \leq C\varepsilon^{k}\|p\|_{C^{k}(\mc{X})}$ for any $z \in B(0,1)$, and that $d_x^jf(\cdot)$ is a $j$-homogeneous function, we have that
\begin{align}
|J_2(x)| & \leq \sum_{j = 1}^{s - 1} \frac{1}{j!}\int \Bigl|\bigl(d_x^{j}u\bigr)(x' - x)\Bigr| \cdot \Bigl|r_{x'}^{k}(x;p)\Bigr| \cdot \biggl|\eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr)\biggr| \,dx' \nonumber \\
& = \varepsilon^{d} \sum_{j = 1}^{s - 1} \frac{1}{j!}\int_{B(0,1)} \Bigl|\bigl(d_x^{j}u\bigr)(\varepsilon z)\Bigr| \cdot |r_{x + z\varepsilon}^{k}(x;p)| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz \nonumber \\
& \leq C\varepsilon^{k + 1 + d}\|p\|_{C^{k}(\mc{X})} \sum_{j = 1}^{s - 1} \frac{1}{j!} \int_{B(0,1)} \Bigl|\bigl(d_x^{j}u\bigr)(z)\Bigr| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz \label{pf:kernel_smoothing_bias_2}
\end{align}
Furthermore, for each $j = 1,\ldots,s - 1$ convolution of $d_x^ju$ with $\eta$ increases the $\Leb^2(\mc{X}_{\varepsilon})$ norm by at most a constant, meaning 
\begin{equation}
\label{pf:kernel_smoothing_bias_3}
\begin{aligned}
\int_{\mc{X}_{\varepsilon}} \biggl(\int_{B(0,1)} \Bigl|\bigl(d_x^{j}u\bigr)(z)\Bigr| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz\biggr)^2 \,dx & \leq \int_{\mc{X}_{\varepsilon}} \biggl(\int_{B(0,1)} \Bigl[\bigl(d_x^ju\bigr)(z)\Bigr]^2 \,dz \biggr) \cdot \biggl(\int_{B(0,1)} K(\|z\|) \,dz \biggr) \,dx \\
& \leq \|\eta\|_{\infty}^2  \int_{B(0,1)} \int_{\mc{X}_{\varepsilon}} \Bigl[\bigl(d_x^ju\bigr)(z)\Bigr]^2 \,dx \,dz \\
& \leq \|\eta\|_{\infty}^2 \nu_d \cdot \|d^ju\|_{\Leb^2(\mc{X_{\varepsilon}})}^2.
\end{aligned}
\end{equation}
Combining this with~\eqref{pf:kernel_smoothing_bias_2}, we conclude that
\begin{align*}
\int_{\mc{X}_{\varepsilon}} |J_2(x)|^2 \,dP(x) & \leq C p_{\max} \|p\|_{C^{k}(\mc{X})}^2 \varepsilon^{2(k + 1 + d)} \sum_{j = 1}^{s - 1} \biggl(\frac{1}{j!} \int_{B(0,1)} \Bigl|\bigl(d_x^{j}u\bigr)(z)\Bigr| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz\biggr) \\
& \leq  C p_{\max} \|p\|_{C^{k}(\mc{X})}^2 \|\eta\|_{\infty}^2 \nu_d \varepsilon^{2(k + 1 + d)} \sum_{j = 1}^{s - 1} \cdot \|d^ju\|_{\Leb^2(\mc{X_{\varepsilon}})}^2 \\
& \leq C \varepsilon^{2(k + 1 + d)} \|u\|_{H^s(\mc{X})}^2,
\end{align*}
whence the claim follows since by assumption either $k + 1 > s$ (when $s = 1$), or $k + 1 = s$ (when $s \geq 2$).

\underline{\textit{Upper bound on $J_3(x)$}}.
We begin with a pointwise upper bound on $|J_3(x)|$, 
\begin{align*}
|J_3(x)| & \leq p_{\max} \int_{\mc{X}} \bigl|r_{x'}^s(x;u)\bigr| \biggl|\eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr)\biggr| \,dx' \\
& = p_{\max} \varepsilon^d \int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr| \bigl|\eta(\|z\|)\bigr| \,dz,
\end{align*}
and applying the Cauchy-Schwarz inequality, we deduce a pointwise upper bound on $|J_3(x)|^2$,
\begin{align*}
|J_3(x)|^2 & \leq p_{\max}^2 \varepsilon^{2d} \biggl(\int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr|^2 \,dz\biggr) \cdot \biggl(\int_{B(0,1)} \bigl|\eta(\|z\|)\bigr| \,dz\biggr) \\
& \leq p_{\max}^2 \|\eta\|_{\infty}^2 \nu_d \varepsilon^{2d}  \int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr|^2 \,dz.
\end{align*}
Applying this pointwise over all $x \in \mc{X}_{\varepsilon}$ and integrating, we obtain
\begin{align*}
\int_{\mc{X}_{\varepsilon}} |J_3(x)|^2 \,dP(x) & \leq p_{\max}^3 \|\eta\|_{\infty}^2 \nu_d \varepsilon^{2d} \int_{\mc{X}_{\varepsilon}} \int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr|^2 \,dz \\
& = p_{\max}^3 \|\eta\|_{\infty}^2 \nu_d \varepsilon^{2d} \int_{B(0,1)} \int_{\mc{X}_{\varepsilon}} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr|^2 \,dz \\
& \leq p_{\max}^3 \|\eta\|_{\infty}^2 \nu_d^2 \varepsilon^{2(s + d)} \|d^sf\|_{L^2(\mc{X}_{\varepsilon})},
\end{align*}
with the last inequality following from~\eqref{eqn:sobolev_remainder_term}.

\paragraph{Upper bound on $I_2$.}
We begin by demonstrating that $I_2$ is upper bounded by $C\|u\|_{\Leb^2(\partial_{2\varepsilon}\mc{X})}^2$. Recall that by assumption $d_P(x) \geq \alpha p_{\min} \varepsilon^d$ for each $x \in \mc{X}$. On the other hand, at every $x \in \mc{X}$ we have that
\begin{align*}
\bigl|d_P(x) \cdot L_{P,\varepsilon}u(x)\bigr|^2 & \leq p_{\max}^2 \biggl[\int \Bigl(|u(x)|^2 + |u(x')|^2\Bigr) K^{1/2}\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx'\biggr] \cdot \biggl[\int K^{1/2}\biggl(\frac{\|x' - x|}{\varepsilon}\biggr) \,dx\biggr] \\
& \leq 2 p_{\max}^2 \|\eta\|_{\infty} \varepsilon^d \int \Bigl(|u(x)|^2 + |u(x')|^2\Bigr) K^{1/2}\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx'.
\end{align*}
It follows that
\begin{align*}
I_2 & = \int_{\partial_{\varepsilon}\mc{X}} \bigl|L_{P,\varepsilon}u(x)\bigr|^2 \,dP(x) \\
& \leq \frac{2p_{\max}^2 |\eta\|_{\infty}}{p_{\min}^2 \alpha \varepsilon^d} \int_{\partial_{\varepsilon}\mc{X}} \int_{\mc{X}} \Bigl(|u(x)|^2 + |u(x')|^2\Bigr) K^{1/2}\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \\ 
& \leq \frac{4p_{\max}^2 \|\eta\|_{\infty}^2}{p_{\min}^2 \alpha} \int_{\partial_{2\varepsilon}\mc{X}} |u(x)|^2 \,dx.
\end{align*}
We will now use the zero-trace property of $u$ to show that there exists a number $c$, which depends only on $\mc{X}$ such that for all $\varepsilon \leq c$,
\begin{equation}
\label{pf:kernel_smoothing_bias_5}
\int_{\partial_{\varepsilon}\mc{X}} |u(x)|^2 \,dx \leq C\varepsilon^{2s}.
\end{equation}
We will build to this result by a series of intermediate steps. Many of these steps are related to the proof of Theorem 18.1 in \textcolor{red}{(Leoni)}. 

\underline{\textit{Step 1: Local Patch.}}
To begin, we assume that for some $c_0 > 0$ and a Lipschitz mapping $\phi: \Reals^{d - 1} \to [-c_0,c_0]$, we have that $u \in C_c^{\infty}(U_{\phi}(c_0))$, where 
\begin{equation*}
U_{\phi}(c_0) = \Bigl\{y \in Q(0,c_0): \phi(y_{-d}) \leq y_d\Bigr\}, 
\end{equation*}
and here $Q(0,c_0)$ is the $d$-dimensional cube of side length $c_0$, centered at $0$. We will show that for all $0 < \varepsilon < c_0$, and for the tubular neighborhood $V_{\phi}(\varepsilon) = \{y \in Q(0,c_0): \phi(y_{-d}) \leq y_d \leq \phi(y_{-d}) + \varepsilon\}$, we have that
\begin{equation*}
\int_{V_{\phi}(\varepsilon)} |u(x)|^2 \,dx \leq C\varepsilon^{2s} \|u\|_{H^s(U_{\phi}(c_0))}^2.
\end{equation*}
For a given $y = (y',y_d) \in V_{\phi}(\varepsilon)$, let $y_0 = (y',\phi(y'))$. Taking the Taylor expansion of $u(y)$ around $y = y_0$ because $u$ is compactly supported in $V_{\phi}$ it follows that,
\begin{align*}
u(y) & = u(y_0) + \sum_{j = 1}^{s - 1} \frac{1}{j!} D^{je_d}u(y_0) \bigl(y_d - \phi(y')\bigr)^j + \frac{1}{(s - 1)!}\int_{\phi(y')}^{y_d} (1 - t)^{s - 1} D^{se_d}u(y',z) \bigl(y_d - z\bigr)^{s - 1} \,dz \Longrightarrow\\
|u(y)| & \leq C\varepsilon^{s - 1}\int_{\phi(y')}^{y_d} \bigl|D^{se_d}u(y',z)\bigr| \,dz. 
\end{align*}
Consequently, by squaring both sides and applying Cauchy-Schwarz, we have that
\begin{equation*}
|u(y)|^2 \leq C\varepsilon^{2(s - 1)} \biggl(\int_{\phi(y')}^{y_d} \bigl|D^{se_d}u(y',z)\bigr| \,dz\biggr)^2 \leq C\varepsilon^{2s - 1} \int_{\phi(y')}^{y_d} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dz.
\end{equation*}
Applying this bound for each $y \in V_{\phi}(\varepsilon)$, and then integrating, we obtain
\begin{align}
\int_{V_{\phi}(\varepsilon)} |u(y)|^2 \,dy & \leq \int_{Q_{d - 1}(c_0)} \int_{\phi(y')}^{\phi(y') + \varepsilon} |u(y',y_d)|^2 \,dy_d \,dy' \nonumber \\
& \leq C\varepsilon^{2s - 1}\int_{Q_{d - 1}(c_0)}  \int_{\phi(y')}^{\phi(y') + \varepsilon} \int_{\phi(y')}^{y_d} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dz \,dy_d \,dy' \label{pf:kernel_smoothing_bias_4}
\end{align}
where we have written $Q_{d - 1}(0,c_0)$ for the $d - 1$ dimensional cube of side length $c_0$, centered at $0$. Exchanging the order of the inner two integrals then gives
\begin{align*}
\int_{\phi(y')}^{\phi(y') + \varepsilon} \int_{\phi(y')}^{y_d} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dz \,dy_d & = \int_{\phi(y')}^{\phi(y') + \varepsilon} \int_{z}^{\varepsilon} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dy_d \,dz \\
& \leq C \varepsilon \int_{\phi(y')}^{\phi(y') + \varepsilon} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dz \\
& \leq C \varepsilon \int_{\phi(y')}^{c_0} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dz.
\end{align*}
Finally, plugging back into~\eqref{pf:kernel_smoothing_bias_4}, we conclude that
\begin{equation*}
\int_{V_{\phi}(\varepsilon)} |u(y)|^2 \,dy \leq C \varepsilon^{2s} \int_{Q_{d - 1}(0,c_0)} \int_{\phi(y')}^{c_0} \bigl|D^{se_d}u(y',z)\bigr|^2 \,dz \,dy' \leq C \varepsilon^{2s} |u|_{H^s(U_{\phi}(c_0))}^2.
\end{equation*}

\underline{\textit{Step 2: Rigid motion of local patch.}} Now, suppose that at a point $x_0 \in \partial \mc{X}$, there exists a rigid motion $T: \Rd \to \Rd$ for which $T(x_0) = 0$, and a number $C_0$ such that for all $\varepsilon \cdot C_0 \leq c_0$, 
\begin{equation*}
T\bigl(Q_{T}(x_0,c_0) \cap \partial_{\varepsilon}\mc{X}\bigr) \subseteq V_{\phi}\bigl(C_0\varepsilon\bigr) \quad\textrm{and}\quad T\bigl(Q_T(x_0,c_0) \cap \mc{X}\bigr) = U_{\phi}(c_0).
\end{equation*}
Here $Q_{T}(x_0,c_0))$ is a (not necessarily coordinate-axis-aligned) cube of side length $c_0)$, centered at $x_0$. Define $v(y) := u(T^{-1}(y))$ for $y \in U_{\phi}(c_0)$. If $u \in C_c^{\infty}(\mc{X})$, then $v \in C_c^{\infty}(U_{\phi}(c_0))$, and moreover $\|v\|_{H^s(U_{\phi}(c_0))}^2 = \|u\|_{H^s(Q_{T}(x_0,c_0) \cap \mc{X})}^2$. Therefore, using the upper bound that we derived in Step 1,
\begin{equation*}
\int_{V_{\phi}(C_0 \cdot \varepsilon)} |v(y)|^2 \,dy \leq C \varepsilon^{2s} \|v\|_{H^s(U_{\phi}(c_0))}^2,
\end{equation*}
we conclude that
\begin{align*}
\int_{Q_{T}(x_0,c_0) \cap \partial_{\varepsilon}\mc{X}} |u(x)|^2 \,dx & = \int_{T(Q_T(x_0,c_0)) \cap \partial_{\varepsilon}\mc{X})} |v(y)|^2 \,dy \\
& \leq \int_{V_{\phi}(C_0 \cdot \varepsilon)} |v(y)|^2 \,dy \\
& \leq C \varepsilon^{2s} \|v\|_{H^s(U_{\phi}(c_0))}^2 = C \varepsilon^{2s} \|u\|_{H^s(Q_{T}(x_0,c_0)) \cap \mc{X})}^2 \leq C \varepsilon^{2s} \|u\|_{H^s(\mc{X})}^2.
\end{align*}

\underline{\textit{Step 3: Lipschitz domain}}.
Finally, we deal with the case where $\mc{X}$ is assumed to be an open, bounded subset of $\Rd$, with Lipschitz boundary. In this case, at every $x_0 \in \partial \mc{X}$, there exists a rigid motion $T_{x_0}: \Rd \to \Rd$ such that $T_{x_0}(x_0) = 0$, a number $c_0(x_0)$, a Lipschitz function $\phi_{x_0}:\Reals^{d - 1} \to [-c_0,c_0]$, and a number $C_0(x_0)$, such that for all $\varepsilon \cdot C_0(x_0) \leq c_0(x_0)$,
\begin{equation*}
T\bigl(Q_{T}(x_0,c_0(x_0)) \cap \partial_{\varepsilon}\mc{X}\bigr) \subseteq V_{\phi}\bigl(C_0(x_0) \cdot \varepsilon\bigr) \quad\textrm{and}\quad T\bigl(Q_T(x_0,c_0(x_0)) \cap \mc{X}\bigr) = U_{\phi}(c_0(x_0)).
\end{equation*}
Therefore for every $x_0 \in \partial \mc{X}$, it follows from the previous step that
\begin{equation*}
\int_{Q_{T_{x_0}}(x_0,c_0(x_0)) \cap \partial_{\varepsilon}\mc{X}} |u(x)|^2 \,dx \leq C(x_0) \varepsilon^{2s} \|u\|_{H^s(\mc{X})}^2,
\end{equation*}
where on the right hand side $C(x_0)$ is a constant that may depend on $x_0$, but not on $u$ or $\varepsilon$.

We conclude by taking a collection of cubes that covers $\partial_{\varepsilon}\mc{X}$ for all $\epsilon$ sufficiently small. First, we note that by a compactness argument there exists a finite subset of the collection of cubes $\{Q_{T_{x_0}}(x_0,c_0(x_0)/2): x_0 \in \partial\mc{X} \}$ which covers $\partial \mc{X}$, say $Q_{T_{x_1}}(x_1,c_0(x_1)/2),\ldots, Q_{T_{x_N}}(x_N,c_0(x_N)/2)$. Then, for any $\varepsilon \leq \min_{i = 1,\ldots,N} c_0(x_i)/2$, it follows from the triangle inequality that
\begin{equation*}
\partial_{\varepsilon}\mc{X} \subseteq \bigcup_{i = 1}^{N} Q_{T_{x_i}}(x_i, c_0(x_i)).
\end{equation*}
As a result,
\begin{equation*}
\int_{\partial_{\varepsilon}\mc{X}} |u(x)|^2 \leq \sum_{i = 1}^{N} \int_{Q_{T_{x_i}}(x_i, c_0(x_i)) \cap \partial_{\varepsilon}(\mc{X})} |u(x)|^2 \leq  \varepsilon^{2s} \|u\|_{H^s(\mc{X})}^2 \sum_{i = 1}^{N}C_0(x_i),
\end{equation*}
which proves the claim of~\eqref{pf:kernel_smoothing_bias_5}.

\end{document}
