\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbb{I}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Notes for Week 6/13/19 - 6/18/19}
\author{Alden Green}
\date{\today}
\maketitle

Let $f,g$ be $L^2$ density functions supported on the $d$-dimensional unit cube $[0,1]^d$, meaning
\begin{equation*}
\int_{[0,1]^d} f(x)^2 dx, \int_{[0,1]^d} g(x)^2 dx < \infty, \quad \text{and} \quad \int_{[0,1]^d} f(x) dx, \int_{[0,1]^d} g(x) dx = 1.
\end{equation*}
with $f$,$g$ bounded above and below, $0 < p_{\min} < f(x), g(x) < p_{\max} < \infty$ for all $x \in [0,1]^d$.

We observe data $(X,\ell)$, a design matrix and associated labels, specified as follows. We let $x_1, \ldots, x_n$ be the rows of $X$, each sampled independently from $\mu = (f + g)/2$. For each $i = 1,\ldots,n$, we then sample $\ell_i$ according to
\begin{equation*}
\ell_i =
\begin{cases}
1,~ \textrm{with probability} \frac{f(x_i)}{f(x_i) + g(x_i)} \\
-1,~ \textrm{with probability} \frac{g(x_i)}{f(x_i) + g(x_i)}
\end{cases}
\end{equation*}
and let $\ell = (\ell_i)$. 

Our statistical goal is hypothesis testing: that is, we wish to construct a test function $\phi$ which differentiates between
\begin{equation*}
\mathbb{H}_0: f = g \text{ and } \mathbb{H}_1: f \neq g.
\end{equation*}

For a given function class $\Hclass$, some $\epsilon > 0$, and test function $\phi$ a Borel measurable function of the data with range $\set{0,1}$, we evaluate the quality of the test using \emph{worst-case risk}
\begin{equation*}
R_{\epsilon}^{(t)}(\phi; \Hclass) = \sup_{f \in \Hclass} \Ebb_{f,f}^{(t)}(\phi) + \sup_{ \substack{f,g \in \Hclass \\ \delta(f,g) \geq \epsilon } } \Ebb_{f,g}^{(t)}(1 - \phi)
\end{equation*} 
where 
\begin{equation*}
\delta^2(f,g) = \int_{\D} (f - g)^2 dx.
\end{equation*}
We will consider the function class $\mathcal{H} := \mathcal{H}_{1}^d(L)$, the $d$-dimensional Lipschitz ball with norm $L$. Formally, a function $f \in \mathcal{H}$ if for all $x,y \in [0,1]^d$, $\abs{f(x) - f(y)} \leq L \norm{x - y}$.\footnote{Note that the Lipschitz requirement, along with the restriction that $f$ be a density function over a bounded domain, imply that $f$ has finite $L^2$ norm.}

\section{Test Statistic}

Let $r > 0$ be a bandwidth parameter, let $\eta_r: \mathcal{X} \times \mathcal{X} \to [0,\infty)$ be a Mercer kernel given by
\begin{equation*}
\eta_r(x,y) =
\begin{cases}
1, \quad & \textrm{$\norm{x - y} \leq r$} \\
0, \quad & \textrm{$\norm{x - y} > r$},
\end{cases}
\end{equation*}
and let $\widetilde{A}$ be an $n \times n$ matrix with entries $\widetilde{A}_{ij} = n^{-1}\eta(x_i,x_j)$. We obtain $A$ from $\widetilde{A}$ be setting all diagonal entries of $\widetilde{A}$ to be zero. Our test statistic is then
\begin{equation*}
T_{\eta} := \ell^T A \ell.
\end{equation*}

\section{Theory}

We show that a simple test based on the statistic $T_{\eta}$, with proper choice of the bandwidth parameter $r$, achieves minimax optimality over the class of Lipschitz functions $\mathcal{H}_{1}^{d}(L)$.

\begin{theorem}
	\label{thm: mmd_minimax_optimal}
	Fix $r = n^{-2/(4+d)}$, and consider the test 
	\begin{equation*}
	\phi_{\eta,\tau} = \1\left\{ T_{\eta} \geq \tau \right\}, \quad \tau = a n^{-d/(4+d)}
	\end{equation*}
	with $a \geq 1$. There are constants $c_1$ depending only on $p_{\max}$, and $c_2$ depending only on $(L,p_{\max},p_{\min})$, such that
	\begin{equation*}
	R_{\epsilon}^{n}(\phi_{\eta,\tau}; \mathcal{H}_{1}^{d}(L)) \leq c_1/a^2, \quad \text{if}~ \epsilon \geq c_2 a n^{-2/(4 + d)}
	\end{equation*}
\end{theorem}
\begin{proof}
	Our first objective is to show that, for an appropriate choice of $c_1$, $\Ebb_{f,f}^{n}(\phi_{\eta,\tau}) \leq \frac{c_1}{2a^2}$ for all $f \in \mathcal{H}_{1}^{d}(L)$. This is equivalent to proving that
	\begin{equation}
	\label{eqn: mmd_minimax_optimal_2}
	\Pbb \left(T_{\eta} \geq \tau \right) \leq c_1/2a^2
	\end{equation}
	which we now proceed to show.
	
	From Lemma \ref{lem: mmd_expectation}, we have that $\Ebb_{f,f}(T_{\eta}) = 0$. Therefore, by Chebyshev's inequality, 
	\begin{equation}
	\label{eqn: mmd_minimax_optimal_1}
	\Pbb \left(T_{\eta} \geq \tau \right) \leq \Var(T_{\eta})/\tau^2
	\end{equation}
	In Lemma \ref{lem: var_mmd}, we have upper bounded $\Var(T_{\eta})$ as function of $r$ and $\delta^2(f,g)$. Under the null hypothesis $f = g$, $\delta^2(f,g) = 0$, and along with our choice of $r = n^{-2/(d + 4)}$, the upper bound on $\Var(T_{\eta})$ reduces to
	\begin{equation*}
	\Var(T_{\eta}) \leq \frac{p_{\max}^2}{2} n^{-2d/(d + 4)}
	\end{equation*}
	By our choice of $\tau = a n^{-d/(4 + d)}$, we therefore have
	\begin{equation*}
	\Var(T_{\eta})/\tau^2 \leq \frac{p_{\max}^2}{2a^2}
	\end{equation*}
	and therefore \eqref{eqn: mmd_minimax_optimal_2} holds with $c_1 := p_{\max}^2$.
	
	We turn now to bound the type II error, $\Ebb_{f,g}^{(n)}(\phi_{\eta,\tau})$, for all $f,g \in \mathcal{H}_{1}^{d}(L)$ such that $\delta^2(f,g) \geq \epsilon = c_2 n^{-2/(4+d)}$. Our goal now will be to show
	\begin{equation*}
	\Pbb \left(T_{\eta} \leq \tau \right) \leq c_1/2a^2.
	\end{equation*}
	By Chebyshev's inequality we obtain
	\begin{equation}
	\label{eqn: mmd_minimax_optimal_4}
	\Pbb(T_{\eta} \leq \tau) \leq \frac{\Var(T_{\eta})}{(\Ebb_{f,g}(T_{\eta}) - \tau)^2}
	\end{equation}
	We obtain the following lower bound on $\Var(T_{\eta})$ in Lemma \ref{lem: var_mmd} in terms of $r$ and $\delta(f,g)$:
	\begin{equation*}
	\Var(T_{\eta}) \leq 2n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right) + \frac{p_{\max}^2}{2}r^d
	\end{equation*}
	We will split our analysis into cases, based on which term on the right hand side of the preceding equation dominates. \textcolor{red}{TODO: Complete this proof.}
	
	\paragraph{Case 1, 1st term dominates:}
	We assume
	\begin{equation*}
	2n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right) > \frac{p_{\max}^2}{2}r^d,
	\end{equation*}
	so that the variance is upper bounded
	\begin{equation*}
	\Var(T_{\eta}) \leq 4 n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right).
	\end{equation*}
	Additionally, in the following case, we will show that for a sufficiently large choice of $c_2$, depending on $p_{\max}, p_{\min}, L$ and $d$, we obtain
	\begin{equation*}
	\Ebb_{f,g} T_{\eta} \geq 2 \tau
	\end{equation*}
	so that 
	\begin{equation*}
	(\Ebb_{f,g} T_{\eta} - \tau)^2 \geq \left(\Ebb_{f,g} T_{\eta}\right)^2/4.
	\end{equation*}
	In Lemma \ref{lem: mmd_expectation}, moreover, we prove the following lower bound on $\Ebb_{f,g} T_{\eta}$ as a function of $r$ and $\delta(f,g)$:
	\begin{equation*}
	\Ebb_{f,g} T_{\eta} \geq \frac{(n - 1)p_{\min}^2}{ p_{\max}^2} \left(\delta^2(f,g)\frac{r^d}{2^d} - L\delta(f,g)r^{d + 1}\right).
	\end{equation*}
	We can therefore upper bound $\Var(T_{\eta})/(\Ebb_{f,g}T_{\eta} - \tau)^2$,
	\begin{align*}
	\Var(T_{\eta})/(\Ebb_{f,g}T_{\eta} - \tau)^2 \leq \frac{p_{\max}^6}{p_{\min}^6}  \frac{16 n r^{2d} \delta(f,g) \left(\delta(f,g) + Lr\right)}{(n-1)^2 r^{2d} \delta^2(f,g) \left(\delta(f,g){2^{-d}} - Lr\right)^2}
	\end{align*}
	
	\paragraph{Case 2, 2nd term dominates:}
	We first lower bound the denominator, and then upper bound the numerator. In Lemma \ref{lem: mmd_expectation}, we prove the following lower bound on $\Ebb T_{\eta}$ when $f,g \in \mathcal{H}_1^{d}(L)$, stated as a function of $r$ and $\delta^2(f,g)$:
	\begin{equation}
	\label{eqn: mmd_minimax_optimal_3}
	\Ebb_{f,g} T_{\eta} \geq \frac{(n - 1)p_{\min}^2}{ p_{\max}^2} \left(\delta^2(f,g)\frac{r^d}{2^d} - L\delta(f,g)r^{d + 1}\right).
	\end{equation}
	Given the choice of bandwidth parameter $r = n^{-2/(4+d)}$ and the lower bound $\delta(f,g) \geq c_2 a n^{-2/(d + 4)}$, we may further simplify the expression on the right hand side of \eqref{eqn: mmd_minimax_optimal_3},
	\begin{align*}
	\frac{(n - 1)p_{\min}^2}{ p_{\max}^2} \left(\delta^2(f,g)\frac{r^d}{2^d} - L\delta(f,g)r^{d + 1}\right) & = \frac{(n - 1)p_{\min}^2}{ p_{\max}^2} n^{-2d/(d+4)} \delta(f,g) \left(\frac{\delta(f,g)}{2^d} - Ln^{-2/(d + 4)} \right) \\
	& \geq \frac{(n - 1)p_{\min}^2}{ p_{\max}^2} c_2 a n^{-(2d + 2)/(d+4)} \left(\frac{c_2 a n^{-2/(d + 4)}}{2^d} - Ln^{-2/(d + 4)} \right) \\
	& \geq 2 a^2 n^{1 - (2d + 2)/(d + 4) - 2/(d + 4)} = 2 a^2 n^{-d/(d + 4)}.
	\end{align*}
	where the last inequality follows when $c_2$ is chosen to be sufficiently large relative to $p_{\min}, p_{\max}, L$ and $d$. Given our choice $\tau = a n^{-d/(d + 4)}$, the denominator of \eqref{eqn: mmd_minimax_optimal_4} can therefore be lower bounded
	\begin{equation*}
	(\Ebb_{f,g}(T_{\eta}) - \tau)^2 \geq (2a^2 - a)^2 n^{-2d/d(+4)} \geq a^4 n^{-2d/d(+4)} 
	\end{equation*}
	where the last inequality follows from the fact $2a^2 - a \geq a^2$ for all $a \geq 1$. 
	
	We turn now to upper bounding the numerator of \eqref{eqn: mmd_minimax_optimal_4}. In Lemma \ref{lem: var_mmd} we give the following upper bound on $\Var_{f,g} T_\eta$ as a function of $r$ and $\delta(f,g)$, 
	\begin{equation*}
	\Var_{f,g} T_\eta \leq 2n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right) + \frac{p_{\max}^2}{2}r^d 
	\end{equation*}
	Similar to before, we apply our choice of $r$ and lower bound on $\delta(f,g)$ and obtain
	\begin{equation*}
	2n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right) + \frac{p_{\max}^2}{2}r^d \leq 
	\end{equation*}
\end{proof}

\section{Supporting Theory}

We begin by recalling Chebyshev's inequality, which we will use to bound deviations of $T_{\eta}$ from its expectation.
\begin{lemma}[Chebyshev]
	\label{lem: chebyshev}
	Let $X$ be a random variable with $\Ebb X < \infty$ and $0 < \Var(X) < \infty$. Then,
	\begin{equation*}
	\Pbb(\abs{X - \Ebb X} \geq a) \leq \frac{\Var(X)}{a^2}.
	\end{equation*}
\end{lemma}

In order to employ Lemma \ref{lem: chebyshev}, we must bound the expectation and variance of $T_{\eta}$ under the null and alternative hypotheses. We begin with the expectation $\Ebb T_{\eta}$. 
\begin{lemma}
	\label{lem: mmd_expectation}
	Under the null hypothesis $h = g$, for any $h, g \in \mathcal{H}_{1}^{d}(L)$,
	\begin{equation*}
	\Ebb T_{\eta} = 0.
	\end{equation*}
	
	Under the alternative hypothesis $h \neq g$, for any $h, g \in \mathcal{H}_{1}^{d}(L)$, 
	\begin{equation*}
	\Ebb T_{\eta} \geq \frac{(n - 1)p_{\min}^2}{ p_{\max}^2} \left(\delta^2(f,g)\frac{r^d}{2^d} - L\delta(f,g)r^{d + 1}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	Let $\thetast$ be a vector in $\Reals^{n}$, with entries $ \thetast_i = \Ebb(\ell_i|x_i)$. Letting $w = \ell - \thetast$, we note that conditional on $X$, the noise component $w$ consists of $n$ independent and zero mean terms.
	
	We expand the test statistic
	\begin{equation*}
	T_{\eta} = (\thetast)^T A \thetast + w^T A w + 2 w^T A \thetast
	\end{equation*}
	and note that by the law of iterated expectation, 
	\begin{equation*}
	\Ebb(T_{\eta}) = \Ebb(T_{\eta}|X) = \Ebb\left( (\thetast)^T A \thetast \right)
	\end{equation*}
	
	Now, let $X$ and $Y$ be independent random variables with distribution $\mu$, and let $\Delta(x) := f(x) - g(x)$. By the linearity of expectation,
	\begin{align*}
	\Ebb (\thetast)^T A \thetast & = \sum_{i \neq j = 1}^{n} \Ebb( \thetast_i \thetast_j A_{ij}) \\
	& = (n - 1) \Ebb \bigl(\thetast_1 \thetast_2 \eta_r(x_1, x_2) \bigr) \\
	& \geq \frac{4(n - 1)}{p_{\max}^2} \Ebb(\Delta(x) \Delta(y) \eta_r(X,Y))
	\end{align*} 
	
	We can lower bound $\Ebb(\Delta(x) \Delta(y) \eta_r(X,Y))$ by $\delta^2(f,g)$, minus a remainder term, as follows
	\begin{align*}
	\Ebb(\Delta(x) \Delta(y) \eta_r(X,Y)) & = \iint \Delta(x) \Delta(y) \eta_r(x,y) d\mu(x) d\mu(x) \\
	& \geq \frac{p_{\min}^2}{4} \iint \Delta(x) \Delta(y) \eta_r(x,y) dx dy \\
	& \geq \frac{p_{\min}^2}{4} \left( \iint \Delta^2(x) \eta_r(x,y) dx dy - \iint \abs{\Delta(x)} \abs{\Delta(x) - \Delta(y)} \eta(x,y) dx dy \right)
	\end{align*}
	To lower bound the first term, observe that for any $x \in [0,1]^d$,
	\begin{equation*}
	\int_{[0,1]^d} \eta_r(x,y) dx \geq \frac{r^d}{2^d}
	\end{equation*}
	and therefore
	\begin{equation*}
	\iint \Delta^2(x) \eta_r(x,y) dx dy \geq \frac{r^d}{2^d} \int_{[0,1]^d} \Delta^2(x) dx \geq \frac{r^d}{2^d} \delta^2(f,g).
	\end{equation*}
	
	To upper bound the remainder term, observe that $\Delta \in \mathcal{H}_{1}^{d}(2L)$. Using this fact, we obtain the upper bound
	\begin{align*}
	\iint \abs{\Delta(x)} \abs{\Delta(x) - \Delta(y)} \eta(x,y) dx dy & \leq : \iint \abs{\Delta(x)} \norm{x - y} \eta(x,y) dx dy \\
	& \leq L r^{d + 1} \int_{[0,1]^d} \abs{\Delta(x)} dx \\
	& \leq L r^{d + 1} \delta(f,g).
	\end{align*}
	where the last inequality follows from Jensen's inequality.
\end{proof}

\begin{lemma}
	\label{lem: var_mmd}
	For any $h,g \in \mathcal{H}_1^{d}(L)$,
	\begin{equation*}
	\Var(T_{\eta}) \leq 2n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right) + \frac{p_{\max}^2}{2}r^d
	\end{equation*}
\end{lemma}
\begin{proof}
	We begin by computing the second moment $\Ebb T_{\eta}^2$. Expanding the quadratic form, we obtain
	\begin{align}
	\Ebb T_{\eta}^2 =  \sum_{i,i' = 1}^{n} \sum_{j,j' = 1}^{n} \Ebb(\ell_i \ell_{i'} \ell_{j} \ell_{j'} A_{ii'} A_{jj'})
	\end{align}
	
	We next compute this expectation, based on the number of distinct indices $i,i',j,j'$. Let $X, X', Y, Y'$ be four independent random variables, each with distribution $\mu$, and write $\widetilde{\Delta} := 2 \Delta / \mu$.  Then, applying the law of conditional expectation, we obtain
	\begin{equation*}
	n^2 \Ebb(\ell_i \ell_{i'} \ell_{j} \ell_{j'} A_{ii'} A_{jj'}) = 
	\begin{cases}
	\Ebb(\widetilde{\Delta}(X) \wt{\Delta}(X') \eta_r(X,X'))^2, \quad & \textrm{if}~ \set{i,i',j,j'} ~\textrm{contains four distinct elements}, \\
	\Ebb(\widetilde{\Delta}(X') \wt{\Delta}(Y') \eta_r(X,X') \eta_r(X,Y')), \quad & \textrm{if}~ \set{i,i',j,j'} ~\textrm{contains three distinct elements and}~ i = j, \\
	\Ebb\biggl(\eta_r(X,Y)^2\biggr), \quad  & \textrm{if}~ \set{i,i',j,j'} ~\textrm{contains two distinct elements and}~ i = j. \\
	0, \quad & \textrm{if}~ i = i' ~\textrm{or}~ j = j'.
	\end{cases}
	\end{equation*}
	and as a result we obtain
	\begin{align}
	\label{eqn: var_mmd_1}
	n^2 \Ebb T_{\eta}^2 = & n(n-1)(n-2)(n-3)\Ebb\biggl(\widetilde{\Delta}(X) \wt{\Delta}(X') \eta_r(X,X')\biggr) + \nonumber \\
	& \quad 2n(n-1)(n-2) \Ebb \biggl(\widetilde{\Delta}(X') \wt{\Delta}(Y') \eta_r(X,X') \eta_r(X,Y') \biggr) + \nonumber \\
	& \quad \quad 2n(n-1) \Ebb\biggl(\eta_r(X,Y)^2\biggr).
	\end{align}
	
	The first term -- covering the case when $\set{i,i',j,j'}$ contains four distinct elements --  will be canceled out by the mean, as we will see, and we do not simplify it further. 
	
	The second term we upper bound by methods similar to those used to obtain the upper bound in Lemma \ref{lem: mmd_expectation}. We will need the following upper bound,
	\begin{align*}
	\int \eta(x,x') \eta(x,y') \dx & \leq r^d \1(\norm{x' - y'} \leq 2r) \\
	& = r^d \eta_{2r}(x',y').
	\end{align*}
	We proceed,
	\begin{align}
	\label{eqn: var_mmd_2}
	\Ebb \biggl(\widetilde{\Delta}(X') \wt{\Delta}(Y') \eta_r(X,X') \eta_r(X,Y') \biggr) & \leq \frac{p_{\max}^2}{p_{\min}^2} \iiint \Delta(x') \Delta(y') \eta(x,x') \eta(x,y') \dx \dypr \dxpr \nonumber\\
	& \leq \frac{p_{\max}^2}{p_{\min}^2} r^d \iint \Delta(x') \Delta(y') \eta_{2r}(x',y') \dypr \dxpr \nonumber \\
	& \leq \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right).
	\end{align}
	
	The following upper bound on the third term is a straightforward consequence of the fact $\eta_r^2 = \eta_r$, and therefore
	\begin{equation}
	\label{eqn: var_mmd_3}
	\Ebb(\eta_r(X,Y)^2) = \Ebb(\eta_r(X,Y)) \leq \frac{p_{\max}^2}{4} r^d. 
	\end{equation}
	
	Finally, we have that
	\begin{equation}
	\label{eqn: var_mmd_4}
	n^2 \Ebb(T_{\eta}^2) = \left(n(n-1) \Ebb(\widetilde{\Delta}(X) \wt{\Delta}(X') \eta_r) \right)^2 .
	\end{equation}
	
	Combining \eqref{eqn: var_mmd_1}-\eqref{eqn: var_mmd_4}, we obtain
	\begin{equation*}
	\Var(T_\eta) \leq 2n \frac{p_{\max}^2}{p_{\min}^2} r^d \left(r^d \delta^2(f,g) + Lr^{d+1}\delta(f,g)\right) + \frac{p_{\max}^2}{2}r^d.
	\end{equation*}
\end{proof}




\end{document}