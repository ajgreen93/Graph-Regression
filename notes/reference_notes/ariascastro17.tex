\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbb{I}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Matrices
\newcommand{\Xbf}{\mathbf{X}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}


\begin{document}
	
\title{Notes on 'Remember the Curse of Dimensionality: The Case of Goodness-of-Fit Testing in Arbitrary Dimension'}
\author{Alden Green}
\date{\today}
\maketitle

Define the \emph{worst-case risk} of a test $\phi$ (Borel-measurable function to $[0,1]$) to be
\begin{equation*}
R_{\epsilon}^{(m)}(\phi; f_0; \mathcal{H}) = \Ebb_{f_0}^{(m)} \phi + \sup_{\substack{f \in \Hclass \\ \delta(f,f_0) \leq \epsilon}} \set{\Ebb_f^{(m)} ( 1 - \phi)}
\end{equation*}
where $\delta(f,g) = \int (f-g)^2$.

The \emph{minimax risk} is then
\begin{equation*}
R_{\epsilon}^{(m)}(f_0; \mathcal{H}) = \inf_{\phi} R_{\epsilon}^{(m)}(\phi; f_0; \mathcal{H}).
\end{equation*}

\section{One-sample goodness of fit problem for Holder class}

Let $\Hclass_s^d(L)$ be the \emph{Holder class} of functions $f: [0,1]^d \to \Reals$ such that
\begin{equation*}
\abs{f^{\floor{s}}(x) - f^{\floor{s}}(y)} \leq L \norm{x - y}^{s - \floor{s}} \tag{for all $x,y \in [0,1]^d$}
\end{equation*}
and additionally
\begin{equation*}
\norm{f^{(s')}}_{\infty} \leq L,~~ \forall s' \in \set{0,\cdots,\floor{s}}
\end{equation*}

\begin{theorem}[One-sample lower bound over Holder densities]
	\label{thm: one_sample_Holder_lower_bound}
	For the one-sample problem
	\begin{equation*}
	f_0 = \text{uniform distribution over} ~[0,1]^d~ \text{known}
	\end{equation*}
	under known Holder regularity, there is a constant $c > 0$ depending only on $(s,d,L)$ such that
	\begin{equation*}
	\R_{\epsilon}^{(m)}(f_0; \Hclass_s^d(L)) \geq 1/2~ \text{for all $\epsilon \leq cm^{-\frac{2s}{4s + d}}$}
	\end{equation*}
\end{theorem}

\subsection{One-sample chi-squared test}

Let the \emph{one sample chi-squared statistic} be given by
\begin{equation*}
\Gamma_{\kappa}^{one} = \sum_{k \in [\kappa]^d} \abs{M_{k,\kappa} - m\kappa^{-d}}^2
\end{equation*}
where
\begin{equation*}
M_{k,\kappa} = \# \set{x_i: x_i \in \left(\frac{k-1}{\kappa}, \frac{k}{\kappa}\right]}
\end{equation*}

For now, we treat $s$ as known, and set
\begin{equation*}
\kappa = \kappa(s,d) := \floor{m^{\frac{2}{4s+d}}}
\end{equation*}

\begin{theorem}[One-sample chi-squared test]
	\label{thm:one_sample_chi_squared_test}
	In the one-sample problem under known Holder regularity, consider the chi-squared test $\phi_{\kappa,\tau} = \1\left\{\Gamma_{\kappa}^{one} > \tau \right\}$. There are constants $c_1$ depending on only $(s,L)$ and $c_2$ depending on only $(s,d,L)$ such that for $\tau = m + a m \kappa^{-d/2}$ with $a \geq 1$
	\begin{equation*}
	R_{\epsilon}^{(m)}(\phi_{\kappa,\tau}; f_0, \mathcal{H}_s^d(L)) \leq c_1/a^2
	\end{equation*}
	for any $\epsilon \geq c_2 a m^{-\frac{2s}{4s+d}}$.
\end{theorem}

\section{Two-Sample Testing}
In the two sample case, we observe data $x_1, \ldots, x_n$ and $y_1, \ldots, y_m$. For a given test (Borel measurable function of the data) $\phi$, define the worst case risk to be
\begin{equation*}
R_{\epsilon}^{(m,n)}(\phi; \mathcal{H}) = \sup_{f \in \Hclass} \Ebb_{f,f}(\phi) + \sup_{ \substack{f,g \in \Hclass \\ \delta(f,g) \geq \epsilon^2} } \Ebb_{f,g}(1 - \phi)
\end{equation*}
and the minimax risk to be
\begin{equation*}
R_{\epsilon}^{(m,n)}(\Hclass) = \inf_{\phi} R_{\epsilon}^{(m,n)}(\phi; \mathcal{H}).
\end{equation*}

It is intuitively clear that the two-sample problem is at least as hard as the one-sample problem.

\begin{lemma}[Two-sample is harder than one-sample.]
	\label{lem: two_sample_harder_one_sample}
	For any class $\Hclass$, pseudo-metric $\delta$, $\epsilon > 0$, and $f_0 \in \Hclass$,
	\begin{equation*}
	R_{\epsilon}^{(m)}(f_0; \Hclass) \leq R_{\epsilon}^{(m,n)}(\Hclass)
	\end{equation*}
\end{lemma}

\begin{theorem}[Two-sample lower bound over Holder densities]
	\label{thm: two_sample_Holder_lower_bound}
	For the two-sample problem under known Holder regularity, there exists constant $c$ depending only on $(s,d,L)$ such that
	\begin{equation*}
	R_{\epsilon}^{(m,n)}(\Hclass_s^d(L)) \geq 1/2
	\end{equation*}
	for any $\epsilon \leq c(m \wedge n)^{-\frac{2s}{4s + d}}$. 
\end{theorem}

\subsection{Two-sample chi-squared test.}
Define the \emph{two-sample chi-squared} test statistic
\begin{equation*}
\Gamma_{\kappa} = \sum_{k \in [\kappa]^d} (M_{k, \kappa} - N_{k, \kappa})^2
\end{equation*}
where
\begin{equation*}
M_{k,\kappa} = \# \set{x_i: x_i \in \left(\frac{k-1}{\kappa}, \frac{k}{\kappa}\right]},~~ N_{k,\kappa} = \# \set{y_i: y_i \in \left(\frac{k-1}{\kappa}, \frac{k}{\kappa}\right]}
\end{equation*}
are bin counts, and for simplicity we let $m = n$ so no normalization is needed.

\begin{theorem}[Two-sample chi-squared test.]
	\label{thm: two_sample_chi_squared_test}
	For the two-sample testing problem under known Holder regularity, let $\phi_{k, \kappa} = \mathbb{I}(\Gamma_{\kappa} > \tau)$. There are constants $c_1$ depending only on $L$, and $c_2$ depending only on $(s,d,L)$ such that for $\tau = 2m + a m \kappa^{-d/2}$
	\begin{equation*}
	R_{\epsilon}^{(m,m)}(\phi_{k, \kappa}) \leq c_1/a^2
	\end{equation*}
	whenever $\epsilon \geq a c_2 m^{-\frac{2s}{4s + d}}$. 
\end{theorem}

\section{Proofs}

\begin{proof}[Proof of Theorem \ref{thm: one_sample_Holder_lower_bound}]
	Let $h: \Rd \to \Reals$ be infinitely differentiable with support on $[0,1]^d$ such that $\int h = 0$, $\int h^2 = 1$. Let $\kappa \geq 1$ be an integer, and for $j \in \mathbb{Z}^d$, define
	\begin{equation*}
	h_{j,\kappa}(x) = \kappa^{d/2} h(\kappa x - j + 1)
	\end{equation*}
	supported on $[\frac{(j - 1)}{\kappa}, \frac{j}{\kappa}]$. (Note that $\norm{h_{j,\kappa}} = 1$)
	
	For $\eta = (\eta_1, \ldots, \eta_{\kappa^d}) \in \set{-1, +1}^{\kappa^d}$ and $\rho > 0$ to be defined later, define
	\begin{equation*}
	f_\eta = f_0 + \rho \sum_{j \in [\kappa]^d} \eta_j h_{j,\kappa}(x)
	\end{equation*}
	and note that since $\int h_{j,\kappa} = 0$, $\int f_{\eta} = 1$. Additionally, because the $h_{j,\kappa}$'s have disjoint support,
	\begin{itemize}
		\item 
		If $\rho \kappa^{d/2} \norm{h}_{\infty} \leq 1$,
		\begin{equation*}
		f_{\eta} \geq 0
		\end{equation*}
		
		\item
		For $C := 4 \norm{h^{(\floor{s})}}_{\infty} \vee 2 \norm{h^{(\floor{s } + 1)}}_{\infty}$, if $\rho \kappa^{d/2 + s} C \leq L$, then
		\begin{equation*}
		h_{j,\kappa}^{(\floor{s})} \in \Hclass_s^d(L)
		\end{equation*}
	\end{itemize}
	
	To see the second point, for arbitrary $x,y \in [0,1]^d$ let $x \in [\frac{k-1}{\kappa}, \frac{k}{\kappa}]$, $y \in [\frac{l-1}{\kappa}, \frac{l}{\kappa}]$. Then
	\begin{align*}
	\abs{f_{\eta}^{(\floor{s})}(x) - f_{\eta}^{(\floor{s})}(y)} & \leq 2 \rho \kappa^{d/2 + \floor{s}} \bigl(\abs{h^{(\floor{s})}(\kappa x - k  + 1) - h^{(\floor{s})}(\kappa y - k  + 1)} + \\
	& \abs{h^{(\floor{s})}(\kappa x - l  + 1) - h^{(\floor{s})}(\kappa y - k + 1)} \bigr) \\
	& \leq 2 \rho \kappa^{d/2 + \floor{s}} \left( 2 \kappa \norm{h^{(\floor{s} + 1)} }_{\infty} \norm{x - y} \wedge 4 \norm{h^{(\floor{s})}}_{\infty} \right) \\
	& \leq  2 \rho \kappa^{d/2 + \floor{s}} \left( \left[2 \norm{h^{(\floor{s} + 1)}}_{\infty} \vee 4 \norm{h^{(\floor{s})}}_{\infty} \right] \left[1 \wedge \kappa \norm{x - y}\right]  \right) \\
	& \leq L \norm{x - y}^{s - \floor{s}}
	\end{align*}
	where the step follows from $(1 \wedge u) \leq u^a$ for all $u > 0, 0 < a \leq 1$.
	
	Take $\rho$ small enough to satisfy the above conditions, let $\epsilon = \rho \kappa^{d/2}$. Note that
	\begin{equation*}
	\norm{f_0 - f_{\eta}}_2^2 \leq \rho^2 \sum_{j \in [\kappa]^d} \norm{h_{j,\kappa}}_2^2 = \rho^2 \kappa^d = \epsilon^2.
	\end{equation*}
	
	The minimax risk is lower bounded by the Bayes risk; in this case, consider  the uniform prior distribution over $\set{f_{\eta}: \eta \in \set{-1,1}^{[\kappa]^d}}$. The Bayes risk is achieved by the likelihood ratio test $\set{W > 1}$, where
	\begin{equation*}
	W = \frac{1}{2^{\kappa^d}} \sum_{\eta \in \set{-1,1}^{\kappa^d}} \prod_{i = 1}^{m} f_{\eta}(x_i)
	\end{equation*}
	and its known that the risk of the likelihood ratio test is lower bounded by $1 - \sqrt{\frac{1}{2} \Var_{f_0}(W)}$.\footnote{Compare the $\chi^2$-divergence and $TV$-distance, and use the lower bound on risk $R \geq 1/2 - 1/2\norm{P_0 - P_1}_{TV}$.}
	
	We can compute $\Var_{f_0}(W) \leq \exp \{(\rho^2 m)^2\kappa_d \}$ for $rho^2 m \leq 1$. Choosing $\kappa = \floor{m^{2/(4s + d)}}$ and $\rho = c m^{-(2s + d)/(4s + d)}$, we have that the upper bound on $\Var_{f_0}(W)$ is $1$, and so the minimax risk is at least $1/2$. $\epsilon = \kappa^{d/2} \rho = cm^{\frac{-2s}{4s + d}}$. It can be verified that the choice of $\rho$ satisfies the necessary conditions imposed above. 
\end{proof}

\subsection{Proof of Theorem \ref{thm: two_sample_chi_squared_test}}

\paragraph{Test error for chi-squared test over discrete distributions}

Let $p$ and $q$ be discrete distributions over $\mathcal{K}$, and let
\begin{equation*}
T = \sum_{k \in \mathcal{K}} (M_k - N_k)^2
\end{equation*}
where
\begin{equation*}
M_k = \#\{A_i = k\},~ N_k = \#\{B_j = k\}
\end{equation*}
for $A_1, \ldots, A_m \sim p, B_1, \ldots, B_m \sim q$ independent.

\begin{lemma}[Moment bounds for $T$]
	We have
	\label{lem: moment_bounds_for_T}
	\begin{align*}
	\Ebb(T) & = 2m + m^2 \langle (p-q)^2  \rangle - m(\langle p^2 \rangle + \langle q^2 \rangle) \\
	\Var(T) & = 2m^2 \langle (p + q)^2 \rangle + 4m^3(\langle (p + q)(p - q)^2 \rangle + 2 \langle pq \rangle \langle (p-q^2)\rangle)
	\end{align*}
\end{lemma}

\begin{corollary}
	Consider testing within the class of probability distributions $r$ on $\mathcal{K}$ such that $\norm{r}_{\infty} \leq \eta$ for some $\eta > 0$. There are universal constants $\nu_1$ and $\nu_2$ such that for any $a > 0$, the test with rejection region
	\begin{equation*}
	\set{T - 2m \geq a m \sqrt{\eta}}
	\end{equation*}
	has size at most $\nu_1/a^2$ and power at least $1 - \nu_1/a^2$ against alternatives satisfying
	\begin{equation*}
	\norm{p - q}^2 \geq \nu_2(a \sqrt{\eta} \vee a^2 \eta \vee \eta) / m
	\end{equation*} 
\end{corollary}

\paragraph{Approximation Error.}
\begin{lemma}[Approximation error of binning]
	\label{lem: approximation_error_of_binning}
	For a continuous function $h: [0,1]^d \to \Reals$ and an integer $\kappa \geq 2$, define
	\begin{equation*}
	W_{\kappa}[h] = \sum_{k \in [\kappa^d]} \kappa^d \int_{H_k} h(x) dx \1(H_k)
	\end{equation*}
	Then there are constants $b_1, b_2 > 0$ depending only on $(s,d,L)$ such that
	\begin{equation*}
	\norm{W_k[h]}_2 \geq b_1 \norm{h}_2 - b_2 \kappa^{-s},~ \forall h \in H_s^d(L) 
	\end{equation*}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem: approximation_error_of_binning}]
	
	\begin{lemma}[Taylor expansion]
		\label{lem: taylor_expansion}
		Fix any $h \in \Hclass_s^d(L)$ and any $x_0 \in [0,1]^d$. Let $u$ denote the $\floor{s}$-th order Taylor expansion of $h$ around $x_0$. Then, there is a constant $L'$ depending only on $(s,d,L)$ such that
		\begin{equation*}
		\abs{h(x) - u(x)} \leq L' \norm{x - x_0}^s \tag{$\forall u \in [0,1]^d$}
		\end{equation*}
	\end{lemma}

	Fix $h \in \Hclass_s^d(L)$, and let $u_j$ be the $\floor{s}$-th order Taylor expansion around $(j - 1)r/\kappa$, for some $j = (j_1,\ldots,j_d)$, $r \geq 1$ is an integer. Then, define
	\begin{equation*}
	u = \sum_{j} u_j \1_{\widetilde{\Hclass}_j},~ \widetilde{\Hclass}_j = \prod_{l = 1}^{m} \widetilde{\Hclass}_{j_l},~ \widetilde{\Hclass}_{j_l} = \left( \frac{(j_l - 1)r}{\kappa}, \frac{j_l r}{\kappa} \right]
	\end{equation*}
	Then,
	\begin{equation*}
	\abs{u(x) - h(x)} \leq L'\left(\frac{2\sqrt{d} r}{\kappa}\right)^s =: c_1 \kappa^{-s}
	\end{equation*}
	and as a result
	\begin{align*}
	\norm{W_{\kappa}[h]}_2 & \geq \norm{W_{\kappa}[u]}_2 - \norm{W_{\kappa}[h] - W_{\kappa}[u]}_2 \\
	& \geq \norm{W_{\kappa}[u]}_2 - \norm{h - u}_2 \\
	& \geq \norm{W_{\kappa}[u]}_2 - c_1 \kappa^{-s}
	\end{align*}
	
	\begin{lemma}[Averaging of polynomial preserves norm]
		\label{lem: averaging_of_polynomial_preserves_norm}
		Let $\Pclass_m^d$ denote the class of polynomials on $\Reals^d$ of degree at most $m$. For a given partition $\mathcal{Q} = (Q_i)$, define
		\begin{equation*}
		W_{\mathcal{Q}}[v] = \sum_{Q_i \in \mathcal{Q}} \frac{1}{\abs{Q_i}} \left(\int_{Q_j} v(x) dx\right) \1_{Q_i}.
		\end{equation*}
		Then there exists constant $c_2 > 0$ depending only on $(d,m)$ such that, when $\max_{j} \diam(Q_j) \leq c_2$,
		\begin{equation*}
		\norm{W_{\mathcal{Q}}[v]} \geq c_2 \norm{v}_2,~ \text{for all $v \in \Pclass_m^d$}.
		\end{equation*}
	\end{lemma}
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm: two_sample_chi_squared_test}]
	
Define
\begin{equation*}
H_k = \left(\frac{k-1}{\kappa}, \frac{k}{\kappa}\right],~ p_k = \int_{H_k} f(x) dx,~ q_k = \int_{H_k} g(x) dx
\end{equation*}
and let $p = (p_k)_{k \in [\kappa]^d}$, $q = (q_k)_{k \in [\kappa]^d}$. 
	
Application of Lemma \ref{lem: approximation_error_of_binning} to $f - g$ yields
\begin{equation*}
\norm{p - q}^2 \geq \kappa^{-d} \left( b_1^* \norm{f - g}_2 - b_2^* \kappa^{-s} \right)^2.
\end{equation*}

Recall that $\norm{g - f}_2 \geq \epsilon \geq c_1 a m^{-\frac{2s}{4s + d}}$. Since $\kappa^{-s} = m^{-\frac{2s}{4s + d}}$, an appropriate choice of $c_1$, independent of $\kappa$, yields
\begin{equation*}
\norm{p - q}^2 \geq \kappa^{-d} \epsilon^2
\end{equation*}

\end{proof}

\section{Relevant Citations}

\begin{enumerate}
	\item A distribution free version of the smirnov two-sample test in the p-variate case (Bickel 69)
	\item Optimal kernel choice for large-scale two-sample tests. (Gretton 12)
	\item Permutation tests for equality of distributions in high-dimensional settings (Hall 02)
\end{enumerate}

\end{document}