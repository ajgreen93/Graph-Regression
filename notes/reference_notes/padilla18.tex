\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left( #1 \right)}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\1}{\mathbb{I}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Matrices
\newcommand{\Xbf}{\mathbf{X}}

%%% Sets
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}


\begin{document}
	
\title{Notes on 'Adaptive Non-Parametric Regression With the \emph{K}-NN Fused Lasso'}
\author{Alden Green}
\date{\today}
\maketitle

Let $\Xbf = x_1, \ldots, x_n$ be sampled i.i.d from $\mu$ with density function $p(\cdot)$ over some subset $\Xset$ of Euclidean space, and suppose
\begin{equation*}
y_i = f_0(x_i) + \epsilon_i, ~~~\epsilon_i \overset{i.i.d}{\sim}SG(\sigma^2)
\end{equation*}
holds for some unknown $f_0$. Let $\widehat{\theta}$ be the solution to the \emph{fused lasso}
\begin{equation*}
\widehat{\theta} := \argmin_{\theta \in \Reals^n} \set{\frac{1}{2} \norm{y - \theta}_2^2 + \lambda \norm{\nabla_G \theta}_1} 
\end{equation*}
where $\lambda > 0$ is a tuning parameter, and $\nabla_G$ is an oriented incidence matrix of the graph $G$. 

The \emph{$K$-NN-FL} estimator computes the fused lasso over the $K$-NN graph $G_K$ of $\Xbf$. The \emph{$\epsilon$-FL} estimator computes the fused lasso over the $\epsilon$ graph $G_{\epsilon}$.

The assumptions required for Theorems \ref{thm: padilla18_theorem_1} and \ref{thm: padilla18_theorem_2} are as follows.
\begin{enumerate}[(a)]
	\item For all $x \in \Xset$
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty
	\end{equation*}
	\item The base measure $\mu$ in $\Xset$ satisfies
	\begin{equation*}
	r^d c_{1,d} \leq \mu(B_r(x)) \leq c_{2,d} r^d \tag{$\forall x \in \Xset$}
	\end{equation*}
	\item There exists a homeomorphism (continuous bijection with continuous inverse) $h: \Xset \to [0,1]^d$ such that
	\begin{equation*}
	L_{\min} d_{\Xset}(x,x') \leq \norm{h(x) - h(x')}_2  \leq L_{\max} d_{\Xset}(x,x') \tag{$\forall x,x' \in \Xset$}
	\end{equation*}
	\item $g_0$ is piecewise Lipschitz\footnote{Technically, the requirement is slightly weaker than piecewise Lipschitz.}, meaning there exists a set $\Sset \subset (0,1)^d$ such that
	\begin{enumerate}
		\item $\nu(\Sset) = 0$.
		\item $\mu\Bigl( h^{-1}\bigl(S_{\epsilon} \cup ([0,1]^d \setminus \Omega_{\epsilon})\bigr) \Bigr) \leq C_{\Sset} \epsilon$
		\item There exists a positive constant $L_0$ such that if $z$ and $z'$ belong to the same connected component of $\Omega_{\epsilon} \setminus B_{\epsilon}(\Sset)$, then 
		\begin{equation*}
		\abs{g(z) - g(z')} \leq L_0 \norm{z - z'}_2
		\end{equation*}
	\end{enumerate}
	where $\Omega_{\epsilon} = [0,1]^d \setminus B_{\epsilon}(\partial [0,1]^d)$.
\end{enumerate}

\begin{theorem}
	\label{thm: padilla18_theorem_1}
	Let $K \asymp \log^{1 + 2r} n$ for some $r > 0$, Then under Assumptions \textcolor{red}{1-3}, with an appropriate choice of the tuning parameter $\lambda$, the \textcolor{red}{$K$-NN-FL} estimator $\widehat{\theta}$ satisfies
	\begin{equation*}
	\norm{\widehat{\theta} - \textcolor{red}{\theta^{\star}}}_n^2 = O_{\Pbb}\left(\frac{\log^{1 + 2r} n}{n} + \frac{\log^{1.5 + r} n}{n}\norm{\nabla_{G_K} \theta^{\star}}_1 \right)
	\end{equation*}
	
	This upper bound also holds for $\epsilon$-NN-FL if we replace $\norm{\nabla_{G_K} \theta^{\star}}_1 $ with $\norm{\nabla_{G_{\epsilon}} \theta^{\star}}_1$.
\end{theorem}

\begin{theorem}
	\label{thm: padilla18_theorem_2}
	Under Assumptions \textcolor{red}{1-5}, with an appropriate choice of the tuning parameter $\lambda$, the \textcolor{red}{$K$-NN-FL} estimator $\widehat{\theta}$ satisfies
	\begin{equation*}
	\norm{\widehat{\theta} - \textcolor{red}{\theta^{\star}}}_n^2 = \widetilde{O}_{\Pbb}\left( \frac{1}{n^{1/d}} \right).
	\end{equation*}
\end{theorem}

\section{Proofs}

To ease proofs, we will assume $\Xset = [0,1]^d$. 

Construct $G_{lat} = (V_{lat}, E_{lat})$ a lattice graph with equal side lengths in $[0,1]^d$, where
\begin{align*}
V_{lat} & = P_{lat}(N) := \set{\left(\frac{i_1}{N} - \frac{1}{2N}, \cdots, \frac{i_d}{N} - \frac{1}{2N}\right): i_1, \ldots, i_d \in \set{1, \ldots, N}} \\
(z,z') & \in E_{lat}~ \text{if and only if $\norm{z - z'} \leq \frac{1}{N}$}
\end{align*}
where $z$ and $z' \in P_{lat}(N)$.

Denoting $I = P_{lat}$, we define
\begin{equation*}
P_I(x) = \argmin \set{\norm{x - z'}_{\infty}, z' \in P_{lat}(N)}
\end{equation*}

Then, let $C(z) = \set{x \in [0,1]^d: z =  P_I(x)}$ be the collection of cells associated with the mesh $P_{lat}(N)$, noting that $\set{C(z): z \in P_{lat}(N)}$ defines a partition over $[0,1]^d$.

\paragraph{Quantization.}

For a given $\theta \in \Reals^n$, the \emph{quantization} $\theta_I \in \Reals^n$
\begin{equation*}
(\theta_{I})_i := \theta_j,~ \text{ where }~ x_j = \argmin_{x_l, l \in [n]} \norm{P_I(x_i) - x_l}_{\infty}
\end{equation*}
is constant over every cell $C(z)$. We now induce a signal in $\Reals^{N^d}$ corresponding to the elements in $I$. Let $\set{z_1, \ldots, z_{N^d}} = I$. Then
we write
\begin{equation*}
I_k = \set{i \in [n]: P_I(x_i) = z_k} 
\end{equation*}
for $k = 1,\ldots, N^d$. Define $\theta^{I} \in \Reals^{N^d}$ by
\begin{equation*}
(\theta^{I})_k := 
\begin{cases}
(\theta_I)_i, x_i \in I_k \\
0, I_k = \emptyset
\end{cases}
\end{equation*}
where we note that $(\theta^{I})$ is well-defined since $(\theta_I)_i = (\theta_I)_j$ if $x_i$ and $x_j$ are both in $I_k$.

\subsection{Controlling counts of mesh}

Define the event $\Omega$ as: ``If $x_i \in C(z_k)$ and $x_i \in C(z_l)$ for $z_k, z_l \in I$ with $\norm{z_k - z_l}_2 \leq \frac{1}{N}$, then $x_i$ and $x_j$ are connected in the $K$-NN graph.'' Then,
\begin{lemma}
	\label{lem: mesh_counts}
	Take Assumptions \textcolor{red}{1-3}, and additionally assume that $N$ in the construction of $G_{lat}(N)$ is chosen as
	\begin{equation}
	\label{eqn: N_lower_bound}
	N \geq \left[\frac{3\sqrt{d}(2 c_{2,d} p_{\max})^{1/d} n^{1/d}}{L_{\min} K^{1/d}}\right].
	\end{equation}
	Then,
	\begin{equation*}
	\Pbb(\Omega) \geq 1 - n \exp(-K / 3).
	\end{equation*}
\end{lemma}

\subsection{Bounding Empirical Process}

\begin{lemma}
	
\end{lemma}

\subsection{Mesh embedding for $K$-NN graph}

\begin{lemma}
	\label{lem: quantization_error}
	Fix $N$ to satisfy \eqref{eqn: N_lower_bound}, and let us assume that the event $\Omega$ from Lemma \ref{lem: mesh_counts} holds. Denote $I = P_{lat}(N)$ to be the mesh. Then, for all $e \in \Reals^n$, it holds that
	\begin{equation*}
	\abs{e^T(\theta - \theta_I)} \leq 2 \norm{e}_{\infty} \norm{\nabla_{G_K} \theta}_1, \tag{$\forall \theta \in \Reals^n$}
	\end{equation*}
	Moreover, 
	\begin{equation*}
	\norm{D \theta^I}_1 \leq \norm{\nabla_{G_K} \theta}_1, \tag{$\forall \theta \in \Reals^n$}
	\end{equation*}
	where $D$ is the incidence matrix of $G_{lat}$.
\end{lemma}
\begin{proof}
	Clearly
	\begin{equation*}
	\dotp{\epsilon^T}{\theta - \theta_{I}} \leq \norm{\epsilon}_{\infty} \cdot \norm{\theta - \theta_I}_1
	\end{equation*}
	Then, for every $i = 1, \ldots, n$, the event $\Omega$ implies that there exists a $j \in [n]$ such that
	\begin{equation*}
	(\theta_I)_i = \theta_j,~~ (x_i,x_j) \in E_{G_K}
	\end{equation*}
	and therefore
	\begin{equation*}
	\norm{\theta - \theta_I}_1 \leq \norm{\nabla_{G_K} \theta}_1
	\end{equation*}
\end{proof}

\subsection{Bounding empirical process}

\begin{lemma}
	Conditional on the event $\Omega$, we have that
	\begin{equation*}
	\dotp{\epsilon}{\widehat{\theta}_I - \widehat{\theta}_I^{\star}} \leq \max_{u \in I} \sqrt{\abs{C(u)}} \left( \norm{\Pi \widetilde{\epsilon}}_2 \norm{\widehat{\theta} - \theta^{\star}}_2  + \norm{(D^{\dagger})^T \widetilde{\epsilon}}_{\infty} \left[ \norm{\nabla_{G_K} \widehat{\theta} }_1 + \norm{\nabla_{G_K} \theta^{\star} }_1  \right]\right)
	\end{equation*}
	where $\widetilde{\epsilon}$ is an independent, mean-zero vector of subgaussian random variables, $\abs{C(u)} := \sum_{i \in [n]} \1(x_i \in C(u))$, and $D^{\dagger}$ is the pseudoinverse of the incidence matrix $D$ of $G_{lat}$.
\end{lemma}
\begin{proof}
	Writing
	\begin{equation*}
	\widetilde{\epsilon}_l = \left[\max_{u \in I} \abs{C(u)} \right]^{-1/2} \sum_{x_j \in I_l} \epsilon_j
	\end{equation*}
	we have
	\begin{align*}
	\dotp{\epsilon}{\widehat{\theta}_I - \theta_I^{\star}} = \left[\max_{u \in I} \abs{C(u)}\right]^{1/2} \dotp{\widetilde{\epsilon}}{\widehat{\theta}^{I} - \theta^{\star,I}}
	\end{align*}
	Now, divide up $\widetilde{\epsilon}$ into 
	\begin{equation*}
	\widetilde{\epsilon} = P_{1}(\widetilde{\epsilon}) + P_{1^{\perp}}(\widetilde{\epsilon})
	\end{equation*}
	where $P_1$ is the projection onto the span of $\mathbf{1}$ the constant vector, and $P_{1^{\perp}}$ the projection onto the space orthogonal to $\mathbf{1}$. Note that $P_{1^{\perp}}(x) = (D^{\dagger} D)^T x$. Then, we have
	\begin{align*}
	\dotp{P_{1^{\perp}}(\widetilde{\epsilon})}{\widehat{\theta}^{I} - \theta^{\star,I}} & = \dotp{(D^{\dagger} D)^T \widetilde{\epsilon}}{\widehat{\theta}^{I} - \theta^{\star,I}} \\
	& = \dotp{(D^{\dagger})^T \widetilde{\epsilon}}{D(\widehat{\theta}^{I} - \theta^{\star,I})} \\
	& \leq \norm{(D^{\dagger})^T \widetilde{\epsilon}}_{\infty} \norm{ D(\widehat{\theta}^{I} - \theta^{\star,I})  }_1
	& \leq \norm{(D^{\dagger})^T \widetilde{\epsilon}}_{\infty} \left[ \norm{\nabla_{G_K} \widehat{\theta} }_1 + \norm{\nabla_{G_K} \theta^{\star} }_1 \right]
	\end{align*}
	where the last inequality follows from the triangle inequality and Lemma \ref{lem: quantization_error}.
	
	On the other hand,
	\begin{equation*}
	\dotp{P_{1}(\widetilde{\epsilon})}{\widehat{\theta}^{I} - \theta^{\star,I}} \leq \norm{P_{1}(\widetilde{\epsilon})}_2 \norm{\widehat{\theta}^{I} - \theta^{\star,I}}_2
	\end{equation*}
	and so the desired result follows.
\end{proof}

\subsection{Proof of Theorem \ref{thm: padilla18_theorem_1}}
We begin with a basic inequality
\begin{equation*}
\frac{1}{2} \norm{\widehat{\theta} - \theta^*}_n^2 \leq \frac{1}{n} \dotp{\epsilon}{\widehat{\theta} - \theta^{\star}} + \lambda_n \left(\norm{\nabla_G \theta^*}_1 - \norm{\nabla_G \widehat{\theta}}_1 \right)
\end{equation*}

We split up the \textcolor{red}{empirical process},
\begin{equation*}
\dotp{\epsilon}{\widehat{\theta} - \theta^{\star}} = \dotp{\epsilon}{\widehat{\theta} - \widehat{\theta}_I} + \dotp{\epsilon}{\widehat{\theta}_I - \theta_I^{\star}} + \dotp{\epsilon}{\theta_I^{\star} - \theta^{\star}}
\end{equation*}
Hereafter in the proof, we condition on the event $\Omega$. Lemma \ref{lem: quantization_error} gives us bounds on the first and third terms
\begin{align*}
\dotp{\epsilon}{\widehat{\theta} - \widehat{\theta}_I} & \leq 2 \norm{\epsilon}_{\infty} \cdot \norm{\nabla_{G_K} \widehat{\theta} }_1 \\
\dotp{\epsilon}{\theta^{\star} - \theta_I^{\star}} & \leq 2 \norm{\epsilon}_{\infty} \cdot \norm{\nabla_{G_K} \theta^{\star} }_1
\end{align*}




\end{document}