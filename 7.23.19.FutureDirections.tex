\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\distiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbb{I}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Future Directions}
\author{Alden Green}
\date{\today}
\maketitle

Throughout, let $G = (V,E)$ be a connected, undirected graph with Laplacian matrix $L = D - A$. Write $L = U \mu U^T$ for the spectral decomposition of $L$. Let the normalized Laplacian matrix be $N = D^{-1/2} L D^{-1/2}$, and write $N = V \Lambda V^T$ for the spectral decomposition of $N$. Let $M = D^{-1/2}ND^{1/2}$ be the random walk Laplacian (i.e. $M = I - W$) where $W$ is the random walk matrix.

\section{Graph clustering with \textrm{PPR}}

We begin with concrete tasks, specific to the editing of our paper ``Local Spectral Clustering of Density Upper Level Sets.'' 

\begin{enumerate}
	\item Rewrite introduction to carefully introduce and motivate our goals. Why, in light of what is already known about consistency of spectral clustering, consistency of density clustering in the flat and hierarchical setting, and spectral clustering in the nonparametric mixture models setting, is our study worthwhile?
	\item Relatedly, we should include a more extensive literature review to place our work in better context.
	\item Examples. To start, the Gaussian mixture and uniform mixture (with noise) case. 
\end{enumerate}

Equally concrete extensions, although not necessarily to be included in the aforementioned paper, include:

\begin{enumerate}
	\item Negative examples. Can we prove a lower bound on the symmetric set distance metric between \textrm{PPR}-recovered set and any high density set? Can we prove that the \textrm{PPR}-recovered set has higher normalized cut than a given high density set?
\end{enumerate}

Next, we detail some more abstract potential areas to study.

\subsection{Connection between spectral, local, and resistance-distance embeddings.}

We start by establishing a connection between spectral, random walk based (local), and resistance-distance based clustering approaches, based on embeddings associated with each approach.

\paragraph{Spectral Embedding.}
We start by recalling the definition of spectral clustering. Let $V_k = (v_1 \ldots v_k)$ be the $n \times k$ matrix containing the first $k$ eigenvectors of $N$. Letting $n = \abs{V}$, the spectral embedding into $\Reals^{k - 1}$ based on $V_k$ is given by $z_i = (v_{2i}, \ldots, v_{ki})$, so that
\begin{equation*}
\norm{z_i - z_j}_2^2 = (e_i - e_j)^T V_k V_k^T (e_i - e_j)
\end{equation*}
where $e_i$ is the $i$th standard basis vector in $\Reals^n$.

\paragraph{Resistance Distance Embedding.}
Let $N^{\dagger} = \sum_{k = 2}^{n} (1/\lambda_k) v_k v_k^T$ be the pseudoinverse of the Laplacian matrix. The resistance distance embedding into $\Reals^{n - 1}$ is given by $z_i = (\lambda_{2}^{-1/2}/\sqrt{d_i} u_{2i}, \ldots, \lambda_{n}^{-1/2}/\sqrt{d_i} u_{ni})$, so that
\begin{equation*}
\norm{z_i - z_j}_2^2 = (e_i - e_j)^T D^{-1/2} N^{\dagger} D^{-1/2} (e_i - e_j)
\end{equation*}
the latter being exactly the formula for resistance distance.

As shown by \citep{vonluxburg14}, the resistance distance embedding is not useful when $G$ is a neighborhood graph. 

\paragraph{\textrm{PPR} resistance distance embedding.}

Given a parameter $\alpha > 0$ and seed node $e_i$, the \textrm{PPR} random walk is defined by the recursive relation:
\begin{equation*}
\textrm{pr}_{i}(\alpha) = \alpha e_i + (1 - \alpha) \textrm{pr}_{i}(\alpha) W
\end{equation*}

The following is a representation of the \textrm{PPR}~vector in terms of the inverse of the normalized Laplacian, from \citep{chung10}:
\begin{equation*}
\frac{(1 - \alpha)}{2\alpha}\textrm{pr}_{i}(\alpha) = e_i D^{-1/2}\left(N + \alpha I\right)^{-1} D^{1/2}
\end{equation*}
Following \citep{chung10}, we build to a notion of \textrm{PPR} resistance distance. We define the (normalized) \textrm{PPR} hitting time to be
\begin{equation*}
h_{\alpha}(i,j) = \frac{(\textrm{pr}_{j}(\alpha))_j}{d_j} -  \frac{(\textrm{pr}_{j}(\alpha))_i}{d_i}
\end{equation*}
and the \textrm{PPR} effective resistance to be
\begin{equation*}
R_{\alpha}(i,j) := h_{\alpha}(i,j) + h_{\alpha}(j,i).
\end{equation*}
It can be verified that
\begin{equation*}
R_{\alpha}(i,j) = \frac{(1- \alpha)}{2\alpha} (e_i - e_j)^T D^{-1/2} (N + \alpha I)^{-1} D^{-1/2} (e_i - e_j).
\end{equation*}

Therefore, the \textrm{PPR} resistance distance induces an embedding, $z_i = \bigl( (\lambda_2 + \alpha)^{-1/2}/\sqrt{d_i})v_{2i}, \ldots, (\lambda_n + \alpha)^{-1/2}/\sqrt{d_i})v_{ni} \bigr)$. 

Several directions are now worth mentioning:
\begin{itemize}
	\item These relations hint at why clustering based on PPR or spectral embeddings may be useful, even though clustering based on the resistance distance is not. Both of the former two methods involve regularization over the eigenvalues, and in particular ensure that the top eigenvalue does not dominate. We could more formally study the properties of \textrm{PPR} resistance distance embedding -- framing it either via comparison to spectral embedding, or by contrasting it with (not \textrm{PPR}) resistance distance embeddings.
	\item This also is an interesting--if vague--connection between our work here and developments on graph testing. In both cases, shrinking the eigenvalues of $\Linv$ has proved essential to meaningful developments.
	\item This would also be a natural area to study if we decided to use embedding (synthesis) as an overarching theme.
\end{itemize}

\subsection{Effect of normalization of Laplacian on recovery of high-density clusters.}

We know (see \citep{vonluxburg07}) that the optimization problem
\begin{equation}
\label{eqn:laplacian_energy}
\min_{\mathbf{1}^T z = 0} z^T L z
\end{equation}
is a relaxation of the (NP-hard) problem
\begin{equation}
\label{eqn:ratio_cut}
\min_{\mathbf{1}^T z = 0, z_i \in \set{0,1}} z^T L z
\end{equation}
We term the solution to \eqref{eqn:ratio_cut} the minimum \textit{RatioCut}, noting that the minimization problem is equivalent to finding $\min_{A \subseteq V} \mathrm{RatioCut}(A)$ where $\mathrm{RatioCut}(A)$ is defined as
\begin{equation}
\mathrm{RatioCut}(A) := \mathrm{cut}(A,A^c)\left(\frac{1}{\abs{A}} + \frac{1}{\abs{A^c}}\right).
\end{equation}

Let $N = D^{-1/2}LD^{-1/2}$ be the normalized Laplacian. The optimization problem
\begin{equation}
\label{eqn:norm_laplacian_energy}
\min_{\mathbf{1}^T z = 0} z^T N z
\end{equation}
is a relaxation of the (NP-hard) problem
\begin{equation}
\label{eqn:normalized_cut}
\min_{\mathbf{1}^T z = 0, z_i \in \set{0,1}} z^T N z
\end{equation}
which is equivalent to finding $A \subseteq V$ which miniminizes $\mathrm{NCut}{A}$, for
\begin{equation*}
\mathrm{NCut}(A) := \mathrm{cut}(A,A^c)\left(\frac{1}{\vol(A)} + \frac{1}{\vol(A)}\right).
\end{equation*}

We immediately note that for two subsets $A$ and $A'$ which have equivalent \textrm{RatioCut}, whichever subset with higher average degree will have the lower \textrm{NCut}. In this sense, therefore, the problem \eqref{eqn:norm_laplacian_energy} seems to place ``more emphasis'' on finding a cluster with high average density than does the problem \eqref{eqn:laplacian_energy}. 
\begin{itemize}
	\item Can we extend this intuition to a formal mathematical statement?
	\item Is there a normalization of $L$ (for example, of the form $N^{\alpha} = D^{-\alpha}LD^{-\alpha}$) such that a problem equivalent to \eqref{eqn:norm_laplacian_energy} or \eqref{eqn:laplacian_energy} (or indeed \eqref{eqn:normalized_cut} or \eqref{eqn:ratio_cut}) recovers exactly a high density cluster?
	\item Can we make some practical recommendation about what the ``right'' normalization of the Laplacian is?
\end{itemize}

\subsection{Normalized cut recovery of the density cluster tree.}

In \cite{chaudhuri2010}, the authors state that analysis is made cleaner/easier by working with a hierarchical cluster tree object, rather than the flat density clustering problem. How can we make use of this in the study of the spectral properties of density clusters?

Let $C_f$ be the hierarchical cluster tree for a density function $f: [0,1] \to \Reals_{\geq 0}$ of a distribution $P$. Additionally, let $\eta_r: [0,1] \times [0,1]$ by the kernel function $\eta_r(x,y) = \mathbf{1}(\norm{x - y} \leq r)$.  For $A \subseteq [0,1]$, let the \emph{local normalized cut} functional be given by,
\begin{align*}
\mathrm{cut}_{r,\Pbb}(A) = \int_{A} \int_{A^c} \eta_r(x,y) \,dP(y) \,dP(x), & \quad \quad \mathrm{vol}_{r,\Pbb}(A) = \int_{A} \int_{[0,1]} \eta_r(x,y) \,dP(y) \,dP(x) \\ \mathrm{LCut}_{r,\Pbb}(A) & = \frac{\mathrm{cut}_{r,\Pbb}(A)}{\mathrm{vol}_{r,\Pbb}(A)}.
\end{align*}

Observe that if $C \subseteq C'$ are both objects in $C_f$, then for sufficiently small $r > 0$, $\mathrm{LCut}_{r,\Pbb}(C) \geq \mathrm{LCut}_{r,\Pbb}(C')$. In other words, the $\mathrm{LCut}$ functional respects the partial ordering induced by $C_f$ over sets $C \in C_f$. 

\begin{itemize}
	\item How can we derive algorithmic consequences from this fact? To be concrete, can I design an algorithm which, given the normalized cut of every set $A \subseteq [0,1]$, can reconstruct the cluster tree $C_f$? If not, what additional information do I need?
	\item Clearly for $d \geq 1$, the \textrm{NCut} functional does not in general respect the partial ordering induced by $C_f$. Can we still do something; e.g. can we consistently estimate those relations in $C_f$ which are respected by the functional $\mathrm{LCut}_{r,\Pbb}(\cdot)$?
\end{itemize}

\section{Spectral Graph Testing.}

We observe $G = (V,E)$ with $V = [n]$, and in addition observe
\begin{equation*}
y_i = \beta_i + \sigma \varepsilon_i, \quad \varepsilon_i \sim \overset{\textrm{i.i.d}}{\sim} \mathcal{N}(0,1),
\end{equation*}

\begin{itemize}
	\item We have derived minimax optimal convergence rates, using a projection kernel onto eigenvectors of $L$, when $\beta = (\beta_1, \ldots, \beta_n)$ is in a discrete Sobolev class, assuming appropriate tail decay of the eigenvalues $\mu_i$. Can we extend this to the setting where $G$ is a random neighborhood graph, and $\beta = (f(x_1), \ldots, f(x_n))$ for $f$ in a continuous Sobolev class?
	\item We know that simply considering an IPM-type statistic of the form
	\begin{equation*}
	T = \sup_{\theta: \norm{B\theta}_2^2 \leq C_{n,r}^2} \dotp{\theta}{y}
	\end{equation*}
	will not lead to a minimax optimal test. When $C$ is chosen to contain $\beta$ with high probability, the function class $\set{\theta: \norm{B\theta}_2^2 \leq C^2}$ is too rich, and the variance of $T_C$ therefore too large. 
	
	Consider instead the test statistic: for some $C > 0$
	\begin{equation*}
	T_C = \sup_{\theta: \norm{(B + C I)\theta}_2^2 \leq C_{n,r}^2} \dotp{\theta}{y}
	\end{equation*}
	which restricts the function class under consideration. 
	Can we show that this test statistic -- based on a regularization, rather than truncation, of the eigenvalues -- achieves similar results to the aforementioned projection kernel?
	\item In what practical cases does such a test statistic outperform other related test statistics? As some examples, \citep{ingster2009} proposes a test based on Fourier series projection kernel. \citep{ariascastro2018} analyzes the chi-squared test. \citep{schilling1986} proposes a two-sample test based on nearest neighbors, which can be easily converted into a goodness-of-fit test. \citep{gretton2012} analyzes the MMD in an appropriate Reproducing Kernel Hilbert Space. There are others. 
	\item Can we derive minimax optimal convergence rates when $\beta$ is in a discrete total variation class? This entails both proving upper bounds and matching lower bound. 
	\item Can we extend this to the random neighborhood graph setting, where $f$ is a bounded variation function?
\end{itemize}

\section{Embedding / Other topic to focus on here.}

TODO. (One note: maybe it's worth thinking about ways that our grid inequality can be of use?)



\bibliographystyle{plainnat}
\bibliography{future_directions_bibliography}

\end{document}

