\section{Introduction}
\label{sec:introduction}
Two fundamental statistical goals are to learn and leverage structure of an unknown distribution on the basis of samples. Neighborhood graph Laplacians, and their associated spectra, have proven to be powerful and general tools for both purposes. They have been successfully used for various statistical tasks such as (spectral) clustering, manifold learning, level-set estimation, semi-supervised learning, etc.

By now there exists a rich literature \citep{koltchinskii2000,belkin07,vonluxburg2008,burago2014,shi2015,singer2017,garciatrillos18,trillos2019, calder2019, cheng2021,dunson2021} explaining this practical success from a theoretical perspective. Loosely speaking, these works justify graph spectral methods by showing that the eigenvectors and eigenvalues of graph Laplacians are empirical approximations of population-level objects: the eigenvalues and eigenfunctions of a density-weighted Laplacian operator. These population-level objects in turn characterize various structural aspects of the density, such as its high- and low-density regions, the shape and intrinsic dimension of its support, and so forth. Together, these insights give a reasonably complete explanation of how graph spectra \emph{learn} the structure of an underlying distribution. In contrast, in this paper we develop theory showing that graph spectral methods can effectively \emph{leverage} learned structure to perform downstream (supervised) learning tasks. Our main goal is to show that spectral projection procedures---which use eigenvectors of a neighborhood graph Laplacian as features in linear regression---are statistically optimal methods for two problems in nonparametric regression: estimation and goodness-of-fit testing.

The regression methods we consider are based on \emph{Laplacian eigenmaps}, first introduced by~\cite{belkin03a}.\footnote{Technically speaking, as used by~\cite{belkin03a} ``Laplacian eigenmaps'' refers to a spectral embedding procedure. What we call Laplacian eigenmaps is a two-step regression method: the first step is this spectral embedding, and the second step computes the estimator $\wh{f}$ by running ordinary least squares in embedding space.  Since in this work we only ever consider the embedding within the context of this two-step regression method, we take the liberty of referring to the entire procedure as Laplacian eigenmaps.} Given samples $(X_1,Y_1),\ldots,(X_n,Y_n)$, Laplacian eigenmaps first forms a neighborhood graph, with vertices at the design points $\{X_1,\ldots,X_n\}$ and weighted edges between pairs of sufficiently close points $X_i$ and $X_j$. The Laplacian eigenmaps estimator $\wh{f}$ is then computed using the spectral decomposition of an (unweighted) Laplacian defined over this neighborhood graph, by projecting the response vector ${\bf Y} = (Y_1,\ldots,Y_n)$ onto a subspace spanned by a certain number of ``low-frequency'' graph Laplacian eigenvectors; more precisely, the eigenvectors corresponding to the smallest eigenvalues of the graph Laplacian. The Laplacian eigenmaps test statistic $\wh{T} = \|\wh{f}\|_n^2$ can in turn be used as a statistic to detect whether any signal is present, that is, whether the regression function $f_0(x) = \mathbb{E}[Y|X = x]$ is anywhere non-zero. (The precise definition of these methods is given in Section~\ref{subsec:laplacian_eigenmaps}.) We study these methods through the lens of a minimax analysis. Our major contribution is to provide upper bounds on the error of the Laplacian eigenmaps estimator and test which imply that they achieve the minimax optimal rates of convergence over natural smoothness classes.

More specifically, we consider the worst-case risk of Laplacian eigenmaps over normed balls in Sobolev spaces. Roughly speaking, for a domain $\mc{X} \subseteq \Rd$, the order-$s$ (Hilbert-)Sobolev space $H^s(\mc{X})$ consists of those functions which are $s$-times weakly differentiable, with order-$1$ up through order-$s$ derivatives bounded in $L^2(\mc{X})$ norm. Equivalently, Sobolev spaces can be defined in a spectral manner;  letting $\psi_1,\psi_2,\ldots$ be the eigenfunctions of a Laplacian operator defined with respect to $\mc{X}$, Sobolev spaces consist of those functions $f$ whose Fourier coefficients $\theta_k := \int_{\mc{X}} f \psi_k$ are bounded in a particular sequence norm (see Section~\ref{subsec:spectral_projection}). The minimax estimation and testing rates for Sobolev spaces are generally speaking well-known---when using mean-squared error as the loss, the estimation rate is $n^{-2s/(2s + d)}$, and the testing rate is $n^{-4s/(4s + d)}$---and are achieved by various methods. However, since Sobolev spaces admit a spectral characterization in terms of $\psi_1,\psi_2,\ldots$, spectral projection methods that use these eigenfunctions as features are a particularly natural choice for regression over Sobolev spaces, and are in a sense canonical estimators for these function classes. 
% (AG 8/13/21): Last clause makes a strong statement that I personally believe, but may irritate certain readers. Keep or remove?
% (AG 8/18/21): I do not love the name ``classical spectral projection method''. It doesn't roll off the tongue, and ``classical'' doesn't really convey what the method is. Is there a better name?

Laplacian eigenmaps serves as a data-dependent approximation to these population-level spectral projection methods.\footnote{We use the term ``population-level'' to emphasize that these methods use eigenfunctions which are the infinite-data limits of graph Laplacian eigenvectors.} This data dependency can be very useful. As the name suggests, population-level spectral methods require a priori knowledge of the design distribution, since they are defined with respect to the eigenfunctions of a Laplacian operator which itself depends on this distribution. For this reason, these methods are typically studied only under restrictive conditions on the design; for instance, that the design points are uniformly distributed over the unit cube. In many situations such restrictive conditions are not satisfied, and it may not be reasonable to assume prior knowledge of the design distribution. In contrast to population-level methods, Laplacian eigenmaps implicitly learns structure from the design points $\{X_1,\ldots,X_n\}$, and can thus be used even when the design distribution is unknown. Of course, this comes with a trade off. Intuitively, using an empirical approximation to the ``right'' population-level basis will incur some additional error. The question is whether, notwithstanding this additional error, Laplacian eigenmaps shares the strong statistical properties of its population-level counterpart.

We find that in many cases, the answer is yes: under appropriate regularity conditions, when correctly tuned Laplacian eigenmaps is minimax optimal over Sobolev spaces. Specifically, our main contributions are as follows:
\begin{itemize}
	\item Over the Sobolev space $H^{s}(\mc{X})$, we establish that the Laplacian eigenmaps estimator $\wh{f}$ has in-sample mean-squared error of at most on the order of $n^{-2s/(2s + d)}$, for any number of derivatives $s \in \mathbb{N}$ and dimension $d$. A test based on the statistic $\|\wh{f}\|_n^2$ has a squared critical radius on the order of $n^{-4s/(4s + d)}$, for any number of derivatives $s \in \mathbb{N}$ and dimension $d \in \{1,2,3,4\}$. 
	\item We also consider the behavior of Laplacian eigenmaps when the data satisfies a \emph{manifold hypothesis}, meaning the design points $\{X_1,\ldots,X_n\}$ belong to some low-dimensional submanifold of $\Rd$. In this case, it is known that the minimax rates over Sobolev spaces depend on the intrinsic dimension $m$ of the manifold: explicitly, the estimation rate is $n^{-2s/(2s + m)}$ and the testing rate is $n^{-4s/(4s + m)}$. We give upper bounds showing that Laplacian eigenmaps achieves these rates so long as $s \in \{1,2,3\}$. One can interpret these results as precisely quantifying a way in which Laplacian eigenmaps leverages structure, by adapting to the intrinsic dimension of the design.
	\item The estimator $\wh{f}$ is defined only in-sample. We propose a simple out-of-sample extension based on kernel smoothing, and show that this extension adds negligible error, resulting in an estimator which is defined on all of $\mc{X}$ and has out-of-sample mean-squared error also on the order of $n^{-2s/(2s + d)}$.
\end{itemize}
In all cases, our bounds also depend optimally on the radius $M$ of the Sobolev ball under consideration.

To derive these results, we develop a framework for analysis reminiscent of that  classically used to analyze population-level spectral projection estimators, but ultimately quite different. The chief difference is in the approximation error (bias). For our purposes we must establish that with high probability, the eigenvectors of graph Laplacians (which are data-dependent and hence random objects) can be used to effectively approximate continuum Sobolev functions, whereas in the classical analysis an upper bound on the approximation error follows immediately from the aforementioned spectral characterization of Sobolev spaces. To tightly upper bound the approximation error of Laplacian eigenmaps, we rely on several recent results regarding concentration of the spectra of graph Laplacians around their population limits, and prove some additional results of this type which may be of independent interest. 

For some values of $s$ (number of derivatives) and $d$ (dimension), there do exist gaps between our upper bounds on the error of Laplacian eigenmaps and the minimax rates. Although we do not give corresponding lower bounds verifying the tightness of our analysis, we believe these gaps reflect the true behavior of the method rather than some looseness in our analysis, and we comment more on this at relevant parts in the text. For completeness, we summarize all of our upper bounds---those which match the minimax rates, and those which do not---in Tables~\ref{tbl:estimation_rates} and~\ref{tbl:testing_rates}.
\begin{table}
	\begin{center}
		\begin{tabular}{p{.2\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			$s \leq 3$ & $\bm{n^{-2s/(2s + d)}}$ & $\bm{n^{-2s/(2s + m)}}$ \\
			$s > 3$  & $\bm{n^{-2s/(2s + d)}}$ & $n^{-6/(6 + m)}$
		\end{tabular}
	\end{center}
	\caption{Summary of Laplacian eigenmaps estimation rates over Sobolev balls. Bold font marks minimax optimal rates. In each case, rates hold for all $d \in \mathbb{N}$ (under Model~\ref{def:model_flat_euclidean}), and for all $m \in \mathbb{N}, 1 < m < d$ (under Model~\ref{def:model_manifold}). Although we suppress it for simplicity, in all cases when the Laplacian eigenmaps estimator is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal}
	\label{tbl:estimation_rates}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{p{.175\textwidth} p{.175\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Dimension & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			\multirow{2}{*}{$s = 1$} & $\dim(\mc{X}) < 4$ & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $\dim(\mc{X}) \geq 4$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s = 2$ or $3$} & $\dim(\mc{X}) \leq 4$  & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $4 <\dim(\mc{X}) < 4s$  & $n^{-2s/(2(s - 1) + d)}$ & $n^{-2s/(2(s - 1) + m)}$\\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s > 3$} & $\dim(\mc{X}) \leq 4$ & $\bm{n^{-4s/(4s + d)}}$ & $n^{-12/(12 + d)}$ \\
			& $4 < \dim(\mc{X}) < 4s$ & $n^{-2s/(2(s - 1) + d)}$ & $n^{-6/(4 + m)}$ \\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
		\end{tabular}
	\end{center}
	\caption{Summary of Laplacian eigenmaps testing rates over Sobolev balls. Bold font marks minimax optimal rates. Rates when $d > 4s$ assume that $f_0 \in L^4(\mc{X})$, and depend on $\|f_0\|_{L^4(\mc{X})}$. Although we suppress it for simplicity, in all cases when othe Laplacian eigenmaps test is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal.}
	\label{tbl:testing_rates}
\end{table}

\paragraph{Related work.}

Most work on supervised learning using graphs adopts a \emph{fixed design} perspective, treating the design points $X_1 = x_1,\ldots,X_n = x_n$ as vertices of a fixed graph, and carrying out inference with respect to the conditional mean vector $(f_0(x_1),\ldots,f_0(x_n))$. In this setting, matching upper and lower bounds have been established that certify the optimality of graph-based methods for estimation \citep{wang2016,hutter2016,sadhanala16,sadhanala17,kirichenko2017,kirichenko2018}) and testing \citep{sharpnack2010identifying,sharpnack2013b,sharpnack2013,sharpnack2015} over different ``function'' classes (in quotes because these classes really model the $n$-dimensional vector of evaluations). This setting is quite general, because the graph need not be a geometric graph defined on a vertex set which belongs to Euclidean space. On the other hand, depending on the data collection process, it may be unnatural to model the design points as being a priori fixed, and the estimand as being a vector which exhibits a discrete notion of ``smoothness'' over this fixed design. Instead, we adopt the \emph{random design} perspective, and seek to estimate a function that we assume exhibits a more classical notion of smoothness. 

In the random design setting the analysis of graph-based approaches to supervised learning is more challenging, and only a few works have studied the matter. \cite{zhou2011} provide an asymptotic analysis of the Laplacian eigenmaps estimator in the semi-supervised setting, as the number of unlabeled samples tends to infinity; in this case the estimator is equivalent to population-level spectral projection, and there is no extra error incurred by using data-dependent eigenvectors. \cite{lee2016} consider the \emph{diffusion maps} estimator---which uses the eigenvectors of a differently normalized graph Laplacian---in a similar supervised setting to our own, but prove suboptimal rates of convergence. 

Finally, \citet{trillos2020,green2021} study \emph{graph Laplacian regularization}, a graph-based penalized least squares method which uses a quadratic form involving the graph Laplacian as a penalty. Laplacian regularization can, like Laplacian eigenmaps, be cast as a spectral method. The difference is that Laplacian regularization induces some smooth shrinkage based on the eigenvalue, whereas Laplacian eigenmaps corresponds to a hard spectral cutoff. Interestingly, this difference leads to a stark contrast in the ultimate statistical properties of the two approaches. For instance, in~\cite{green2021} Laplacian regularization is shown to be optimal only for the first-order Sobolev space $H^1(\mc{X})$, and even then only when $d \in \{1,2,3,4\}$. In contrast, in this work we show that the Laplacian eigenmaps estimator is optimal for all $s$ and $d$.

\paragraph{Roadmap.}
We now outline the structure of the rest of this paper. In Section~\ref{sec:setup_main_results}, we give our formal modeling assumptions, and precisely define the Laplacian eigenmaps methods we study. Propositions~\ref{prop:spectral_series_estimation} and ~\ref{prop:spectral_series_testing}, in Section~\ref{subsec:spectral_projection}, show that under rather general (nonparametric) conditions on the design distribution, population-level spectral projection methods achieve minimax rates of convergence over Sobolev classes. Then in Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps},~\ref{sec:manifold_adaptivity}, and \ref{sec:out_of_sample}, we give our main upper bounds on the error of Laplacian eigenmaps (summarized above); these upper bounds hold under similarly general conditions, and imply that Laplacian eigenmaps estimators and tests are also minimax rate-optimal. In Section~\ref{sec:experiments} we examine the empirical behavior of Laplacian eigenmaps, and show that even at moderate sample sizes Laplacian eigenmaps is competitive with population-level spectral projection. We conclude with some discussion in Section~\ref{sec:discussion}. 

\paragraph{Notation.}
We frequently refer to various classical function classes. For an open set $\mc{X}$ equipped with a volume form $d\mu$, we let $L^2(\mc{X})$ denote the set of functions $f$ for which $\|f\|_{L^2(\mc{X})}^2 := \int f^2 \,d\mu  < \infty$, and equip $L^2(\mc{X})$ with the norm $\|\cdot\|_{L^2(\mc{X})}$. We define $\dotp{f}{g}_P := \int fg\,dP$, and let $L^2(P)$ contain those functions $f$ for which $\|f\|_P^2 := \dotp{f}{f}_P$ is finite. Equivalently, we let $L^2(P_n)$ consist of those ``functions'' $f: \set{X_1,\ldots,X_n} \to \Reals$ for which the empirical norm $\|f\|_{n}^2 := \frac{1}{n}\sum_{i = 1}^{n} \bigl(f(X_i)\bigr)^2 < \infty$. When there is no chance of confusion, we will sometimes associate functions in $L^2(P_n)$ with vectors in $\Reals^n$, and vice versa. We use $C^k(\mc{X})$ to refer to functions which are $k$ times continuously differentiable in $\mc{X}$, either for some integer $k \geq 1$ or for $k = \infty$. We let $C_c^{\infty}(\mc{X})$ represent those functions in $C^{\infty}(\mc{X})$ with support $V$ compactly contained in $\mc{X}$, meaning $\wb{V}$ is compact and $\wb{V} \subseteq \mc{X}$. We write $\partial f/\partial r_i$ for the partial derivative of $f$ in the $i$th standard coordinate of $\Rd$, and use the multi-index notation $D^{\alpha}f := \partial^{|\alpha|}f/\partial^{\alpha_1}x_1\ldots\partial^{\alpha_d}x_d$ for multi-indices $\alpha \in \Reals^d$. Recall that for a given multi-index $\alpha \in \mathbb{N}^d$, a function $f$ is \emph{$\alpha$-weakly differentiable} if there exists some $h \in L^1(\mc{X})$ such that
\begin{equation*}
\int_{\mc{X}} h g = (-1)^{|\alpha|} \int_{\mc{X}} f D^{\alpha}g, \quad \textrm{for every $g \in C_c^{\infty}(\mc{X})$.}
\end{equation*}
If such a function $h$ exists, it is the $\alpha$th weak partial derivative of $f$, and denoted by $D^{\alpha}f := h$. For functions $f$ which are $|\alpha|$-times classically differentiable, this coincides with the classical definition of derivative, and so we use the same notation for both.

We write $\|\cdot\| = \|\cdot\|_2$ for Euclidean norm, $|\cdot| = \|\cdot\|_1$ for $\ell_1$ norm, and $d_{\mc{X}}(x',x)$ for the geodesic distance between points $x$ and $x'$ on a manifold $\mc{X}$. Then for a given $\delta > 0$, $B(x,\delta)$ is the radius-$\delta$ ball with respect to Euclidean distance, whereas $B_{\mc{X}}(x,\delta)$ is the radius-$\delta$ ball with respect to geodesic distance. Letting $T_x(\mc{X})$ be the tangent space at a point $x \in \mc{X}$, we write $B_m(v,\delta) \subset T_x(\mc{X})$ for the radius-$\delta$ ball centered at $v \in T_x(\mc{X})$.

For sequences $(a_n)$ and $(b_n)$, we use the asymptotic notation $a_n \lesssim b_n$ to mean that there exists a number $C$ such that $a_n \leq C b_n$ for all $n$. We write $a_n \asymp b_n$ when $a_n \lesssim b_n$ and $b_n \lesssim a_n$. On the other hand we write $a_n = o(b_n)$ when $\lim a_n/b_n = 0$, and likewise $a_n = \omega(b_n)$ when $\lim a_n/b_n = \infty$. Finally $a \vee b := \max\{a,b\}$ and $a \wedge b := \min\{a,b\}$.


