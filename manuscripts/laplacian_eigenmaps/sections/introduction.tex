\section{Introduction}
\label{sec:introduction}
Two fundamental goals of statistics, data science, and machine learning are to learn and leverage structure of an unknown distribution, on the basis of samples drawn from said distribution. Neighborhood graph Laplacians, and their associated spectra, have proven to be powerful and general tools for both purposes. They have been successfully used for various statistical tasks such as (spectral) clustering, manifold learning, level-set estimation, semi-supervised learning, etc.

By this point there exists a rich literature (e.g. \citep{koltchinskii2000,belkin07,vonluxburg2008,burago2014,shi2015,singer2017,garciatrillos18,trillos2019, calder2019, cheng2021,dunson2021}) explaining this practical success from a theoretical perspective. Loosely speaking, these works justify graph spectral methods by showing that the eigenvectors and eigenvalues of graph Laplacians are empirical approximations of population-level objects: the eigenvalues and eigenfunctions of a density-weighted Laplace-Beltrami operator. These population-level objects in turn characterize various structural aspects of the density, such as its high- and low-density regions, the shape and intrinsic dimension of its support, and so forth. Together, these insights give a reasonably complete explanation of how graph spectra \emph{learn} the structure of an underlying distribution. In contrast, in this paper we develop theory showing that graph spectral methods can effectively \emph{leverage} structure to perform downstream (supervised) learning tasks. Our main goal is to show that spectral projection procedures for nonparametric estimation and testing---which use eigenvectors of a neighborhood graph Laplacian as features in linear regression---are statistically optimal. 

The regression methods we consider are based on \emph{Laplacian eigenmaps}, first introduced by~\cite{belkin03a} \footnote{Technically speaking, as used by~\cite{belkin03a} ``Laplacian eigenmaps'' refers to embedding each design point $X_i$ according to coordinates defined by the first-$K$ graph Laplacian eigenvectors; that is, mapping $X_i \mapsto (v_{i,1},\ldots,v_{i,K})$. What we call Laplacian eigenmaps is a two-step supervised method: the first step is the embedding step, and the second step computes the estimate $\wh{f}$ using linear regression.  Since in this work we only ever deal with the supervised method, and since the choice of embedding step is the far more important of the two steps, we take the liberty of referring to the entire procedure as Laplacian eigenmaps}. Given samples $(X_1,Y_1),\ldots,(X_n,Y_n)$, Laplacian eigenmaps first constructs a neighborhood graph---with vertices at the design points $X_1,\ldots,X_n$ and weighted edges between pairs of sufficiently close points $X_i$ and $X_j$---and computes the corresponding (unweighted) graph Laplacian. The Laplacian eigenmaps estimate $\wh{f}$ is simply the projection of the response vector ${\bf Y} = (Y_1,\ldots,Y_n)$ onto the subspace spanned by a subset of the graph Laplacian eigenvectors. The empirical norm $\|\wh{f}\|_n^2$ can be used as a statistic to test whether any signal is present: that is, whether the regression function $f_0(x) = \mathbb{E}[Y|X = x]$ is anywhere non-zero. (The precise definition of these methods is given in Section~\ref{}.) We study these methods through the lens of a minimax analysis, and give upper bounds on the error of the Laplacian eigenmaps estimator and test which imply that they are minimax optimal over certain smoothness classes.

More specifically, we consider the worst-case risk of Laplacian eigenmaps over normed balls in Sobolev spaces. Roughly speaking, for a domain $\mc{X} \subseteq \Rd$, the order-$s$ (Hilbert-)Sobolev space $H^s(\mc{X})$ consists of those functions which are $s$-times weakly differentiable, with derivatives bounded in $L^2(\mc{X})$ norm. Under standard regularity conditions, the minimax estimation and testing rates for Sobolev spaces are well-known: when using mean-squared error as the loss, the estimation rate is $n^{-2s/(2s + d)}$, and the testing rate is $n^{-4s/(4s + d)}$.  These rates are achieved by various estimators, but classical spectral projection methods\footnote{We use ``classical'' to refer to spectral projection methods using eigenfunctions of a (fixed) Laplacian or Laplace-Beltrami operator, as distinct from Laplacian eigenmaps which uses eigenvectors of a (data-dependent) graph Laplacian matrix.}---which use the eigenfunctions $\psi_1,\psi_2,\ldots$ of a Laplace-Beltrami operator defined on $\mc{X}$---are particularly well-suited for regression over Sobolev spaces. This is because Sobolev spaces can be equivalently defined in a spectral manner: they consist of those functions $f$ whose Fourier coefficients $\theta_k := \int_{\mc{X}} f \psi_k$ are bounded in a particular sequence norm. For this reason, classical spectral projection methods have excellent statistical properties; see e.g.~\cite{tsybakov2008_book} and \cite{ingster2009} for analysis of estimation and testing risk in the special case where the design distribution is uniform over the unit cube, and Propositions~\ref{prop:spectral_series_estimation} and~\ref{prop:spectral_series_testing} in Section~\ref{subsec:minimax_rates_sobolev} of our paper, which give equivalent upper bounds under more general conditions. However, these classical spectral methods require a priori knowledge of the design distribution, which is often not available.

On the other hand, Laplacian eigenmaps implicitly learns the structure of the design distribution, and serves as a data-dependent approximation to classical spectral projection that can be used even when the design distribution is unknown. Of course, this comes with a tradeoff. Intuitively, using an empirical approximation to the ``right'' population-level basis will incur some additional error; the question is whether this additional error is enough to meaningfully degrade the overall statistical properties. 

We find that in many cases, the answer is no: under appropriate regularity conditions, when correctly tuned Laplacian eigenmaps is minimax optimal over Sobolev spaces. Specifically, our main contributions are as follows:
\begin{itemize}
	\item Over the Sobolev space $H^{s}(\mc{X})$, we establish the Laplacian eigenmaps estimator $\wh{f}$ has in-sample mean-square error on the order of $n^{-2s/(2s + d)}$, for any number of derivatives $s \in \mathbb{N}$ and dimension $d$. A test based on the statistic $\|\wh{f}\|_n^2$ has a critical radius on the order of $n^{-4s/(4s + d)}$, for any number of derivatives $s \in \mathbb{N}$ and dimension $d \in \{1,2,3,4\}$. 
	\item The estimator $\wh{f}$ is defined only in-sample. We propose a simple out-of-sample extension based on kernel smoothing, and show that this extension adds negligible error, resulting in an estimator which is defined on all of $\mc{X}$ and has out-of-sample mean-squared error also on the order of $n^{-2s/(2s + d)}$.
	\item We also consider the behavior of Laplacian eigenmaps when the data satisfies a \emph{manifold hypothesis}, meaning the design points $X_1,\ldots,X_n$ belong to some low-dimensional submanifold of $\Rd$. In this case, it is known that the minimax rates over Sobolev spaces depend on the intrinsic dimension $m$ of the manifold: explicitly, the estimation rate is $n^{-2s/(2s + m)}$ and the testing rate is $n^{-4s/(4s + m)}$. We give upper bounds implying that Laplacian eigenmaps achieves these rates so long as $s \in \{1,2,3\}$, and demonstrating that Laplacian eigenmaps is a~\emph{domain adaptive} estimator.
\end{itemize}
(In all cases, our bounds also depend optimally on the radius $M$ of the Sobolev ball under consideration.)

To derive these results, we develop an analysis framework reminiscent of that used in the analysis classical spectral projection estimators, but ultimately quite different. The chief difference is in the approximation error (bias). For our purposes we must establish that, with high probability, the eigenvectors of graph Laplacians effectively approximate continuum Sobolev functions, whereas in the classical case this follows immediately from the aforementioned spectral characterization of Sobolev spaces. To tighly upper bound the approximation error of Laplacian eigenmaps, we rely on several recent results regarding concentration of the spectra of graph Laplacians, and prove some additional results which may be of independent interest. 

Our work establishes that Laplacian eigenmaps shares many of the optimality properties of classical spectral projection. However, for some values of $s$ (number of derivatives) and $d$ (dimension), there do exist gaps between our upper bounds on the error of Laplacian eigenmaps and the minimax rates. Although we do not give corresponding lower bounds verifying the tightness of our analysis, we believe these gaps reflect the true behavior of the method rather than some looseness in our analysis; we comment more on this at relevant parts in the text. For completeness, we summarize all of our upper bounds---those which match the minimax rates, and those which do not---in Tables~\ref{tbl:estimation_rates} and~\ref{tbl:testing_rates}.
\begin{table}
	\begin{center}
		\begin{tabular}{p{.2\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			$s \leq 3$ & $\bm{n^{-2s/(2s + d)}}$ & $\bm{n^{-2s/(2s + m)}}$ \\
			$s > 3$  & $\bm{n^{-2s/(2s + d)}}$ & $n^{-6/(6 + m)}$
		\end{tabular}
	\end{center}
	\caption{Summary of Laplacian eigenmaps estimation rates over Sobolev balls. Bold font marks minimax optimal rates. In each case, rates hold for all $d \in \mathbb{N}$ (under Model~\ref{def:model_flat_euclidean}), and for all $m \in \mathbb{N}, 1 < m < d$ (under Model~\ref{def:model_manifold}). Although we suppress it for simplicity, in all cases when the Laplacian eigenmaps estimator is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal, as long as $n^{-1/2} \lesssim M \lesssim n^{1/d}$.}
	\label{tbl:estimation_rates}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{p{.175\textwidth} p{.175\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Dimension & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			\multirow{2}{*}{$s = 1$} & $\dim(\mc{X}) < 4$ & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $\dim(\mc{X}) \geq 4$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s = 2$ or $3$} & $\dim(\mc{X}) \leq 4$  & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $4 <\dim(\mc{X}) < 4s$  & $n^{-2s/(2(s - 1) + d)}$ & $n^{-2s/(2(s - 1) + m)}$\\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s > 3$} & $\dim(\mc{X}) \leq 4$ & $\bm{n^{-4s/(4s + d)}}$ & $n^{-12/(12 + d)}$ \\
			& $4 < \dim(\mc{X}) < 4s$ & $n^{-2s/(2(s - 1) + d)}$ & $n^{-6/(4 + m)}$ \\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
		\end{tabular}
	\end{center}
	\caption{Summary of Laplacian eigenmaps testing rates over Sobolev balls. Bold font marks minimax optimal rates. Rates when $d > 4s$ assume that $f_0 \in L^4(P,M)$. Although we suppress it for simplicity, in all cases when the Laplacian eigenmaps test is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal, as long as $n^{-1/2} \lesssim M \lesssim n^{1/d}$.}
	\label{tbl:testing_rates}
\end{table}

\paragraph{Related work.}

The majority of work on supervised learning using graphs adopts a \emph{fixed design} perspective, treating the design points $x_1,\ldots,x_n$ as vertices of a fixed graph, and carrying out inference with respect to the conditional mean vector $(f_0(x_1),\ldots,f_0(x_n))$. In this setting, tight upper bounds have been established that certify the optimality of graph-based methods for estimation \citep{wang2016,hutter2016,sadhanala16,sadhanala17,kirichenko2017,kirichenko2018}) and testing \citep{sharpnack2010identifying,sharpnack2013b,sharpnack2013,sharpnack2015} over different ``function'' classes (in quotes because these classes really model the $n$-dimensional vector of evaluations). In particular,~\citet{sadhanala16} and~\citet{sharpnack2015} analyze the Laplacian eigenmaps estimator and test statistic, respectively. This setting is quite general, because the graph need not be a geometric graph defined on a vertex set which belongs to Euclidean space. On the other hand, depending on the data-collection process, it may be unnatural to model the design points as being a priori fixed, and the estimand as being a vector which exhibits a discrete notion of ``smoothness'' over this fixed design. Instead, we adopt the \emph{random design} perspective, and seek to estimate a function that we assume exhibits a more classical notion of smoothness. 

In the random design setting the analysis is more challenging, and only a few works have studied the matter. \cite{zhou2011} provide an asymptotic analysis of the Laplacian eigenmaps estimator in the semi-supervised setting, as the number of unlabeled samples tends to infinity; in this case the estimator reduces to the classical spectral projection estimator, and there is no extra approximation error due to using data-dependent eigenvectors. \cite{lee2016} consider the \emph{diffusion maps} estimator---which uses the eigenvectors of a differently normalized graph Laplacian---in a similar supervised setting to our own, but prove suboptimal rates of convergence relative to the minimax rates. Finally, \citet{trillos2020,green2021} study a penalized least squares method for estimation, which uses a quadratic form involving the Laplacian as a penalty. This method is optimal only for the first-order Sobolev space $H^1(\mc{X})$, and even then only when $d \in \{1,2,3,4\}$, in contrast to the Laplacian eigenmaps estimator which is optimal for all $s$ and $d$; we discuss this striking contrast more in Section~\ref{sec:discussion}.

\paragraph{Roadmap.}
We now outline the structure of the rest of this paper. In Section~\ref{sec:setup_main_results}, we formally setup the models and methods we study. Propositions~\ref{prop:spectral_series_estimation} and ~\ref{prop:spectral_series_testing}, in Section~\ref{sec:spectral_projection}, show that under rather general conditions on the design distribution, classical spectral projection methods achieve minimax rates of convergence. Then in Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps}, \ref{sec:out_of_sample} and and~\ref{sec:manifold_adaptivity}, we give various upper bounds on the error of Laplacian eigenmaps (summarized above), as well as summarizing our analysis at a high-level and providing some key intermediary results. In Section~\ref{sec:experiments} we examine the empirical behavior of Laplacian eigenmaps, and show that even at moderate sample sizes Laplacian eigenmaps is competitive with classical spectral projection. We conclude with some discussion in Section~\ref{sec:discussion}, comparing Laplacian eigenmaps to a few selected estimators and commenting on some computational considerations.

\paragraph{Notation.}
We frequently refer to various classical function classes. For an open set $\mc{X}$ equipped with a volume form $d\mu$, we let $L^2(\mc{X})$ denote the set of functions $f$ for which $\|f\|_{L^2(\mc{X})}^2 := \int f^2 \,d\mu  < \infty$, and equip $L^2(\mc{X})$ with the norm $\|\cdot\|_{L^2(\mc{X})}$. We define $\dotp{f}{g}_P := \int fg\,dP$, and let $L^2(P)$ contain those functions $f$ for which $\|f\|_P^2 := \dotp{f}{f}_P$ is finite. Finally, we let $L^2(P_n)$ consist of those ``functions'' $f: \set{X_1,\ldots,X_n} \to \Reals$ for which the empirical norm $\|f\|_{n}^2 := \frac{1}{n}\sum_{i = 1}^{n} \bigl(f(X_i)\bigr)^2 < \infty$. When there is no chance of confusion, we will sometimes associate functions in $L^2(P_n)$ with vectors in $\Reals^n$, and vice versa. We use $C^k(\mc{X})$ to refer to functions which are $k$ times continuously differentiable in $\mc{X}$, either for some integer $k \geq 1$ or for $k = \infty$. We let $C_c^{\infty}(\mc{X})$ represent those functions in $C^{\infty}(\mc{X})$ with support $V$ compactly contained in $\mc{X}$, meaning $\wb{V} \subseteq \mc{X}$. We write $\partial f/\partial r_i$ for the partial derivative of $f$ in the $i$th standard coordinate of $\Rd$, and use the multi-index notation $D^{\alpha}f := \partial^{|\alpha|}f/\partial^{\alpha_1}x_1\ldots\partial^{\alpha_d}x_d$ for multi-indices $\alpha \in \Reals^d$.

We write $\|\cdot\| = \|\cdot\|_2$ for Euclidean norm, $|\cdot| = \|\cdot\|_1$ for $\ell_1$ norm, and $d_{\mc{X}}(x',x)$ for the geodesic distance between points $x$ and $x'$ on a manifold $\mc{X}$. Then for a given $\delta > 0$, $B(x,\delta)$ is the radius-$\delta$ ball with respect to Euclidean distance, whereas $B_{\mc{X}}(x,\delta)$ is the radius-$\delta$ ball with respect to geodesic distance. Letting $T_x(\mc{X})$ be the tangent space at a point $x \in \mc{X}$, we write $B_m(v,\delta) \subset T_x(\mc{X})$ for the radius-$\delta$ ball centered at $v \in T_x(\mc{X})$.

For sequences $(a_n)$ and $(b_n)$, we use the asymptotic notation $a_n \lesssim b_n$ to mean that there exists a number $C$ such that $a_n \leq C b_n$ for all $n$. We write $a_n \asymp b_n$ when $a_n \lesssim b_n$ and $b_n \lesssim a_n$. On the other hand we write $a_n = o(b_n)$ when $\lim a_n/b_n = 0$, and likewise $a_n = \omega(b_n)$ when $\lim a_n/b_n = \infty$. Finally $a \vee b := \max\{a,b\}$ and $a \wedge b := \min\{a,b\}$.


