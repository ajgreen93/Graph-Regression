\section{Discussion}
\label{sec:discussion}

\subsection{Comparison with other estimators}
In this paper, we have motivated Laplacian eigenmaps by viewing it as a noisy approximation of a classical spectral projection method, which is its most obvious counterpart. We have shown that Laplacian eigenmaps inherits the optimality properties of its more classical counterpart. We now discuss the relationship between Laplacian eigenmaps and three other approaches to nonparametric regression. The first two---nonparametric least squares and kernel smoothing---are classical, whereas the third---Laplacian smoothing---makes use of the graph Laplacian in a different way than Laplacian eigenmaps.

% (SB via AG): Change the name of this to ``spectral projection'' or something like that.
\paragraph{Nonparametric least squares.}
The standard recommended alternative to spectral projection methods, when the distribution $P$ is considered non-uniform or unknown, is to do least-squares. For example, suppose instead of knowing $\psi_1,\psi_2,\ldots$, we had access only to eigenfunctions $\phi_1,\phi_2,\ldots$ of an unweighted Laplace-Beltrami operator $\Delta$. Then letting $\Phi_K = \mathrm{span}\{\phi_1,\ldots,\phi_K\}$, the least-squares estimator and test statistic
\begin{equation*}
\wt{f}_{\mathrm{LS}} := \argmin_{f \in \Phi_K} \|Y - f\|_n^2,\textrm{and}~~\wt{T}_{\mathrm{LS}} = \|\wt{f}\|_{\nu}^2
\end{equation*}
are still rate-optimal over $H_0^s(\mc{X})$. This holds true for both Model~\ref{def:model_flat_euclidean} and~\ref{def:model_manifold}, and is a consequence of our assumption that $p$ is bounded away from $0$.

However, this is not a totally satisfactory fix. For one thing, the least squares approach just outlined requires that we know the domain $\mc{X}$, in the strong sense that we know the Laplace-Beltrami operator $\Delta$ defined on $\mc{X}$. Domain knowledge is generally precious information, and such strong knowledge of $\mc{X}$ seems particularly unrealistic in the case where $\mc{X}$ is a manifold, and $\Delta$ is the manifold Laplace-Beltrami operator. Additionally, even if we know $\mc{X}$, diagonalizing the Laplace-Beltrami operator $\Delta$ is quite difficult for all but a few special domains, such as the unit cube $\mc{X} = [0,1]^d$ or torus $\mc{X} = \mathbb{T}^d$.

% (SB via AG): Maybe also worth discussing the ill-posed counterpart?

\paragraph{Kernel smoothing.}
It is also natural to ask whether the two stage estimator $T_{n,h}\wh{f}$ defined in Section~\ref{sec:out_of_sample} has any advantage over the simpler approach of directly kernel smoothing the responses, i.e. using the estimator $T_{n,h}Y$ (possibly for a different choice of $h$). In Appendix~\ref{subsec:eigenmaps_beats_kernel_smoothing}, we answer this question in the affirmative, by giving a simple example of a sequence of densities and regression functions $\{(p^{(n)}, f_0^{(n)}: n \in \mathbb{N}\}$ such that $\Ebb\|f_0 - T_{n,h}\wh{f}\|_P^2$ is of a strictly lower order than $\inf_{h'} \Ebb\|f_0 - T_{h',n}Y\|_P^2$. This is possible because Laplacian eigenmaps induces a completely different bias than kernel smoothing. For example, when $f_0$ and $p$ satisfy the so-called \emph{cluster} assumption--- e.g. $f_0$ is piecewise constant in high-density regions (clusters) of $p$--- then the bias of Laplacian eigenmaps can much smaller than that of kernel smoothing (for equivalent levels of variance). 

We emphasize that this does not contradict the well-known fact that kernel smoothing is an optimal method for nonparametric regression over e.g. H\"{o}lder balls. It simply reflects that in the standard nonparametric regression setup---which we adopt in the main part of this paper, and in which $P$ is assumed to be equivalent to Lebesgue measure---the biases of Laplacian eigenmaps and kernel smoothing are equivalent. On the other hand, when $f_0$ and $p$ satisfy some special relationship, such as the cluster assumption, the biases of these two methods can be quite different. There has been some work \textcolor{red}{(Rigollet, Wasserman, Niyogi, El Alaoui)} analyzing semi-supervised learning under various instantiations of the cluster assumption. However, a comprehensive analysis of various methods, including Laplacian eigenmaps and kernel smoothing, in this setting remains outstanding.

\paragraph{Laplacian smoothing.}
As mentioned previously, graph Laplacian smoothing uses the graph Laplacian $L_{n,\varepsilon}$ to form the penalty in a penalized least squares estimator. It can be calculated by one solve of a sparse, diagonally dominant linear system. Thus it should be much faster to compute than Laplacian eigenmaps, in which one must find (many) eigenvectors of $L_{n,\varepsilon}$. On the other hand, Laplacian smoothing is known to be minimax rate-optimal only in very limited regimes (over the Sobolev spaces $H^1(\mc{X})$ for $1 \leq d < 4$.) In contrast, we have shown that Laplacian eigenmaps is minimax rate-optimal for all $d$ (when $s = 1$) . We have also shown that Laplacian eigenmaps can adapt to higher-order smoothness, i.e. it can be minimax rate-optimal when $s > 1$. Thus, the known statistical properties of Laplacian eigenmaps are much stronger than those of Laplacian smoothing. 

\subsection{Future Work}
We view our work can be viewed as a contribution both to the fields of nonparametric regression with series estimators, and to graph-based learning. We end our discussion by mentioning some open work in each of these directions. 

% (SB via AG): Add references, and clarify what is meant by used to.
Much is known about classical spectral projection methods beyond their rate optimality. For instance: such estimators and tests exhibit \emph{sharp optimality}, meaning their risk is within a $(1 + o(1))$ factor of the optimal risk; they can adapt to unknown smoothness of the regression function; they can be used to estimate smooth functionals of the regression function;  finally, they can be used to form confidence sets in $L^2(P)$. It would be interesting to see if Laplacian eigenmaps could replicate the performance of classical methods in any, or all, of these problems.

On the other hand, there are many variants of Laplacian eigenmaps worth considering. For instance, one can change the graph under consideration (e.g. by using the k-nearest neighbors), or the normalization of the graph Laplacian $L_{n,\varepsilon}$ (e.g. by using the symmetric normalized Laplacian). The former is practically useful, because it typically leads to connected graphs while always ensuring a given level of edge sparsity. In the latter, the graph Laplacian converges to a different limiting operator, which possesses different eigenvectors than $\Delta_P$ and thereby induces a different bias. We believe that under the setup we consider here, both methods will continue to be optimal.
