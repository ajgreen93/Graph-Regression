\section{Discussion}
\label{sec:discussion}

In this work, we have derived upper bounds on the rates of convergence for methods based on Laplacian Eigenmaps, which imply that in various settings these methods are minimax rate-optimal over Sobolev classes. Importantly, these upper bounds hold under nonparametric conditions on the design density $p$, and allow for $p$ to be unknown and, potentially, supported on a low-dimensional manifold. Our results help explain the practical success of methods which leverage graph Laplacian eigenvectors in downstream learning problems. They also distinguish Laplacian Eigenmaps from more traditional spectral projection procedures, which rely on a density-dependent basis and thus require the density be known a priori.

Of course, there do exist other methods for nonparametric regression which achieve optimal rates of convergence under similar (or indeed weaker) conditions on $p$. These include other graph-based approaches---graph Laplacian regularization---methods besides spectral projection---e.g. kernel smoothing, local polynomial regression, thin-plate splines---and continuum spectral projection methods which use the eigenfunctions of an operator defined independently of $p$. To be clear, we do not advocate Laplacian eigenmaps over these alternatives. Rather, we view our results as theoretically justifying a place for Laplacian eigenmaps in the nonparametric regression toolbox. 

That being said, Laplacian eigenmaps can have advantages over each of the aforementioned approaches. We now conclude by outlining some of these advantages, limiting our discussion to estimation:
\begin{itemize}
	\item \emph{Optimality over low-smoothness Sobolev spaces}. Graph Laplacian regularization achieves minimax optimal rates over $H^1(\mc{X})$ only when $d \in \{1,2,3,4\}$ \citep{sadhanala16, green2021}; when $d \geq 5$, graph Laplacian regularization appears to be suboptimal. In contrast, Laplacian eigenmaps is optimal for all dimensions $d$.  An even more stark contrast can be drawn between Laplacian eigenmaps and the first order thin-plate spline, the solution of the variation problem
	\begin{equation}
	\underset{f \in H^1(\mc{X})}{\textrm{minimize}} \quad  \|{\bf Y} - f\|_n^2 + \lambda \|f\|_{H^1(\Reals^d)}^2,
	\end{equation}
	which is optimal and indeed well-posed only when $d = 1$. This appears to be related to a general phenomenon described by~\cite{dhillon2013,dicker2017}, in which principal components analysis followed by ordinary least-squares is shown to be always competitive with, and sometimes much better than, ridge regression. 
	\item \emph{Manifold adaptivity}. An oft-recommended alternative to the population-level spectral projection methods considered in Section~\ref{subsec:spectral_projection} is to run ordinary-least squares regression using eigenfunctions of a density-independent operator, such as the Laplacian operator $\Delta = \sum_{i = 1}^{d} \partial^2f/\partial x_i^2$. Under the conditions of Model~\ref{def:model_flat_euclidean}, such a method will indeed be rate-optimal, though the upper bounds may come with undesirably large constants if $p$ is very non-uniform. However, in contrast with Laplacian eigenmaps, under Model~\ref{def:model_manifold} this method cannot achieve the faster minimax rates of convergence.
	\item \emph{Density adaptivity}. It is natural to ask whether the two stage estimator $T_{n,h}\wh{f}$ defined in Section~\ref{sec:out_of_sample} has any advantage over the simpler approach of directly kernel smoothing the responses, i.e. using the estimator $T_{n,h}{\bf Y}$ (possibly for a different choice of $h$). In Appendix~\ref{subsec:eigenmaps_beats_kernel_smoothing}, we answer this question in the affirmative, by giving a simple example of a sequence of densities and regression functions $\{(p^{(n)}, f_0^{(n)}: n \in \mathbb{N}\}$ such that $\Ebb\|f_0 - T_{n,h}\wh{f}\|_P^2$ is of a strictly lower order than $\inf_{h'} \Ebb\|f_0 - T_{h',n}{\bf Y}\|_P^2$. This is possible because Laplacian eigenmaps induces a completely different bias than kernel smoothing. For example, when $f_0$ and $p$ satisfy the so-called \emph{cluster} assumption--- e.g. $f_0$ is piecewise constant in high-density regions (clusters) of $p$--- then the bias of Laplacian eigenmaps can much smaller than that of kernel smoothing (for equivalent levels of variance). 
	
	We emphasize that this does not contradict the well-known optimality properties of kernel smoothing over e.g. H\"{o}lder balls. Rather, in the standard nonparametric regression setup---which we adopt in the main part of this paper, and in which $P$ is assumed to be equivalent to Lebesgue measure---the biases of Laplacian eigenmaps and kernel smoothing happen to be equivalent. But in general this is not the case.
\end{itemize}
Grounding each of these three points on a firmer and more complete theoretical basis would be, in our view, a valuable direction for future work.