\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue
}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{tikz}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}
\newcommand{\Lap}{{\bf L}}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{FS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Minimax-optimal regression over Sobolev Spaces with Neighborhood Graphs}
\author{Alden Green}
\date{\today}
\maketitle

\section{Introduction}
\label{sec:introduction}

\textbf{(1) Geometric graphs.} Suppose one observes data $X_1,\ldots,X_n$ sampled independently from an unknown distribution $P$, and forms a geometric graph $G$---with edges $e_{ij}$ corresponding to proximity between samples $X_i$ and $X_j$---over the observed data. Geometric graphs encode information about $P$ in an extremely general manner, and have thus been leveraged to conduct many different fundamental statistical tasks. These include clustering, semi-supervised learning, classification, and regression---both estimation and goodness-of-fit testing. Though much theoretical work has been done on the consistency of graph-based learning methods---and more recently, rates of convergence have been established for some problems---little is known so far about their optimality, even for classic statistical tasks. 

\textbf{(2) What we study: Regression and Laplacian eigenmaps.} In this paper we focus on regression, where in addition to the design points $X_1,\ldots,X_n$ one observes real-valued responses $Y_1,\ldots,Y_n$, and seeks to infer the regression function $f_0(x) := E[Y|X = x]$. The method we will study is \emph{Laplacian eigenmaps}, first introduced by \textcolor{red}{(Belkin 2003)}, which constructs an estimator $\wh{f}$ using eigenvectors of the graph Laplacian $L$ as features in a least-squares problem. The graph Laplacian $L$ is a difference operator, acting on functions $f: X \to \Reals$, 
\begin{equation}
\label{eqn:graph_laplacian}
Lf(X_i) = \sum_{j = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr)e_{ij},
\end{equation}
The Laplacian $L$ can be seen as a discretization of the Laplace-Beltrami operator \textcolor{red}{(...)}. The eigenvectors of $L$ are an orthonormal basis of $L^2(X)$, which can seen as estimates of the $L^2(P)$ orthonormal basis \textcolor{red}{(...)}. Their corresponding eigenvalues  are estimates of~\textcolor{red}{(...)}, and provide a notion of smoothness for each eigenvector---the smaller the eigenvalue, the smoother the eigenvector. 

\textbf{(3) Why we study.} Laplacian eigenmaps is thus a type of \emph{spectral series} estimator. Spectral series estimators are one of the most classical ways to perform non-parametric regression. Traditional spectral series estimators use an \emph{a priori defined basis} \textcolor{red}{(...)}, and are typically defined with respect to some reference measure $Q$. In contrast, eigenvectors of the graph Laplacian are data-dependent, and adapt to the geometry of the distribution $P$ in a rich manner. 

The study of (classical) spectral series estimators is rich, and their theoretical optimality is well-understood \textcolor{red}{(references)}. However, in practice these estimators suffer from some serious drawbacks.

\textbf{(4) Our contributions.} Laplacian eigenmaps avoid these drawbacks \textcolor{red}{(...)} On the other hand their statistical properties are not as well understood \textcolor{red}{(...)}. The primary contribution of our paper is to fill this theoretical gap, by answering the following question:

\begin{quote}
	\textcolor{red}{(TODO)}
\end{quote}

We show that under various classical assumptions on the regression function $f_0$ and design distribution $P$, Laplacian eigenmaps is statistically minimax optimal. \textcolor{red}{(TODO): Add a richer summary of your major results here.} 

\textbf{(5) Organization.}

\section{Setup and Background}
\label{sec:setup_main_results}

In this section, we give a precise setup of the non-parametric regression problems and methods we will subsequently consider, review the minimax rates for non-parametric regression over Sobolev spaces, and summarize our main results.

\subsection{Non-parametric regression with Laplacian Eigenmaps}
\label{sec:regression_laplacian_eigenmaps}

\textbf{(1) Formal setup.}
Throughout, we assume that we observe independent random samples $(X_1,Y_1),\ldots,(X_n,Y_n)$, where $X_1,\ldots,X_N$ are drawn i.i.d from a distribution $P$ supported on a compact set $\mc{X} \subseteq \Rd$, and 
\begin{equation*}
Y_i = f_0(X_i) + \varepsilon_i,
\end{equation*}
with $\varepsilon_i \sim N(0,1)$ independent Gaussian noise with unit variance.

\textbf{(2) Laplacian eigenmaps.}

\subsection{Sobolev Classes}
\label{sec:sobolev}
We now review the Sobolev classes $H^s(\mc{X})$. 

\textbf{(1) Sobolev norms, semi-norms, and balls.} 

\textbf{(2) Zero-trace conditions.}

\subsection{Minimax Rates}
\label{subsec:minimax_rates_sobolev}

\textbf{(1) Estimation rates.}

\textbf{(2) Testing rates.}

\textbf{(3) Conditions under which spectral series methods achieve these rates.}

\textbf{(4) Comparison with the conditions necessary for Laplacian Eigenmaps.}

\subsection{Our contributions}
The following items summarize our major results.
\begin{itemize}
	\item \textbf{Minimax optimal estimation.} With high probability, $\norm{\wh{f} - f_0}_{n}^2 \lesssim n^{-2/(2 + d)}$ over all $f_0 \in H^1(\Xset;M)$. 
	\item \textbf{Minimax optimal testing.}
	A test constructed using $T_{\LE}$ has non-trivial power whenever $f_0 \in H^1(\Xset;M)$ satisfies $\norm{f_0}_{\Leb^2(\Xset)} \gtrsim n^{-2/(4 + d)}$ and $d < 4$.
	\item \textbf{Taking advantage of higher-order smoothness conditions.} When the regression function $f_0 \in H^s(\Xset)$ and additionally the density $p \in C^{s - 1}(\mc{X})$, with high probability the error $\norm{\wh{f}_{\LE} - f_0}_{n}^2 \lesssim n^{-2s/(2s + d)}$ over all $f_0 \in H_0^s(\Xset;M)$. Additionally, a test constructed using $T_{\LE}$ has non-trivial power whenever $f_0 \in H_0^s(\Xset;M)$ satisfies $\norm{f_0}_{\Leb^2(\Xset)} \gtrsim n^{-2s/(4s + d)}$ and $d < 4$.
	\item \textbf{Manifold adaptivity.}
	If $\mc{X} \subset \Rd$ is a submanifold of dimension $m < d$, each of the aforementioned rates hold with $d$ replaced by $m$.
\end{itemize}
\textbf{(1) Major takeaway.} Viewed as a whole, our results show that Laplacian eigenmaps retain the strong theoretical properties of classic orthogonal series approaches, while also automatically \emph{adapting} to an unknown design distribution $P$.

\subsection{Related Work}
\begin{itemize}
	\item \cite{lee2016} considers a similar estimator --- the notable difference being the normalization of the Laplacian $L$ --- but get suboptimal rates for the supervised problem. 
	\item \cite{zhou2011} consider a similar estimator, but in the semi-supervised setting. They consider the limit as the number of unlabeled points grows to infinity, in which case the estimator is simply a classic spectral series estimator, and the analysis is standard. 
\end{itemize}

\section{Minimax Optimality of Laplacian Eigenmaps}
\label{sec:minimax_optimal_laplacian_eigenmaps}

In this section, we will show that the estimator $\wh{f}$, and a test using the statistic $\wh{T}$, achieve optimal estimation and goodness-of-fit testing rates over Sobolev classes. Throughout this section, we will assume $\mc{X}$ is a \textcolor{red}{regular, $d$-dimensional domain} and $P$ has a \textcolor{red}{regular density with respect to Lebesgue measure.}

\subsection{First-order Sobolev classes}
\label{sec:first_order_sobolev_classes}
To begin, we handle the case where $f_0 \in H^1(\mc{X})$.

\textbf{(1) Laplacian eigenmaps is an optimal estimator.} Under appropriate choices of $r$ and $\kappa$, the estimator $\wh{f}$ has (in-sample) optimal risk (up to constant factors) $\|\wh{f} - f_0\|_n^2 \lesssim n^{-2/(2 + d)}$. 

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_fo}
	\textcolor{red}{(TODO)}
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_fo}
	\|\wh{f} - f_0\|_n^2 \lesssim n^{-2/(2 + d)}
	\end{equation}
\end{theorem}
Some remarks:
\begin{itemize}
	\item \textbf{(1.1) How to tune hyper-parameters.} In practice, one typically tunes the hyper-parameters of an algorithm by sample-splitting or (more usually) by cross-validation. However, because $\wh{f}$ is defined only in-sample such approaches cannot be straightforwardly applied to select the graph radius $r$, or number of eigenvectors $\kappa$. We discuss out-of-sample extensions of $\wh{f}$ later, in Section~\ref{sec:out_of_sample}.
	\item \textbf{(1.2) The dependence on failure probability can be improved.} The upper bound on in-sample risk given by Equation~\eqref{eqn:laplacian_eigenmaps_estimation_fo} holds with probability $1 - C\delta$, and depends on a factor of $1/\delta$. When $f \in C^1(\mc{X})$, we can show that this depends on a factor of $(1 + 1/\delta)$, and holds with probability at least $1 - C\delta^2/n$. 
\end{itemize}

\textbf{(2) Laplacian eigenmaps is an optimal test.} Consider the test $\phi = \1\{\wh{T} \geq t\}$, where $t$ is the data-dependent threshold~\textcolor{red}{(...)}. The choice of $t$ guarantees that $\phi$ is a level-$\alpha$ test. Under appropriate choices of $r$ and $\kappa$, the test $\phi$ has non-negligible power (up to constant factors) against alternatives separated the null by at least $\|f_0\|_{\Leb^2(\mc{X})} \gtrsim n^{-4/(4 + d)}$. 

\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_fo}
	\textcolor{red}{(TODO)}
\end{theorem}
Some remarks:
\begin{itemize}
	\item \textbf{(2.1) When $d \geq 4$, the testing problem is different.} When $d \geq 4$, it is not the case that $\Leb^4(\mc{X})$ continuously embeds into $H^1(\mc{X})$, and testing rates over $H^1(\mc{X})$ are not known. On the other hand, if we explicitly assume $f_0 \in \Leb^4(\mc{X})$, then simply taking $T = \|Y\|_n^2$ achieves the optimal rates.
	\item \textbf{(2.2) Both of the remarks after Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} also apply to testing.}
\end{itemize}

\subsection{Higher-order Sobolev classes}
\label{sec:higher_order_sobolev_classes}
In this section, we handle the case where $f_0 \in H_0^s(\mc{X})$.

\textbf{(1) Laplacian eigenmaps is an optimal estimator.}  Under appropriate choices of $r$ and $\kappa$, the estimator $\wh{f}$ has (in-sample) optimal risk (up to constant factors) $\|\wh{f} - f_0\|_n^2 \lesssim n^{-2s/(2s + d)}$. 
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_ho}
	Under appropriate conditions, with probability at least $1 - \delta$,
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_ho}
	\|\wh{f} - f_0\|_n^2 \lesssim n^{-2s/(2s + d)}
	\end{equation}
\end{theorem}
Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, in combination with Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo}, implies that Laplacian eigenmaps is a rate-optimal estimator over Sobolev classes, for all scalings of $s$ and $d$. Some other remarks:
\begin{itemize}
	\item \textbf{(1.1) Optimal rates, no RKHS.} It is worth pointing out that we do not require that $2s > d$, a condition often seen in the literature. When $2s \leq d$, the Sobolev space $H^s(\mc{X})$ is quite irregular. For instance, it is not a Reproducing Kernel Hilbert Space, nor does it embed into any H\"{o}lder space. This renders certain versions of the nonparametric regression problem ill-posed---for instance when the design points $X$ are assumed to be fixed, or loss is measured in $\Leb^{\infty}$ norm. It also takes many estimators ``off the table'', most notably including RKHS-based methods such as smoothing splines. However, at these scales traditional spectral series estimators still obtain the optimal rates for random design regression measured in $\Leb^2$ norm. Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho} shows that the same is true with respect to Laplacian eigenmaps.
	\item \textbf{(1.2) The zero-trace condition.} The zero-trace condition can likely be weakened. However, requiring some boundary condition on $f_0$ and its first through $(k -1 )$th-order derivatives is unavoidable. \textcolor{red}{Explain why: graphs are implicitly boundaryless objects, and the eigenvectors of $L$ converge to those eigenvectors of $\Delta_P$ which satisfy boundary conditions. Repeat that this is a standard assumption made when analyzing spectral series estimators}.
	\item \textbf{(1.3) The bounded norm and smooth density conditions.} We require that $f_0$ be bounded in the Sobolev norm $\|f_0\|_{H^s(\mc{X})}$---as opposed to the semi-norm $|f_0|_{H^s(\mc{X})}$--- and that $p \in C^{s - 1}(\mc{X})$. Both requirements are related and fundamental, as we shall now explain. As previously mentioned, $L$ converges to a weighted-version of the Laplace operator $\Delta_P$ (also known as a Fokker-Planck) operator. The operator $\Delta_P$ includes both first and second-order derivatives,
	\begin{equation}
	\label{eqn:fokker_planck_1}
	\Delta_Pf= \frac{-1}{p} \mathrm{div}(p \nabla f) = \Delta f - \frac{\nabla p^{\top} \nabla f}{p},
	\end{equation}
	and therefore a function $f \in H_0^s(\mc{X})$ has finite energy $\|\Delta_P f\|_{P}^2$ only if the first-order partial derivatives of $f$ and $p$ are bounded. \textcolor{red}{This discussion---and in particular the last sentence---are obviously not quite correct. This is in part because you realize now that the requirements are not in fact fundamental. Instead, the requirement should merely be that $\|\Delta_P^kf\|_{\Leb^2(P)}^2$ is bounded---here $k = s/2$ and we assume $s$ is even---and the upper bounds should be in terms of this semi-norm.}.
	\item \textbf{(1.4) Both of the remarks after Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} continue to apply.}
\end{itemize}

\textbf{(1.4) Rates for testing with higher-order smoothness assumptions.} For testing, we can get the optimal rate $n^{-4s/(4s + d)}$ when $d \leq 4$. When $4 < d \leq 4s$, we get the rate~\textcolor{red}{(...)} Recall that the rate $n^{-4s/(4s + d)}$ is optimal whenever $d \leq 4s$. Therefore there are scalings of $s$ and $d$ for which we do not get the optimal rate. We do not know whether this due to looseness in our proof techniques, or a problem with the test itself.

\subsection{Analysis}
\label{sec:analysis}

\textbf{(1) Bias and variance terms, for estimation and testing}. We analyze the estimation error of $\wh{f}$ by first condition on the design points $X$ and computing \emph{design-dependent} bias and variance terms. For estimation, we have
\begin{equation*}
\Ebb\bigl[\|\wh{f} - f_0\|_n^2|X\bigr] \leq C\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{\kappa}^s} + \frac{\kappa}{n}
\end{equation*}
For testing, we have~\textcolor{red}{(...)}.

In the above, the semi-norm $\dotp{L^s f_0}{f_0}_n$ and eigenvalue $\lambda_{\kappa}$ are random variables that depend the design $X$. It remains to derive suitable upper and lower bounds on these quantities. 

\textbf{(2) Upper bound on the semi-norm. } Lemma~\ref{lem:graph_seminorm} gives a sufficient upper bound on $\dotp{L^s f_0}{f_0}_n$. Take $\beta := s - 1$. 
\begin{lemma}
	\label{lem:graph_seminorm} 
	Suppose $f \in H_0^s(\mc{X})$, that $p \in C^{\beta}(\mc{X})$, and that $r \gtrsim n^{-1/d}$. Then with probability at least $1 - \delta$, it holds that 
	\begin{equation}
	\label{eqn:graph_seminorm_1}
	\dotp{L^s f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^s} \|p\|_{C^{\beta}} + \textcolor{red}{(?)}
	\end{equation}
	Therefore, if additionally $r \gtrsim n^{-1/(2\beta + d)}$, then 
	\begin{equation}
	\label{eqn:graph_seminorm_2}
	\dotp{L^s f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^s} \|p\|_{C^{\beta}}
	\end{equation}
\end{lemma}

\textbf{(3) Lower bound on the eigenvalue.} On the other hand, recent work \textcolor{red}{(Burago14, Shi16, Garcia Trillos 18)} has analyzed the convergence of $\lambda_{k}$ towards $\lambda_{k}(\Delta_P)$. They provide explicit bounds on the relative error $|\lambda_{k} - \lambda_{k}(\Delta_P)|/\lambda_{k}(\Delta_P)$, showing that the relative error is small for sufficiently large $n$, small $\varepsilon$, and any $1 \leq k \leq n$ that is not too large relative to $n$. These results are actually stronger than are necessary to establish Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_estimation_ho}---note that we need only show that $\lambda_{\kappa}$ is lower bounded by the right order of magnitude---but unfortunately they all assume $P$ is supported on a manifold without boundary. We will instead use the results of \textcolor{red}{(Green 2021)}, whose assumptions match our own, and who give a weaker bound on $\lambda_k/\lambda_k(\Delta_P)$ that will nevertheless suffice for our purposes. 

\begin{lemma}
	\textcolor{red}{(TODO)}
	With probability at least $1 - \textcolor{red}{(?)}$,
	\begin{equation*}
	\lambda_k \geq c \cdot \min\Bigl\{\lambda_k(\Delta_P), \frac{1}{r^{2}} \Bigr\}
	\end{equation*}
\end{lemma}
By Weyl's Law, $c k^{2/d} \leq \lambda_{k}(\Delta_P) \leq Ck^{2/d}$, and as a result $\lambda_{\kappa} \geq C\lambda_{\kappa}(\Delta_P)$ as long as $r \geq \kappa^{-1/d}$. 

\textbf{(1.2) Traps we avoid.} It is tempting to use recent results regarding spectral convergence to analyze the estimator $\wh{f}$ and test $\wh{T}$. This suffers from an accumulation of error problem. 

It is also key that we directly analyze the semi-norm $\dotp{L^s f}{f}_n$, rather than invoking the pointwise convergence of $L^{s}f \to \Delta_P^{s}f$ to obtain a bound on $\dotp{L^s f}{f}_n$. Recall that we assume $f$ has bounded derivatives up to (and including) order $s$. This suffices to gain control of the semi-norm $\dotp{L^s f}{f}_n$, because $\dotp{L^s f}{f}_n$ approximates the $\Leb^2$ norm of an order $s$ differential operator, and so we have exactly as many derivatives as we need. On the other hand, $L^s$ approximates an order $2s$ differential operator, and $\Delta_P^sf$ is only a well-defined limiting object if we assume $f$ has order $2s$ derivatives. 


\section{Manifold Adaptivity}
\label{sec:manifold_adaptivity}

The optimal rates for nonparametric regression suffer from a bona fide curse of dimensionality, and are thus difficult to reconcile with the practical success of many methods for nonparametric regression. One explanation for this seeming paradox is the \emph{manifold hypothesis}, where the design points $X$ are assumed to lie on or near a manifold $\mc{X}$ of intrinsic dimension $m \ll d$. Under the manifold hypothesis nonparametric regression is demonstrably easier, and in particular it is known \citep{bickel2007,ariascastro2018} that in this setting the optimal rates over $H^s(\Xset)$ scale like $n^{-2s/(2s + m)}$ (for estimation) and $n^{-4s/(4s + m)}$ (for testing). 

On the other hand, a theory has been developed~\citep{belkin03,belkin05,niyogi2013} establishing the the neighborhood graph $G_{n,r}$ can ``learn'' the manifold $\Xset$ in various senses, so long as $\Xset$ is locally linear. In this section, we contribute to this line of work by showing that under the manifold hypothesis, Laplacian eigenmaps achieve the sharper minimax estimation and testing rates. First, we give some relevant background on Riemmanian manifolds, and Sobolev classes defined upon them.

\subsection{Riemmanian manifolds}

\subsection{Error rates under the manifold hypothesis}

\section{Out-of-sample error}
\label{sec:out_of_sample}
Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps} and~\ref{sec:manifold_adaptivity} highlights the statistical optimality of nonparametric regression with Laplacian eigenmaps. However, with regards to estimation, the error has always been measured in $\|\cdot\|_n$ norm---in words, the average squared loss incurred over the $n$ observed design points. This is somewhat unusual. Instead, it is more typical to consider $L^2(P)$ error---meaning error with respect to the $\|\cdot\|_{P}$ norm---in large part because $L^2(P)$ error is closely tied to the expected prediction error an estimator would make at a new point $X_{n + 1}$.

Of course, the Laplacian eigenmaps estimator is only defined at the observed design points $X_1,\ldots,X_n$, and to measure its error in $L^2(P)$ norm we must first give a means of extending it to the rest of $\Xset$. We propose a simple method, essentially kernel smoothing, to do the job, which can be applied to any estimator at data, including Laplacian eigenmaps. We show that the smoothed version of $\wh{f}$ has optimal $L^2(P)$ error. 

\subsection{Extending Laplacian Eigenmaps with Kernel Smoothing}

\begin{theorem}
	\label{thm:kernel_smoothing}
	\textcolor{red}{(TODO)}
\end{theorem}

Some remarks:
\begin{itemize}
	\item \textbf{Tuning parameters by sample-splitting.}
	\item \textbf{Other extensions.} Comment that there does not exist any canonical way to extend Laplacian based estimators such as $\wh{\theta}$ to an ambient domain. Point out that there exist various proposals to extend eigenvectors in the literature, such as $1$-nearest-neighbor regression (piecewise constant extrapolation over Voronoi cells), convolution with a specialized kernel, or Nystr\"{o}m extension. Explain that each of these is designed for a highly specialized purpose, such as suitability for mathematical analysis or computational efficiency, and in any case are suggested only for eigenvectors rather than a regression estimate. State explicitly that we choose kernel smoothing for its simplicity, ubiquity, and strong theoretical properties under our assumptions, but that considerations of extensions for specifically tailored to Laplacian based estimators could be of interest.
\end{itemize}


\subsection{Why not just do kernel smoothing?}
On the other hand, the allusion to kernel smoothing raises a very natural question: why should we prefer to first compute $\wh{\theta}$ and smooth it out, as opposed to simply performing kernel smoothing over the original responses $Y$. We now answer this question, by \textcolor{red}{(...)}.

\section{Experiments}
\label{sec:experiments}

\section{Discussion}
\label{sec:discussion}


\bibliographystyle{plainnat}
\bibliography{../../../graph_testing_bibliography} 

\end{document}