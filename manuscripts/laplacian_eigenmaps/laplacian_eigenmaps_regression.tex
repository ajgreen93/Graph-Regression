\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue
}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{bm}
\usepackage{multirow}

\usepackage{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}
\DeclareMathAccent{\wc}{0}{mathx}{"71}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\RD}{\Reals^D}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}
\newcommand{\Lap}{{\bf L}}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{FS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Minimax-optimal Laplacian Eigenmaps regression over Sobolev Spaces with Neighborhood Graphs}
\author{Alden Green}
\date{\today}
\maketitle

\section{Introduction}
\label{sec:introduction}

Suppose we observe data $X_1,\ldots,X_n$ sampled independently from an unknown distribution $P$. As a replacement for $P$, we form a geometric graph $G$ over the observed data, with weighted edges $W_{ij}$ corresponding to proximity between samples $X_i$ and $X_j$. Geometric graphs encode information about $P$--- for instance, the number of connected components of its support, and the shape of each connected component---in an extremely general manner, and can be leveraged to conduct many different fundamental statistical tasks. These include clustering, manifold learning, semi-supervised learning, classification, and regression---both estimation and goodness-of-fit testing. Though much theoretical work has been done on the consistency of learning methods defined with respect to geometric graphs---and more recently, rates of convergence have been established for some methods---little is known so far about their optimality, even for classic statistical tasks. 

In this paper we focus on the theoretical statistical properties of \textcolor{red}{graph-based regression}. We assume that in addition to the design points $X_1,\ldots,X_n$ one observes real-valued responses $Y_1,\ldots,Y_n$, and seeks to learn the unknown regression function $f_0(x) := E[Y|X = x]$. The methods we will study are based on \emph{Laplacian eigenmaps}, first introduced by \cite{belkin03a}, which takes the eigenvectors of the graph Laplacian $L$. The Laplacian $L$ is a difference operator, acting on functions $f: \{X_1,\ldots,X_n\} \to \Reals$ as follows,
\begin{equation}
\label{eqn:graph_laplacian}
Lf(X_i) = \sum_{j = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr)W_{ij}. 
\end{equation}
The Laplacian $L$ can be seen as a discretization of the Laplace-Beltrami operator $\Delta_P$, defined as
\begin{equation}
\label{eqn:fokker_planck_1}
\Delta_Pf= \frac{-1}{p} \mathrm{div}(p^2 \nabla f) = \Delta f - 2\frac{\nabla p^{\top} \nabla f}{p}.
\end{equation}
The eigenvectors of $L$ form an orthonormal basis of $L^2(X)$, and are approximations to the eigenvectors of $\Delta_P$. Their corresponding eigenvalues are estimates of~\textcolor{red}{(...)}, and provide a notion of smoothness for each eigenvector---the smaller the eigenvalue, the smoother the corresponding eigenvector. 

Laplacian eigenmaps is thus an example of a \emph{spectral series} estimator. A spectral series estimator is a special case of an orthogonal series estimator, one of the most classical methods of non-parametric regression. Traditionally, orthogonal series estimators are defined with respect to a fixed set of basis functions, orthogonalized in $L^2(Q)$ with respect to some pre-determined reference measure $Q$. Spectral series estimators take a particular orthogonal basis, the eigenfunctions of $\Delta_Q$. In contrast, eigenvectors of the graph Laplacian are data-dependent objects, and adapt to the geometry of the design distribution $P$ in a rich manner. 

The theoretical properties of (classical) spectral series methods are various, and are in general extremely well-understood \textcolor{red}{(references)}. In particular, they are minimax rate optimal for nonparametric regression over H\"{o}lder and Sobolev spaces. However, in practice these methods suffer from some serious drawbacks. As a practical matter,  even finding the eigenfunctions of $\Delta_Q$ is in general non-trivial. The statistical optimality of such estimators also hinges on the reference measure $Q$ being equal to $P$, an unrealistic assumption when $P$ is unknown. If $Q \neq P$, the standard recommendation is to instead perform nonparametric least squares---this can be statistically rate-optimal, but is numerically unstable when the dimension $d$ is even moderately large. 

\textcolor{red}{(TODO)}: Change the preceding paragraph.

Laplacian eigenmaps avoids these drawbacks \textcolor{red}{(...)} On the other hand its statistical properties are not as well understood \textcolor{red}{(...)}. Thus, there exists a theoretical gap. 

\paragraph{Our contributions.} The primary contribution of our paper is to fill this theoretical gap, by answering the following question:

\begin{quote}
	\textcolor{red}{(TODO)}
\end{quote}

Broadly speaking, we show that when the regression function $f_0$ is smooth, in the Sobolev sense of having weak derivatives bounded in $\Leb^2$-norm, Laplacian eigenmaps methods are statistically minimax optimal, for both estimation and goodness-of-fit testing. This statement holds for different relations between the dimension $d$ and number of derivatives $s$, depending on the problem (estimation or testing).
\begin{table}
	\begin{center}
		\begin{tabular}{p{.2\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			$s \leq 3$ & $\bm{n^{-2s/(2s + d)}}$ & $\bm{n^{-2s/(2s + m)}}$ \\
			$s > 3$  & $\bm{n^{-2s/(2s + d)}}$ & $n^{-6/(6 + m)}$
		\end{tabular}
	\end{center}
	\caption{Summary of estimation rates over Sobolev balls. Bold font marks minimax optimal rates. In each case, rates hold for all $d \in \mathbb{N}$ (under Model~\ref{def:model_flat_euclidean}), and for all $d, m \in \mathbb{N}$ (under Model~\ref{def:model_manifold}). Although we suppress it for simplicity, in all cases the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal, as long as $M \gtrsim n^{-1/2}$.}
	\label{tbl:estimation_rates}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{p{.175\textwidth} p{.175\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Dimension & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			\multirow{2}{*}{$s = 1$} & $\dim(\mc{X}) < 4$ & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $\dim(\mc{X}) \geq 4$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s = 2$ or $3$} & $\dim(\mc{X}) \leq 4$  & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $4 <\dim(\mc{X}) < 4s$  & $n^{-2s/(2(s - 1) + d)}$ & $n^{-2s/(2(s - 1) + m)}$\\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s > 3$} & $\dim(\mc{X}) \leq 4$ & $\bm{n^{-4s/(4s + d)}}$ & $n^{-12/(12 + d)}$ \\
			& $4 < \dim(\mc{X}) < 4s$ & $n^{-2s/(2(s - 1) + d)}$ & $n^{-6/(4 + m)}$ \\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
		\end{tabular}
	\end{center}
	\caption{Summary of testing rates over Sobolev balls. Bold font marks minimax optimal rates. Rates when $d > 4s$ assume that $f_0 \in L^4(P,M)$. Although we suppress it for simplicity, in all cases the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal, as long as $M \gtrsim n^{-1/2}$.}
	\label{tbl:testing_rates}
\end{table}

\paragraph{Related Work.} 
\begin{itemize}
	\item \textcolor{red}{(TODO)} Summarize what is known on fixed graphs.
	\item \textcolor{red}{von Luxburg, Belkin, Slepcev} show that for a fixed index $k$, the eigenvector $v_k$ converges to the eigenfunction $\psi_k$, in the sense that \textcolor{red}{(...)}. \textcolor{red}{Shi, Burago, Garcia Trillos, Calder, Cheng} build on this, giving finite sample bounds, rates of convergence, and making statements uniform over $1 \leq k \leq k_{\max}$. This suggests that Laplacian eigenmaps may be good alternatives to classical spectral series methods when the latter cannot be implemented (e.g. because $P$ or $\mc{X}$ are unknown.) 
	\item There has been limited analysis of the estimator $\wh{f}$. \cite{zhou2011} consider a similar estimator, but in the semi-supervised setting where the number of unlabeled points grows to infinity. In this case the estimator reduces to a classical spectral series estimator, and there is no error incurred by approximating $\Delta_P$. \cite{lee2016} consider the \emph{diffusion maps} estimator---which uses the eigenvectors of a different normalization of the Laplacian $L$---in both the supervised and semi-supervised setups, but they derive suboptimal rates for the supervised problem. As far as we know, there has been no analysis of the test statistic $\wh{T}$ in the random design framework which we study.
	\item \textcolor{red}{(TODO)}: Alternatively, \cite{trillos2020} \textcolor{blue}{(Green 2021)} use the graph Laplacian operator to induce a penalty over functions $f: \{\bf X\} \to \Reals$. The \emph{Laplacian smoothing} estimator $\wh{f}_{\mathrm{LS}}$ is obtained by minimizing the sum of this penalty along with a data-fidelity term,
	\begin{equation*}
	\wh{f}_{\mathrm{LS}} := \|Y - f\|_n^2 + \lambda \dotp{L_{n,\varepsilon}f}{f}_n.
	\end{equation*}
	\textcolor{blue}{(Green 2021)} show that the resulting estimator is minimax optimal, but only for $s = 1$ and $d \leq 4$. In contrast, the Laplacian eigenmaps estimator is optimal for all $s$ and $d$. 
\end{itemize}

\textbf{(5) Organization.}

\textcolor{red}{(TODO)}

\textbf{(6) Notation.}

\textcolor{red}{TODO}: 

\begin{itemize}
	\item Define $\Leb^2(\mc{X}), \Leb^2(P)$ and $\Leb^2(P_n)$. Define $C^k(\mc{X})$, its norm, and its normed ball.
	\item Define asymptotic notation: $\ll$, $\lesssim$ and $\asymp$.
	\item Define min and max notation: $\vee$ and $\wedge$.
\end{itemize}



\section{Setup and Background}
\label{sec:setup_main_results}

In this section, we begin by giving a precise definition of our framework, and the Laplacian eigenmaps methods we study. We then review minimax rates for non-parametric regression over Sobolev spaces.

\subsection{Non-parametric regression with random design}
\label{sec:regression_laplacian_eigenmaps}

We will operate in the usual setting of non-parametric regression with random design, in which we observe independent random samples $(X_1,Y_1),\ldots,(X_n,Y_n)$. The design points $X_1,\ldots,X_n$ are sampled from a distribution $P$ with support $\mc{X} \subseteq \Rd$, and the responses follow
\begin{equation}
\label{eqn:model}
Y_i = f_0(X_i) + w_i,
\end{equation}
with regression function $f_0: \mc{X} \to \Reals$, and $w_i \sim N(0,1)$ independent Gaussian noise. For simplicity we will assume throughout that the noise has unit-variance, but all of our results extend in a straightforward manner to the case where the variance is equal to a known positive value. 

We now formulate two models, which differ in the assumed nature of the support $\mc{X}$ of the design distribution $P$: the \emph{flat Euclidean} and \emph{manifold} models.

\begin{definition}[Flat Euclidean model]
	\label{def:model_flat_euclidean}
	The data $(X_1,Y_1),\ldots,(X_n,Y_n)$ are sampled according to~\eqref{eqn:model}. The support $\mc{X}$ of the design distribution $P$ is an open, connected, and bounded subset of $\Rd$, with Lipschitz boundary. The distribution $P$ admits a density $p$ with respect to the $d$-dimensional Lebesgue measure $\nu$, which is bounded away from $0$ and $\infty$,
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty, \quad \textrm{for all $x \in \mc{X}$.}
	\end{equation*}
\end{definition}

In the following, recall that the injectivity radius of a $m$-dimensional Riemannian manifold $\mc{X}$ is the maximum value of $\delta$ such that the exponential map $\exp_x: B_m(0,\delta) \subset T_x(\mc{X}) \to B_{\mc{X}}(x,\delta) \subset \mc{X}$ is a diffeomorphism for all $x \in \mc{X}$.
\begin{definition}[Manifold model]
	\label{def:model_manifold}
	The data $(X_1,Y_1),\ldots,(X_n,Y_n)$ are sampled according to~\eqref{eqn:model}. 
	The support $\mc{X}$ of the design distribution $P$ is a closed, connected, smooth and boundaryless Riemannanian manifold embedded in $\Rd$, of intrinsic dimension $1 \leq m < d$. The injectivity radius of $\mc{X}$ is upper bounded by a constant $i_0$. The design distribution $P$ admits a density $p$ with respect to the volume form $d\mu$ induced by the Riemannian structure, which is bounded away from $0$ and $\infty$,
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty, \quad \textrm{for all $x \in \mc{X}$.}
	\end{equation*}
\end{definition}

\subsection{Laplacian eigenmaps}
We now formally define the estimator and test statistic we study. Both are derived from eigenvectors of the neighborhood graph Laplacian operator. For a positive, symmetric kernel $\eta: [0,\infty) \to [0,\infty)$, and a bandwidth parameter $\varepsilon > 0$, the \emph{neighborhood graph Laplacian} operator $L_{n,\varepsilon}$ is
\begin{equation}
\label{eqn:neighborhood_graph_laplacian}
L_{n,\varepsilon}u(x) = \frac{1}{n\varepsilon^{2 + \mathrm{dim}(\mc{X})}} \sum_{i = 1}^{n} \bigl(u(x) - u(X_j)\bigr) \eta\biggl(\frac{\|x - X_j\|}{\varepsilon}\biggr)
\end{equation}
(Here $\mathrm{dim}(\mc{X})$ stands for the dimension of $\mc{X}$. It is equal to $d$ under the assumptions of Model~\ref{def:model_flat_euclidean}, and equal to $m$ under the assumptions of Model~\ref{def:model_manifold}. The pre-factor $(n\varepsilon^{2 + \mathrm{dim}(\mc{X})})^{-1}$ is purely for convenience in taking limits as $n \to \infty, \varepsilon \to 0$). 

Although~\eqref{eqn:neighborhood_graph_laplacian} makes sense for any $x \in \mc{X}$ and $u: \mc{X} \to \Reals$, we will typically think of $x \in \{X_1,\ldots,X_n\}$ and $u = (u(X_1),\ldots,u(X_n)) \in \Leb^2(P_n)$), so that $L_{n,\varepsilon}$ is equivalently a matrix in $\Reals^{n \times n}$. Indeed, viewed as a matrix $L_{n,\varepsilon}$ is a symmetric and positive semi-definite, and can thus be diagonalized by its eigenpairs, solutions $(\lambda,v)$ to the eigenproblem
\begin{equation*}
\frac{1}{n}L_{n,\varepsilon}v = \lambda v, \quad \|v\|_n^2 = 1.
\end{equation*}
We will assume without loss of generality that each eigenvalue $\lambda$ of $L_{n,\varepsilon}$ has algebraic multiplicity $1$, and so we can index the eigenpairs $(\lambda_1,v_1),\ldots,(\lambda_n,v_n)$ in ascending order of eigenvalue, $0 = \lambda_1 < \ldots < \lambda_n$. 

The Laplacian eigenmaps estimator $\wh{f}$ simply projects the response vector ${\bf Y}$ onto the first $K$ eigenvectors of $L_{n,\varepsilon}$: letting $V_K \in \Reals^{n \times K}$ be the matrix with columns $v_1,\ldots,v_K$, we have that
\begin{equation}
\label{eqn:laplacian_eigenmaps_estimator}
\wh{f} := \sum_{k = 1}^{K} \dotp{Y}{v_k}_{n} v_k = \frac{1}{n} V_K V_K^{\top} Y.
\end{equation} 
If $\wh{f}$ is a reasonable estimate of $f_0$, then the Laplacian eigenmaps test statistic
\begin{equation}
\label{eqn:laplacian_eigenmaps_test}
\wh{T} := \|\wh{f}\|_n^2
\end{equation}
is in turn a reasonable estimate of $\|f_0\|_{P}^2$, and can be used to distinguish whether or not $f_0 = 0$.

It may be helpful to comment briefly on the term ``Laplacian eigenmaps'', which we use a bit differently than is typical in the literature. Laplacian eigenmaps typically refers to an algorithm for embedding, which maps each design point $X_1,\ldots,X_n$ to $\Reals^K$ according to $X_i \mapsto (v_{1,i}, \ldots, v_{K,i})$. Viewing this mapping as a feature map, we can then interpret the estimator $\wh{f}$ as the least-squares solution to a linear regression problem with responses $Y_1,\ldots,Y_n$ and features $v_1,\ldots,v_K$. Often, the Laplacian eigenmaps embedding is viewed as a tool for dimensionality reduction, wherein it is implicitly assumed that $K \ll d$. We will neither explicitly nor implicitly take $K < d$; indeed, the embedding perspective is not particularly illuminating in what follows, and we do not henceforth make reference to it. Instead, we use ``Laplacian eigenmaps'' to directly refer to the estimator $\wh{f}$ or test statistic $\wh{T}$. 

On a different note, it is worth explicitly pointing out that $\wh{f}$ is defined only in-sample, that is, only at the design points $X_1,\ldots,X_n$. There is no straightforward or agreed upon way to extend $\wh{f}$ out-of-sample, and we address the problem of out-of-sample estimation in some detail later in Section~\ref{sec:out_of_sample}. 

\subsection{Sobolev Classes}
\label{sec:sobolev}
Before we give an overview of our main results, we will first pause to review the definition of Sobolev classes. We divide our discussion into two cases---the flat Euclidean case, and the manifold case, corresponding to Models~\ref{def:model_flat_euclidean} and Model~\ref{def:model_manifold}.

\paragraph{Flat Euclidean case.}
Take $\mc{X}$ to be an open, connected set with Lipschitz boundary, as in Model~\ref{def:model_flat_euclidean}. Let $Q$ be a measure supported on a subset of $\mc{X}$, that is absolutely continuous with respect to the full-dimensional Lebesgue measure $\nu$. Recall that for a given multiindex $\alpha \in \mathbb{N}^d$, a function $f$ is \emph{$\alpha$ weakly differentiable} if there exists some $h \in L^1(Q)$ such that
\begin{equation*}
\int_{\mc{X}} h g = (-1)^{|\alpha|} \int_{\mc{X}} f D^{\alpha}g, \quad \textrm{for every $g \in C_0^{\infty}(\mc{X})$.}
\end{equation*}
If such a function $h$ exists, it is the $\alpha$th weak partial derivative of $f$, and denoted by $D^{\alpha}f = h$. If $f$ is $\alpha$ weakly differentiable for all $|\alpha| = j$, then $d^jf := \sum_{|\alpha| = j}D^{\alpha}f$ is the sum of all weak partial derivatives of order $j$. 

For an integer $s \geq 1$, the Sobolev class $H^s(Q)$ consists of all those functions $f \in \Leb^2(Q)$ which have weak derivatives of all orders $j = 1,\ldots,s$, all of which also belong to $\Leb^2(Q)$. The $j$th order semi-norm for $f \in H^s(Q)$ is $|f|_{H^j(Q)} := \|d^jf\|_{L^2(Q)}$. The corresponding norm
\begin{equation*}
\|f\|_{H^s(Q)}^2 := \|f\|_{\Leb^2(Q)}^2 + \sum_{j = 1}^{s} |f|_{H^j(Q)}^2,
\end{equation*}
which induces the ball
\begin{equation*}
H^s(Q;M) := \bigl\{f \in H^s(Q): \|f\|_{H^s(Q)} \leq M\bigr\}.
\end{equation*} 
In the special case where $Q$ is the uniform measure over $\mc{X}$, we use the notation $H^s(\mc{X}) := H^s(Q)$. Finally, we note that $H^s(Q)$ is the completion of $C^{(\infty)}(\mc{X})$ with respect to the $\|\cdot\|_{H^s(Q)}$ norm, so that $C^{\infty}(\mc{X})$ is dense in $H^s(\mc{X})$. 

\paragraph{Manifold case.}

\textcolor{red}{(TODO)}

\paragraph{Boundary conditions.}
In the flat Euclidean case, when $s > 1$ we will assume that $f_0$ satisfies some boundary conditions. In particular, we will assume that $f_0$ belongs to the set of \emph{zero trace} Sobolev functions $H_0^{s}(P)$, defined as the closure of $C_0^s(\mc{X})$ with respect to $\|\cdot\|_{H^s(P)}$ norm. Of course, under Model~\ref{def:model_manifold} the domain $\mc{X}$ is without boundary, and so boundary conditions are irrelevant.

Let us explain the reason we require boundary conditions in the flat Euclidean case when $s > 1$, by considering an alternative way to characterize Sobolev spaces using a relation to sequence spaces.\footnote{This is equivalent to characterizing Sobolev spaces using the Fourier transform, but we find it more transparent to instead work directly with sequence spaces.} Let $(\lambda_1(\Delta_P),\psi_1),(\lambda_2(\Delta_P),\psi_2),\ldots$ be the solutions to the Laplace-Beltrami eigenvector equation (with Neumann boundary conditions), 
\begin{equation*}
\Delta_P\psi_k = \lambda_{k}(\Delta_P) \psi_k, \quad \frac{\partial}{\partial {\bf n}}\psi_k = 0~~\textrm{on $\partial \mc{X}$},
\end{equation*}
where $\partial/(\partial {\bf n})$ is the partial derivative operator in the direction of the normal vector $\mathbf{n}$ to the boundary of $\mc{X}$.
We define an ellipsoid $\Theta \subset \ell^2(\mathbb{N})$  using the eigenvalues $\lambda_1(\Delta_P),\lambda_2(\Delta_P),\ldots$ as $\Theta := \{\theta \in \ell^2(\mathbb{N}): \sum_{k = 1}^{\infty} \lambda_k(\Delta_P)^{2s} \theta_k^2 < \infty\}$. Then, the function class $\mc{H}^s(P) = \{f = \sum_{k = 1}^{\infty} \theta_k \psi_k: \theta \in \Theta\}$ consists precisely of those Sobolev functions $f \in H^s(P)$ which satisfy the following \emph{Neumann} boundary conditions,
\begin{equation}
\label{eqn:condition}
\frac{\partial^j}{\partial {\bf n}^j}f = 0 \quad \textrm{for $j = 1,3,5,\ldots,\floor{s/2}$.}
\end{equation}
As we have already reviewed, the eigenvectors $v_k$ of the graph Laplacian $L_{n,\varepsilon}$ converge to $\psi_k$ as $n \to \infty, \varepsilon \to 0$. As a result, is natural that they accurately approximate only those functions $f_0 \in \mc{H}^s(P) \subset H^s(P)$. Although the zero-trace boundary condition is more restrictive than~\eqref{eqn:condition}---since it also requires that derivatives of even-order be equal to $0$---the point is that some kind of boundary condition will be necessary in order to obtain optimal rates for spectral series methods, including Laplacian eigenmaps. We will stick to the zero-trace condition, since it greatly eases some of the steps in our proofs.

\subsection{Minimax Rates}
\label{subsec:minimax_rates_sobolev}
We now review the minimax estimation and goodness-of-fit testing rates over Sobolev balls. We pay special attention to the spectral series methods which achieve these rates, because, as we have already seen, these methods are very related to Laplacian eigenmaps.

\paragraph{Estimation rates over Sobolev balls.}
In the estimation problem, we ask for an estimator---formally, a Borel measurable function $\wh{f}$, that maps from $\mc{X}$ to $\Reals$---which is close to the regression function $f_0$ with respect to the squared norm $\|\wh{f} - f_0\|_{P}^2$. Under Model~\ref{def:model_flat_euclidean}, the minimax estimation rate over the Sobolev ball is\footnote{Perhaps surprisingly, we were unable to find a reference that explicitly states the rate in~\eqref{eqn:sobolev_space_minimax_estimation_rate} in our general setup, for all values of $s$ and $d$---but the rate certainly does hold.  Momentarily we will introduce a spectral series estimator $\wt{f}$ which achieves this rate, implying the upper bound. The lower bound in~\eqref{eqn:sobolev_space_minimax_estimation_rate} can be obtained from the analogous lower bound over $C_0^s(\mc{X})$\textcolor{red}{(reference)}, and the fact that there exists a continuous embedding of $C_0^s(\mc{X})$ into $\mc{H}^s(P)$.}
\begin{equation}
\label{eqn:sobolev_space_minimax_estimation_rate}
\inf_{\wh{f}} \sup_{f_0} \|\wh{f} - f_0\|_P^2 \asymp M^2(M^2n)^{-2s/(2s + d)}
\end{equation}
here the infimum is taken over all estimators $\wh{f}$, the supremum over all $f_0 \in H^1(\mc{X};M)$ (first-order case) or $f_0 \in H_0^s(\mc{X};M)$ (higher-order case).

The upper bound in~\eqref{eqn:sobolev_space_minimax_estimation_rate} can be certified by a particular spectral series estimator.  Under the assumptions of Model~\ref{def:model_flat_euclidean}, the eigenvalues $\lambda_k(\Delta_P)$ satisfy the Weyl's Law asymptotic scaling $\lambda_{k}(\Delta_P) \asymp k^{2/d}$, a fact that we discuss in more detail later. A key consequence of this scaling is that the Sobolev ball $H_0^s(P) \subset \mc{H}^s(P)$ continuously embeds into the Sobolev ellipsoid $\mc{\wt{H}}^s(P) = \{f = \sum_{k = 1}^{\infty} \theta_k \psi_k: \sum_{k = 1}^{\infty} (k - 1)^{2s/d} \theta_k^2\}$. By reducing the regression problem to an estimation problem in a Gaussian sequence model \textcolor{red}{(see e.g. Tsybakov,Johnstone,Gine)}, it can be shown the estimator
\begin{equation}
\label{eqn:spectral_series_estimator}
\wt{f} := \sum_{k = 1}^{K} \dotp{Y}{\psi_k}_n \psi_k,
\end{equation}
achieves the minimax rate over $\mc{H}^s(P;M)$, and the same upper bound on the minimax rate therefore applies to $H_0^s(P;M)$. 

\paragraph{Goodness-of-fit testing rates over Sobolev balls.}
In the goodness-of-fit testing problem, we ask for a test function---formally, a Borel measurable function $\phi$ that takes values in $\{0,1\}$--- which can distinguish between the hypotheses
\begin{equation}
\mathbf{H}_0: f_0 = f_0^{\star}, ~~\textrm{versus}~~ \mathbf{H}_a: f_0 \in \mc{F} \setminus \{f_0^{\star}\}.
\end{equation} 
Typically, the null hypothesis $f_0 = f_0^{\star} \in \mc{F}$ reflects the absence of interesting structure, and $\mc{F} \setminus  \{f_0^{\star}\}$ is a set of smooth departures from this null. To fix ideas, as in \citet{ingster2009} we focus on the problem of \emph{signal detection} in Sobolev spaces, where $f_0^{\star} = 0$ and $\mc{F} = \mc{H}^s(\mc{X};M)$ is a Sobolev ball. This is without loss of generality since our test statistic and its analysis are easily modified to handle the case when $f_0^{\star}$ is not $0$, by simply subtracting $f_0^{\star}(X_i)$ from each observation $Y_i$.

The Type I error of a test $\phi$ is $\mathbb{E}_0[\phi]$, and if $\mathbb{E}_0[\phi] \leq a$ for a given $a \in (0,1)$ we refer to $\phi$ as a level-$a$ test\footnote{We reserve the more common symbol $\alpha$ for multi-indices, so as to avoid confusion}. The worst-case risk of $\phi$ over $\mc{F}$ is
\begin{equation*}
R_n(\phi, \mc{F}, \epsilon) := \sup\Bigl\{\mathbb{E}_{f_0}[1 - \phi]: f_0 \in \mc{F}, \|f_0\|_{P} > \epsilon\Bigr\},
\end{equation*}
and for a given constant $b \in (0,1)$, the minimax critical radius $\epsilon(\mc{F})$ is the smallest value of $\epsilon$ such that some level-$a$ test has worst-case risk of at most $b$. Formally,
\begin{equation*}
\epsilon(\mc{F}) := \inf\Bigl\{\epsilon > 0: \inf_{\phi} R_n(\phi,\mc{F},\epsilon) \leq b \Bigr\},
\end{equation*} 
where in the above the infimum is over all level-$a$ tests $\phi$, and $\Ebb_{f_0}[\cdot]$ is the expectation operator under the regression function $f_0$.\footnote{Clearly, the minimax critical radius $\epsilon(\mc{F})$ depends on $a$ and $b$. However, we adopt the typical convention of treating $\alpha,b \in (0,1)$ and  as small but fixed positive constants; hence they will not affect the testing error rates, and we suppress them notationally.} See \citet{ingster82,ingster87,ingster2012} for a more extended treatment of the minimax paradigm in nonparametric testing. 

Testing whether a regression function $f_0$ is equal to $0$ is an easier problem than estimating $f_0$, and so the minimax testing critical radius is much smaller than the minimax estimation rate. \citet{ingster2009} show that in the special case where $\mc{X} = [0,1]^d$ and $P = \nu$,
\begin{equation}
\label{eqn:sobolev_space_testing_critical_radius}
\epsilon^2\bigl(\mc{H}^s(P;M)\bigr) \asymp M^2(M^2n)^{-4s/(4s + d)}~~\textrm{for $1 \leq d < 4s$,}
\end{equation}
and the same result holds under the more general setup of Model~\ref{def:model_flat_euclidean}. The test \citet{ingster2009} use to certify the upper bound in~\eqref{eqn:sobolev_space_testing_critical_radius} is a spectral series method, which uses the squared $L^2(P)$ norm of $\wt{T} = \|\wt{f}\|_{P}^2$ as a test statistic. 

When $4s \geq d$ the functions in $H^s(\Xset)$ are very irregular---formally speaking $H^s(\Xset)$ does not continuously embed into $\Leb^4(\Xset)$ when $4s \geq d$---and as far as we know, the minimax testing rates in this regime have not been worked out. However, if one explicitly assumes that $f_0 \in \Leb^4(\mc{X})$ then the critical radius is given by $\epsilon^2 \asymp Mn^{-1/2}$. As a sanity check note that this is strictly worse than the rate in~\eqref{eqn:sobolev_space_testing_critical_radius}. As we discuss after our first main theorem regarding testing (Theorem~\ref{thm:laplacian_eigenmaps_testing_fo}), this rate is achievable by a test based on $\wh{T}$.

\paragraph{Manifold setup.}
Under Model~\ref{def:model_manifold}, both~\eqref{eqn:sobolev_space_minimax_estimation_rate} and~\eqref{eqn:sobolev_space_testing_critical_radius} continue to hold, but with the ambient dimension $d$ replaced everywhere by the intrinsic dimension $m$. The estimator $\wt{f}$ and a test using the statistic $\wt{T}$---with $\psi_1,\psi_2,\ldots$ now the eigenfunctions of the manifold weighted Laplace-Beltrami operator $\Delta_P$---continue to achieve the optimal rates. \textcolor{red}{(TODO): Find references.}

\paragraph{In-sample mean squared error.}

The error in~\eqref{eqn:sobolev_space_minimax_estimation_rate} is measured with respect to $\|\cdot\|_P^2$ norm. Since $\wh{f}$ is defined only at the design points $X_1,\ldots,X_n$, we will measure estimation rates in $\|\cdot\|_n^2$ error. The lower bounds used to show~\eqref{eqn:sobolev_space_minimax_estimation_rate} will still hold when in-sample loss is used rather than out-of-sample loss \textcolor{red}{(?)}, and we verify matching upper bounds in the sections to come. Thus the minimax rates are unchanged---we call estimators which achieve these optimal rates \emph{in-sample minimax rate-optimal estimators}.

There is one other subtlety introduced by the use of in-sample mean squared error. Technically speaking, elements $f \in H^s(P)$ are equivalence classes, defined only up to a set of measure zero. Thus one cannot speak of the pointwise evaluation $f_0(X_i)$, as we do by defining our target of estimation to be $f_0(X_i)$, $i=1,\ldots,n$, until one selects \emph{representatives}. When $s > d/2$, every element $f$ of $H^s(P)$ admits a continuous version $f^{\ast}$, and it is standard to pick this as the favored representative. When $s \leq d/2$, some elements in $H^s(P)$ do not have any continuous version; however they admit a \emph{quasi-continuous} version \citep{evans15} known as the \emph{precise representative}, and we use this representative. To be clear, however, it does not really matter which representative we choose. Since all versions agree except on a set of measure zero, and since $P$ is absolutely continuous with respect to Lebesgue measure (in Model~\ref{def:model_flat_euclidean}) or the volume form $d\mu$ (in Model~\ref{def:model_manifold}), with probability $1$ any two versions $g_0, h_0 \in f_0$ will satisfy $g_0(X_i) = h_0(X_i)$ for all $i = 1,\ldots,n$. The bottom line is that we can use the notation $f_0(X_i)$ without fear of ambiguity or confusion.

% TODO: AG, get rid of this!

%\subsection{Our contributions}
%The following items summarize our major results. In all of them we assume~\ref{asmp:domain} and~\ref{asmp:density} hold, but not that any of $P$, $\mc{X}$, $\Delta_P$ or $\Delta$ are known. Put $r := s - 1$.
%\begin{itemize}
%	\item \textbf{Minimax optimal estimation.} For any $f_0 \in H^1(\Xset;M)$, with high probability $\norm{\wh{f} - f_0}_{n}^2 \lesssim M^{2d/(2 + d)}n^{-2/(2 + d)}$.
%	\item \textbf{Minimax optimal testing.}
%	A test constructed using $\wh{T}$ has non-trivial power whenever $f_0 \in H^1(\Xset;M)$ satisfies $\norm{f_0}_{P}^2 \gtrsim M^{2d/(2 + d)}n^{-4/(4 + d)}$ and $d < 4$.
%	\item \textbf{Higher-order smoothness conditions.} When the regression function $f_0 \in H_0^s(\Xset;M)$ and additionally the density $p \in C^{r}(\mc{X};M)$, with high probability the error $\norm{\wh{f} - f_0}_{n}^2 \lesssim M^{2d/(2s + d)}n^{-2s/(2s + d)}$. Additionally, a test constructed using $\wh{T}$ has non-trivial power whenever $f_0 \in H_0^s(\Xset;M)$ satisfies $\norm{f_0}_{P}^2 \gtrsim M^{2d/(4s + d)}n^{-4s/(4s + d)}$ and $d < 4$.
%	\item \textbf{Manifold adaptivity.}
%	If $\mc{X} \subset \Rd$ is a manifold of dimension $m < d$, and if $H^s(\mc{X})$ for $s \leq 4$, then each of the aforementioned rates hold with $d$ replaced by $m$.
%\end{itemize}
%In the statements above, estimation error is measured in-sample, using the empirical squared-norm $\|\cdot\|_n^2$.\footnote{To state the obvious: this is very different than measuring the \emph{training error} $\|Y - \wh{f}\|_n^2$.} It is necessary to measure loss in-sample because the Laplacian eigenmaps estimator $\wh{f}$ is defined only at the observed design points $X_1,\ldots,X_n$. That being said, often one would like an upper bound on the prediction risk, or equivalently the out-of-sample loss $\|\cdot\|_P^2$. We therefore also introduce a simple, intuitive way of extending $\wh{f}$ to a function defined over all of $\mc{X}$, and show that this extension is minimax rate-optimal with respect to error measured in $L^2(P)$ norm.

\section{Minimax Optimality of Laplacian Eigenmaps}
\label{sec:minimax_optimal_laplacian_eigenmaps}

As previously explained, Laplacian eigenmaps is a discrete and noisy approximation to a spectral series method, of particular use when the design distribution $P$ is unknown. Our goal is to show that Laplacian eigenmaps are rate optimal methods, notwithstanding the extra error incurred by this approximation. In this section and the following one, we will see that this is indeed the case: the estimator $\wh{f}$, and a test using the statistic $\wh{T}$, achieve optimal estimation and goodness-of-fit testing rates over Sobolev classes. 

In this Section we will cover the flat Euclidean case, where we observe data $(X_1,Y_1),\ldots,(X_n,Y_n)$ according to Model~\ref{def:model_flat_euclidean}. We will divide our theorem statements based on whether we assume the regression function $f_0$ belongs to the first order Sobolev class ($s = 1$) or a higher-order Sobolev class ($s > 1$), since the details of the two settings are somewhat different.

\subsection{First-order Sobolev classes}
\label{sec:first_order_sobolev_classes}
We begin by assuming $f_0 \in H^1(P)$. We show that $\wh{f}$ and a test based on $\wh{T}$ are minimax optimal, for all values of $d$, and under no additional assumptions on the data generating process, i.e. on either $P$ or $f_0$.

\paragraph{Estimation.} When the graph radius $\varepsilon$ and number of eigenvectors $K$ are chosen appropriately, we show in Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} that the estimator $\wh{f}$ achieves the minimax rate over $H^1(P;M)$, when error is measured in $\|\cdot\|_{n}$.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{0}
	\item 
	\label{asmp:parameters_estimation_fo} 
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy the following inequalities:
	\begin{equation}\\
	\label{eqn:radius_fo} 
	\biggl(\frac{\log n}{n}\biggr)^{1/d} \ll \varepsilon \ll \min\{1,K^{-1/d}\},
	\end{equation}
	and 
	\begin{equation}
	\label{eqn:eigenvector_estimation_fo} 
	K \asymp \min\Bigl\{(M^2n)^{d/(2 + d)} \vee 1, n\Bigr\}
	\end{equation}
\end{enumerate}
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_fo}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally $f_0 \in H^1(P,M)$. There exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds all $n$ larger than $N$ and for any $\delta \in (0,1)$: if the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_fo}, then
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_fo}
	\|\wh{f} - f_0\|_n^2 \leq C\Bigl(\frac{1}{\delta}M^2(M^2n)^{-2/(2 + d)} \wedge 1\Bigr) \vee \frac{1}{n},
	\end{equation}
	with probability at least $1 - \delta - Cn\exp(-cn\varepsilon^d)$.
\end{theorem}
From~\eqref{eqn:laplacian_eigenmaps_estimation_fo} it follows immediately that with high probability $\|\wh{f} - f_0\|_n^2 \lesssim M^2(M^2n)^{-2/(2 + d)}$ whenever $n^{-1/2} \lesssim M \lesssim n^{1/d}$, and is thus an in-sample minimax rate-optimal estimator.

Some other remarks:
\begin{itemize}
	\item When $M \ll n^{-1/2}$ or $M \gg n^{1/d}$, the rate in~\eqref{eqn:laplacian_eigenmaps_estimation_fo} looks different than~\eqref{eqn:sobolev_space_minimax_estimation_rate}. In particular, when $M \ll n^{-1/2}$, then computing Laplacian eigenmaps with $K = 1$ achieves the parametric rate $\|\wh{f} - f_0\|_n^2 \lesssim n^{-1}$, and the zero-estimator $\wh{f} = 0$ achieves the better rate $\|\wh{f} - f_0\|_n^2 \lesssim M^2$, but neither match the minimax rate in~\eqref{eqn:sobolev_space_minimax_estimation_rate}. On the other hand, when $M \gg n^{1/d}$, then computing Laplacian eigenmaps with $K = n$ achieves the rate $\|\wh{f} - f_0\|_n^2 \asymp 1$, which is better than the rate in~\eqref{eqn:sobolev_space_minimax_estimation_rate}. However, in truth these are edge cases, which do not fall neatly into the framework of nonparametric regression.
	\item The lower bound on $\varepsilon$ imposed by~\eqref{eqn:radius_fo} is on the order of the connectivity threshold, the smallest length scale at which the resulting graph will still, with high probability, be connected. On the other hand, as we will see in Section~\ref{subsec:analysis}, the upper bound on $\varepsilon$ is needed to ensure that the graph eigenvalue $\lambda_K$ is on the same order as the continuum eigenvalue $\lambda_K(\Delta_P)$. Finally, we choose $K \asymp (M^2n)^{2d/(2 + d)}$ to optimally trade-off bias and variance.
	\item We note that the ranges~\eqref{eqn:radius_fo} and~\eqref{eqn:eigenvector_estimation_fo} depend on unknown quantities such as the dimension $d$ and radius of the Sobolev ball $M$. In practice, one typically tunes hyper-parameters by sample-splitting or cross-validation. However, because the estimator $\wh{f}$ is defined only in-sample, we cannot use such methods to select the graph radius $\varepsilon$, or number of eigenvectors $K$. We return to this issue in Section~\ref{sec:out_of_sample}, when we propose an out-of-sample extension of $\wh{f}$.
	\item The upper bound given in Equation~\eqref{eqn:laplacian_eigenmaps_estimation_fo} holds with probability $1 - \delta - Cn\exp(-cn\varepsilon^d)$. Under the stronger assumption that $f_0 \in C^1(\mc{X};M)$ we can replace the factor of $\delta$ by the sharper $\delta^2/n$, which is less than $\delta$ because $\delta \in (0,1)$. 
\end{itemize}

\paragraph{Testing.} Consider the test $\varphi = \1\{\wh{T} \geq t_{a}\}$, where $t_{a}$ is the threshold
\begin{equation*}
t_{a} := \frac{K}{n} + \frac{1}{n}\sqrt{\frac{2K}{a}}.
\end{equation*}
This choice of threshold $t_{a}$ guarantees that $\varphi$ is a level-$a$ test. Moreover, when $\varepsilon$ and $K$ are chosen appropriately, the test $\varphi$ has negligible Type II error against alternatives separated from the null by at least $\|f_0\|_{P}^2 \gtrsim M^2(M^2n)^{-4/(4 + d)}$, whenever $d < 4$. 

\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{1}
	\item 
	\label{asmp:parameters_testing_fo}
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy~\eqref{eqn:radius_fo}. Additionally,
	\begin{equation}
	\label{eqn:eigenvector_testing_fo}
	K \asymp \min\Bigl\{(M^2n)^{2d/(4 + d)} \vee 1, n\Bigr\}.
	\end{equation}
\end{enumerate}
In the following, let $p(d) = \min\{1,2/d\}$.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_fo}
	Fix $a,b \in (0,1)$. Suppose Model~\ref{def:model_flat_euclidean}. Then $\mathbb{E}_0[\varphi] \leq a$, i.e $\varphi$ is a level-$a$ test. Suppose additionally $f_0 \in H^1(P,M)$, and that $d < 4$. Then there exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds for all $n$ larger than $N$: if the Laplacian eigenmaps test $\varphi$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_testing_fo}, and if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_fo}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-4/(4 + d) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{p(d)}} \biggr) \vee \frac{1}{n}
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}
Although~\eqref{eqn:laplacian_eigenmaps_testing_criticalradius_fo} involves taking the maximum of several different terms, the important takeaway of Theorem~\ref{thm:laplacian_eigenmaps_testing_fo} is that if $n^{-1/2} \lesssim M^2 \ll n^{1/4}$ ($d = 1$) or $n^{-1/2} \lesssim M^2 \ll n^{2/d - 1/2}$ ($d = 2$ or $d = 3$), then $\varphi$ is a minimax rate-optimal test over $H^1(P,M)$.

Some other remarks:
\begin{itemize}
	\item When $d \geq 4$, as we have already mentioned $H^1(\mc{X})$ does not continuously embed into $\Leb^4(\mc{X})$. This has severe consequences for test statistics based on the $L^2(P_n)$-norm---such as $\wh{T}$---or $L^2(P)$ norm---such as $\wt{T}$---since they may no longer have a finite variance. Indeed, as far we know when $d \geq 4$ the optimal testing rates over $H^1(\mc{X})$ are not known. On the other hand, if we explicitly assume $f_0 \in \Leb^4(\mc{X})$, then we show in Appendix~\textcolor{red}{(?)} that the Laplacian eigenmaps test, with $K = n$, is minimax optimal whenever $\|f_0\|_n^2 \gtrsim n^{-1/2}$. 
	\item The requirement that $\|f_0\|_P^2 \gtrsim n^{-p(d)}$ is necessary in order for us to establish that with probability at least $1 - 5\delta$,
	\begin{equation}
	\label{eqn:small_ball_estimate}
	\|f_0\|_n^2 \geq \frac{1}{\delta}\|f_0\|_P^2.
	\end{equation}
	This requirement prevents us from showing that $\varphi$ is a minimax optimal test when $M \gtrsim n^{1/4}$ ($d = 1$) or $M \gtrsim n^{2/d - 1/2}$ ($d = 2$ or $d = 3$). Of course, note that we do show $\varphi$ is an optimal test when $M \asymp 1$, which is by far the most commonly considered case. On the other hand, if we assume $f_0 \in C^1(\mc{X};M)$, then we can establish~\eqref{eqn:small_ball_estimate}, and indeed stronger estimates, without any restrictions on $\|f_0\|_P^2$. As a result, under this stronger assumption we can show that $\varphi$ is an optimal test for all $n^{-1/2} \lesssim M^2 \lesssim n^{1/d}$, matching our results in the estimation case.
\end{itemize}

\subsection{Higher-order Sobolev classes}
\label{sec:higher_order_sobolev_classes}
We now assume that the regression function displays some higher-order regularity and is zero-trace, i.e. $f_0 \in H_0^s(P)$. At first glance, it is not at all clear that Laplacian eigenmaps \emph{can} take advantage of this additional regularity, and it is especially unclear whether it can do so in an optimal manner. As $s \to \infty$ and $d$ remains fixed, the optimal estimation and testing rates approach the parametric rate $n^{-1}$. On the other hand, the sharpest known results show that Laplacian eigenvectors $v_k$ converge to continuum eigenfunctions $\psi_k$---and thus, for a fixed value of $k$, $\wh{f}$ converges to $\wt{f}$---only at a rate of \textcolor{red}{(?)}, which is obviously much slower than $n^{-1}$. 

Nevertheless, as we will establish momentarily, remarkably Laplacian eigenmaps methods continue to be optimal for all orders of $s$, as long as the design density is itself also sufficiently regular, $p \in C^{s - 1}(\mc{X})$. In estimation, this is the case for any dimension $d$, whereas in testing it is the case only when $d \leq 4$. 

\paragraph{Estimation.}
In order to show that $\wh{f}$ is an optimal estimator over $H_0^s(P;M)$, we will require that $\varepsilon$ be meaningfully larger than the lower bound in~\ref{asmp:parameters_estimation_fo}.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:parameters_estimation_ho}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy
	\begin{equation}
	\label{eqn:radius_ho}
	\max\biggl\{\biggl(\frac{\log}{n}\biggr)^{1/d}, (M^2n)^{-1/(2(s - 1) + d)}\biggr\} \ll \varepsilon \ll \min\{1, K^{-1/d}\}
	\end{equation}
	and
	\begin{equation*}
	K \asymp (M^2n)^{2d/(2s + d)}.
	\end{equation*}
\end{enumerate}
It is important to note that the two restrictions of~\ref{asmp:parameters_estimation_ho} are not mutually exclusive. So long as $M^2 \gg n^{-1}$, then $(M^2n)^{-2/(2(s - 1) + d)} \ll (M^2n)^{-2/(2s + d)}$ for all values of $s$ and $d$.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_ho}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally $f_0 \in H_0^s(P,M)$ and $p \in C^{s - 1}(\mc{X})$. There exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds all for all $n$ larger than $N$ and for any $\delta \in (0,1)$: if the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_ho}, then
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_ho}
	\|\wh{f} - f_0\|_n^2 \leq C\Bigl(\frac{1}{\delta}M^2(M^2n)^{-2s/(2s + d)} \wedge 1\Bigr) \vee \frac{1}{n},
	\end{equation}
	with probability at least $1 - \delta - Cn\exp(-cn\varepsilon^d)$.
\end{theorem}
Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, in combination with Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo}, implies that in the flat Euclidean setting Laplacian eigenmaps is a rate-optimal estimator over Sobolev classes, for all values of $s$ and $d$. Some other remarks:
\begin{itemize}
	\item We do not require that the regularity of the Sobolev space satisfy $s > d/2$, a condition often seen in the literature. In the sub-critical regime $s \leq d/2$, the Sobolev space $H^s(\mc{X})$ is quite irregular. It is not a Reproducing Kernel Hilbert Space (RKHS), nor does it continuously embed into $C^0(\mc{X})$, much less into any H\"{o}lder space. As a result, for certain versions of the nonparametric regression problem---e.g. when loss is measured in $\Leb^{\infty}$ norm, or when the design points $\{X_1,\ldots,X_n\}$ are assumed to be fixed---in a minimax sense even consistent estimation is not possible. Likewise, certain estimators are ``off the table'', most notably RKHS-based methods such as thin-plate splines of degree $k \leq d/2$. Nevertheless, for random design regression with error measured in $\Leb^2(P)$-norm, the spectral series estimator $\wt{f}$ defined in~\eqref{eqn:spectral_series_estimator} obtains the ``usual'' minimax rates $n^{-2s/(2s + d)}$ for all values of $s$ and $d$. Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo} and \ref{thm:laplacian_eigenmaps_estimation_ho} show that the same is true with respect to Laplacian eigenmaps, with error measured in $\Leb^2(P_n)$-norm.
	\item \textcolor{red}{(TODO 1): This entire comment is too vague right now. I'll leave it in, in case Siva + Ryan think it is sufficiently valuable to edit and make more precise.} The lower bound on $\varepsilon$ means that the graph $G$ will be quite dense, with average degree growing polynomially in $n$ as $n \to \infty$. Consequently, exactly computing the eigenvectors $v_k$ is more computationally intensive than if the graph radius were on the order of the connectivity threshold $\varepsilon \asymp (\log n/n)^{1/d}$. Recall that in the first-order case, we established in Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} that as long as $\varepsilon \gg (\log n/n)^{1/d}$---meaning average degree growing like $\omega(\log n)$---the estimator $\wh{f}$ could still be optimal. Thus Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo} and~\ref{thm:laplacian_eigenmaps_estimation_ho} can be viewed as revealing a tradeoff between statistical and computational efficiency; although to be clear, we have no theoretical evidence that Laplacian eigenmaps \emph{fails} to adapt to higher-order smoothness when $\varepsilon \asymp (\log n/n)^{1/d}$---we simply cannot prove that it succeeds. 
	\textcolor{red}{(TODO 2): If we decide it is worth our time to investigate the optimal choice of graph radius empirically, we can add a line here mentioning this.}
	
	Suppose one does choose $\varepsilon$ meaningfully larger than the connectivity threshold, as our theory requires when $s > 1$. We now discuss a procedure to efficiently compute an approximation to the Laplacian eigenmaps estimate, without meaningfully increasing the statistical error incurred: \emph{edge sparsification}. For a given $\delta \in (0,1)$, various algorithms have been designed \textcolor{red}{(references)} to efficiently remove many edges from the graph $G$, resulting in a sparser graph $\wc{G}$ with Laplacian $\wc{L}_{n,\varepsilon}$, while only perturbing the spectrum of the Laplacian by a factor of at most $\sigma \geq 1$, meaning
	\begin{equation*}
	\frac{1}{\sigma} \cdot u^{\top} \wc{L}_{n,\varepsilon} u \leq u^{\top} L_{n,\varepsilon} u \leq \sigma \cdot u^{\top} \wc{L}_{n,\varepsilon}u \quad \textrm{for all $u \in \Reals^n$.}
	\end{equation*}
	The graph $\wc{G}$ is said to be $\sigma$-spectrally similar to $G$. Let $\wc{f}$ be the Laplacian eigenmaps estimator computed using the eigenvectors of the sparsified graph Laplacian $\wc{L}_{n,\varepsilon}$ . Because $\wc{G}$ is sparser than $G$, it can be (much) faster to compute the eigenvectors of $\wc{L}_{n,\varepsilon}$ than the eigenvectors of $L_{n,\varepsilon}$, and consequently much faster to compute $\wc{f}$ than $\wh{f}$\textcolor{red}{(reference needed)}. Statistically speaking, as we show in Appendix~\textcolor{red}{(?)} $\wc{f}$ has $L^2(P_n)$-error of at most $\sigma^{2s}$ times the error of $\wh{f}$. Therefore, as long as $\sigma \asymp 1$ the estimator $\wc{f}$ will also be rate-optimal. Finally, geometric graphs such as $G$ exhibit a special structure; very roughly speaking, no one edge is a bottleneck in geometric graphs. As pointed out by~\citet{sadhanala16b}, in this case there exist far simpler and faster methods for sparsification than those of~\textcolor{red}{(references)}; at least empirically, these methods appear to do the job.
	\item The requirement $p \in C^{s - 1}(\mc{X})$ is a strong condition, but is essential to showing that $\wh{f}$ enjoys faster rates of convergence when $s > 1$. We give more detail about this when we discuss our analysis in Section~\ref{subsec:analysis}. 
\end{itemize}

\paragraph{Testing.} The test $\varphi$ can adapt to the higher-order smoothness of $f_0$, when $\varepsilon$ and $K$ are chosen correctly.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{3}
	\item 
	\label{asmp:parameters_testing_ho}
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy~\eqref{eqn:radius_ho}. Additionally,
	\begin{equation}
	\label{eqn:eigenvector_testing_ho}
	K \asymp \min\Bigl\{(M^2n)^{2d/(4s + d)} \vee 1, n\Bigr\}.
	\end{equation}
\end{enumerate}
When $d \leq 4$, for any value of $s \in \mathbb{N}$ it is possible to choose $\varepsilon$ and $K$ such that both~\eqref{eqn:radius_ho} and~\eqref{eqn:eigenvector_testing_ho} are satisfied, and our next theorem establishes that in this situation $\varphi$ is an optimal test. In what follows, let $p(d) = \min\{1,2s/d\}$. 
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_ho}
	Fix $a,b \in (0,1)$. Suppose Model~\ref{def:model_flat_euclidean}. Then $\mathbb{E}_0[\varphi] \leq a$, i.e $\varphi$ is a level-$a$ test. Suppose additionally $f_0 \in H_0^s(P,M)$, that $p \in C^{s-1}(\mc{X})$, and that $d \leq 4$. Then there exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds for all $n$ larger than $N$: if the Laplacian eigenmaps test $\varphi$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_testing_ho}, and if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_ho}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-4s/(4s + d) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{p(d)}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}
Similarly to the first-order case, the main takeaway from Theorem~\ref{thm:laplacian_eigenmaps_testing_ho} is that $\varphi$ is a minimax optimal test over $H_0^s(P)$ when $n^{-1/2} \ll M^2 \ll n^{1/4}$ (if $2s < d$), or if $n^{-1/2} \ll M^2 \ll (2/d - 1/2)$ (if $2s \geq d$.)

However, unlike the first-order case, when $4 < d < 4s$ the minimax testing rate over $H_0^s(P)$ is still $n^{-4s/(4s + d)}$. Unfortunately, we can no longer claim that $\varphi$ is an optimal test in this regime. There do not exist any choices of $\varepsilon$ and $K$ which satisfy both~\eqref{eqn:radius_ho} and~\eqref{eqn:eigenvector_testing_ho}. As a result we can't balance testing bias and variance (defined momentarily in~\eqref{eqn:testing_biasvariance}). It follows from Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho} and the triangle inequality that, choosing $K \asymp (n)^{-2s/(2s + d)}$ as is optimal for estimation, $\varphi$ must have small Type II error whenever $\|f_0\|_P^2 \gtrsim n^{-2s/(2s + d)}$. In fact, we can derive slightly better (though still not optimal) rates.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_ho_suboptimal}
	Under the same setup as Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, but with $d > 4$. If the Laplacian eigenmaps test $\varphi$ is computed with $K$ satisfying~\eqref{eqn:eigenvector_testing_ho}, and $\varepsilon \asymp K^{-1/d}$, and if 
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_ho_suboptimal}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-2s/(2(s - 1) + d) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{p(d)}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}
We do not know whether the problem here is the test itself, our some looseness in our theoretical bounds, and leave the matter for future work.

A last point: as mentioned at the beginning of this section, the estimation and testing rates we obtain in Theorems~\ref{thm:laplacian_eigenmaps_estimation_ho}-\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal} are superior to those implied by the concentration of graph Laplacian eigenvectors $v_k$ around eigenfunctions $\psi_k$ of the weighted Laplace-Beltrami operator $\Delta_P$. This implies (at least) one out of the two following explanations.
\begin{enumerate}
	\item The convergence rates of $v_k \to \psi_k$ are faster than currently known,  particularly when $p$ exhibits higher-order smoothness.
	\item The approximation properties of $v_1,\ldots,v_K$ are much stronger than is implied by the concentration of each individual eigenvector $v_k$ around its continuum counterpart $\psi_k$. This is reminiscent of similar phenomena exhibited by kernel machines \textcolor{red}{(Belkin 2019)}.
\end{enumerate}
Identifying which explanation is responsible would, in our view, be a useful step towards understanding the success of graph Laplacian methods. \textcolor{red}{(TODO): Too vague and speculative. Maybe worth deleting entirely?}

\subsection{Analysis}
\label{subsec:analysis}

We now outline the high-level strategy we follow when proving each of Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal}. We analyze the estimation error of $\wh{f}$, and the testing error of $\wh{\varphi}$, by first conditioning on the design points ${\bf X}$ and deriving \emph{design-dependent} bias and variance terms. For estimation, we have that with probability at least $1 - \exp(-K)$,
\begin{equation}
\label{eqn:estimation_biasvariance}
\|\wh{f} - f_0\|_n^2 \leq \underbrace{\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{K}^s}}_{\textrm{bias}} + \underbrace{\frac{5K}{n} \vphantom{\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{K}^s}}}_{\textrm{variance}}.
\end{equation}
For testing, we have that $\varphi$ (which is a level-$a$ test by construction) also has small Type II Error, $\Ebb_{f_0}[1 - \phi] \leq b/2$, if 
\begin{equation}
\label{eqn:testing_biasvariance}
\|f_0\|_n^2 \geq  \underbrace{\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{K}^s}}_{\textrm{bias}} + \underbrace{32\frac{\sqrt{2K}}{n}\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr]}_{\textrm{variance}}.
\end{equation}
The graph quadratic form $\dotp{L^s f_0}{f_0}_n$, eigenvalue $\lambda_{K}$, and empirical squared norm $\|f_0\|_n^2$ are each random variables that depend the random design points $X_1,\ldots,X_n$. We now establish suitable upper and lower bounds on these quantities. 

\paragraph{Estimates on graph quadratic forms.}
We begin by restating in Proposition~\ref{prop:graph_seminorm_fo} an upper bound on the first-order quadratic form $\dotp{L f_0}{f_0}_n$ from \textcolor{blue}{Green et al. 2021}.
\begin{proposition}[Lemma~1 of \textcolor{blue}{Green et al. 2021}]
	\label{prop:graph_seminorm_fo}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally $f_0 \in H^1(P,M)$. There exist constants $c,C$ that do not depend on $f_0$ such that the following statement holds for any $\delta \in (0,1)$: if $\varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_seminorm_fo}
	\dotp{L_{n,\varepsilon}f_0}{f_0}_n \leq \frac{C}{\delta} M^2,
	\end{equation}
	with probability at least $1 - \delta$.
\end{proposition}

In this work, we establish that a similar result holds for the higher-order case $s > 1$. 

\begin{proposition}
	\label{prop:graph_seminorm_ho} 
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally that $f_0 \in H_0^s(P;M)$ and $p \in C^{s - 1}(\mc{X})$. Then there exist constants $c$ and $C$ that do not depend on $f_0$ such that the following statement holds for any $\delta \in (0,1)$: if $Cn^{-1/(2(s - 1) + d)} < \varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_seminorm_ho}
	\dotp{L_{n,\varepsilon}^s f}{f}_n \leq \frac{C}{\delta} M^2 \|p\|_{C^{s}(\mc{X})}^2,
	\end{equation}
	with probability at least $1 - \delta$.
\end{proposition}
The proof of Proposition~\ref{prop:graph_seminorm_ho} can be divided into two steps. In the first step, we show that when $Cn^{-1/(2(s - 1) + d)}$, then with probability at least $1 - \delta$ the graph semi-norm $\dotp{L^s f}{f}_n$ is upper bounded by $1/\delta$ times a non-local continuum energy $E_{\varepsilon}^{(s)}(f)$, defined as
\begin{equation*}
E_{\varepsilon}^{(s)}(f) := \dotp{L_{P,\varepsilon}^sf}{f}_{P}.
\end{equation*}
Here $L_{P,\varepsilon}$ is a non-local approximation to $\Delta_P$, 
\begin{equation}
\label{eqn:nonlocal_laplacian}
L_{P,\varepsilon}f(x) := \frac{1}{\varepsilon^{d + 2}}\int_{\mc{X}}\bigl(f(z) - f(x)\bigr) \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dP(x).
\end{equation}
In the second step, we show that whenever $\varepsilon$  is smaller than some contanst $c$, then $E_{\varepsilon}^{(s)}(f)$ is in turn upper bounded by a constant times $M^2 \|p\|_{C^{s - 1}(P)}^2$. Note that neither quantity here is random; this is a purely approximation-theoretic statement. Together, these steps establish Proposition~\ref{prop:graph_seminorm_ho}.

The analysis of $\dotp{L_{n,\varepsilon}^sf_0}{f_0}$ when $s > 1$ is substantially trickier than the first-order case, for several reasons. First of all, we note that when $s > 1$ the graph energy $\dotp{L_{n,\varepsilon}^s f}{f}_n$ is itself a biased estimate of the non-local energy $E_{\varepsilon}^{(s)}(f)$, in contrast to the first order case where $\Ebb\bigl[\dotp{L_{n,\varepsilon} f}{f}_n\bigr] = E_{\varepsilon}(f)$. In the language of classical statistics, $\dotp{L_{n,\varepsilon}^s f}{f}_n$ is a $V$-statistic, that is the sum of an unbiased estimator of $E_{\varepsilon}^{(s)}(f)$ (in other words, a $U$-statistic) plus some higher-order, pure bias terms. These pure bias terms are negligible only when $\varepsilon \gg n^{-1/(2r + d)}$. 

On the other hand, in order to give an estimate of $E_{\varepsilon}^{(s)}(f)$ in terms of the integrated, squared derivatives of $f_0$ and $p$, we first show that $L_{P,\varepsilon}^jf(x) \to \Delta_P^jf(x)$ as $\varepsilon \to 0$; here $j = (s - 1)/2$ when $s$ is odd and $j = (s - 2)/2$ when $s$ is even. In words, we show that a $j$th-order iterated difference operator approximates a $2j$-th order differential operator.  In order to establish this fact, we require that on the one hand that $p \in C^{s - 1}(\mc{X})$, and on the other hand that $f$ be zero-trace. Crucially we do not try to show $L_{P,\varepsilon}^sf(x) \to \Delta_{P}^sf(x)$: though this may seem like a natural first step, note carefully that the latter object is an order-$2s$ differential operator, whereas we assume that $f$ has only $s$ bounded derivatives. 

% AG: I think this following might just be garbage, but I will leave it in in case it piques Ryan or Siva's interest. 

% Proposition~\ref{prop:graph_seminorm_ho}, and its proof, show that the iterated graph Laplacian $L_{n,\varepsilon}^s$ does a reasonable job of approximating a higher-order differential operator. The conditions we require --- that $\varepsilon$ be sufficiently large, that $f$ be zero-trace, and that $p$ have regularity of order $s - 1$ --- indicate that the iterated graph Laplacian is an imperfect, if adequate, tool for this job. This is no fault of the graph Laplacian. Rather, as pointed out by \textcolor{red}{(Sadhanala et al. 2017)} its reflects that graph Laplacians were designed for a very general circumstance in which the only notions of derivation are first-differentials, divergences, and compositions of the two. Indeed, viewed in this light it is a pleasant surprise that iterated graph Laplacians approximate higher-order differentials at all. When one assumes Euclidean structure, as we do in this paper, there exist many methods (e.g. local linear embeddings, Hessian local linear embedding, local tangent space alignment) which leverage this structure to approximate differential operators in a more sophisticated manner. However, thus far little theoretical investigation has been done into even the pointwise behavior of these approaches.%

\paragraph{Neighborhood graph eigenvalue.}
On the other hand, several recent works \citep{burago2014,garciatrillos18,calder2019} have analyzed the convergence of $\lambda_{k}$ towards $\lambda_{k}(\Delta_P)$. They provide explicit bounds on the relative error $|\lambda_{k} - \lambda_{k}(\Delta_P)|/\lambda_{k}(\Delta_P)$, which show that the relative error is small for sufficiently large $n$ and small $\varepsilon$. Crucially, the guarantees hold simultaneously for all $1 \leq k \leq k_{\max}$, where $k_{\max}$ must satisfy $\lambda_{k_{\max}}(\Delta_P) \ll \varepsilon^{-2}$. These results are actually stronger than are necessary to establish Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_testing_ho}---in order to get rate-optimality, we need only show that $\lambda_{K}/\lambda_K(P)$ is $\Omega_P(1)$---but unfortunately they all assume $P$ is supported on a manifold without boundary (i.e. they assume Model~\ref{def:model_manifold} rather than Model~\ref{def:model_flat_euclidean}). 

In the case where $\mc{X}$ is assumed to have a boundary, the graph Laplacian $L$ is a reasonable approximation of the operator $\Delta_P$ only at interior points $x$ such that $B(x,\varepsilon) \subseteq \mc{X}$. In contrast, at points $x$ near the boundary $L_{n,\varepsilon}$ is known to approximate a completely different operator altogether \citep{belkin2012}. This is reminiscent of the boundary effects present in the analysis of kernel smoothing. Thus proving convergence of $\lambda_k$ to a continuum limit becomes a substantially more challenging problem.  We will instead use the results of \textcolor{red}{(Green et al. 2021)}, whose assumptions match our own, and who give a weaker bound on $\lambda_k/\lambda_k(\Delta_P)$ that will nevertheless suffice for our purposes. 

\begin{proposition}[Lemma~2 of \textcolor{blue}{(Green et al. 2021)}]
	\label{prop:graph_eigenvalue}
	Suppose Model~\ref{def:model_flat_euclidean}. Then there exist constants $c$ and $C$ such that the following statement holds: if $C(\log n/n)^{1/d} < \varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_eigenvalue}
	\lambda_k \geq c \cdot \min\Bigl\{\lambda_k(\Delta_P), \frac{1}{\varepsilon^{2}} \Bigr\} \quad \textrm{for all $1 \leq k \leq n$,}
	\end{equation}
	with probability at least $1 - Cn\exp\{-n\varepsilon^d\}$. 
\end{proposition}
Note immediately that $\lambda_0(\Delta_P) = \lambda_0 = 0$. Furthermore, Weyl's Law \textcolor{red}{(Dunlop, Wu)} tells us that under Model~\ref{def:model_flat_euclidean}, $k^{2/d} \lesssim \lambda_{k}(\Delta_P) \lesssim k^{2/d}$ for all $k \in \mathbb{N}, k > 1$. Combining these statements with~\eqref{eqn:graph_eigenvalue}, we conclude that $\lambda_{K} = \Omega_P(K^{2/d})$ so long as $K \lesssim \varepsilon^{-d}$. 

\paragraph{Empirical norm.}
\textcolor{red}{(TODO)}


Invoking the bounds of Propositions~\ref{prop:graph_seminorm_fo}-\ref{prop:graph_eigenvalue} inside the bias-variance tradeoffs~\eqref{eqn:estimation_biasvariance} and~\eqref{eqn:testing_biasvariance} and then choosing $K$ to balance bias and variance (if possible), leads to the conclusions of Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal}.


\section{Manifold Adaptivity}
\label{sec:manifold_adaptivity}

In this section we consider the manifold setting, where $(X_1,Y_1),\ldots,(X_n,Y_n)$ are observed according to Model~\ref{def:model_manifold}. \citet{belkin03,belkin05,niyogi2013} establish that the neighborhood graph $G_{n,r}$ can ``learn'' the manifold $\Xset$ in various senses, so long as $\Xset$ is locally linear. We build on this work by showing that when $f_0 \in H^s(P)$ and $P$ is supported on a manifold, Laplacian eigenmaps achieve the sharper minimax estimation and testing rates reviewed in Section~\ref{subsec:minimax_rates_sobolev}.

\subsection{Error rates under the manifold hypothesis}
We now show that Laplacian eigenmaps achieves the faster minimax rates under the manifold hypothesis, when the regression function $f_0 \in H^s(P;M)$, $s \leq 4$, and, in the testing problem, $m \leq 4$. This section will proceed in a similar fashion to Section~\ref{sec:higher_order_sobolev_classes}, except with the ambient dimension $d$ replaced by intrinsic dimension $m$. 

\paragraph{Estimation.}
To ensure that $\wh{f}$ is an in-sample minimax rate-optimal estimator, we choose the graph radius $\varepsilon$ and number of eigenvectors $K$ as in~\ref{asmp:parameters_estimation_ho}, except with ambient dimension $d$ replaced by the intrinsic dimension $m$.

\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp:parameters_estimation_manifold}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy
	\begin{equation}
	\label{eqn:radius_estimation_manifold}
	\max\biggl\{\biggl(\frac{\log}{n}\biggr)^{1/m}, n^{-1/(2(s - 1) + m)}\biggr\} \ll \varepsilon \ll \min\{1, K^{-1/m}\}
	\end{equation}
	and
	\begin{equation*}
	K \asymp (M^2n)^{2m/(2s + m)}.
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_manifold}
	Suppose Model~\ref{def:model_manifold}, and additionally $f_0 \in H^s(P,M)$ and $p \in C^{s - 1}(\mc{X})$ for $s < 4$. There exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds all for all $n$ larger than $N$ and for any $\delta \in (0,1)$: if the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_manifold}, then
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_manifold}
	\|\wh{f} - f_0\|_n^2 \leq C\Bigl(\frac{1}{\delta}M^2(M^2n)^{-2s/(2s + m)} \wedge 1\Bigr) \vee \frac{1}{n},
	\end{equation}
	with probability at least $1 - \delta - Cn\exp(-cn\varepsilon^m)$.
\end{theorem}

\paragraph{Testing.}
Likewise, to construct a minimax optimal test using $\wh{T}$, we choose $\varepsilon$ and $K$ as in~\ref{asmp:parameters_testing_fo}, except with the ambient dimension $d$ replaced by the intrinsic dimension $m$.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp:parameters_testing_manifold}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy~\eqref{eqn:radius_estimation_manifold}. Additionally, the
	\begin{equation*}
	K \asymp C_2 (M^2n)^{2m/(4s + m)}
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_manifold}
	Fix $a,b \in (0,1)$. Suppose Model~\ref{def:model_manifold}. Then $\mathbb{E}_0[\varphi] \leq a$, i.e $\varphi$ is a level-$a$ test. Suppose additionally $f_0 \in H^s(P,M)$, that $p \in C^{s-1}(\mc{X})$, and that $s < 4$ and $m \leq 4$. Then there exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds for all $n$ larger than $N$: if the Laplacian eigenmaps test $\varphi$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_testing_manifold}, and if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_manifold}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-4s/(4s + m) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{p(m)}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}

\begin{itemize}
	\item The proofs of Theorems~\ref{thm:laplacian_eigenmaps_estimation_manifold} and~\ref{thm:laplacian_eigenmaps_testing_manifold} follow very similarly to the full-dimensional setting. The difference is that when $\mc{X}$ is a manifold with intrinsic dimension $m$, we can prove analogous results to Propositions~\ref{prop:graph_seminorm_fo}-\ref{prop:graph_eigenvalue}, but with the ambient dimension $d$ replaced by the intrinsic dimension $m$. 
	\item Unlike in the full-dimensional case, our upper bounds on the estimation and testing error of Laplacian eigenmaps match the minimax rate only when $s < 4$.  Our upper bounds when $s \geq 4$ follow from the embedding $H^s(P;M) \subset H^{3}(P;M)$, i.e they match the rates we get just by assuming that the $3$rd order derivative is bounded, and are clearly suboptimal.
	
	We now explain this discrepancy. At a high level, thinking of the graph $G$ as an estimate of the manifold $\mc{X}$, we incur some error by using Euclidean distance rather than geodesic distance to form the edges of $G$. This is in contrast with the full-dimensional setting, where the Euclidean metric exactly coincides with the geodesic distance for all points $x,z \in \mc{X}$ that are sufficiently close to each other and far from the boundary of $\mc{X}$. This extra error incurred in the manifold setting by using the ``wrong distance'' dominates when $s \geq 4$. 
	
	As this explanation suggests, by building $G$ using the geodesic distance one could avoid this error, and would obtain superior rates of convergence. However this is not an option for us, as we assume $\mc{X}$---and in particular its geodesics---are unknown. Likewise, traditional spectral series estimators achieve the minimax rate for all values of $s$ and $m$, but are undesirable for the same reason---we do not want to assume that $\mc{X}$ is known. It is not clear whether this gap between spectral series and Laplacian eigenmaps estimators---or more generally, between estimators which assume the manifold is known, and those which do not---is real, or a product of loose upper bounds. 
	
	\item Finally, when $m > 4$, we get an upper bound on testing error equivalent to that of Theorem~\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal}, except with the ambient dimension $d$ replaced by intrinsic dimension $m$.
\end{itemize}

\textcolor{red}{(TODO)}
\begin{itemize}
	\item \textcolor{red}{Give a reference regarding the optimality of spectral series estimators on known manifolds.}
\end{itemize}

\section{Out-of-sample error}
\label{sec:out_of_sample}
Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps} and~\ref{sec:manifold_adaptivity} show that $\wh{f}$ is a minimax optimal estimator over Sobolev spaces. However, as mentioned previously we have measured loss \emph{in-sample}---that is, measured in $\Leb^2(P_n)$ norm---whereas \emph{out-of-sample} error---error measured in $L^2(P)$ norm---is the more typical metric in the random design setup.

Of course, the Laplacian eigenmaps estimator is only defined at the observed design points $X_1,\ldots,X_n$, and to measure its error in $L^2(P)$ norm we must first extend it to the rest of $\Xset$. We propose a simple method, kernel smoothing, to do the job. The method can applied to any estimator defined at the design points, including Laplacian eigenmaps, and we show that a smoothed version $\wt{f}$ of our original estimator $\wh{f}$ has optimal $L^2(P)$ error. For simplicity, in this section we stick to the full-dimensional setting, but everything can be adapted to the manifold setting in a straightforward way.

\paragraph{Extension by kernel smoothing.}
We now formally define our approach to extension by kernel smoothing. For a kernel function $\psi(\cdot): [0,\infty) \to (-\infty,+\infty)$, bandwidth $h > 0$, and a distribution $Q$, the \emph{Nadaraya-Watson kernel smoother} $T_{\varepsilon,Q}$ is given by
\begin{equation*}
\bigl(T_{h,Q}f)(x) := \frac{1}{d_Q(x)} \int_{\Omega} f(z)\psi\biggl(\frac{\|z - x\|}{h}\biggr) \,dQ(z)
\end{equation*}
where $d_Q(x) := \int_{\Omega} \psi\bigl(\|z - x\|/\varepsilon\bigr) \,dQ(z)$. We denote $T_{\varepsilon,P_n}$ by $T_{\varepsilon,n}$, and $d_{P_n}$ by $d_n$. 

The estimated function $\wt{f} \in L^2(P)$ we will consider is a kernel smoother passed over $\wh{f}$, meaning
\begin{equation}
\label{eqn:kernel_smoother_laplacian_eigenmaps}
\wt{f}(x) := \bigl(T_{h,n}\wh{f}\bigr)(x).
\end{equation}
It is useful to make a few comments about $\wt{f}$. First of all, while in principle~\eqref{eqn:kernel_smoother_laplacian_eigenmaps} makes sense at any point $x \in \Rd$, we will always think of $\wt{f}$ as a function mapping $\mc{X}$ to $\Reals$. Additionally, note we may use the same kernel for smoothing as was used to construct the graph, i.e. take $\psi = \eta$. Likewise, we can set $h = \varepsilon$. However, we will see that for theoretical purposes these are not always the best choices. 

\paragraph{Out-of-sample error of kernel smoothed Laplacian Eigenmaps.}

In Lemma~\ref{lem:kernel_smoothing_laplacian_eigenmaps}, we consider an arbitrary estimator $\wc{f} \in L^2(P_n)$. We show that the out-of-sample error $\|T_{n,h}\wc{f} - f_0\|_P^2$ can be upper bounded by three terms--- (a constant times) the in-sample error of $\wc{f} - f_0$, and variance and bias terms associated with a noiseless version of kernel smoothing. We shall assume the following conditions on $\psi$.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{1}
	\item
	\label{asmp:kernel}
	The kernel function $\psi$ is supported on a subset of $[0,1]$. Additionally, $\psi$ is Lipschitz continuous on $[0,1]$, and is normalized so that
	\begin{equation*}
	\int_{-\infty}^{\infty} \psi(|s|) \,ds = 1.
	\end{equation*}
\end{enumerate}
\begin{lemma}
	\label{lem:kernel_smoothing_laplacian_eigenmaps}
	Suppose $\wc{f} \in L^2(P_n)$, $f_0 \in H^1(P)$ and $p \in C^1(P)$. Under Assumptions~\ref{asmp:domain}, \ref{asmp:density}, and~\ref{asmp:kernel}, it holds that
	\begin{equation}
	\label{eqn:kernel_smoothing_laplacian_eigenmaps}
	\|T_{n,h}\wc{f} - f_0\|_P^2 \leq C\biggl(\|\wc{f} - f_0\|_n^2 + \frac{1}{\delta} \cdot \frac{h^2}{nh^d} |f|_{H^1(\mc{X})}^2 + \frac{1}{\delta}\|T_{h,P}f_0 - f_0\|_P^2\biggr),
	\end{equation}
	with probability at least $1 - \delta - Ch^d\exp\{-Cnh^d\}$. 
\end{lemma}
Notice that the variance term in the above is smaller than the typical variance term for kernel smoothing of noisy data, by a factor of $h^2$. The bias term, on the other hand, is standard. A standard analysis shows that the $\|T_{h,P}f_0 - f_0\|_P^2 \lesssim \varepsilon^{2s}$ when $\psi$ is an order-$s$ kernel, meaning
\begin{equation*}
\int_{-\infty}^{\infty} \psi(|s|) = 1,~~\int_{-\infty}^{\infty} t^j \psi(|t|) \,dt = 0~~\textrm{for $j = 1,\ldots,s - 1$},~~\textrm{and}~~\int_{-\infty}^{\infty} t^s \psi(|t|) \,dt < \infty.
\end{equation*}
\begin{lemma}
	\label{lem:kernel_smoothing_bias}
	Suppose the domain $\mc{X}$, the distribution $P$, and the kernel $\psi$  satisfy~\ref{asmp:domain}, \ref{asmp:density}, and~\ref{asmp:kernel}, respectively. 
	\begin{itemize}
		\item If $f_0 \in H^1(P)$, it follows that
		\begin{equation*}
		\|T_{h,P}f_0 - f_0\|_P^2 \leq C h^{2} |f|_{H^1(P)}^2.
		\end{equation*}
		\item If $f_0 \in H_0^{s}(P)$, $p \in C^{s}(\mc{X})$, and $\psi$ is an order-$s$ kernel, it follows that
		\begin{equation*}
		\|T_{h,P}f_0 - f_0\|_P^2 \leq C h^{2s} |f|_{H^s(P)}^2.
		\end{equation*}
	\end{itemize}
\end{lemma}
Choosing $h \asymp n^{-1/(2(s - 1) + d)}$ balances the kernel smoothing bias and variance terms in~\eqref{eqn:kernel_smoothing_laplacian_eigenmaps}. Then, applying the bounds in Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} (when $s = 1$) or Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho} (when $s > 1$), we conclude that $T_{h,n}\wh{f}$ is an optimal estimator, with respect to error measured in $L^2(P)$ norm.

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_out_of_sample}
	Suppose we observe data $(X_1,Y_1),\ldots,(X_n,Y_n)$ according to~\eqref{eqn:model}. Suppose that for some $s \in \mathbb{N}$, the regression function $f_0 \in H^s(P;M)$ (if $s = 1$) or $H_0^s(P;M)$ (if $s > 1$). Suppose also that the distribution $P$ has support $\mc{X}$ which satisfies~\ref{asmp:domain} and density $p \in C^s(\mc{X};M)$ which satisfies~\ref{asmp:density}. Then if the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_ho}, then the out-of-sample extension $T_{h,n}\wh{f}$ computed with bandwidth $h \asymp n^{-1/(2(s - 1) + d)}$ satisfies
	\begin{equation*}
	\|T_{h,n}\wh{f} - f_0\|_P^2 \leq \frac{C}{\delta}M^{2d/(2s + d)}n^{-2s/(2s + d)},
	\end{equation*}
	with probability at least $1 - \delta - Cn\exp\{-Cn\varepsilon^d\} - Ch^d\exp\{-Cnh^d\}$. 
\end{theorem}
Some remarks:
\begin{itemize}
	\item \textbf{Tuning parameters by sample-splitting.} Extending the estimator $\wh{f}$ to be defined out-of-sample allows us to select the hyper-parameters $\varepsilon$ and $K$ using sample splitting or cross validation. For instance, we can (i) split the sample into two halves, (ii) use the first half to compute $T_{h,n}\wh{f}$ for various values of $\varepsilon$, $h$, and $K$, (iii) choose the optimal values of these three hyperparameters by minimizing error on the held out set. This is by far the most common used way to choose hyperparameters \textcolor{red}{(references)}, and it addresses a serious limitation of Laplacian eigenmaps. Moreover, it is known \textcolor{red}{(references)} that choosing hyper-parameters by minimizing error over a held-out set results in estimators that optimally adapt to the order of regularity $s$---that is, that are rate-optimal (up to $\log$ factors), even when $s$ is unknown. We believe that similar arguments will imply that $T_{h,n}\wh{f}$ is adaptive in this sense, though we do not pursue the details.
	\item \textbf{Other extensions.} We consider extension by kernel smoothing because it is a simple and statistically optimal procedure that does not require any knowledge of the domain $\mc{X}$ or distribution $P$---as we have argued, this latter property is one of the main selling points of Laplacian eigenmaps as a tool for nonparametric regression. 
	
	That being said, there exist (many) other approaches to extending (or interpolating) a function $f$ based on the evaluations $\{f(X_1),\ldots,f(X_n)\}$, and we now comment on a few of these alternatives. One approach is to perform minimum norm interpolation, where one defines a normed space $(\mc{F},\|\cdot\|_{\mc{F}})$ and then solves the optimization problem
	\begin{equation*}
	\min_{u \in \mc{F}} \|u\|_{\mc{F}},~~\textrm{such that}~~ u(X_i) =  f(X_i)~~\textrm{for $i = 1,\ldots,n$.}
	\end{equation*}
	A particularly popular version of this general approach takes $\mc{F}$ to be an RKHS \textcolor{red}{(Rieger2008, Belkin2018)}, which encompasses thin-plate spline interpolation (where $\mc{F} = H^s(\mc{X})$ for $s > d/2$) as a special case. Naturally, this approach works well when $f$ is close to a function $u \in \mc{F}$ with reasonably small norm.  This holds true when $f \in H^s(P;M)$ and $s > d/2$, but as already discussed when $s \leq d/2$ the Sobolev space $H^s(P)$ is not an RKHS, and in fact when $s \leq d/2$ thin-plate spline interpolation is ill-posed \citep{green93}. Another, arguably simpler approach is to extend $f$ to be piecewise constant on the Voronoi tessellation induced by $X_1,\ldots,X_n$, or equivalently to perform $1$-nearest neighbors regression on $f$. However, this approach is theoretically optimal only when $f \in C^1(P)$, in contrast to the kernel smoothing method we propose and study.
	%Finally, the Nystr\"{o}m method extends  eigenvectors $v_k$ to functions $f_k: \mc{X} \to \Reals$ as follows,
	%\begin{equation*}
	%L_{n,\varepsilon}f_k = \lambda_k f_k~~\textrm{on $\mc{X}$, such that}~~f_k(X_i) = %v_k(X_i)~~\textrm{for $i = 1,\ldots,n$.}
	%\end{equation*}
	%Here $L_{n,\varepsilon}$ should be thought of as an operator acting on functions $f \in C(\mc{X})$ as follows,
	%\begin{equation*}
	%L_{n,\varepsilon}f(x) = \sum_{i = 1}^{n} (f(x) - f(X_i))\eta\Bigl(\frac{\|X_i - x\|}{\varepsilon}\Bigr)
	%\end{equation*}
	% The Nystr\"{o}m approach to extension makes intriguing (re-)use of the graph Laplacian $L_{n,\varepsilon}$ and its eigenvectors $v_k$. Similar methods have been analyzed in the context of kernel ridge regression (references), but little is known about its application to graph Laplacians.	
\end{itemize}

\section{Experiments}
\label{sec:experiments}

\textcolor{red}{(TODO)}: Come up with a set of experiments that might be interesting to run. 

\begin{itemize}
	\item \textbf{Laplacian eigenmaps approximates spectral series.} $1$d unit cube, uniform density, ``interesting'' regression function. Compare Laplacian eigenmaps with spectral series estimator, including their MSE curves, and their fits at several different values of $K$. 
	\item \textbf{Laplacian eigenmaps beats nonparametric least squares.} $1$d unit cube, very non-uniform density, same regression function. Make the same three comparisons, but in this case with respect to the least-squares estimator.
	\item \textbf{Investigate our theoretical bounds on hyperparameters.} $1$d unit cube, uniform density, ``smooth'' regression function. Investigate the MSE heat map of Laplacian eigenmaps, as a function of $K$ and $\varepsilon$.
	\item Swiss roll, density \textcolor{red}{(?)}, regression function \textcolor{red}{(?)}. Evaluate Laplacian eigenmaps in-sample error, and Laplacian-eigenmaps + kernel smoothing out-of-sample error.
\end{itemize}

\section{Discussion}
\label{sec:discussion}

\subsection{Comparison with other estimators}
In this paper, we have motivated Laplacian eigenmaps by viewing it as a noisy approximation of a classical spectral series method, which is its most obvious counterpart. We now discuss the relationship between Laplacian eigenmaps and three other approaches to nonparametric regression. The first two---nonparametric least squares and kernel smoothing---are classical, whereas the third---Laplacian smoothing---makes use of the graph Laplacian in a different way than Laplacian eigenmaps.

\paragraph{Nonparametric least squares.}
The standard recommended alternative to spectral series methods, when the distribution $P$ is considered non-uniform or unknown, is to do least-squares. Concretely, suppose instead we had access only to eigenfunctions $f_1',f_2',\ldots$ of the unweighted Laplace-Beltrami operator $\Delta = \Delta_{\nu}$. Then letting $F_K = \mathrm{span}\{f_1,\ldots,f_K\}$, the least-squares estimator and test statistic
\begin{equation*}
\wt{f}_{\mathrm{LS}} := \argmin_{f \in F_K} \|Y - f\|_n^2,\textrm{and}~~\wt{T}_{\mathrm{LS}} = \|\wt{f}\|_{\nu}^2
\end{equation*}
are still rate-optimal over $H_0^s(P)$. However, this is not a totally satisfactory fix. For one thing, the least squares approach still requires that we know $\Delta$, and is not well suited for the case where $\mc{X}$ is a manifold, and $\Delta$ is the manifold Laplace-Beltrami operator. Additionally, diagonalizing even the unweighted Laplace-Beltrami operator $\Delta$ is quite difficult for all but a few special domains, such as $\mc{X} = [0,1]^d$.

\paragraph{Kernel smoothing.}
It is also natural to ask whether the two stage estimator $T_{h,n}\wh{f}$ defined in Section~\ref{sec:out_of_sample} has any advantage over the simpler approach of directly kernel smoothing the responses, i.e. using the estimator $T_{h,n}Y$ (possibly for a different choice of $h$). In Appendix~\textcolor{red}{(?)}, we answer this question in the affirmative, by giving a simple example of a sequence of densities and regression functions $\{(p^{(n)}, f_0^{(n)}: n \in \mathbb{N}\}$ such that $\|f_0 - T_{h,n}\wh{f}\|_P^2 \ll \inf_{h'} \|f_0 - T_{h',n}Y\|_P^2$. This is possible because Laplacian eigenmaps induces a completely different bias than kernel smoothing. In particular, when $f_0$ and $p$ satisfy the so-called \emph{cluster} assumption--- i.e. $f_0$ is piecewise constant in high-density regions of $p$--- then the bias of Laplacian eigenmaps can much smaller than that of kernel smoothing (for equivalent levels of variance). 

We emphasize that this does not contradict the well-known fact that kernel smoothing is an optimal method for nonparametric regression over e.g. H\"{o}lder balls. It simply reflects that in the standard nonparametric regression setup---which we adopt in the main part of this paper, and in which $P$ is assumed to be equivalent to Lebesgue measure---the biases of Laplacian eigenmaps and kernel smoothing are equivalent. There has been some work \textcolor{red}{(Rigollet, Wasserman, Niyogi, El Alaoui)} analyzing semi-supervised learning in the case where $f_0$ and $p$ satisfy some special relationship, such as the cluster assumption. However, a comprehensive analysis comparing the error of Laplacian based methods (e.g. eigenmaps) to that of other methods remains outstanding.

\subsection{Future Work}
As mentioned at the outset, our work can be viewed as a contribution both to the fields of nonparametric regression with series estimators, and to graph-based learning. Fittingly, we end our discussion by mentioning some open work in each of these directions. 

Much is known about classical spectral series methods beyond their rate optimality. For instance, such estimators and tests exhibit \emph{sharp optimality}, meaning their risk can be bounded to within a $(1 + o(1))$ factor of the optimal risk. They can adapt to unknown smoothness of the regression function. They can be used to estimate smooth functionals of the regression function, or alternatively to form confidence sets in $L^2(P)$. It would be interesting to see if Laplacian eigenmaps could replicate the performance of classical methods in any, or all, of these problems.

On the other hand, there are many variants of Laplacian eigenmaps worth considering. For instance, one can change the graph under consideration (e.g. by using the k-nearest neighbors), or the normalization of the graph Laplacian $L_{n,\varepsilon}$ (e.g. by using the symmetric normalized Laplacian). The former is practically useful, because it typically leads to connected graphs while always ensuring a given level of edge sparsity. In the latter, the graph Laplacian converges to a different limiting operator, which possesses different eigenvectors than $\Delta_P$ and thereby induces a different bias. We believe that under the setup we consider here, both methods will continue to be optimal.


\bibliographystyle{plainnat}
\bibliography{../../graph_regression_bibliography} 

\appendix

\input{appendix.tex}

\end{document}