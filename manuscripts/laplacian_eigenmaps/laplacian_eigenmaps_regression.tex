\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue
}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{bm}
\usepackage{multirow}
\usepackage[font={small,it}]{caption}

\usepackage{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}
\DeclareMathAccent{\wc}{0}{mathx}{"71}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\RD}{\Reals^D}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}
\newcommand{\bj}{{\bf j}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}
\newcommand{\Lap}{{\bf L}}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{FS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Minimax-optimal Laplacian Eigenmaps regression over Sobolev Spaces with Neighborhood Graphs}
\author{Alden Green}
\date{\today}
\maketitle

\section{Introduction}
\label{sec:introduction}

Suppose we observe data $X_1,\ldots,X_n \in \Reals^d$, sampled independently from an unknown distribution $P$. As a replacement for $P$, we form a geometric graph $G$ over the observed data, with vertices at $X_1,\ldots,X_n$ and weighted edges $W_{ij}$ corresponding to proximity between samples $X_i$ and $X_j$. Geometric graphs encode both local information and global information about $P$ in an extremely general manner. For this reason they have been leveraged to conduct many different fundamental statistical tasks, such as clustering, manifold learning, semi-supervised learning, classification, and regression \textcolor{red}{(TODO): references}. Substantial theoretical progress has been made regarding the consistency of \emph{graph-based learning}, that is, learning algorithms defined with respect to geometric graphs \textcolor{red}{(TODO): references}. This work sheds light on why such procedures work, by showing that they converge to interesting \emph{continuum} limits as $n \to \infty$. However, thus far little has been said regarding the optimality of graph-based learning methods, even for classic statistical tasks. 

In this paper we focus on the theoretical statistical properties of a particularly popular graph-based learning method for regression. We assume that in addition to the design points $X_1,\ldots,X_n$ one observes real-valued responses $Y_1,\ldots,Y_n$, and seeks to learn the unknown regression function $f_0(x) := E[Y|X = x]$. The specific graph-based method we study is \emph{Laplacian eigenmaps}, first introduced by \cite{belkin03a}, which projects the response vector ${\bf Y} = (Y_1,\ldots,Y_n)$ onto the span of eigenvectors of a graph Laplacian. We focus on the the unnormalized graph Laplacian $L$, which is a difference operator acting on vectors $u \in \Reals^n$ as follows,
\begin{equation}
\label{eqn:graph_laplacian}
(Lf)_i = \sum_{j = 1}^{n} (u_i - u_j)W_{ij}. 
\end{equation}
The graph Laplacian $L$ is a discrete approximation to a weighted continuum Laplacian operator $\Delta_P$, defined when $P$ admits a differentiable density $p$ as
\begin{equation}
\label{eqn:fokker_planck_1}
\Delta_Pf= -\frac{1}{p} \mathrm{div}(p^2 \nabla f).
\end{equation}
The eigenvectors of $L$ form an orthonormal basis of $\Reals^n$, and serve as estimates of eigenfunctions of $\Delta_P$. Their corresponding eigenvalues are likewise estimates of eigenvalues of $\Delta_P$, and give a notion of smoothness to each eigenvector---roughly speaking, the smaller the eigenvalue, the smoother the corresponding eigenvector. 

We can therefore view Laplacian eigenmaps as a twist on a very classical approach to nonparametric regression: \emph{spectral projection}, or more generally \emph{orthogonal series}, regression. Classically, in orthogonal series regression one fixes a reference measure $Q$, takes an ordered orthonormal basis $\psi_1,\psi_2,\ldots$ of $L^2(Q)$, computes empirical Fourier coefficients $\dotp{{\bf Y}}{\psi_k}_n$, and uses the first few terms in the resulting Fourier series to construct an estimator. Regression by spectral projection is a special case of this general setup, where one takes $\psi_k$ to be the $k$th eigenfunction of the continuum Laplacian operator $\Delta_Q$. In contrast to this classical approach---in which the reference measure $Q$ and resulting Laplacian $\Delta_Q$ are determined a priori---in Laplacian eigenmaps the eigenvectors of the graph Laplacian serve as the basis. These eigenvectors are data-dependent objects, and adapt to the geometry of the unknown design distribution $P$ in a rich manner. For instance, they respect the cluster structure of $P$, meaning that if the density $p$ has multiple connected components, the first few eigenvectors of $L$ will (with high probability) be piecewise constant over each such component \textcolor{blue}{(vonLuxburg 2009)}. On the other hand if $P$ is supported on some low-dimensional manifold, the graph Laplacian eigenvectors concentrate around the eigenfunctions of a manifold Laplace-Beltrami operator, and thus give a principled embedding of potentially high dimensional design points $X_i$ into a lower dimensional space \citep{belkin03a}.

Thus Laplacian eigenmaps is a data-dependent alternative to classical spectral projection estimators. This data-dependency is appealing. Classical spectral projection estimators possess attractive theoretical properties for nonparametric regression---in particular, they are minimax rate-optimal for regression over H\"{o}lder and Sobolev spaces \cite{tsybakov08,johnstone2011,gine16}---but in practice suffer from some serious drawbacks. A basic difficulty is that finding the eigenfunctions of $\Delta_Q$ is in general non-trivial, so that it may not be possible to compute the estimator in the first place. Moreover, spectral projection estimators make sense (and are minimax optimal) only if the reference measure $Q$ is very close or equal to $P$, since otherwise the basis functions are orthogonalized with respect to the wrong measure. For these reasons, such estimators are typically proposed and studied under very restrictive conditions on the design points: for instance, that they are equally spaced fixed grid points, or that they are random but uniformly distributed on the unit cube $[0,1]^d$. There do exist fixes to the issues just raised, for instance using nonparametric least-squares. We will compare Laplacian eigenmaps to nonparametric least squares in more detail later, and for now merely point out that it fundamentally changes the estimator under consideration.  \textcolor{red}{(For Ryan + Siva): I attempted to edit this but still don't like it.}

In contrast, Laplacian eigenmaps directly approximates a spectral projection method: it projects the responses onto an orthogonal basis (eigenvectors of the graph Laplacian) that approximates the smooth eigenfunctions of $\Delta_P$.  Laplacian eigenmaps is perfectly well-defined, and indeed straightforward to compute, when the design $P$ is unknown. This includes the situation where $P$ may be non-uniform, or even concentrated on a low-dimensional manifold. On the other hand, because graph Laplacian eigenvectors depend on the random design points $X_1,\ldots,X_n$ in \textcolor{red}{(non-trivial ways)}, it is substantially more difficult to analyze Laplacian eigenmaps than to analyze classical spectral projection methods. For this reason, the theoretical statistical properties of Laplacian eigenmaps, and in particular its optimality as a method for nonparametric regression with random design, remain poorly understood.

\paragraph{Our contributions.} The primary contribution of our paper is to fill this theoretical gap, by answering the following question:

\begin{quote}
	Is Laplacian eigenmaps an optimal method for nonparametric regression over Sobolev functions?
\end{quote}

Broadly seeking, our answer is yes. We consider two different data models, one of which assumes the support $\mc{X}$ of the distribution $P$ is a full-dimensional, flat Euclidean domain, and the other of which assumes that $\mc{X}$ is a manifold of small intrinsic dimension $m$. We show that when the regression function $f_0$ is smooth, in the Sobolev sense of having weak derivatives bounded in $\Leb^2(\mc{X})$ norm, Laplacian eigenmaps methods are statistically minimax optimal, for both estimation and goodness-of-fit testing. Our statements hold for different relations between the dimension $d$ (or $m$) and number of derivatives $s$, depending on the problem (estimation or testing), and are summarized in Tables~\ref{tbl:estimation_rates} and~\ref{tbl:testing_rates}.
\begin{table}
	\begin{center}
		\begin{tabular}{p{.2\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			$s \leq 3$ & $\bm{n^{-2s/(2s + d)}}$ & $\bm{n^{-2s/(2s + m)}}$ \\
			$s > 3$  & $\bm{n^{-2s/(2s + d)}}$ & $n^{-6/(6 + m)}$
		\end{tabular}
	\end{center}
	\caption{Summary of estimation rates over Sobolev balls. Bold font marks minimax optimal rates. In each case, rates hold for all $d \in \mathbb{N}$ (under Model~\ref{def:model_flat_euclidean}), and for all $m \in \mathbb{N}, 1 < m < d$ (under Model~\ref{def:model_manifold}). Although we suppress it for simplicity, in all cases when the Laplacian eigenmaps estimator is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal, as long as $n^{-1/2} \lesssim M \lesssim n^{1/d}$.}
	\label{tbl:estimation_rates}
\end{table}

\begin{table}
	\begin{center}
		\begin{tabular}{p{.175\textwidth} p{.175\textwidth} | p{.14\textwidth} p{.12\textwidth} }
			Smoothness order & Dimension & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
			\hline
			\multirow{2}{*}{$s = 1$} & $\dim(\mc{X}) < 4$ & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $\dim(\mc{X}) \geq 4$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s = 2$ or $3$} & $\dim(\mc{X}) \leq 4$  & $\bm{n^{-4s/(4s + d)}}$ & $\bm{n^{-4s/(4s + m)}}$ \\
			& $4 <\dim(\mc{X}) < 4s$  & $n^{-2s/(2(s - 1) + d)}$ & $n^{-2s/(2(s - 1) + m)}$\\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
			\hline
			\multirow{3}{*}{$s > 3$} & $\dim(\mc{X}) \leq 4$ & $\bm{n^{-4s/(4s + d)}}$ & $n^{-12/(12 + d)}$ \\
			& $4 < \dim(\mc{X}) < 4s$ & $n^{-2s/(2(s - 1) + d)}$ & $n^{-6/(4 + m)}$ \\
			& $\dim(\mc{X}) \geq 4s$ & $\bm{n^{-1/2}}$ & $\bm{n^{-1/2}}$ \\
		\end{tabular}
	\end{center}
	\caption{Summary of Laplacian eigenmaps testing rates over Sobolev balls. Bold font marks minimax optimal rates. Rates when $d > 4s$ assume that $f_0 \in L^4(P,M)$. Although we suppress it for simplicity, in all cases when the Laplacian eigenmaps test is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal, as long as $n^{-1/2} \lesssim M \lesssim n^{1/d}$.}
	\label{tbl:testing_rates}
\end{table}

\paragraph{Related Work.}
There is an incredible amount of work on the properties of classical spectral projection methods, which go far beyond their optimality for nonparametric regression. We will not attempt to summarize this work, and instead refer to \citet{wasserman2006,gyorfi2006,tsybakov08,johnstone2011,gine16} and the references therein.

More recent work has considered regression on a fixed graph, where one treats the design points $x_1,\ldots,x_n$ as vertices in a fixed graph $G$, and carries out inference with respect to the function evaluations $(f_0(x_i))_{i = 1}^{n}$ By this point there exists a relatively mature theory describing this setting. Tight upper bounds have been established that certify the optimality of graph-based methods for both nonparametric estimation \citep{wang2016,hutter2016,sadhanala16,sadhanala17,kirichenko2017,kirichenko2018}) and testing (e.g., \citet{sharpnack2010identifying,sharpnack2013b,sharpnack2013,sharpnack2015} over different ``function'' classes (in quotes because these classes really model the $n$-dimensional vector of evaluations) We call particular attention to~\citet{sadhanala16} and~\citet{sharpnack2015}, who analyze the Laplacian eigenmaps estimator and test statistic, respectively. This setting is quite general, because the graph need not be a geometric graph defined on a vertex set which belongs to Euclidean space. On the other hand, in many situations it may be somewhat to unnatural to assume that the design points are a priori fixed, and that the regression function $f_0$ exhibits ``smoothness'' over this fixed design. Instead, it may be more reasonable to adopt the \emph{random design} perspective that we work in, and assume that the regression function $f_0$ exhibits a more classical notion of smoothness. 

However, as already mentioned, when the design points are random so too are the graph Laplacian eigenvectors, and grasping their properties is in general non-trivial. For this reason, there has not been much analysis of random design nonparametric regression using Laplacian eigenmaps. \cite{zhou2011} consider the Laplacian eigenmaps estimator, but in the semi-supervised setting, where one additionally observes unlabeled design points $X_{n + 1},\ldots,X_{n + m}$. Their analysis assumed that for a fixed number of labeled samples $n$, the number of unlabeled samples $m$ grows to infinity. In this case the eigenvectors of the graph Laplacian converge to eigenfunctions of a continuum Laplacian, and the analysis of the resulting estimator is identical to that of a classical spectral projection estimator. \cite{lee2016} consider the \emph{diffusion maps} estimator---which uses the eigenvectors of a different normalization of the graph Laplacian $L$---in both the supervised and semi-supervised setups. In the supervised case, they show that the diffusion maps estimator converges to the regression function as the sample size $n \to \infty$, but at a suboptimal rate. As far as we know, there has been no analysis of the test statistic $\wh{T}$ in the random design framework which we study.

There has been much more analysis of the convergence properties of eigenvectors of random graph Laplacians to their continuum limits.  \citet{belkin07,vonluxburg2008,singer2017, garciatrillos18} show that eigenvalue-eigenvector pairs $(\lambda,v)$ of a graph Laplacian converge to eigenvalue-eigenfunction pairs $(\lambda,\psi)$ of a limiting differential or integral operator. \citet{burago2014, shi2015, trillos2019, calder2019, cheng2021} build on these works, by giving finite sample bounds, rates of convergence, and making statements uniform. These results justify our intuition that Laplacian eigenmaps, which projects the responses ${\bf Y}$ onto eigenvectors of a graph Laplacian, is approximating a classical spectral projection method, which projects ${\bf Y}$ onto eigenfunctions of the limiting differential operator $\Delta_P$. In fact, more formally these results imply that for a fixed number of eigenvectors $K > 0$, the Laplacian eigenmaps estimator converges to its classical counterpart as $n \to \infty$. By taking $K \to \infty$ sufficiently slowly with $n$, we can even conclude that Laplacian eigenmaps will be a consistent estimator of $f_0$. Unfortunately, the resulting rates of convergence implied by this approach are severely suboptimal, compared to the minimax rates. Our analysis showing that Laplacian eigenmaps achieves minimax optimal rates of convergence will use some of the results mentioned above, but will overall proceed by a very different route than the one just considered.

Finally, we point out that there are other ways to use neighborhood graphs, and specifically graph Laplacians, for nonparametric regression. For instance, \cite{trillos2020} \textcolor{blue}{(Green 2021)} use the graph Laplacian to induce a penalty over functions $f: \{\bf X\} \to \Reals$. The \emph{Laplacian smoothing} estimator $\wh{f}_{\mathrm{LS}}$ is obtained by minimizing the sum of this penalty with a data-fidelity term,
\begin{equation*}
\wh{f}_{\mathrm{LS}} := \|Y - f\|_n^2 + \lambda \dotp{L_{n,\varepsilon}f}{f}_n.
\end{equation*}
\textcolor{blue}{(Green 2021)} show that the resulting estimator is minimax optimal, but only for $s = 1$ and $d \leq 4$. In contrast, we show in this work that the Laplacian eigenmaps estimator is optimal for all $s$ and $d$.

\paragraph{Organization.}

We now outline the structure of the rest of this paper. In Section~\ref{sec:setup_main_results}, we formally define the regression problem and estimator we consider. We also give some background about minimax regression over Sobolev spaces, and recall the classical spectral projection estimators which achieve minimax rates of convergence. In Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps} and~\ref{sec:manifold_adaptivity}, we establish our main results regarding the optimality of Laplacian eigenmaps. Since thhe Laplacian eigenmaps estimator $\wh{f}$ is defined only at the design points $X_1,\ldots,X_n$, in Section~\ref{sec:out_of_sample} we propose a method for out-of-sample extension of $\wh{f}$, and show that it has optimal out-of-sample error. In Section~\ref{sec:experiments} we examine the empirical behavior of Laplacian eigenmaps, and compare it to some natural competitors for nonparametric regression. We conclude with some discussion in Section~\ref{sec:discussion}.

\paragraph{Notation.}
We frequently refer to various classical function classes. For a domain $\mc{X}$ with volume form $d\mu$, we let $L^2(\mc{X})$ denote the set of functions $f$ for which $\|f\|_{L^2(\mc{X})}^2 := \int f^2 \,d\mu  < \infty$, and equip $L^2(\mc{X})$ with the norm $\|\cdot\|_{L^2(\mc{X})}$. We define $\dotp{f}{g}_P := \int fg\,dP$, and let $L^2(P)$ contain those functions $f$ for which $\|f\|_P^2 := \dotp{f}{f}_P$ is finite. Finally, we let $L^2(P_n)$ consist of those ``functions'' $f: \set{X_1,\ldots,X_n} \to \Reals$ for which the empirical norm $\|f\|_{n}^2 := \frac{1}{n}\sum_{i = 1}^{n} \bigl(f(X_i)\bigr)^2 < \infty$. When there is no chance of confusion, we will sometimes associate functions in $L^2(P_n)$ with vectors in $\Reals^n$, and vice versa. We use $C^k(\mc{X})$ to refer to functions which are $k$ times continuously differentiable in $\mc{X}$, either for some integer $k \geq 1$ or for $k = \infty$. We let $C_c^{\infty}(\mc{X})$ represent those functions in $C^{\infty}(\mc{X})$ which are compactly contained in $\mc{X}$. We write $\partial f/\partial r_i$ for the partial derivative of $f$ in the $i$th standard coordinate of $\Rd$, and use the multi-index notation $D^{\alpha}f := \partial^{|\alpha|}f/\partial^{\alpha_1}x_1\ldots\partial^{\alpha_d}x_d$ for multi-indices $\alpha \in \Reals^m$.

We write $\|\cdot\|$ for Euclidean, and $d_{\mc{X}}(x',x)$ for the geodesic distance between points $x$ and $x'$ on a manifold $\mc{X}$. Then for a given $\delta > 0$, $B(x,\delta)$ is the radius-$\delta$ ball with respect to Euclidean distance, whereas $B_{\mc{X}}(x,\delta)$ is the radius-$\delta$ ball with respect to geodesic distance. Letting $T_x(\mc{X})$ be the tangent space at a point $x \in \mc{X}$, we write $B_m(v,\delta) \subset T_x(\mc{X})$ for the radius-$\delta$ ball centered at $v \in T_x(\mc{X})$.

For sequences $(a_n)$ and $(b_n)$, we use the asymptotic notation $a_n \lesssim b_n$ to mean that there exists a number $C$ such that $a_n \leq C b_n$ for all $n$. We write $a_n \asymp b_n$ when $a_n \lesssim b_n$ and $b_n \lesssim a_n$. On the other hand we write $a_n = o(b_n)$ when $\lim a_n/b_n = 0$, and likewise $a_n = \omega(b_n)$ when $\lim a_n/b_n = \infty$. Finally $a \vee b := \max\{a,b\}$ and $a \wedge b := \min\{a,b\}$.

\section{Setup and Background}
\label{sec:setup_main_results}

In this section, we begin by giving a precise definition of our framework, and the Laplacian eigenmaps methods we study. We then review minimax rates for nonparametric regression over Sobolev spaces. We pay special attention to the classical spectral projection methods which achieve these rates, since such methods are closely connected to Laplacian eigenmaps.

\subsection{Nonparametric regression with random design}
\label{sec:regression_laplacian_eigenmaps}

We will operate in the usual setting of nonparametric regression with random design, in which we observe independent random samples $(X_1,Y_1),\ldots,(X_n,Y_n)$. The design points $X_1,\ldots,X_n$ are sampled from a distribution $P$ with support $\mc{X} \subseteq \Rd$, and the responses follow the signal plus noise model
\begin{equation}
\label{eqn:model}
Y_i = f_0(X_i) + w_i,
\end{equation}
with regression function $f_0: \mc{X} \to \Reals$, and $w_i \sim N(0,1)$ independent Gaussian noise. For simplicity we will assume throughout that the noise has unit-variance, but all of our results extend in a straightforward manner to the case where the variance is equal to a known positive value. 

We now formulate two models, which differ in the assumed nature of the support $\mc{X}$ of the design distribution $P$: the \emph{flat Euclidean} and \emph{manifold} models.

\begin{definition}[Flat Euclidean model]
	\label{def:model_flat_euclidean}
	The data $(X_1,Y_1),\ldots,(X_n,Y_n)$ are sampled according to~\eqref{eqn:model}. The support $\mc{X}$ of the design distribution $P$ is an open, connected, and bounded subset of $\Rd$, with Lipschitz boundary. The distribution $P$ admits a Lipschitz density $p$ with respect to the $d$-dimensional Lebesgue measure $\nu$, which is bounded away from $0$ and $\infty$,
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty, \quad \textrm{for all $x \in \mc{X}$.}
	\end{equation*}
\end{definition}

In the following, we recall that the injectivity radius of a $m$-dimensional Riemannian manifold $\mc{X}$ is the maximum value of $\delta$ such that the exponential map $\exp_x: B_m(0,\delta) \subset T_x(\mc{X}) \to B_{\mc{X}}(x,\delta) \subset \mc{X}$ is a diffeomorphism for all $x \in \mc{X}$.
\begin{definition}[Manifold model]
	\label{def:model_manifold}
	The data $(X_1,Y_1),\ldots,(X_n,Y_n)$ are sampled according to~\eqref{eqn:model}. 
	The support $\mc{X}$ of the design distribution $P$ is a closed, connected, smooth and boundaryless Riemannanian manifold embedded in $\Rd$, of intrinsic dimension $1 \leq m < d$. The injectivity radius of $\mc{X}$ is lower bounded by a positive constant $i_0 > 0$. The design distribution $P$ admits a Lipschitz density $p$ with respect to the volume form $d\mu$ induced by the Riemannian structure of $\mc{X}$, which is bounded away from $0$ and $\infty$,
	\begin{equation*}
	0 < p_{\min} \leq p(x) \leq p_{\max} < \infty, \quad \textrm{for all $x \in \mc{X}$.}
	\end{equation*}
\end{definition}

Finally, at various points we will have to assume that the density $p$ also displays different types of higher-order regularity, beyond Lipschitz continuity. When such assumptions are necessary, we always will state them explicitly.

\subsection{Laplacian eigenmaps}
We now formally define the estimator and test statistic we study. Both are derived from eigenvectors of a graph Laplacian.  For a positive, symmetric kernel $\eta: [0,\infty) \to [0,\infty)$, and a radius parameter $\varepsilon > 0$, let $G = ([n],W)$ be the neighborhood graph formed over the design points $\{X_1,\ldots,X_n\}$, with a weighted edge $W_{ij} = \eta(\|X_i - X_j\|/\varepsilon)$ between vertices $i$ and $j$. Then the 
\emph{neighborhood graph Laplacian} $L_{n,\varepsilon}: \Reals^n \to \Reals$ is defined by its action on vectors $u \in \Reals^n$ as
\begin{equation}
\label{eqn:neighborhood_graph_laplacian}
\bigl(L_{n,\varepsilon}u\bigr)_i := \frac{1}{n\varepsilon^{2 + \mathrm{dim}(\mc{X})}} \sum_{j = 1}^{n} \bigl(u_i - u_j\bigr) \eta\biggl(\frac{\|X_i - X_j\|}{\varepsilon}\biggr).
\end{equation}
(Here $\mathrm{dim}(\mc{X})$ stands for the dimension of $\mc{X}$. It is equal to $d$ under the assumptions of Model~\ref{def:model_flat_euclidean}, and equal to $m$ under the assumptions of Model~\ref{def:model_manifold}. The pre-factor $(n\varepsilon^{2 + \mathrm{dim}(\mc{X})})^{-1}$ is purely for convenience in taking limits as $n \to \infty, \varepsilon \to 0$). Written in standard coordinates we have $(n\varepsilon^{\dim(\mc{X}) + 2}) \cdot L_{n,\varepsilon} = D - W$, where $D \in \Reals^{n \times n}$ is the diagonal degree matrix, $D_{ii} = \sum_{i = 1}^{n} W_{ij}$.

The graph Laplacian is a positive semi-definite matrix, and admits the eigendecomposition $L_{n,\varepsilon} = \sum_{k = 1}^{n} \lambda_k v_k v_k^{\top}$, where for each $k = 1,\ldots,n$ the eigenvalue-eigenvector pair $(\lambda_k,v_k)$ satisfies 
\begin{equation*}
\frac{1}{n}L_{n,\varepsilon}v_k = \lambda_k v_k, \quad \|v_k\|_n^2 = 1.
\end{equation*}
We will assume without loss of generality that each eigenvalue $\lambda$ of $L_{n,\varepsilon}$ has algebraic multiplicity $1$, and so we can index the eigenpairs $(\lambda_1,v_1),\ldots,(\lambda_n,v_n)$ in ascending order of eigenvalue, $0 = \lambda_1 < \ldots < \lambda_n$. 

The Laplacian eigenmaps estimator $\wh{f}$ simply projects the response vector ${\bf Y}$ onto the first $K$ eigenvectors of $L_{n,\varepsilon}$: letting $V_K \in \Reals^{n \times K}$ be the matrix with columns $v_1,\ldots,v_K$, we have that
\begin{equation}
\label{eqn:laplacian_eigenmaps_estimator}
\wh{f} := \sum_{k = 1}^{K} \dotp{Y}{v_k}_{n} v_k = \frac{1}{n} V_K V_K^{\top} Y.
\end{equation} 
Thus $\wh{f}$ is equivalently a vector in $\Reals^n$, or a function in $L^2(P_n)$. If $\wh{f}$ is a reasonable estimate of $f_0$, then the Laplacian eigenmaps test statistic
\begin{equation}
\label{eqn:laplacian_eigenmaps_test}
\wh{T} := \|\wh{f}\|_n^2
\end{equation}
is in turn a reasonable estimate of $\|f_0\|_{P}^2$, and can be used to distinguish whether or not $f_0 = 0$.

It may be helpful to comment briefly on the term ``Laplacian eigenmaps'', which we use a bit differently than is typical in the literature. Laplacian eigenmaps typically refers to an algorithm for embedding, which maps each design point $X_1,\ldots,X_n$ to $\Reals^K$ according to $X_i \mapsto (v_{1,i}, \ldots, v_{K,i})$. Viewing this embedding as a feature map, we can then interpret the estimator $\wh{f}$ as the least-squares solution to a linear regression problem with responses $Y_1,\ldots,Y_n$ and features $v_1,\ldots,v_K$. Often, the Laplacian eigenmaps embedding is viewed as a tool for dimensionality reduction, wherein it is implicitly assumed that $K$ is much smaller than $d$. We will neither explicitly nor implicitly take $K < d$; indeed, the embedding perspective is not particularly illuminating in what follows, and we do not henceforth make reference to it. Instead, we use ``Laplacian eigenmaps'' to directly refer to the estimator $\wh{f}$ or test statistic $\wh{T}$. 

\subsection{Sobolev Classes}
\label{sec:sobolev}
We now review the definition of Sobolev classes, dividing our discussion into two cases---the flat Euclidean case and the manifold case, corresponding respectively to Models~\ref{def:model_flat_euclidean} and Model~\ref{def:model_manifold}.

\paragraph{Flat Euclidean case.}
Take $\mc{X}$ to be a open, connected and bounded set with Lipschitz boundary, as in Model~\ref{def:model_flat_euclidean}. Recall that for a given multiindex $\alpha \in \mathbb{N}^d$, a function $f$ is \emph{$\alpha$-weakly differentiable} if there exists some $h \in L^1(\mc{X})$ such that
\begin{equation*}
\int_{\mc{X}} h g = (-1)^{|\alpha|} \int_{\mc{X}} f D^{\alpha}g, \quad \textrm{for every $g \in C_c^{\infty}(\mc{X})$.}
\end{equation*}
If such a function $h$ exists, it is the $\alpha$th weak partial derivative of $f$, and denoted by $D^{\alpha}f := h$. 

\begin{definition}[Sobolev space on an open set]
	\label{def:sobolev_space}
	For an integer $s \geq 1$, a function $f \in L^2(\mc{X})$ belongs to the Sobolev space $H^s(\mc{X})$ if for all $|\alpha| \leq s$, the weak derivatives $D^{\alpha}f$ exists and satisfy $D^{\alpha}f \in L^2(\mc{X})$. The $j$th order semi-norm for $f \in H^s(\mc{X})$ is $|f|_{H^j(\mc{X})} := \sum_{|j| = s}\|D^{\alpha}f\|_{\Leb^2(\mc{X})}$, and the corresponding norm
	\begin{equation*}
	\|f\|_{H^s(\mc{X})}^2 := \|f\|_{\Leb^2(\mc{X})}^2 + \sum_{j = 1}^{s} |f|_{H^j(\mc{X})}^2,
	\end{equation*}
	induces a ball
	\begin{equation*}
	H^s(\mc{X};M) := \bigl\{f \in H^s(\mc{X}): \|f\|_{H^s(\mc{X})} \leq M\bigr\}.
	\end{equation*} 
\end{definition}
We note that $H^s(\mc{X})$ is the completion of $C^{\infty}(\mc{X})$ with respect to the $\|\cdot\|_{H^s(\mc{X})}$ norm, so that $C^{\infty}(\mc{X})$ is dense in $H^s(\mc{X})$. 

\paragraph{Manifold case.}

There are several equivalent ways to define Sobolev spaces on a compact, smooth, $m$-dimensional Riemannian manifold embedded in $\Reals^d$. We will stick with a definition that parallels our setup in the flat Euclidean setting as much as possible. To do so, we first recall the notion of partial derivatives on a manifold, which are defined with respect to a local coordinate system. Letting $r_1,\ldots,r_m$ be the standard basis of $\Reals^m$, for a given chart $(\phi,U)$ (meaning an open set $U \subseteq \mc{X}$, and a smooth mapping $\phi: U \to \Reals^m$) we write $\phi =: (x_1,\ldots,x_m)$ in local coordinates, meaning $x_i = r_i \circ \phi$. Then the partial derivative $\partial f/\partial x_i$ of a function $f$ with respect to $x_i$ at $x \in U$ is
\begin{equation*}
\frac{\partial f}{\partial x_i}(x) := \frac{\partial(f \circ \phi^{-1})}{\partial r_i}\bigl(\phi(x)\bigr).
\end{equation*}
The right hand side should be interpreted in the weak sense of derivative. As before, we use the multi-index notation $D^{\alpha}f := \partial^{|\alpha|}f/\partial^{\alpha_1}x_1\ldots\partial^{\alpha_m}x_m$. 

\begin{definition}[Sobolev space on a manifold]
	\label{def:sobolev_space_manifold}
	A function $f \in \Leb^2(\mc{X})$ belongs to the Sobolev space $H^{s}(\mc{X})$ if for all $\abs{\alpha} \leq s$, the weak derivatives $D^{\alpha}f$ exist and satisfy  $D^{\alpha}f \in \Leb^2(\mc{X})$. The $j$th order semi-norm $|f|_{H^j(\mc{X})}$, the norm $\|f\|_{H^s(\mc{X})}$, and the ball $H^s(\mc{X};M)$ are all defined as in Defintion~\ref{def:sobolev_space}.
\end{definition}
The partial derivatives $D^{\alpha}f$ will depend on the choice of local coordinates, and so will the resulting Sobolev norm~$\|f\|_{H^s(\mc{X})}$. However, regardless of the choice of local coordinates the resulting norms will be equivalent\footnote{Recall that norms $\|\cdot\|_1$ and $\|\cdot\|_2$ on a space $\mc{F}$ are said to be equivalent if there exist constants $c$ and $C$ such that
\begin{equation*}
c \|f\|_1 \leq \|f\|_2 \leq C \|f\|_2 \quad \textrm{for all} f \in \mc{F}.
\end{equation*}} 
and so the ultimate Sobolev space $H^s(\mc{X})$ is independent of the choice of local coordinates. 

% AG: This was my first attempt at defining Sobolev spaces on manifolds, which I keep as a comment for now, and plan to delete.
%
% We stick to an approach that builds on the definition of Sobolev spaces in the flat Euclidean case. Specifically, let $\{\phi_{\beta}, U_{\beta}\}$ be a $C^{\infty}$ atlas of $\mc{X}$, and let $\{h_\beta\}$ be a partition of unity subordinate to $\{(\phi_{\beta}), U_{\beta}\}$.\footnote{Meaning $\mathrm{supp}(h_{\beta}) \subseteq U_{\beta}$ for each $\beta$.} For a function $f \in C^{\infty}(\mc{X})$, define the order-$s$ Sobolev norm to be
%\begin{equation*}
%\|f\|_{H^s(\mc{X})}^2 := \sum_{\beta} \|(fh_{\beta}) \circ %\phi_\beta^{-1}\|_{H^s(\phi(U_\beta))}^2.
%\end{equation*}
%Then the order-$s$ Sobolev space $H^s(\mc{X})$ is defined to be the completion of $C^{\infty}(\mc{X})$ with respect to the norm $\|\cdot\|_{H^s(\mc{X})}$. When the manifold $\mc{X}$ is smooth, as we assume in Model~\ref{def:model_manifold}, then this construction is independent of the choice of atlas $\{\phi_{\beta}, U_{\beta}\}$.

\paragraph{Boundary conditions.}
In the flat Euclidean model (Model~\ref{def:model_flat_euclidean}), in order to show that Laplacian eigenmaps is optimal over $H^{s}(\mc{X})$ for $s > 1$ we will need to assume that the regression function $f_0$ satisfies some boundary conditions. In particular, we will assume that $f_0$ is zero-trace.
\begin{definition}[Zero-trace Sobolev space]
	\label{def:zero_trace_sobolev_space}
	The \emph{order-s zero-trace Sobolev space} $H_0^s(\mc{X})$ is the closure of $C_c^\infty(\mc{X})$ with respect to $\|\cdot\|_{H^s(\mc{X})}$ norm. That is, $f \in H_0^s(\mc{X})$ if $f \in H^s(\mc{X})$ and additionally there exists a sequence $f_1,f_2,\ldots$ of functions in $C_c^{\infty}(\mc{X})$ such that
	\begin{equation*}
	\lim_{k \to \infty}\|f_k - f\|_{H^s(\mc{X})} = 0.
	\end{equation*}
	The normed ball $H_0^{s}(\mc{X};M) := H_0^{s}(\mc{X}) \cap H^{s}(\mc{X};M)$.
\end{definition}
The zero-trace condition can be made more concrete when $f \in C^\infty(\mc{X})$, since we can then speak of the pointwise behavior of $f$ and its derivatives. Letting $\partial/(\partial {\bf n})$ be the partial derivative operator in the direction of the vector $\mathbf{n}$ normal to the boundary of $\mc{X}$, then the zero-trace condition implies that $\partial^{k}f/\partial{\bf n}^k(x) = 0$ for each $k = 0,\ldots,s - 1$, and for all $x \in \partial\mc{X}$.

We now explain why Laplacian eigenmaps should be optimal only when $f_0$ satisfies certain boundary conditions. Let $(\lambda_1(\Delta_P),\psi_1),(\lambda_2(\Delta_P),\psi_2),\ldots$ be the solutions to the weighted Laplacian eigenvector equation with Neumann boundary conditions, 
\begin{equation}
\label{eqn:laplacian_eigenfunction_neumann}
\Delta_P\psi_k = \lambda_{k}(\Delta_P) \psi_k, \quad \frac{\partial}{\partial {\bf n}}\psi_k = 0~~\textrm{on $\partial \mc{X}$}.
\end{equation}
As we have already alluded to, it is known~\citep{garciatrillos18} that each graph Laplacian eigenpair ($\lambda_k$,$v_k$) converges to a corresponding solution ($\lambda_k(\Delta_P)$, $\psi_k$) of~\eqref{eqn:laplacian_eigenfunction_neumann}. Thus it is relevant to consider which Sobolev functions $f \in H^s(M)$ we can reconstruct using the eigenfunctions $\psi_1,\psi_2,\ldots$. To that end, we introduce the spectrally defined Sobolev space
\begin{equation}
\label{eqn:spectral_sobolev_space}
\mc{H}^s(\mc{X}) := \biggl\{f \in L^2(\mc{X}): \sum_{k = 1}^{\infty} \bigl[\dotp{f}{\psi_k}_P\bigr]^2 \cdot\bigl[\lambda_k(\Delta_P)\bigr]^s < \infty \biggr\}.
\end{equation}
Under the conditions $p \in C^{\infty}(\mc{X})$ and $\partial\mc{X} \in C^{1,1}$, \cite{dunlop2020} show the strict inclusion $\mc{H}^{2s}(\mc{X}) \subset H^{2s}(\mc{X})$. More precisely, they show that for any $s > 0$,
\begin{equation}
\label{eqn:spectral_sobolev_space_equivalence}
\mc{H}^{2s}(\mc{X}) = \biggl\{f \in H^{2s}(\mc{X}): \frac{\partial \Delta_P^rf}{\partial {\bf n}} = 0~\textrm{on}~\partial\mc{X},~~\textrm{for all $0 \leq r \leq s - 1$} \biggr\},
\end{equation}
and for any $s \geq 0$, $\mc{H}^{2s + 1}(\mc{X}) = \mc{H}^{2s}(\mc{X}) \cap H^{2s + 1}(\mc{X})$. If $P$ is uniform on $\mc{X}$, then~\eqref{eqn:spectral_sobolev_space} means that a Sobolev function $f \in H^{s}(\mc{X})$ additionally belongs to $\mc{H}^{s}(\mc{X})$ only if all its odd lower order derivatives vanish at the boundary $\partial\mc{X}$.

The bottom line is that the eigenvectors $v_k$ of the graph Laplacian accurately approximate only those functions $f \in H^{s}(\mc{X})$ which satisfy some additional boundary conditions. Although the zero-trace boundary condition is more restrictive than~\eqref{eqn:spectral_sobolev_space_equivalence}---since it also requires that derivatives of even-order be equal to $0$---the point is that some kind of boundary condition will be necessary in order to obtain optimal rates. We will stick to the zero-trace condition, since it greatly eases some of the steps in our proofs.

On the other hand, in the manifold model (Model~\ref{def:model_manifold}) the domain $\mc{X}$ is without boundary---precisely, every point $x \in \mc{X}$ has a neighborhood that is homeomorphic to an open set in $\Reals^m$, for instance the open ball $B(x,\delta)$ for any $\delta$ smaller than the injectivity radius $i_0$---and so boundary conditions are irrelevant. 

\subsection{Minimax Rates and Spectral Series Methods}
\label{subsec:minimax_rates_sobolev}
We now review the minimax estimation and goodness-of-fit testing rates over Sobolev balls. We will pay special attention to certain classical spectral projection methods which achieve these rates. This is because, as we have already discussed, classical spectral projection methods are very related to Laplacian eigenmaps.

\paragraph{Estimation rates over Sobolev balls.}
In the estimation problem, we ask for an estimator---formally, a Borel measurable function $\wh{f}$ that maps from $\mc{X}$ to $\Reals$---which is close to the regression function $f_0$ with respect to the squared norm $\|\wh{f} - f_0\|_{P}^2$. Under Model~\ref{def:model_flat_euclidean}, the minimax estimation rate over the order-$s$ Sobolev ball is
\begin{equation}
\label{eqn:sobolev_space_minimax_estimation_rate}
\inf_{\wh{f}} \sup_{f_0} \Ebb\|\wh{f} - f_0\|_P^2 \asymp M^2(M^2n)^{-2s/(2s + d)};
\end{equation}
here infimum taken over all estimators $\wh{f}$, and the supremum over all $f_0 \in H^1(\mc{X};M)$ (first-order case) or $f_0 \in H_0^s(\mc{X};M)$ (higher-order case), and we assume $n^{-1/2} \lesssim M$.

The lower bound in~\eqref{eqn:sobolev_space_minimax_estimation_rate} is due to~\citep{stone1980} (at least for the case of $M$ constant, as is most typically considered). The upper bound can be certified by a particular spectral projection estimator $\wt{f}$,
\begin{equation}
\label{eqn:spectral_series_estimator}
\wt{f} := \sum_{k = 1}^{K} \dotp{Y}{\psi_k}_n \psi_k,
\end{equation}
where $\psi_k$ are the eigenfunctions of $\Delta_P$ defined in~\eqref{eqn:laplacian_eigenfunction_neumann}. The optimality of spectral projection estimators over Sobolev type spaces is generally well-understood. For instance, see \cite{tsybakov08,johnstone2011,gine16}, who work in the Gaussian sequence model and show that analogous estimators are optimal over Sobolev ellipsoids. However, we have not found an analysis of the specific estimator~\eqref{eqn:spectral_series_estimator} under Model~\ref{def:model_flat_euclidean}, and so for completeness we state the result in the following proposition.
\begin{proposition}
	\label{prop:spectral_series_estimation}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally that $\partial \mc{X} \in C^{1,1}$, that $p \in C^{\infty}(\mc{X})$, and that $f_0 \in H^1(\mc{X};M)$ (first-order case) or $f_0 \in H_0^s(\mc{X};M)$ for some $s > 1$ (higher-order case). Then there exists a constant $C$ which does not depend on $f_0,M$ or $n$ such that the following statement holds: if the spectral projection estimator $\wt{f}$ is computed with parameter $K = \floor{M^2n}^{d/(2s + d)}$, then
	\begin{equation*}
	\Ebb\bigl[\|\wt{f} - f_0\|_P^2\bigr] \leq C \min\bigl\{M^2(M^2n)^{-2s/(2s + d)}, M^2\bigr\}
	\end{equation*}
\end{proposition}
The proof of Proposition~\ref{prop:spectral_series_estimation}, along with proofs of all of our results, can be found in the appendix. It is worth mentioning some aspects of the analysis here, because it sets the stage for the strategy we will use to analyze Laplacian eigenmaps.  

In particular, three essential facts are needed to establish Proposition~\ref{prop:spectral_series_estimation}. 
\begin{enumerate}
	\item The continuous embedding of $H_0^s(\mc{X})$ into $\mc{H}^s(\mc{X})$---recall the latter is defined in~\eqref{eqn:spectral_sobolev_space}---which is a consequence of the zero-trace condition, the conditions on $\partial \mc{X}$ and $p$, and~\eqref{eqn:spectral_sobolev_space_equivalence}.
	\item Weyl's Law, which gives the asymptotic scaling of eigenvalues $\lambda_{k}(\Delta_P) \asymp k^{2/d}$, and allows us to properly control the bias induced by spectral projection.
	\item A local version of Weyl's law, which gives an estimate on $\sum_{k = 1}^{K} \bigl(\psi_k(x)\bigr)^2$, and allows us to appropriately control the difference $\dotp{f_0}{\psi_k}_n - \dotp{f_0}{\psi_k}_P$  between the empirical and population Fourier coefficients.
\end{enumerate}
With these facts in hand the proof of Proposition~\ref{prop:spectral_series_estimation} follows from calculations standard to analysis of the Gaussian sequence model. Our analysis of Laplacian eigenmaps will depend on analogues to the first and second of these facts, with the space $\mc{H}^s(\mc{X})$ and eigenvalues $\lambda_k(\Delta_P)$ replaced by alternatives suitably defined with respect to the neighborhood graph Laplacian $L_{n,\varepsilon}$.

\paragraph{Goodness-of-fit testing rates over Sobolev balls.}
In the goodness-of-fit testing problem, we ask for a test function---formally, a Borel measurable function $\phi$ that takes values in $\{0,1\}$--- which can distinguish between the hypotheses
\begin{equation}
\mathbf{H}_0: f_0 = f_0^{\ast}, ~~\textrm{versus}~~ \mathbf{H}_a: f_0 \in \mc{F} \setminus \{f_0^{\ast}\}.
\end{equation} 
Typically, the null hypothesis $f_0 = f_0^{\ast} \in \mc{F}$ reflects the absence of interesting structure, and $\mc{F} \setminus  \{f_0^{\ast}\}$ is a set of smooth departures from this null. To fix ideas, as in \citet{ingster2009} we focus on the problem of \emph{signal detection} in Sobolev spaces, where $f_0^{\ast} = 0$ and $\mc{F} = \mc{H}^s(\mc{X};M)$ is a Sobolev ball. This is without loss of generality since our test statistic and its analysis are easily modified to handle the case when $f_0^{\ast}$ is not $0$, by simply subtracting $f_0^{\ast}(X_i)$ from each observation $Y_i$.

The Type I error of a test $\phi$ is $\mathbb{E}_0[\phi]$, and if $\mathbb{E}_0[\phi] \leq a$ for a given $a \in (0,1)$ we refer to $\phi$ as a level-$a$ test\footnote{We reserve the more common symbol $\alpha$ for multi-indices, so as to avoid confusion}. The worst-case risk of $\phi$ over $\mc{F}$ is
\begin{equation*}
R_n(\phi, \mc{F}, \epsilon) := \sup\Bigl\{\mathbb{E}_{f_0}[1 - \phi]: f_0 \in \mc{F}, \|f_0\|_{P} > \epsilon\Bigr\},
\end{equation*}
and for a given constant $b \in (0,1)$, the minimax critical radius $\epsilon_n(\mc{F})$ is the smallest value of $\epsilon$ such that some level-$a$ test has worst-case risk of at most $b$. Formally,
\begin{equation*}
\epsilon_n(\mc{F}) := \inf\Bigl\{\epsilon > 0: \inf_{\phi} R_n(\phi,\mc{F},\epsilon) \leq b \Bigr\},
\end{equation*} 
where in the above the infimum is over all level-$a$ tests $\phi$, and $\Ebb_{f_0}[\cdot]$ is the expectation operator under the regression function $f_0$.\footnote{Clearly, the minimax critical radius $\epsilon_n(\mc{F})$ depends on $a$ and $b$. However, we adopt the typical convention of treating $\alpha,b \in (0,1)$ and  as small but fixed positive constants; hence they will not affect the testing error rates, and we suppress them notationally.} We will refer to the rate at which the squared critical radius $\epsilon_n(\mc{F})^2$ tends to $0$ as the minimax testing rate.

Testing whether a regression function $f_0$ is equal to $0$ is an easier problem than estimating $f_0$, and so the minimax testing rate is much smaller than the minimax estimation rate. \citet{ingster2009} give the minimax testing rate in the special case where $\mc{X} = [0,1]^d$ and $P$ is uniform, and we restate their result in the terms and notation of our paper.
\begin{theorem}[Theorem 1 of \citet{ingster2009}]
	\label{thm:spectral_series_testing}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally that $\mc{X} = [0,1]^d$, $P$ is the uniform distribution on $[0,1]^d$, and $f_0 \in H_0^s(\mc{X})$. Then 
	\begin{equation}
	\label{eqn:sobolev_space_testing_critical_radius}
	\epsilon_n^2\bigl(H_0^s(P;1)\bigr) \asymp n^{-4s/(4s + d)}~~\textrm{for $1 \leq d < 4s$.}
	\end{equation}
\end{theorem}
The analysis used by \citet{ingster2009} to show the upper bound in~\eqref{eqn:sobolev_space_testing_critical_radius} relies on a similar trio of facts as used in the proof of Proposition~\ref{prop:spectral_series_estimation}. It is otherwise reminiscent of calculations made in the Gaussian sequence model, which can be found in~\cite{ingster2012}. This analysis can be straightforwardly adapted to handle design distributions $P$ that satisfy the conditions of Model~\ref{def:model_flat_euclidean}, or to handle the case where $M$ is not $1$. Finally, the test \citet{ingster2009} use to certify the upper bound in~\eqref{eqn:sobolev_space_testing_critical_radius} is implicitly a spectral projection method: the test statistic is the $L^2(P_n)$ norm of a particular spectral projection estimator.\footnote{\cite{ingster2009} project the responses onto a the span of trigonometric basis functions. Since $P$ is uniform on the unit cube, such functions are eigenfunctions of $\Delta_P$ when the right boundary conditions are imposed.}

A major difference between testing and estimation over Sobolev spaces is the requirement that $4s > d$. When $4s \leq d$, the functions in $H^s(\Xset)$ are very irregular. Crucially, $H^s(\Xset)$ no longer continuously embeds into $\Leb^4(\Xset)$ when $4s \leq d$, and test statistics using the $L^2(P_n)$ norm are no longer guaranteed to have finite variance.\footnote{Note that this will not affect the analysis for estimation, because for estimation we only need to control the first two moments of $f_0$.} In fact, we have not seen any analysis which describes the minimax rate for nonparametric regression testing over Sobolev spaces in the $4s \leq d$ regime. However, if one explicitly assumes that $f_0 \in \Leb^4(\mc{X})$, then the critical radius is characterized by the dimension-free rate $\epsilon_n^2(\Leb^4(\mc{X};M)) \asymp Mn^{-1/2}$.\footnote{As a sanity check note that this is strictly worse than the rate in~\eqref{eqn:sobolev_space_testing_critical_radius}. In other words, whenever $4s > d$, the embedding $H^s(\mc{X}) \subseteq L^4(\mc{X})$ never yields a tight upper bound.} As we discuss after our first main theorem regarding testing with Laplacian eigenmaps (Theorem~\ref{thm:laplacian_eigenmaps_testing_fo}), this rate is achievable by a test based on $\wh{T}$.

\paragraph{Manifold setup.}
Under Model~\ref{def:model_manifold}, both~\eqref{eqn:sobolev_space_minimax_estimation_rate} and~\eqref{eqn:sobolev_space_testing_critical_radius} continue to hold, but with the ambient dimension $d$ replaced everywhere by the intrinsic dimension $m$. The estimator $\wt{f}$ and a test using the statistic $\wt{T}$---with $\psi_1,\psi_2,\ldots$ now the eigenfunctions of the manifold weighted Laplace-Beltrami operator $\Delta_P$---achieve the optimal estimation and testing rates. This is because each of the three facts mentioned after Proposition~\ref{prop:spectral_series_estimation} have analogues when the domain is a smooth manifold (See~\citet{hendriks1990}, who analyzes a spectral projection density estimator, for details.)

\paragraph{In-sample mean squared error.}
As mentioned in our introduction, roughly speaking one of our main conclusions is that the Laplacian eigenmaps estimator $\wh{f}$ is minimax rate-optimal. It is worth being clear about what we do and do not mean by this statement. We do not mean that the estimator $\wh{f}$ will match the upper bound given in~\eqref{eqn:sobolev_space_minimax_estimation_rate}, since such a statement does not make sense when the estimator is defined only at the random design points $X_1,\ldots,X_n$. Instead we will measure loss using the squared $L^2(P_n)$ error. In Section~\ref{sec:out_of_sample} we show that an extension of $\wh{f}$ defined over all $\mc{X}$ has $L^2(P)$ error comparable to the $L^2(P_n)$ error of $\wh{f}$. We also believe that in the random design setting we work in, simple arguments will imply that $L^2(P_n)$ risk has the same minimax rate of convergence as $L^2(P)$ risk. We sketch such an argument in Section~\ref{sec:out_of_sample}, but do not further pursue the details. 

Additionally, we will not actually measure accuracy using the expectation of the loss. Rather, we will give a high-probability bound on $\|\cdot\|_n^2$. For instance, when $f_0 \in H^1(\mc{X};1)$, we will show that with probability $1 - \delta$ the loss $\|\wh{f} - f_0\|_n^2 \leq C_{\delta} n^{-2/(2 + d)}$, for a constant $C_{\delta}$ that depends on $\delta$ but not on $f_0$ or $n$. Thus we give an upper bound on the $(1 - \delta)$th quantile of $\|\wh{f} - f_0\|_n^2$, rather than an upper bound on its expectation. We explain the reason for this in Section~\ref{sec:minimax_optimal_laplacian_eigenmaps}. We also show that if $f_0$ is bounded in a larger norm---for instance, if it is H\"{o}lder rather than Sobolev smooth---then we can obtain bounds on the expected $L^2(P_n)$ loss.

There is one other subtlety introduced by the use of in-sample mean squared error. Technically speaking, elements $f \in H^s(\mc{X})$ are equivalence classes, defined only up to a set of measure zero. Thus one cannot speak of the pointwise evaluation $f_0(X_i)$, as we do by defining our target of estimation to be $f_0(X_i)$, $i=1,\ldots,n$, until one selects \emph{representatives}. When $s > d/2$, every element $f$ of $H^s(\mc{X})$ admits a continuous version $f^{\ast}$, and as is standard we set this to be our favored representative. When $s \leq d/2$, some elements in $H^s(\mc{X})$ do not have any continuous version; however they admit a \emph{quasi-continuous} version \citep{evans15} known as the \emph{precise representative}, and we use this representative. To be clear, however, it does not really matter which representative we choose. Since all versions agree except on a set of measure zero, and since $P$ is absolutely continuous with respect to Lebesgue measure (in Model~\ref{def:model_flat_euclidean}) or the volume form $d\mu$ (in Model~\ref{def:model_manifold}), with probability $1$ any two versions $g_0, h_0 \in f_0$ will satisfy $g_0(X_i) = h_0(X_i)$ for all $i = 1,\ldots,n$. The bottom line is that we can use the notation $f_0(X_i)$ without fear of ambiguity or confusion.

Finally, we note that for testing none of these comments are relevant. We will show that our test has small worst case risk whenever $\epsilon \gtrsim \epsilon_n(H_0^s(\mc{X};M))$, thus establishing that it is a minimax optimal test in the usual sense.

\section{Minimax Optimality of Laplacian Eigenmaps}
\label{sec:minimax_optimal_laplacian_eigenmaps}

As previously explained, Laplacian eigenmaps is a discrete and noisy approximation to a spectral projection method using the eigenfunctions of $\Delta_P$. This is particularly useful when $P$ is unknown, or when the eigenfunctions of $\Delta_P$ cannot be explicitly computed. Our goal is to show that Laplacian eigenmaps methods are rate-optimal, notwithstanding the potential extra error incurred by this approximation. In this section and the following one, we will see that this is indeed the case: the estimator $\wh{f}$, and a test using the statistic $\wh{T}$, achieve optimal estimation and goodness-of-fit testing rates over Sobolev classes. 

In this section we will cover the flat Euclidean case, where we observe data $(X_1,Y_1),\ldots,(X_n,Y_n)$ according to Model~\ref{def:model_flat_euclidean}. We will divide our theorem statements based on whether we assume the regression function $f_0$ belongs to the first order Sobolev class ($s = 1$) or a higher-order Sobolev class ($s > 1$), since the details of the two settings are somewhat different.

\subsection{First-order Sobolev classes}
\label{sec:first_order_sobolev_classes}
We begin by assuming $f_0 \in H^1(\mc{X})$. We show that $\wh{f}$ and a test based on $\wh{T}$ are minimax optimal, for all values of $d$, and under no additional assumptions (beyond those of Model~\ref{def:model_flat_euclidean}) on the data generating process, i.e. on either $P$ or $f_0$.

\paragraph{Estimation.} When the kernel $\eta$, graph radius $\varepsilon$, and number of eigenvectors $K$ are chosen appropriately, we show in Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} that the estimator $\wh{f}$ achieves the minimax rate over $H^1(\mc{X};M)$, when error is measured in squared $L^2(P_n)$ norm.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{0}
	\item
	\label{asmp:kernel_flat_euclidean}
	The kernel function $\eta$ is a nonincreasing function supported on $[0,1]$. Its restriction to $[0,1]$ is Lipschitz, and $\eta(1) > 0$. Additionally, it is normalized so that
	\begin{equation*}
	\int_{\Rd} \eta(\|z\|) \,dz = 1.
	\end{equation*}
	and we assume \smash{$\sigma_{\eta} := \frac{1}{d}\int_{\Rd} \|x\|^2 \eta(\|x\|) \,dx < \infty$}.
\end{enumerate}
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{0}
	\item 
	\label{asmp:parameters_estimation_fo} 
	For constants $c_0$ and $C_0$, the graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy the following inequalities:
	\begin{equation}\\
	\label{eqn:radius_fo} 
	C_0\biggl(\frac{\log n}{n}\biggr)^{1/d} \leq \varepsilon \leq c_0\min\{1,K^{-1/d}\},
	\end{equation}
	and 
	\begin{equation}
	\label{eqn:eigenvector_estimation_fo} 
	K = \min\Bigl\{\floor{(M^2n)^{d/(2 + d)}} \vee 1, n\Bigr\}.
	\end{equation}
\end{enumerate}
We comment on these assumptions after stating our first main theorem, regarding the estimation error of Laplacian eigenmaps.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_fo}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally $f_0 \in H^1(\mc{X},M)$. There are constants $c,C$ and $N$ (not depending on $f_0$, $M$ or $n$), such that the following statement holds for all $n \geq N$ and any $\delta \in (0,1)$: if the Laplacian eigenmaps estimator $\wh{f}$ is computed with kernel $\eta$ satisfying~\ref{asmp:kernel_flat_euclidean}, and parameters $\varepsilon$ and $K$ satisfying~\ref{asmp:parameters_estimation_fo}, then
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_fo}
	\|\wh{f} - f_0\|_n^2 \leq C\Bigl(\frac{1}{\delta}M^2(M^2n)^{-2/(2 + d)} \wedge 1\Bigr) \vee \frac{1}{n},
	\end{equation}
	with probability at least $1 - \delta - Cn\exp(-cn\varepsilon^d) - \exp(-K)$.
\end{theorem}
From~\eqref{eqn:laplacian_eigenmaps_estimation_fo} it follows immediately that with high probability $\|\wh{f} - f_0\|_n^2 \lesssim M^2(M^2n)^{-2/(2 + d)}$ whenever $n^{-1/2} \lesssim M \lesssim n^{1/d}$. 

Some other remarks:
\begin{itemize}
	\item When $M = o(n^{-1/2})$, then computing Laplacian eigenmaps with $K = 1$ achieves the parametric rate $\|\wh{f} - f_0\|_n^2 \lesssim n^{-1}$, and the zero-estimator $\wh{f} = 0$ achieves the better rate $\|\wh{f} - f_0\|_n^2 \lesssim M^2$. However, we do not know what the minimax rate is in this regime. On the other hand, when $M = \omega(n^{1/d})$, then computing Laplacian eigenmaps with $K = n$ achieves the rate $\|\wh{f} - f_0\|_n^2 \lesssim 1$, which is better than the rate in~\eqref{eqn:sobolev_space_minimax_estimation_rate}. This is because we are evaluating error in $L^2(P_n)$ rather than $L^2(P)$. However, in truth these are edge cases, which do not fall neatly into the framework of nonparametric regression. 
	\item The assumptions placed on the kernel function $\eta$ are needed for technical reasons. They can likely be weakened, although we note that they are already fairly general. The lower bound on $\varepsilon$ imposed by~\eqref{eqn:radius_fo} is on the order of the connectivity threshold, the smallest length scale at which the resulting graph will still be connected with high probability. On the other hand, as we will see in Section~\ref{subsec:analysis}, the upper bound on $\varepsilon$ is needed to ensure that the graph eigenvalue $\lambda_K$ is of at least the same order as the continuum eigenvalue $\lambda_K(\Delta_P)$. Finally, we choose $K = (M^2n)^{2d/(2 + d)}$ (when possible) to optimally trade-off bias and variance.
	\item We note that the ranges~\eqref{eqn:radius_fo} and~\eqref{eqn:eigenvector_estimation_fo} depend on quantities, such as the dimension $d$ and radius of the Sobolev ball $M$, which are usually unknown. In practice, one typically tunes hyper-parameters by sample-splitting or cross-validation. However, because the estimator $\wh{f}$ is defined only in-sample, we cannot use such methods to select the graph radius $\varepsilon$, or number of eigenvectors $K$. We return to this issue in Section~\ref{sec:out_of_sample}, when we propose an out-of-sample extension of $\wh{f}$. 
	
	\textcolor{red}{(TODO): Ryan, you asked about AIC, but I'm not clear about whether you want me to (a) make some remark about it, (b) actually go through the work of giving guarantees, or (c) were just curious.}
	
	\item The upper bound given in equation~\eqref{eqn:laplacian_eigenmaps_estimation_fo} holds with probability $1 - \delta - Cn\exp(-cn\varepsilon^d)$. Under the stronger assumption that $f_0 \in C^1(\mc{X};M)$ we can replace the factor of $\delta$ by the sharper $\delta^2/n$, which is less than $\delta$ because $\delta \in (0,1)$. Then a routine calculation, which we carry out in Appendix~\ref{subsec:upper_bound_l2pn_risk}, shows that the expected $L^2(P_n)$ loss is on the same order as~\eqref{eqn:laplacian_eigenmaps_estimation_fo}, matching the minimax rate.
\end{itemize}

\paragraph{Testing.} Consider the test $\varphi = \1\{\wh{T} \geq t_{a}\}$, where $t_{a}$ is the threshold
\begin{equation*}
t_{a} := \frac{K}{n} + \frac{1}{n}\sqrt{\frac{2K}{a}}.
\end{equation*}
This choice of threshold $t_{a}$ guarantees that $\varphi$ is a level-$a$ test. Moreover, when $\varepsilon$ and $K$ are chosen appropriately, the test $\varphi$ has negligible Type II error against alternatives separated from the null by at least $\|f_0\|_{P}^2 \gtrsim M^2(M^2n)^{-4/(4 + d)}$, whenever $d < 4$. 

\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{1}
	\item 
	\label{asmp:parameters_testing_fo}
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy~\eqref{eqn:radius_fo}. Additionally,
	\begin{equation}
	\label{eqn:eigenvector_testing_fo}
	K = \min\Bigl\{\floor{(M^2n)^{2d/(4 + d)}} \vee 1, n\Bigr\}.
	\end{equation}
\end{enumerate}
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_fo}
	Fix $a,b \in (0,1)$. Suppose Model~\ref{def:model_flat_euclidean}. Then $\mathbb{E}_0[\varphi] \leq a$, i.e $\varphi$ is a level-$a$ test. Suppose additionally $f_0 \in H^1(\mc{X},M)$, and that $d < 4$. Then there exist constants $C$ and $N$ that do not depend on $f_0$, such that the following statement holds for all $n$ larger than $N$: if the Laplacian eigenmaps test $\varphi$ is computed with kernel $\eta$ satisfying~\ref{asmp:kernel_flat_euclidean}, and parameters $\varepsilon$ and $K$ satisfying~\ref{asmp:parameters_testing_fo}, and if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_fo}
	\|f_0\|_P^2 \geq C\biggl(\Bigl(M^2(M^2n)^{-4/(4 + d) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{2/d}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}
Although~\eqref{eqn:laplacian_eigenmaps_testing_criticalradius_fo} involves taking the maximum of several different terms, the important takeaway of Theorem~\ref{thm:laplacian_eigenmaps_testing_fo} is that if $n^{-1/2} \lesssim M \lesssim n^{1/d}$, then $\varphi$ has small worst-case risk as long as $f_0$ is separated from $0$ by at least $M^2(M^2n)^{-4/(4 + d)}$. Note that unlike in the estimation setting---where we measured loss in $L^2(P_n)$ error---the separation in~\eqref{eqn:laplacian_eigenmaps_testing_criticalradius_fo} is measured in $L^2(P)$ norm. Thus~\eqref{eqn:laplacian_eigenmaps_testing_criticalradius_fo} implies that $\varphi$ is a minimax rate-optimal test over $H^1(\mc{X};M)$, in the usual sense.

Some other remarks:
\begin{itemize}
	\item As mentioned previously, when $d \geq 4$ the first order Sobolev space $H^1(\mc{X})$ does not continuously embed into $\Leb^4(\mc{X})$, and we do not know the optimal rates for regression testing over $H^1(\mc{X},M)$. On the other hand, if we explicitly assume $f_0 \in \Leb^4(\mc{X})$, then the Laplacian eigenmaps test with $K = n$, has small type II error whenever $\|f_0\|_P^2 \gtrsim Mn^{-1/2}$. Note that when $K = n$ the Laplacian eigenmaps test statistic is nothing but the squared $L^2(P_n)$ norm of ${\bf Y}$, $\wh{T} = \|{\bf Y}\|_n^2$. See \textcolor{blue}{(Green et al. 2021)} for details.
	\item As in the estimation setting, the range of Sobolev ball radii $n^{-1/2} \lesssim M \lesssim n^{1/d}$ for which Theorem~\ref{thm:laplacian_eigenmaps_testing_fo} implies that $\varphi$ is a rate-optimal test covers all those cases for which the critical radius $\epsilon(H^1(\mc{X};M))$ is both $\Omega(1/n)$ and $O(1)$.
\end{itemize}

\subsection{Higher-order Sobolev classes}
\label{sec:higher_order_sobolev_classes}
We now consider the situation where the regression function displays some higher-order regularity. For reasons already discussed, we also assume the zero-trace boundary condition, i.e. $f_0 \in H_0^s(\mc{X})$. We show that Laplacian eigenmaps methods continue to be optimal for all orders of $s$, as long as the design density is itself also sufficiently regular, $p \in C^{s - 1}(\mc{X})$. In estimation, this is the case for any dimension $d$, whereas in testing it is the case only when $d \leq 4$. 

\paragraph{Estimation.}
In order to show that $\wh{f}$ is an optimal estimator over $H_0^s(\mc{X};M)$, we will require that $\varepsilon$ be meaningfully larger than the lower bound in~\ref{asmp:parameters_estimation_fo}.
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{2}
	\item 
	\label{asmp:parameters_estimation_ho}
	For constants $c_0$ and $C_0$, the graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy
	\begin{equation}
	\label{eqn:radius_ho}
	C_0\max\biggl\{\biggl(\frac{\log}{n}\biggr)^{1/d}, (M^2n)^{-1/(2(s - 1) + d)}\biggr\} \leq \varepsilon \leq c_0\min\{1, K^{-1/d}\}
	\end{equation}
	and
	\begin{equation*}
	K = \min\Bigl\{\floor{(M^2n)^{d/(2s + d)}} \vee 1,n\Bigr\}
	\end{equation*}
\end{enumerate}
Crucially, when $n$ is sufficiently large the two conditions in~\ref{asmp:parameters_estimation_ho} are guaranteed to not be mutually exclusive. This is because so long as $M^2 = \omega(n^{-1})$ then $(M^2n)^{-2/(2(s - 1) + d)} = o((M^2n)^{-2/(2s + d)})$, regardless of $s$ and $d$.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_ho}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally $f_0 \in H_0^s(\mc{X},M)$ and $p \in C^{s - 1}(\mc{X})$. There exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds all for all $n$ larger than $N$ and for any $\delta \in (0,1)$: if the Laplacian eigenmaps estimator $\wh{f}$ is computed with kernel $\eta$ satisfying~\ref{asmp:kernel_flat_euclidean}, and parameters $\varepsilon$ and $K$ satisfying~\ref{asmp:parameters_estimation_ho}, then
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_ho}
	\|\wh{f} - f_0\|_n^2 \leq C\Bigl(\frac{1}{\delta}M^2(M^2n)^{-2s/(2s + d)} \wedge 1\Bigr) \vee \frac{1}{n},
	\end{equation}
	with probability at least $1 - \delta - Cn\exp(-cn\varepsilon^d) - \exp(-K)$.
\end{theorem}
Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, in combination with Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo}, implies that in the flat Euclidean setting Laplacian eigenmaps is an in-sample minimax rate-optimal estimator over Sobolev classes, for all values of $s$ and $d$. Some other remarks:
\begin{itemize}
	\item We do not require that the regularity of the Sobolev space satisfy $s > d/2$, a condition often seen in the literature. In the sub-critical regime $s \leq d/2$, the Sobolev space $H^s(\mc{X})$ is quite irregular. It is not a Reproducing Kernel Hilbert Space (RKHS), nor does it continuously embed into $C^0(\mc{X})$, much less into any H\"{o}lder space. As a result, for certain versions of the nonparametric regression problem---e.g. when loss is measured in $\Leb^{\infty}$ norm, or when the design points $\{X_1,\ldots,X_n\}$ are assumed to be fixed---in a minimax sense even consistent estimation is not possible. Likewise, certain estimators are ``off the table'', most notably RKHS-based methods such as thin-plate splines of degree $k \leq d/2$. Nevertheless, for random design regression with error measured in $\Leb^2(P)$-norm, the spectral projection estimator $\wt{f}$ defined in~\eqref{eqn:spectral_series_estimator} obtains the ``usual'' minimax rates $n^{-2s/(2s + d)}$ for all values of $s$ and $d$. Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo} and \ref{thm:laplacian_eigenmaps_estimation_ho} show that the same is true with respect to Laplacian eigenmaps, with error measured in $\Leb^2(P_n)$-norm.
	\item The requirement $p \in C^{s - 1}(\mc{X})$ is a strong condition, but is essential to showing that $\wh{f}$ enjoys faster rates of convergence when $s > 1$. We explain why in Section~\ref{subsec:analysis}, where we discuss our analysis.
\end{itemize}

\paragraph{Testing.} The test $\varphi$ can adapt to the higher-order smoothness of $f_0$, when $\varepsilon$ and $K$ are chosen correctly.
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{3}
	\item 
	\label{asmp:parameters_testing_ho}
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy~\eqref{eqn:radius_ho}. Additionally,
	\begin{equation}
	\label{eqn:eigenvector_testing_ho}
	K = \min\Bigl\{\floor{(M^2n)^{2d/(4 + d)}} \vee 1, n\Bigr\}.
	\end{equation}
\end{enumerate}
When $d \leq 4$, for any value of $s \in \mathbb{N}$ when $n$ is sufficiently large it is possible to choose $\varepsilon$ and $K$ such that both~\eqref{eqn:radius_ho} and~\eqref{eqn:eigenvector_testing_ho} are satisfied, and our next theorem establishes that in this situation $\varphi$ is an optimal test.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_ho}
	Fix $a,b \in (0,1)$. Suppose Model~\ref{def:model_flat_euclidean}. Then $\mathbb{E}_0[\varphi] \leq a$, i.e $\varphi$ is a level-$a$ test. Suppose additionally $f_0 \in H_0^s(\mc{X},M)$, that $p \in C^{s-1}(\mc{X})$, and that $d \leq 4$. Then there exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds for all $n \geq N$: if the Laplacian eigenmaps test $\varphi$ is computed with kernel $\eta$ satisfying~\ref{asmp:kernel_flat_euclidean}, and parameters $\varepsilon$ and $K$ satisfying~\ref{asmp:parameters_testing_ho}, and if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_ho}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-4s/(4s + d) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{2s/d}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}
Similarly to the first-order case, the main takeaway from Theorem~\ref{thm:laplacian_eigenmaps_testing_ho} is that $\varphi$ is a minimax optimal test over $H_0^s(\mc{X})$ when $n^{-1/2} \lesssim M^2 \lesssim n^{1/d}$. However, unlike the first-order case, when $4 < d < 4s$ the minimax testing rate over $H_0^s(\mc{X})$ is still on the order of $M^2(M^2n)^{-4s/(4s + d)}$. Unfortunately, we can no longer claim that $\varphi$ is an optimal test in this regime.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_ho_suboptimal}
	Under the same setup as Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, but with $4 < d < 4s$. If the Laplacian eigenmaps test $\varphi$ is computed with kernel $\eta$ satisfying~\ref{asmp:kernel_flat_euclidean}, number of eigenvectors $K$ satisfying~\eqref{eqn:eigenvector_testing_ho}, and $\varepsilon = (M^2n)^{-1/(2(s - 1) + d)}$, and if 
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_ho_suboptimal}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-2s/(2(s - 1) + d) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{2s/d}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}
Note that as a consequence of Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, if we choose $K = n^{-2s/(2s + d)}$ then $\varphi$ must have small Type II error whenever $\|f_0\|_P^2 \gtrsim n^{-2s/(2s + d)}$. Theorem~\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal} shows that $\varphi$ can achieve better, but still not optimal, rates. As a technical matter, the problem is that when $d > 4$ there do not exist any choices of $\varepsilon$ and $K$ which satisfy both~\eqref{eqn:radius_ho} and~\eqref{eqn:eigenvector_testing_ho}, and as a result we cannot optimally balance (our upper bound on) testing bias and variance (defined momentarily in~\eqref{eqn:testing_biasvariance}). Although we suspect $\varphi$ is truly suboptimal when $d > 4$, technically speaking~\eqref{eqn:testing_biasvariance} gives only an upper bound on testing bias, and thus we cannot rule out that the test $\varphi$ is optimal for all $4 < d < 4s$. We leave the matter to future work.

That being said, it is somewhat remarkable that Laplacian eigenmaps \emph{can} take advantage of higher-order smoothness, and especially surprising that it can do so in an optimal manner. The sharpest known results show that the graph Laplacian eigenvectors $v_k$ converge to eigenfunctions $\psi_k$ at a rate of \textcolor{red}{(?)}. Naively applying these results, one can show that $\wh{f}$ to $\wt{f}$, but only at a rate far slower than the optimal rates for regression. Of course when the index $K$ increases with $n$, as is necessary to optimally balance bias and variance, the issue only gets worse. Clearly, as a method for regression, the rate of convergence of Laplacian eigenmaps is much better than the rate implied by (what is currently known about) the concentration of individual eigenvectors around their continuum limits.

\subsection{Analysis}
\label{subsec:analysis}

We now outline the high-level strategy we follow when proving each of Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal}. We analyze the estimation error of $\wh{f}$, and the testing error of $\wh{\varphi}$, by first conditioning on the design points $X_1,\ldots,X_n$ and deriving \emph{design-dependent} bias and variance terms. For estimation, we have that with probability at least $1 - \exp(-K)$,
\begin{equation}
\label{eqn:estimation_biasvariance}
\|\wh{f} - f_0\|_n^2 \leq \underbrace{\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{K}^s}}_{\textrm{bias}} + \underbrace{\frac{5K}{n} \vphantom{\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{K}^s}}}_{\textrm{variance}}.
\end{equation}
For testing, we have that $\varphi$ (which is a level-$a$ test by construction) also has small Type II Error, $\Ebb_{f_0}[1 - \phi] \leq b/2$, if 
\begin{equation}
\label{eqn:testing_biasvariance}
\|f_0\|_n^2 \geq  \underbrace{\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{K}^s}}_{\textrm{bias}} + \underbrace{32\frac{\sqrt{2K}}{n}\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr]}_{\textrm{variance}}.
\end{equation}
The quadratic form $\dotp{L_{n,\varepsilon}^s f_0}{f_0}_n$, eigenvalue $\lambda_{K}$, and empirical squared norm $\|f_0\|_n^2$ are each random variables that depend the random design points $X_1,\ldots,X_n$. We proceed to establish suitable upper and lower bounds on these quantities. 

\paragraph{Estimates on graph quadratic forms.}
In Proposition~\ref{prop:graph_seminorm_fo} we restate an upper bound on the Dirichlet energy $\dotp{L_{n,\varepsilon}f}{f}_n$ from \textcolor{blue}{Green et al. 2021}. 
\begin{proposition}[Lemma~1 of \textcolor{blue}{Green et al. 2021}]
	\label{prop:graph_seminorm_fo}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally $f \in H^1(\mc{X})$. There exist constants $c,C$ that do not depend on $f$ or $n$ such that the following statement holds for any $\delta \in (0,1)$: if $\eta$ satisfies~\ref{asmp:kernel} and $\varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_seminorm_fo}
	\dotp{L_{n,\varepsilon}f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^1(\mc{X})}^2,
	\end{equation}
	with probability at least $1 - \delta$.
\end{proposition}
Proposition~\ref{prop:graph_seminorm_fo} follows by upper bounding the expectation of $\dotp{L_{n,\varepsilon}f}{f}_n$, which is the Dirichlet energy  $E_{P,\varepsilon}(f) := \dotp{L_{P,\varepsilon}f}{f}_P$, by (a constant times) the squared Sobolev norm $\|f\|_{H^1(\mc{X})}^2$.

In this work, we establish that an analogous bound holds for $\dotp{L_{n,\varepsilon}^sf_0}{f_0}_n$ when $s > 1$. We call this quantity the order-$s$ \emph{graph Sobolev semi-norm}.
\begin{proposition}
	\label{prop:graph_seminorm_ho} 
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally that $f \in H_0^s(\mc{X})$ and $p \in C^{s - 1}(\mc{X})$. Then there exist constants $c$ and $C$ that do not depend on $f_0$ or $n$ such that the following statement holds for any $\delta \in (0,1)$: if $\eta$ satisfies~\ref{asmp:kernel_flat_euclidean} and $Cn^{-1/(2(s - 1) + d)} < \varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_seminorm_ho}
	\dotp{L_{n,\varepsilon}^s f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^s(\mc{X})}^2 ,
	\end{equation}
	with probability at least $1 - \delta$.
\end{proposition}
We now summarize the techniques used to prove Proposition~\ref{prop:graph_seminorm_ho}, which will help explain what role the conditions on $f_0$,$p$ and $\varepsilon$ play. To upper bound $\dotp{L_{n,\varepsilon}^sf}{f}_n$ in terms of $\|f\|_{H^s(\mc{X})}^2$, we introduce an intermediate quantity: the \emph{order-s non-local Sobolev seminorm} $\dotp{L_{P,\varepsilon}^sf}{f}_{P}$. This seminorm is defined with respect to $L_{P,\varepsilon}$, which is a non-local approximation to $\Delta_P$, 
\begin{equation}
\label{eqn:nonlocal_laplacian}
L_{P,\varepsilon}f(x) := \frac{1}{\varepsilon^{d + 2}}\int_{\mc{X}}\bigl(f(z) - f(x)\bigr) \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dP(x).
\end{equation}
Then the proof of Proposition~\ref{prop:graph_seminorm_ho} proceeds according to the following steps.
\begin{itemize}
	\item First we note that $\dotp{L_{n,\varepsilon}^s f}{f}_n$ is itself a biased estimate of the non-local seminorm $\dotp{L_{P,\varepsilon}^sf}{f}_{P}$. Specifically, $\dotp{L_{n,\varepsilon}^s f}{f}_n$ is a $V$-statistic, meaning it is the sum of an unbiased estimator of $\dotp{L_{P,\varepsilon}^sf}{f}_{P}$ (in other words, a $U$-statistic) plus some higher-order, pure bias terms. We show that these pure bias terms are negligible when $\varepsilon = \omega(n^{-1/(2(s - 1) + d)})$. 
	\item For $x$ sufficiently in the interior of $\mc{X}$, we show that $L_{P,\varepsilon}^jf(x) \to \sigma_{\eta}^j \Delta_P^jf(x)$ as $\varepsilon \to 0$. Here $j = (s - 1)/2$ when $s$ is odd and $j = (s - 2)/2$ when $s$ is even. This step bears some resemblance to the analysis of the bias term in kernel smoothing, and requires that $p \in C^{s-1}(\mc{X})$.
	\item On the other hand for $x$ sufficiently near the exterior of $\mc{X}$, $L_{P,\varepsilon}^jf(x)$ does not converge to $\Delta_P^jf(x)$. Instead, we use the zero-trace property of $f$ to show that $L_{P,\varepsilon}^jf(x)$ is small.
	\item Finally, we combine the results of previous two steps to deduce an upper bound on $\dotp{L_{P,\varepsilon}^sf}{f}_{P}$ in terms of the squared Sobolev norm $\|f\|_{H^s(\mc{X})}^2$.  Roughly speaking, when $s$ is odd, $\dotp{L_{P,\varepsilon}^sf}{f}_P = E_{P,\varepsilon}(L_{P,\varepsilon}^jf) \approx \sigma_{\eta}^{2j}E_{P,\varepsilon}(\Delta_P^jf)$, whereas when $s$ is even $\dotp{L_{P,\varepsilon}^sf}{f}_P = \|L_{P,\varepsilon}L_{P,\varepsilon}^{j}f\|_{P}^2 \approx \sigma_{\eta}^{2j}\|L_{P,\varepsilon} \Delta_Pf\|_P^2$. Reasoning in this way, we can translate estimates of $L_{P,\varepsilon}^jf$ into an upper bound on the order-$s$ non-local Sobolev seminorm, even though $s > j$.
\end{itemize}
Together, these steps establish Proposition~\ref{prop:graph_seminorm_ho}. It is worth pointing out that we do not try to show $L_{P,\varepsilon}^sf(x) \to \Delta_{P}^sf(x)$. This may seem like a natural first step towards a simple proof that $\dotp{L_{P,\varepsilon}^sf}{f}_{P} \to \dotp{\Delta_P^sf}{f}_{P}$. The problem is that $\Delta_P^s$ is an order-$2s$ differential operator, whereas we assume that $f$ has only $s$ bounded derivatives. Instead we go for the slightly more complicated approach outlined above. 

% AG: I think this following might just be garbage, but I will leave it in in case it piques Ryan or Siva's interest. 

% Proposition~\ref{prop:graph_seminorm_ho}, and its proof, show that the iterated graph Laplacian $L_{n,\varepsilon}^s$ does a reasonable job of approximating a higher-order differential operator. The conditions we require --- that $\varepsilon$ be sufficiently large, that $f$ be zero-trace, and that $p$ have regularity of order $s - 1$ --- indicate that the iterated graph Laplacian is an imperfect, if adequate, tool for this job. This is no fault of the graph Laplacian. Rather, as pointed out by \textcolor{red}{(Sadhanala et al. 2017)} its reflects that graph Laplacians were designed for a very general circumstance in which the only notions of derivation are first-differentials, divergences, and compositions of the two. Indeed, viewed in this light it is a pleasant surprise that iterated graph Laplacians approximate higher-order differentials at all. When one assumes Euclidean structure, as we do in this paper, there exist many methods (e.g. local linear embeddings, Hessian local linear embedding, local tangent space alignment) which leverage this structure to approximate differential operators in a more sophisticated manner. However, thus far little theoretical investigation has been done into even the pointwise behavior of these approaches.%

\paragraph{Neighborhood graph eigenvalue.}
On the other hand, several recent works \citep{burago2014,garciatrillos18,calder2019} have analyzed the convergence of $\lambda_{k}$ towards $\lambda_{k}(\Delta_P)$. They provide explicit bounds on the relative error $|\lambda_{k} - \lambda_{k}(\Delta_P)|/\lambda_{k}(\Delta_P)$, which show that the relative error is small for sufficiently large $n$ and small $\varepsilon$. Crucially, the guarantees hold simultaneously for all $1 \leq k \leq K$ as long as $\lambda_{K}(\Delta_P) = O(\varepsilon^{-2})$. These results are actually stronger than are necessary to establish Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_testing_ho}---in order to get rate-optimality, we need only show that for the relevant values of $K$, $\lambda_{K}/\lambda_K(P) = \Omega_P(1)$---but unfortunately they all assume $P$ is supported on a manifold without boundary (i.e. they assume Model~\ref{def:model_manifold} rather than Model~\ref{def:model_flat_euclidean}). 

In the case where $\mc{X}$ is assumed to have a boundary, the graph Laplacian $L_{n,\varepsilon}$ is a reasonable approximation of the operator $\Delta_P$ at points $x \in \mc{X}$ for which $B(x,\varepsilon) \subseteq \mc{X}$. In contrast, at points $x$ near the boundary of $\mc{X}$, the graph Laplacian is known to approximate a different operator altogether \citep{belkin2012}. This is reminiscent of the boundary effects present in the analysis of kernel smoothing. Thus proving convergence of $\lambda_k$ to a continuum limit becomes a substantially more challenging problem when $\mc{X}$ has a boundary. Rather than establishing such a result, we will instead directly use Lemma~2 of \textcolor{red}{(Green et al. 2021)}, whose assumptions match our own, and who give a weaker bound on $\lambda_k/\lambda_k(\Delta_P)$ that will nevertheless suffice for our purposes. 

\begin{proposition}[Lemma~2 of \textcolor{blue}{(Green et al. 2021)}]
	\label{prop:graph_eigenvalue}
	Suppose Model~\ref{def:model_flat_euclidean}. Then there exist constants $c$ and $C$ such that the following statement holds: if $\eta$ satisfies~\ref{asmp:kernel_flat_euclidean} and $C(\log n/n)^{1/d} < \varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_eigenvalue}
	\lambda_k \geq c \cdot \min\Bigl\{\lambda_k(\Delta_P), \frac{1}{\varepsilon^{2}} \Bigr\} \quad \textrm{for all $1 \leq k \leq n$,}
	\end{equation}
	with probability at least $1 - Cn\exp\{-c n\varepsilon^d\}$. 
\end{proposition}
Note immediately that $\lambda_0(\Delta_P) = \lambda_0 = 0$. Furthermore, Weyl's Law (Lemma~\ref{lem:weyl}) tells us that under Model~\ref{def:model_flat_euclidean}, $k^{2/d} \lesssim \lambda_{k}(\Delta_P) \lesssim k^{2/d}$ for all $k \in \mathbb{N}, k > 1$. Combining these statements with~\eqref{eqn:graph_eigenvalue}, we conclude that $\lambda_{K} = \Omega_P(K^{2/d})$ so long as $K \lesssim \varepsilon^{-d}$. 

\paragraph{Empirical norm.}
Finally, in order to show that $\varphi$ has small Type II error whenever~$\|f_0\|_P$ is greater than the critical radius given by~\eqref{eqn:sobolev_space_testing_critical_radius}, we require a lower bound on $\|f_0\|_n^2$ in terms of $\|f_0\|_P^2$. In Proposition~\ref{prop:empirical_norm_sobolev} we establish that such a one-sided bound holds, whenever $\|f_0\|_P$ is sufficiently large.
\begin{proposition}
	\label{prop:empirical_norm_sobolev}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally that $f \in H^s(\Xset,M)$ for some $s > d/4$. There exist constants $c$ and $C$ that do not depend on $f_0$ or $n$ such that the following statement holds for any $\delta > 0$:  if
	\begin{equation}
	\label{eqn:empirical_norm_sobolev_1}
	\norm{f}_{P} \geq C M \biggl(\frac{1}{\delta n}\biggr)^{s/d}
	\end{equation}
	then with probability at least $1 - \exp\{-(cn \wedge 1/\delta)\}$.
	\begin{equation}
	\label{eqn:empirical_norm_sobolev}
	\norm{f}_n^2 \geq \frac{1}{2} \|f_0\|_P^2.
	\end{equation}
\end{proposition}
To prove Proposition~\ref{prop:empirical_norm_sobolev}, we use the Gagliardo-Nirenberg interpolation inequality to control the $4$th moment of $f$ in terms of $\|f\|_P$ and $|f|_{H^s(\mc{X})}$, then invoke a one-sided Bernstein's inequality as in \cite[Section 14.2]{wainwright2019}. Note carefully that the statement~\eqref{eqn:empirical_norm_sobolev} is \emph{not} a uniform guarantee over all $f \in H^s(\mc{X};M)$, as such a statement cannot hold in the sub-critical regime ($2s \leq d$). Fortunately, a pointwise bound---meaning a bound that holds with high probability for a single $f \in H^s(\mc{X})$---is sufficient for our purposes.

Finally, invoking the bounds of Propositions~\ref{prop:graph_seminorm_fo}-\ref{prop:empirical_norm_sobolev} inside the bias-variance tradeoffs~\eqref{eqn:estimation_biasvariance} and~\eqref{eqn:testing_biasvariance} and then choosing $K$ to balance bias and variance (when possible), leads to the conclusions of Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal}.

\subsection{Computational considerations}
Recall that when $s = 1$, we have shown that Laplacian eigenmaps is optimal when $\varepsilon \asymp (\log n/n)^{1/d}$ is (up to a constant) as small as possible while still ensuring the graph $G$ is connected. On the other hand, when $s > 1$, we can show Laplacian eigenmaps is optimal only when $\varepsilon = \omega(n^{-c})$ for some $c < 1/d$. For such a choice of $\varepsilon$, the average degree in $G$ will grow polynomially in $n$ as $n \to \infty$, and computing eigenvectors of the Laplacian of a graph will be more computationally intensive than if the graph were sparse \textcolor{red}{(reference)}. Thus Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo} and~\ref{thm:laplacian_eigenmaps_estimation_ho} can be seen as revealing a tradeoff between statistical and computational efficiency; although to be clear, we have no theoretical evidence that Laplacian eigenmaps \emph{fails} to adapt to higher-order smoothness when $\varepsilon \asymp (\log n/n)^{1/d}$---we simply cannot prove that it succeeds. 
\textcolor{red}{(TODO): If we decide it is worth our time to investigate the optimal choice of graph radius empirically, we can add a line here mentioning this.}

Suppose one does choose $\varepsilon$ meaningfully larger than the connectivity threshold, as our theory requires when $s > 1$. We now discuss a procedure to efficiently compute an approximation to the Laplacian eigenmaps estimate, without changing the rate of convergence of the resulting estimator: \emph{edge sparsification}. By now there exist various methods see (e.g., the seminal papers of \citet{spielman2011,spielman2013,spielman2014}, or the overview by \citet{vishnoi2012} and references therein) to efficiently remove many edges from the graph $G$ while only slightly perturbing the spectrum of the Laplacian. Specifically such algorithms take as input a parameter $\sigma \geq 1$, and return a sparser graph $\wt{G}$, $E(\wt{G}) \subseteq E(G)$, with a Laplacian $\wt{L}_{n,\varepsilon}$ satisfying
\begin{equation*}
\frac{1}{\sigma} \cdot u^{\top} \wt{L}_{n,\varepsilon} u \leq u^{\top} L_{n,\varepsilon} u \leq \sigma \cdot u^{\top} \wt{L}_{n,\varepsilon}u \quad \textrm{for all $u \in \Reals^n$.}
\end{equation*}
Let $\wt{f}$ be the Laplacian eigenmaps estimator computed using the eigenvectors of the sparsified graph Laplacian $\wc{L}_{n,\varepsilon}$ . Because $\wt{G}$ is sparser than $G$, it can be (much) faster to compute the eigenvectors of $\wt{L}_{n,\varepsilon}$ than the eigenvectors of $L_{n,\varepsilon}$, and consequently much faster to compute $\wt{f}$ than $\wh{f}$ \textcolor{red}{(reference needed)}. Statistically speaking, letting $\wt{\lambda}_k$ be the $k$th eigenvalue of $\wc{L}_{n,\varepsilon}$, we have that conditional on $X_1,\ldots,X_n$,
\begin{equation*}
\|\wt{f} - f_0\|_n^2 \leq \frac{\dotp{\wt{L}_{n,\varepsilon}^s f_0}{f_0}_n}{\wt{\lambda}_{K + 1}^s} + \frac{5K}{n} \leq \sigma^{2s} \frac{\dotp{\wt{L}_{n,\varepsilon}^s f_0}{f_0}_n}{\wt{\lambda}_{K + 1}^s} + \frac{5K}{n},
\end{equation*}
with probability at least $1 - \exp(-K)$. Consequently $\wt{f}$ has $L^2(P_n)$-error of at most $\sigma^{2s}$ times our upper bound on the error of $\wh{f}$, and for any choice of $\sigma$ that is constant in $n$ the estimator $\wt{f}$ will also be rate-optimal. 

In fact the aforementioned edge sparsification algorithms are overkill for our needs. For one thing, they are designed to work when $\sigma$ is much larger than $1$, whereas in order for $\wc{f}$ to be rate-optimal setting $\sigma$ to be any constant greater than $1$, say $\sigma = 2$, is sufficient. Additionally, edge sparsification algorithms are traditionally designed to work in the worst-case, where no assumptions are made on the structure of the graph $G$. But the geometric graphs we consider in this paper exhibit a special structure, in which very roughly speaking no single edge is a bottleneck. As pointed out by~\citet{sadhanala16b}, in this special case there exist far simpler and faster methods for sparsification, which at least empirically seem to do the job.

\section{Manifold Adaptivity}
\label{sec:manifold_adaptivity}

In this section we consider the manifold setting, where $(X_1,Y_1),\ldots,(X_n,Y_n)$ are observed according to Model~\ref{def:model_manifold}. A theory has been developed \citep{niyogi2008finding,belkin03,belkin05,niyogi2013,balakrishnan2012minimax,balakrishnan2013cluster} establishing that the neighborhood graph $G$ can ``learn'' the manifold $\Xset$ in various senses, so long as $\Xset$ is locally linear. We build on this work by showing that when $f_0 \in H^s(\mc{X})$ and $P$ is supported on a manifold, Laplacian eigenmaps achieve the sharper minimax estimation and testing rates reviewed in Section~\ref{subsec:minimax_rates_sobolev}.

\subsection{Laplacian eigenmaps error rates under the manifold hypothesis}
Unlike in the flat-Euclidean case, since Model~\ref{def:model_manifold} assumes that $\mc{X}$ is boundaryless it is easy to deal with the first-order $(s = 1)$ and higher-order $(s > 1)$ cases all at once. A more important distinction between the results of this section and those of Section~\ref{sec:minimax_optimal_laplacian_eigenmaps} is that we will establish Laplacian eigenmaps is optimal only when the regression function $f_0 \in H^s(\mc{X};M)$ for $s \leq 3$. Otherwise, this section will proceed in a similar fashion to Section~\ref{sec:higher_order_sobolev_classes}.

\paragraph{Estimation.}
To ensure that $\wh{f}$ is an in-sample minimax rate-optimal estimator, we choose the kernel function $\eta$, graph radius $\varepsilon$ and number of eigenvectors $K$ as in~\ref{asmp:parameters_estimation_ho}, except with ambient dimension $d$ replaced by the intrinsic dimension $m$.

\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{4}
	\item 
	\label{asmp:kernel_manifold}
	The kernel function $\eta$ is a nonincreasing function supported on a subset of $[0,1]$. Its restriction to $[0,1]$ is Lipschitz, and $\eta(1/2) > 0$. Additionally, it is normalized so that
	\begin{equation*}
	\int_{\Reals^m} \eta(\|z\|) \,dz = 1,
	\end{equation*}
	and we assume \smash{$\int_{\Reals^m} \|x\|^2 \eta(\|x\|) \,dx < \infty$}.
	\item 
	\label{asmp:parameters_estimation_manifold}
	For a constant $C_0$, the graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy
	\begin{equation}
	\label{eqn:radius_estimation_manifold}
	C_0\max\biggl\{\biggl(\frac{\log}{n}\biggr)^{1/m}, n^{-1/(2(s - 1) + m)}\biggr\} \leq \varepsilon \leq \min\{i_0, K^{-1/m}\}
	\end{equation}
	and
	\begin{equation*}
	K = \min\Bigl\{\floor{(M^2n)^{m/(2s + m)}} \wedge 1,n \Bigr\}.
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_manifold}
	Suppose Model~\ref{def:model_manifold}, and additionally $f_0 \in H^s(\mc{X},M)$ and $p \in C^{s - 1}(\mc{X})$ for $s \leq 3$. There exist constants $c,C$ and $N$ that do not depend on $f_0$, such that the following statement holds all for all $n$ larger than $N$ and for any $\delta \in (0,1)$: if the Laplacian eigenmaps estimator $\wh{f}$ is computed with kernel $\eta$ satisfying~\ref{asmp:kernel_manifold}, and parameters $\varepsilon$ and $K$ satisfying~\ref{asmp:parameters_estimation_manifold}, then
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_manifold}
	\|\wh{f} - f_0\|_n^2 \leq C\Bigl(\frac{1}{\delta}M^2(M^2n)^{-2s/(2s + m)} \wedge 1\Bigr) \vee \frac{1}{n},
	\end{equation}
	with probability at least $1 - \delta - Cn\exp(-cn\varepsilon^m) - \exp(-K)$.
\end{theorem}

\paragraph{Testing.}
Likewise, to construct a minimax optimal test using $\wh{T}$, we choose $\varepsilon$ and $K$ as in~\ref{asmp:parameters_testing_fo}, except with the ambient dimension $d$ replaced by the intrinsic dimension $m$.
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp:parameters_testing_manifold}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy~\eqref{eqn:radius_estimation_manifold}. Additionally, the
	\begin{equation*}
	K = \min\Bigl\{\floor{(M^2n)^{2m/(4s + m)}} \wedge 1,n \Bigr\}.
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_manifold}
	Fix $a,b \in (0,1)$. Suppose Model~\ref{def:model_manifold}. Then $\mathbb{E}_0[\varphi] \leq a$, i.e $\varphi$ is a level-$a$ test. Suppose additionally $f_0 \in H^s(\mc{X},M)$, that $p \in C^{s-1}(\mc{X})$, and that $s \leq 3$ and $m \leq 4$. Then there exist constants $c$, $C$ and $N$ that do not depend on $f_0$, such that the following statement holds for all $n$ larger than $N$: if the Laplacian eigenmaps test $\varphi$ is computed kernel $\eta$ satisfying~\ref{asmp:kernel_manifold}, and parameters $\varepsilon$ and $K$ satisfying~\ref{asmp:parameters_testing_manifold}, and if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_manifold}
	\|f_0\|_P^2 \geq \frac{C}{b}\biggl(\Bigl(M^2(M^2n)^{-4s/(4s + m) } \wedge n^{-1/2}\Bigr)\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr] \vee \frac{M^2}{b n^{2s/d}} \biggr) \vee \frac{1}{n},
	\end{equation}
	then $\Ebb_{f_0}[1 - \phi] \leq b$.
\end{theorem}

\begin{itemize}
	\item The proofs of Theorems~\ref{thm:laplacian_eigenmaps_estimation_manifold} and~\ref{thm:laplacian_eigenmaps_testing_manifold} follow very similarly to the full-dimensional setting. The difference is that when $\mc{X}$ is a manifold with intrinsic dimension $m$, we can prove analogous results to Propositions~\ref{prop:graph_seminorm_fo}-\ref{prop:graph_eigenvalue}, but with the ambient dimension $d$ replaced by the intrinsic dimension $m$. 
	\item Unlike in the full-dimensional case, our upper bounds on the estimation and testing error of Laplacian eigenmaps match the minimax rate only when $s \leq 3$.  Our upper bounds when $s \geq 4$ follow from the embedding $H^s(\mc{X};M) \subset H^{3}(\mc{X};M)$, i.e they match the rates we get just by assuming that the $3$rd order derivative is bounded, and are clearly suboptimal.
	
	We now explain this discrepancy. At a high level, thinking of the graph $G$ as an estimate of the manifold $\mc{X}$, we incur some error by using Euclidean distance rather than geodesic distance to form the edges of $G$. This is in contrast with the full-dimensional setting, where the Euclidean metric exactly coincides with the geodesic distance for all points $x,z \in \mc{X}$ that are sufficiently close to each other and far from the boundary of $\mc{X}$. This extra error incurred in the manifold setting by using the ``wrong distance'' dominates when $s \geq 4$. 
	
	As this explanation suggests, by building $G$ using the geodesic distance one could avoid this error, and might obtain superior rates of convergence. However this is not an option for us, as we assume $\mc{X}$---and in particular its geodesics---are unknown. Likewise, a classical spectral projection estimator, using eigenfunctions of the manifold Laplace-Beltrami operator, will achieve the minimax rate for all values of $s$ and $m$; but this is undesirable for the same reason---we do not want to assume that $\mc{X}$ is known. It is not clear whether this gap between spectral projection and Laplacian eigenmaps estimators---or more generally, between estimators which assume the manifold is known, and those which do not---is real, or a product of loose upper bounds. 
	
	\item Finally, when $m > 4$, we get an upper bound on testing error equivalent to that of Theorem~\ref{thm:laplacian_eigenmaps_testing_ho_suboptimal}, except with the ambient dimension $d$ replaced by intrinsic dimension $m$.
\end{itemize}

\paragraph{Analysis.}
The high-level strategy used to prove Theorems~\ref{thm:laplacian_eigenmaps_estimation_manifold} and~\ref{thm:laplacian_eigenmaps_testing_manifold} is the same as in the flat-Euclidean setting. More specifically, we will use precisely the same bias-variance decompositions~\eqref{eqn:estimation_biasvariance} (for estimation) and~\eqref{eqn:testing_biasvariance} (for testing). The difference will be that our bounds on the graph Sobolev seminorm $\dotp{L_{n,\varepsilon}^sf_0}{f_0}_n$, graph eigenvalue $\lambda_K$, and empirical norm $\|f_0\|_n^2$ will now always depend on the intrinsic dimension $m$, rather than the ambient dimension $d$. The precise results we use are contained in Propositions~\ref{prop:graph_seminorm_manifold}-\ref{prop:empirical_norm_sobolev_manifold}.
\begin{proposition}
	\label{prop:graph_seminorm_manifold} 
	Suppose Model~\ref{def:model_manifold}, and additionally that $f_0 \in H^s(\mc{X};M)$ and $p \in C^{s - 1}(\mc{X})$ for $s = 1,2$ or $3$. Then there exist constants $c_0,C_0$ and $C$ that do not depend on $f_0$, $n$ or $M$ such that the following statement holds for any $\delta \in (0,1)$: if $\eta$ satisfies~\ref{asmp:kernel_manifold} and $C_0n^{-1/(2(s - 1) + m)} < \varepsilon < c_0$, then
	\begin{equation}
	\label{eqn:graph_seminorm_manifold}
	\dotp{L_{n,\varepsilon}^s f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^s(\mc{X})}^2,
	\end{equation}
	with probability at least $1 - 2\delta$.
\end{proposition}

As discussed previously, when $\mc{X}$ is a domain without boundary and $\Delta_P$ is the manifold weighted Laplace-Beltrami operator, appropriate bounds on the graph eigenvalues $\lambda_k$ have already been derived in \citep{burago2014,trillos2019,garciatrillos19}. The precise result we need is a simple consequence of Theorem 2.4 of~\citep{calder2019}.
\begin{proposition}[\textbf{c.f Theorem 2.4 of~\citep{calder2019}}]
	\label{prop:graph_eigenvalue_manifold}
	Suppose Model~\ref{def:model_manifold}. Then there exist constants $c$ and $C$ such that the following statement holds: if $\eta$ satisfies~\ref{asmp:kernel_manifold} and $C(\log n/n)^{1/m} < \varepsilon < c$, then
	\begin{equation}
	\label{eqn:graph_eigenvalue_manifold}
	\lambda_k \geq c \cdot \min\Bigl\{\lambda_k(\Delta_P), \frac{1}{\varepsilon^{2}} \Bigr\} \quad \textrm{for all $1 \leq k \leq n$,}
	\end{equation}
	with probability at least $1 - Cn\exp\{-c n\varepsilon^d\}$. 
\end{proposition}
(For the specific computation used to deduce Proposition~\ref{prop:graph_eigenvalue_manifold} from Theorem 2.4 of~\citep{calder2019}, see~\textcolor{blue}{(Green 2021)}.)

Finally, using a Gagliardo-Nirenberg inequality for functions on compact Riemmanian manifolds, we obtain a lower bound on empirical norm $\|f\|_n$ under the hypotheses of Model~\ref{def:model_manifold}. 
\begin{proposition}
	\label{prop:empirical_norm_sobolev_manifold}
	Suppose Model~\ref{def:model_manifold}, and additionally that $f_0 \in H^s(\Xset,M)$ for some $s > m/4$. There exists a constant $C$ that does not depend on $f_0$ such that the following statement holds for all $\delta > 0$:  if
	\begin{equation}
	\label{eqn:empirical_norm_sobolev_manifold_1}
	\norm{f_0}_{P} \geq \frac{C M}{\delta^{s/m}}n^{-s/m},
	\end{equation}
	then with probability at least $1 - \exp\{-(cn \wedge 1/\delta)\}$,
	\begin{equation}
	\label{eqn:empirical_norm_sobolev_manifold}
	\norm{f_0}_n^2 \geq \frac{1}{2} \|f_0\|_P^2.
	\end{equation}
\end{proposition}

\section{Out-of-sample error}
\label{sec:out_of_sample}
Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps} and~\ref{sec:manifold_adaptivity} show that $\wh{f}$ is a minimax optimal estimator over Sobolev spaces. However, as mentioned previously we have measured loss \emph{in-sample}---that is, measured in $\Leb^2(P_n)$ norm---whereas \emph{out-of-sample} error---error measured in $L^2(P)$ norm---is the more typical metric in the random design setup.

Of course, the Laplacian eigenmaps estimator is only defined at the observed design points $X_1,\ldots,X_n$, and to measure its error in $L^2(P)$ norm we must first extend it to be defined over all of $\Xset$. We propose a simple method, kernel smoothing, to do the job. The method can applied to any estimator defined at the design points, including Laplacian eigenmaps, and we show that a smoothed version of our original estimator $\wh{f}$ has optimal $L^2(P)$ error. For simplicity, in this section we will stick to the flat Euclidean setting, where $(X_1,Y_1),\ldots,(X_n,Y_n)$ are observed according to Model~\ref{def:model_flat_euclidean}.

\paragraph{Extension by kernel smoothing.}
We now formally define our approach to extension by kernel smoothing. For a kernel function $\psi(\cdot): [0,\infty) \to (-\infty,+\infty)$, bandwidth $h > 0$, and a distribution $Q$, the \emph{Nadaraya-Watson kernel smoother} $T_{h,Q}$ is given by
\begin{equation*}
\bigl(T_{Q,h}f)(x) := 
\begin{dcases*}
\frac{1}{d_{Q,h}(x)} \int_{\Omega} f(z)\psi\biggl(\frac{\|z - x\|}{h}\biggr) \,dQ(z), & \textrm{if $d_{Q,h}(x) > 0$,} \\
0, &\textrm{otherwise,}
\end{dcases*}
\end{equation*}
where $d_{Q,h}(x) := \int_{\Omega} \psi\bigl(\|z - x\|/\varepsilon\bigr) \,dQ(z)$. For convenience, we will write $T_{\varepsilon,n}f(x) := T_{\varepsilon,P_n}f(x)$, and $d_{n,h}(x) := n \cdot d_{P_n,h}(x)$.  We extend the Laplacian eigenmaps estimator by passing the kernel smoother $T_{h,n}$ over it, that is we consider the estimator $T_{h,n}\wh{f}$, which is defined at every $x \in \mc{X}$ (indeed, at every $x \in \Rd$). Note that ``extension'' here is a slight abuse of nomenclature, since $T_{h,n}\wh{f}(X_i)$ and $\wh{f}_i$ may not agree in-sample.

\paragraph{Out-of-sample error of kernel smoothed Laplacian eigenmaps.}

In Lemma~\ref{lem:kernel_smoothing_insample}, we consider an arbitrary estimator $\wc{f} \in L^2(P_n)$. We show that the out-of-sample error $\|T_{n,h}\wc{f} - f_0\|_P^2$ can be upper bounded by three terms--- (a constant times) the in-sample error $\|\wc{f} - f_0\|_n^2$, and variance and bias terms that arise naturally in the analysis of kernel smoothing over noiseless data.. We shall assume the following conditions on $\psi$ and $h$.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{2}
	\item
	\label{asmp:kernel}
	The kernel function $\psi$ is supported on a subset of $[0,1]$. Additionally, $\psi$ is Lipschitz continuous on $[0,1]$, and is normalized so that
	\begin{equation*}
	\int_{-\infty}^{\infty} \psi(|z|) \,dz = 1.
	\end{equation*}
\end{enumerate}
\begin{enumerate}[label=(P\arabic*)]
	\setcounter{enumi}{6}
	\item
	\label{asmp:bandwidth}
	For constants $c_0$ and $C_0$, the bandwidth parameter $h$ satisfies
	\begin{equation*}
	C_0\biggl(\frac{\log(1/h)}{n}\biggr)^{1/d} \leq h \leq c_0.
	\end{equation*}
\end{enumerate}
\begin{lemma}
	\label{lem:kernel_smoothing_insample}
	Suppose Model~\ref{def:model_flat_euclidean}, and additionally that $\wc{f} \in L^2(P_n)$, $f_0 \in H^1(\mc{X})$ and $p \in C^1(\mc{X})$. If the kernel smoothing estimator $T_{h,n}\wc{f}$ is computed with kernel $\psi$ satisfying~\ref{asmp:kernel} and bandwidth $h$ satisfying~\ref{asmp:bandwidth}, it holds that
	\begin{equation}
	\label{eqn:kernel_smoothing_insample}
	\|T_{n,h}\wc{f} - f_0\|_P^2 \leq C\biggl(\|\wc{f} - f_0\|_n^2 + \frac{1}{\delta} \cdot \frac{h^2}{nh^d} |f|_{H^1(\mc{X})}^2 + \frac{1}{\delta}\|T_{h,P}f_0 - f_0\|_P^2\biggr),
	\end{equation}
	with probability at least $1 - \delta - Ch^d\exp\{-Cnh^d\}$. 
\end{lemma}
Notice that the variance term in the above is smaller than the typical variance term for kernel smoothing of noisy data, by a factor of $h^2$. On the other hand the bias term is typical. When $\psi$ is an order-$s$ kernel, a standard analysis shows that the $\|T_{h,P}f_0 - f_0\|_P^2 \lesssim \varepsilon^{2s}$.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{3}
	\item
	\label{asmp:ho_kernel}
	The kernel function $\psi$ is an order-$s$ kernel, meaning that it satisfies
	\begin{equation*}
	\int_{-\infty}^{\infty} \psi(|z|) \,dz = 1, \quad \int_{-\infty}^{\infty} z^j \psi(|z|) \,dz = 0 ~~\textrm{for}~j = 1,\ldots, s + d - 2, \quad \textrm{and}~ \int_{-\infty}^{\infty} z^{s + d - 1} \psi(|z|) \,dz < \infty. 
	\end{equation*}
\end{enumerate}

\textcolor{red}{(TODO): I have worked out the math for this result, but I am a little surprised at the requirement that $\psi$ need annihilate polynomials up to degree $s + d - 2$. Will need to double check closely, since I could not find a reference analyzing kernel smoothing using a radially symmetric higher order kernel when $d > 1$.}

Choosing $h \asymp n^{-1/(2(s - 1) + d)}$ balances the kernel smoothing bias and variance terms in~\eqref{eqn:kernel_smoothing_insample}, and implies that
\begin{equation}
\label{eqn:kernel_smoothing_insample2}
\|T_{n,h}\wc{f} - f_0\|_P^2 \leq C\biggl(\|\wc{f} - f_0\|_n^2 + \frac{1}{\delta}n^{-2s/(2(s - 1) + d)}\biggr).
\end{equation}
\eqref{eqn:kernel_smoothing_insample2} tells us that the additional error incurred by passing a kernel smoother over an in-sample estimator $\wc{f}$ is negligible compared to the minimax rate of estimation. Consequently, if $\wc{f}$ converges at the minimax rate in $L^2(P_n)$, then $T_{n,h}\wc{f}$ will converge at the minimax rate in $L^2(P)$. It follows immediately from Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} (when $s = 1$) or Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho} (when $s > 1$), that $T_{h,n}\wh{f}$ achieves the optimal rate of convergence in $L^2(P)$.

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_out_of_sample}
	Suppose Model~\ref{def:model_flat_euclidean}. There exist constants $c$, $C$, and $N$ that do not depend on $f_0$ or $n$ such that each the following statements hold with probability at least $1 - \delta - Cn\exp\{-cn\varepsilon^d\} - Ch^d\exp\{-cnh^d\}$,  for all $n \geq N$ and for any $\delta \in (0,1)$.
	\begin{itemize}
		\item If $f_0 \in H^1(\mc{X};M)$, the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_fo}, and the out-of-sample extension $T_{h,n}\wh{f}$ is computed with bandwidth $h = n^{-1/d}$ and kernel $\psi$ that satisfies~\ref{asmp:kernel}, then
		\begin{equation*}
		\|T_{h,n}\wh{f} - f_0\|_P^2 \leq \frac{C}{\delta}M^2(M^2n)^{-2s/(2s + d)}.
		\end{equation*}
		\item If $f_0 \in H_0^s(\mc{X};M)$ and $p \in C^{s - 1}(\mc{X})$ for some $s \in \mathbb{N}, s > 1$, and the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_ho}, and the out-of-sample extension $T_{h,n}\wh{f}$ is computed with bandwidth $h = n^{-1/(2(s - 1) + d)}$ and kernel $\psi$ that satisfies~\ref{asmp:kernel} and~\ref{asmp:ho_kernel}, then
		\begin{equation*}
		\|T_{h,n}\wh{f} - f_0\|_P^2 \leq \frac{C}{\delta}M^2(M^2n)^{-2s/(2s + d)}.
		\end{equation*}
	\end{itemize}
\end{theorem}
Some remarks:
\begin{itemize}
	\item Since $T_{n,h}\wh{f}$ is defined out-of-sample, we can use sample splitting or cross validation methods to tune hyperparameters, which we could not do for the original estimator $\wh{f}$. For instance, we can (i) split the sample into two halves, (ii) use the first half to compute $T_{h,n}\wh{f}$ for various values of $\varepsilon$, $h$, and $K$, (iii) choose the optimal values of these three hyperparameters by minimizing error on the held out set. Practically speaking, cross-validation is one of the most common approaches to choosing hyperparameters. Theoretically, it is known \textcolor{red}{(references)} that choosing hyper-parameters through sample splitting can result in estimators that optimally adapt to the order of regularity $s$. In other words, it leads to estimators that are rate-optimal (up to $\log n$ factors), even when $s$ is unknown. Similar arguments should imply that $T_{h,n}\wh{f}$ is adaptive in this sense when $\varepsilon,h$ and $K$ are chosen by sample splitting. \textcolor{red}{(TODO): Siva would like me to fill in some details?}
	\item There exist many other approaches to extending a function $f$ using only evaluations $\{f(X_1),\ldots,f(X_n)\}$. We consider extension by kernel smoothing because it is a simple and statistically optimal procedure that does not require any knowledge of the domain $\mc{X}$ or distribution $P$---as we have argued, this latter property is one of the main selling points of Laplacian eigenmaps as a tool for nonparametric regression. 
	
	We now comment on a few alternative methods. One such approach is minimum norm interpolation. Here one defines a normed space $(\mc{F},\|\cdot\|_{\mc{F}})$ and then solves the optimization problem
	\begin{equation*}
	\min_{u \in \mc{F}} \|u\|_{\mc{F}},~~\textrm{such that}~~ u(X_i) =  f(X_i)~~\textrm{for $i = 1,\ldots,n$.}
	\end{equation*}
	A particularly popular version of this general approach takes $\mc{F}$ to be an RKHS \textcolor{red}{(Rieger2008, Belkin2018)}, which encompasses thin-plate spline interpolation (where $\mc{F} = H^s(\mc{X})$ for $s > d/2$) as a special case. Naturally, this approach works well when $f$ is close to a function $u \in \mc{F}$ with reasonably small norm.  This holds true when $f \in H^s(\mc{X};M)$ and $s > d/2$, but as already discussed when $s \leq d/2$ the Sobolev space $H^s(\mc{X})$ is not an RKHS, and in fact when $s \leq d/2$ thin-plate spline interpolation is ill-posed \citep{green93}. Another, arguably simpler approach is to extend $f$ to be piecewise constant on the Voronoi tessellation induced by $X_1,\ldots,X_n$, or equivalently to perform $1$-nearest neighbors regression on $f$. However, this approach is theoretically optimal only when $f_0$ is Lipschitz, in contrast to the kernel smoothing method we propose and study.
	
	None of these three methods are intrinsically linked to Laplacian eigenmaps. This is in one sense a strength, since they can be used to extrapolate any estimator defined only in-sample. But it is also potentially a weakness. Each of these methods have their own approximation and estimation errors (bias and variance) which can be fundamentally different than those of Laplacian eigenmaps, and there is a danger that in extrapolating the Laplacian eigenmaps estimator in this way we are taking the ``worst of both worlds''. Our theory shows that when the data model is Model~\ref{def:model_flat_euclidean} and we perform extrapolation by kernel smoothing, this is not a problem, at least in a minimax sense. 
	
	%Finally, the Nystr\"{o}m method extends  eigenvectors $v_k$ to functions $f_k: \mc{X} \to \Reals$ as follows,
	%\begin{equation*}
	%L_{n,\varepsilon}f_k = \lambda_k f_k~~\textrm{on $\mc{X}$, such that}~~f_k(X_i) = %v_k(X_i)~~\textrm{for $i = 1,\ldots,n$.}
	%\end{equation*}
	%Here $L_{n,\varepsilon}$ should be thought of as an operator acting on functions $f \in C(\mc{X})$ as follows,
	%\begin{equation*}
	%L_{n,\varepsilon}f(x) = \sum_{i = 1}^{n} (f(x) - f(X_i))\eta\Bigl(\frac{\|X_i - x\|}{\varepsilon}\Bigr)
	%\end{equation*}
	% The Nystr\"{o}m approach to extension makes intriguing (re-)use of the graph Laplacian $L_{n,\varepsilon}$ and its eigenvectors $v_k$. Similar methods have been analyzed in the context of kernel ridge regression (references), but little is known about its application to graph Laplacians.	
\end{itemize}

\section{Experiments}
\label{sec:experiments}

\textcolor{red}{(TODO): Include more details.}
In this section we empirically demonstrate that Laplacian Eigenmaps is a very good alternative to spectral projection, even when $n$ is only moderately large. In order to compare the two methods, in our experiments we stick to simple settings where we can compute eigenfunctions of $\Delta_P$, and thus the spectral projection estimator. In general, this is not so easily done: hence the appeal of Laplacian Eigenmaps.

In our first experiment, we compare the mean-squared error of Laplacian eigenmaps to that of its classical counterpart. We vary the sample size from $n = 200$ to $n = 2000$; sample $n$ design points $X_1,\ldots,X_n$ from the uniform distribution on the cube $[-1,1]^d$; and sample responses $Y_i$ according to~\eqref{eqn:model} with regression function $f_0 = M/\sqrt{n} \cdot \sum_{k = 1}^{n/20} k^{-s/d} \psi_k$ (Note that $f_0$ changes with $n$, but always belongs to the Sobolev ball~\textcolor{red}{(?)}.) In Figure~\ref{fig:fig1} we show the mean-squared error of Laplacian eigenmaps and spectral projection as a function of $n$, for different dimensions $d$ and order of smoothness $s$. We see that both estimators have mean-squared error converging to zero at least as fast as the optimal rate. (Here the regression function $f_0$ is a little bit smoother than the worst-case Sobolev function, which explains why the minimax rate is not always saturated). Surprisingly, we also find that Laplacian Eigenmaps slightly but consistently outperforms its continuum counterpart. 
\begin{figure*}[tb]
	\includegraphics[width=.245\textwidth]{figures/mse_by_sample_size_2d.pdf}
	\includegraphics[width=.245\textwidth]{figures/mse_by_sample_size_3d.pdf} 
	\includegraphics[width=.245\textwidth]{figures/mse_by_sample_size_2d_2s.pdf}
	\includegraphics[width=.245\textwidth]{figures/mse_by_sample_size_3d_2s.pdf} 
	\caption{Mean squared error of Laplacian eigenmaps (\texttt{LE}) and a spectral projection estimator (\texttt{SP}) as a function of sample size $n$. Each plot is on the log-log scale, and the results are averaged over 50 repetitions. Both estimators are tuned for optimal average mean squared error. The black line shows the minimax rate (in slope only; the intercept is chosen to match the observed error).}
	\label{fig:fig1}
\end{figure*}

In Figure~\ref{fig:fig2} we compare tests using Laplacian eigenmaps and spectral projection. \textcolor{red}{(TODO): Need to ask Siva for help.}
\begin{figure*}[tb]
	\caption{Worst-case risk Laplacian eigenmaps (\texttt{LE}) and spectral projection (\texttt{SP}) tests, as a function of sample size $n$.}
	\label{fig:fig2}
\end{figure*}

These experiments demonstrate that in terms of statistical error, Laplacian eigenmaps methods are reasonable replacements for spectral projection methods. Laplacian eigenmaps depends on two tuning parameters, and in our final experiment we investigate the importance of both, focusing now on estimation. In Figure~\ref{fig:fig3}, we see how the mean-squared error of Laplacian eigenmaps changes as each tuning parameter is varied. As suggested by our theory, properly choosing the number of eigenvectors $K$ is crucial: the mse curves have a sharply defined minimum. On the other hand, as a function of the graph radius parameter $\varepsilon$ the mse curves are much closer to flat. This is particularly true in the first-order ($s = 1$) case, and still true albeit to a lesser extent when $s = 2$.   These findings reinforce two points we've made previously: first that Laplacian Eigenmaps should be viewed as a type of spectral projection method, in which the more important parameter to carefully tune is the number of eigenvectors $K$; second that the more regularity displayed by the regression function, the more important tuning the graph radius $\varepsilon$ becomes.

\begin{figure*}[tb]
	\includegraphics[width=.245\textwidth]{figures/eigenmaps_parameters/mse_by_number_of_eigenvectors.pdf}
	\includegraphics[width=.245\textwidth]{figures/eigenmaps_parameters/mse_by_radius.pdf} 
	\includegraphics[width=.245\textwidth]{figures/eigenmaps_parameters_2s/mse_by_number_of_eigenvectors.pdf}
	\includegraphics[width=.245\textwidth]{figures/eigenmaps_parameters_2s/mse_by_radius.pdf} 
	\caption{Mean squared error of Laplacian Eigenmaps as a function of hyperparameters. For all experiments, sample size $n = 2000$, and the results are averaged over $50$ repetitions. Colors show~\textcolor{red}{(?)}}
	\label{fig:fig3}
\end{figure*}

\section{Discussion}
\label{sec:discussion}

\subsection{Comparison with other estimators}
In this paper, we have motivated Laplacian eigenmaps by viewing it as a noisy approximation of a classical spectral projection method, which is its most obvious counterpart. We have shown that Laplacian eigenmaps inherits the optimality properties of its more classical counterpart. We now discuss the relationship between Laplacian eigenmaps and three other approaches to nonparametric regression. The first two---nonparametric least squares and kernel smoothing---are classical, whereas the third---Laplacian smoothing---makes use of the graph Laplacian in a different way than Laplacian eigenmaps.

\paragraph{Nonparametric least squares.}
The standard recommended alternative to spectral projection methods, when the distribution $P$ is considered non-uniform or unknown, is to do least-squares. For example, suppose instead of knowing $\psi_1,\psi_2,\ldots$, we had access only to eigenfunctions $\phi_1,\phi_2,\ldots$ of an unweighted Laplace-Beltrami operator $\Delta$. Then letting $\Phi_K = \mathrm{span}\{\phi_1,\ldots,\phi_K\}$, the least-squares estimator and test statistic
\begin{equation*}
\wt{f}_{\mathrm{LS}} := \argmin_{f \in \Phi_K} \|Y - f\|_n^2,\textrm{and}~~\wt{T}_{\mathrm{LS}} = \|\wt{f}\|_{\nu}^2
\end{equation*}
are still rate-optimal over $H_0^s(\mc{X})$. This holds true for both Model~\ref{def:model_flat_euclidean} and~\ref{def:model_manifold}, and is a consequence of our assumption that $p$ is bounded away from $0$.

However, this is not a totally satisfactory fix. For one thing, the least squares approach just outlined requires that we know the domain $\mc{X}$, in the strong sense that we know the Laplace-Beltrami operator $\Delta$ defined on $\mc{X}$. Domain knowledge is generally precious information, and such strong knowledge of $\mc{X}$ seems particularly unrealistic in the case where $\mc{X}$ is a manifold, and $\Delta$ is the manifold Laplace-Beltrami operator. Additionally, even if we know $\mc{X}$, diagonalizing the Laplace-Beltrami operator $\Delta$ is quite difficult for all but a few special domains, such as the unit cube $\mc{X} = [0,1]^d$ or torus $\mc{X} = \mathbb{T}^d$.

\paragraph{Kernel smoothing.}
It is also natural to ask whether the two stage estimator $T_{h,n}\wh{f}$ defined in Section~\ref{sec:out_of_sample} has any advantage over the simpler approach of directly kernel smoothing the responses, i.e. using the estimator $T_{h,n}Y$ (possibly for a different choice of $h$). In Appendix~\ref{subsec:eigenmaps_beats_kernel_smoothing}, we answer this question in the affirmative, by giving a simple example of a sequence of densities and regression functions $\{(p^{(n)}, f_0^{(n)}: n \in \mathbb{N}\}$ such that $\Ebb\|f_0 - T_{h,n}\wh{f}\|_P^2$ is of a strictly lower order than $\inf_{h'} \Ebb\|f_0 - T_{h',n}Y\|_P^2$. This is possible because Laplacian eigenmaps induces a completely different bias than kernel smoothing. For example, when $f_0$ and $p$ satisfy the so-called \emph{cluster} assumption--- e.g. $f_0$ is piecewise constant in high-density regions (clusters) of $p$--- then the bias of Laplacian eigenmaps can much smaller than that of kernel smoothing (for equivalent levels of variance). 

We emphasize that this does not contradict the well-known fact that kernel smoothing is an optimal method for nonparametric regression over e.g. H\"{o}lder balls. It simply reflects that in the standard nonparametric regression setup---which we adopt in the main part of this paper, and in which $P$ is assumed to be equivalent to Lebesgue measure---the biases of Laplacian eigenmaps and kernel smoothing are equivalent. On the other hand, when $f_0$ and $p$ satisfy some special relationship, such as the cluster assumption, the biases of these two methods can be quite different. There has been some work \textcolor{red}{(Rigollet, Wasserman, Niyogi, El Alaoui)} analyzing semi-supervised learning under various instantiations of the cluster assumption. However, a comprehensive analysis of various methods, including Laplacian eigenmaps and kernel smoothing, in this setting remains outstanding.

\paragraph{Laplacian smoothing.}
As mentioned previously, graph Laplacian smoothing uses the graph Laplacian $L_{n,\varepsilon}$ to form the penalty in a penalized least squares estimator. It can be calculated by one solve of a sparse, diagonally dominant linear system. Thus it should be much faster to compute than Laplacian eigenmaps, in which one must find (many) eigenvectors of $L_{n,\varepsilon}$. On the other hand, Laplacian smoothing is known to be minimax rate-optimal only in very limited regimes (over the Sobolev spaces $H^1(\mc{X})$ for $1 \leq d < 4$.) In contrast, we have shown that Laplacian eigenmaps is minimax rate-optimal for all $d$ (when $s = 1$) . We have also shown that Laplacian eigenmaps can adapt to higher-order smoothness, i.e. it can be minimax rate-optimal when $s > 1$. Thus, the known statistical properties of Laplacian eigenmaps are much stronger than those of Laplacian smoothing. 

\subsection{Future Work}
We view our work can be viewed as a contribution both to the fields of nonparametric regression with series estimators, and to graph-based learning. We end our discussion by mentioning some open work in each of these directions. 

Much is known about classical spectral projection methods beyond their rate optimality. For instance: such estimators and tests exhibit \emph{sharp optimality}, meaning their risk is within a $(1 + o(1))$ factor of the optimal risk; they can adapt to unknown smoothness of the regression function; they can be used to estimate smooth functionals of the regression function;  finally, they can be used to form confidence sets in $L^2(P)$. It would be interesting to see if Laplacian eigenmaps could replicate the performance of classical methods in any, or all, of these problems.

On the other hand, there are many variants of Laplacian eigenmaps worth considering. For instance, one can change the graph under consideration (e.g. by using the k-nearest neighbors), or the normalization of the graph Laplacian $L_{n,\varepsilon}$ (e.g. by using the symmetric normalized Laplacian). The former is practically useful, because it typically leads to connected graphs while always ensuring a given level of edge sparsity. In the latter, the graph Laplacian converges to a different limiting operator, which possesses different eigenvectors than $\Delta_P$ and thereby induces a different bias. We believe that under the setup we consider here, both methods will continue to be optimal.


\bibliographystyle{plainnat}
\bibliography{../../graph_regression_bibliography} 

\appendix

\input{appendix.tex}

\end{document}