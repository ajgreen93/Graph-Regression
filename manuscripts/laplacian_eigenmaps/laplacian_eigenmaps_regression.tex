\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue
}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{tikz}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}
\DeclareMathAccent{\wc}{0}{mathx}{"71}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\text{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}
\newcommand{\Lap}{{\bf L}}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{FS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Minimax-optimal Laplacian Eigenmaps regression over Sobolev Spaces with Neighborhood Graphs}
\author{Alden Green}
\date{\today}
\maketitle

\section{Introduction}
\label{sec:introduction}

\textbf{(1) Geometric graphs.} In graph-based learning, one observes data $X_1,\ldots,X_n$ sampled independently from an unknown distribution $P$, and forms a geometric graph $G$---with edges $e_{ij}$ corresponding to proximity between samples $X_i$ and $X_j$---over the observed data. Geometric graphs encode information about $P$ in an extremely general manner, and can be leveraged to conduct many different fundamental statistical tasks. These include clustering, semi-supervised learning, classification, and regression---both estimation and goodness-of-fit testing. Though much theoretical work has been done on the consistency of graph-based learning methods---and more recently, rates of convergence have been established for some problems---little is known so far about their optimality, even for classic statistical tasks. 

\textbf{(2) What we study: Regression and Laplacian eigenmaps.} In this paper we focus on regression where in addition to the design points $X_1,\ldots,X_n$ one observes real-valued responses $Y_1,\ldots,Y_n$, and seeks to learn the unknown regression function $f_0(x) := E[Y|X = x]$. We consider both the estimation and goodness-of-fit testing problems. The methods we will study are based on \emph{Laplacian eigenmaps}, first introduced by \cite{belkin03a}, which projects the response vector $Y = (Y_1,\ldots,Y_n)$ onto the span of the leading eigenvectors of the graph Laplacian $L$. The Laplacian $L$ is a difference operator, acting on functions $f: \{X_1,\ldots,X_n\} \to \Reals$ as follows,
\begin{equation}
\label{eqn:graph_laplacian}
Lf(X_i) = \sum_{j = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr)e_{ij}. 
\end{equation}
The Laplacian $L$ can be seen as a discretization of the Laplace-Beltrami operator $\Delta_P$ \textcolor{red}{(...)}. The eigenvectors of $L$ form an orthonormal basis of $L^2(X)$, and are approximations to the eigenvectors of $\Delta_P$. Their corresponding eigenvalues are estimates of~\textcolor{red}{(...)}, and provide a notion of smoothness for each eigenvector---the smaller the eigenvalue, the smoother the corresponding eigenvector. 

\textbf{(3) Why we study.} Laplacian eigenmaps is thus an example of a \emph{spectral series} estimator. A spectral series estimator is a special case of an orthogonal series estimator, one of the most classical methods of non-parametric regression. Traditionally, orthogonal series estimators are defined with respect to a fixed set of basis functions, orthogonalized with respect to some pre-determined reference measure $Q$. Spectral series estimators take a particular orthogonal basis, the eigenfunctions of $\Delta_Q$. In contrast, eigenvectors of the graph Laplacian are data-dependent objects, and adapt to the geometry of the design distribution $P$ in a rich manner. 

The theoretical properties of (classical) spectral series estimators are by this point well-understood \textcolor{red}{(references)}. In particular, they are minimax optimal for nonparametric regression over H\"{o}lder and Sobolev spaces. However, in practice these estimators suffer from some serious drawbacks. Finding the eigenfunctions of $\Delta_Q$ is in general non-trivial. The statistical optimality of such estimators also hinges on the reference measure $Q$ being equal to $P$, an unrealistic assumption when $P$ is unknown. If $Q \neq P$, the standard recommendation is to instead do least squares---this can be statistically rate-optimal, but is numerically unstable when dimension $d$ is even moderately large.

\textbf{(4) Our contributions.} Laplacian eigenmaps avoid these drawbacks \textcolor{red}{(...)} On the other hand their statistical properties are not as well understood \textcolor{red}{(...)}. The primary contribution of our paper is to fill this theoretical gap, by answering the following question:

\begin{quote}
	\textcolor{red}{(TODO)}
\end{quote}

Broadly speaking, we show that when the regression function $f_0$ is smooth, in the sense of having bounded derivatives in the Sobolev sense, Laplacian eigenmaps is statistically minimax optimal. This statement holds for different relations between the dimension $d$ and number of derivatives $s$, depending on the problem (estimation or testing). \textcolor{red}{(TODO): Add two tables summarizing your major results here.} 

\textbf{(5) Organization.}

\textcolor{red}{(TODO)}

\textbf{(6) Notation.}

\textcolor{red}{TODO}: Define $\Leb^2(\mc{X}), \Leb^2(P)$ and $\Leb^2(P_n)$. 


For a square, symmetric matrix $A \in \Reals^{n \times n}$, an eigenvector $v$ of $A$ is a \textcolor{red}{unit-norm} solution to the eigenproblem
\begin{equation*}
Av = \lambda v,
\end{equation*}
and $\lambda$ is the corresponding eigenvalue.

\section{Setup, Background, and Overview of Results}
\label{sec:setup_main_results}

In this section, we begin by giving a precise definition of Laplacian eigenmaps. We then review minimax rates for non-parametric regression over Sobolev spaces, allowing us to summarize our main results. 

\subsection{Non-parametric regression with Laplacian Eigenmaps}
\label{sec:regression_laplacian_eigenmaps}

\textbf{(1) Formal setup.}
We will operate in the usual setting of non-parametric regression with random design. We observe independent random samples $(X_1,Y_1),\ldots,(X_n,Y_n)$, where the design points $X_1,\ldots,X_N$ are drawn i.i.d from a distribution $P$ supported on a compact set $\mc{X} \subseteq \Rd$, and responses
\begin{equation}
\label{eqn:model}
Y_i = f_0(X_i) + \epsilon_i,
\end{equation}
with regression function $f_0: \mc{X} \to \Reals$, and $\epsilon_i \sim N(0,1)$ independent Gaussian noise. For simplicity we will assume throughout that the noise has unit-variance, but all of our results extend in a straightforward manner to the case where the variance is equal to a known positive value. 

\textbf{(2) Laplacian eigenmaps.} Laplacian eigenmaps constructs an estimate of the regression function $f_0$ using the eigenvectors of a neighborhood graph Laplacian operator. For a positive, radially symmetric kernel $\eta: [0,\infty) \to [0,\infty)$, and a bandwidth parameter $\varepsilon > 0$, the \emph{neighborhood graph Laplacian} operator $L_{n,\varepsilon}$ is
\begin{equation}
\label{eqn:neighborhood_graph_laplacian}
L_{n,\varepsilon}u(x) = \frac{1}{n\varepsilon^{d + 2}} \sum_{i = 1}^{n} \bigl(u(x) - u(X_j)\bigr) \eta\biggl(\frac{\|x - X_j\|}{\varepsilon}\biggr)
\end{equation}
The constant scaling $(n\varepsilon^{d + 2})^{-1}$ is purely for convenience in taking limits as $n \to \infty, \varepsilon \to 0$. Also, although~\eqref{eqn:neighborhood_graph_laplacian} makes sense for any $x \in \mc{X}$ and $u: \mc{X} \to \Reals$, we will typically think of $x \in \{X_1,\ldots,X_n\}$ and $u = (u(X_1),\ldots,u(X_n)) \in \Leb^2({\bf X})$. The graph Laplacian is positive semi-definite, and we will index its eigenpairs $(\lambda_1,v_1),\ldots,(\lambda_n,v_n)$ in ascending order of eigenvalue, $0 = \lambda_1 \leq \ldots \leq \lambda_n$. 

For a given $K \in \{1,\ldots,n\}$, the order-$K$ Laplacian eigenmaps estimator simply projects the response vector ${\bf Y}$ onto the first $K$ eigenvectors of $L_{n,\varepsilon}$: letting $V_K \in \Reals^{n \times K}$ be the matrix with columns $v_1,\ldots,v_K$, we have that
\begin{equation}
\label{eqn:laplacian_eigenmaps_estimator}
\wh{f} := \sum_{k = 1}^{K} \dotp{Y}{v_k}_{n} v_k = \frac{1}{n} V_K V_K^{\top} Y.
\end{equation} 
Since the eigenvectors $v_1,\ldots,v_n$ are orthogonalized with respect to the $L^2(P_n)$ inner product, the estimator $\wh{f}$ is the least-squares solution to the linear regression problem with responses $Y_1,\ldots,Y_n$ and features $v_1,\ldots,v_K$. Note that $\wh{f}$ is defined only in-sample, that is, only at the design points $X_1,\ldots,X_n$---we will consider the question of out-of-sample estimation later in Section~\ref{sec:out_of_sample}. 

If $\wh{f}$ is a reasonable estimate of $f_0$, then the test statistic
\begin{equation}
\label{eqn:laplacian_eigenmaps_test}
\wh{T} := \|\wh{f}\|_n^2
\end{equation}
is in turn a reasonable estimate of $\|f_0\|_{P}^2$, and can be used to distinguish whether or not $f_0 = 0$.

\subsection{Sobolev Classes}
\label{sec:sobolev}
Before we give an overview of our main results, we will first pause to review the Sobolev classes and \textcolor{red}{associated} minimax rates.

\textbf{(1) Sobolev norms, semi-norms, and balls.} 
The Sobolev class $H^s(P)$ consists of all those functions $f \in \Leb^2(P)$ which have weak partial derivatives of all orders $j = 1,\ldots,s$, all of which are bounded in $\Leb^2(P)$ norm. The Sobolev norm $\|f\|_{H^s(P)}$ is defined according to
\begin{equation*}
\|f\|_{H^s(P)}^2 := \|f\|_{\Leb^2(P)}^2 + \sum_{j = 1}^{s} \|D^jf\|_{\Leb^2(P)}^2,
\end{equation*}
where $D^jf := \sum_{|\alpha| = j}D^{\alpha}f$ is the sum of all weak partial derivatives of order $j$, and we use the multiindex notation $D^{\alpha}f = \partial^j f/(\partial^{\alpha_1}x^1\ldots \partial^{\alpha_d}x^d)$ for multiindex $\alpha$. The Sobolev ball of radius $M$ consists of all those functions $f \in H^s(P)$ with Sobolev norm no greater than $M$,
\begin{equation*}
H^s(P;M) := \bigl\{f \in H^s(P): \|f\|_{H^s(P)} \leq M\bigr\}
\end{equation*}

\textcolor{red}{(TODO)}: 
\begin{itemize}
	\item Possibly define the iterated Laplace-Beltrami operator. Point out that for $H^{2s}(P)$ functions, the Sobolev semi-norm can be stated in terms of inner products using the iterated Laplace-Beltrami operator.
	\item Introduce the notation $H^s(\mc{X}) = H^s(\nu)$, where $\nu$ is Lebesgue measure.
\end{itemize}

\textbf{(2) Zero-trace and periodic conditions.}
When $s \geq 1$, our results regarding Laplacian eigenmaps will require some boundary assumptions on $f_0$. In particular, we will assume that $f_0$ belongs to the set of zero trace Sobolev functions $H_0^{s}(P)$, meaning \textcolor{red}{roughly} that $f_0 \in H^s(P)$ additionally satisfies,
\begin{equation}
\label{eqn:zero_trace}
D^jf(x) = 0,~~\textrm{for all $x \in \partial \mc{X}$ and $j = 0,\ldots,s - 1$.}
\end{equation}
\textcolor{red}{(TODO)}: This is in spirit right but technically incoherent unless $f \in C^{\infty}(\mc{X})$. Need to use the formal definition of trace.

Let us justify why we impose a boundary condition such as zero-trace. For this part only we concentrate on the special case of $d = 1$ and the domain $\mc{X} = [0,1]$. In this case, there exists another way to define Sobolev space, via a sequence space representation (equiv Fourier transform). Let $\psi_1,\psi_2,\ldots$ denote the trigonometric basis of $\Leb^2([0,1])$,\footnote{To be explicit, $\psi_1(x) := 1$, $\psi_{2k}(x) := \sqrt{2} \cos(2\pi k x)$ and $\psi_{2k + 1}(x) := \sqrt{2} \sin(2\pi k x)$} and let $\Theta \subset \ell^2(\mathbb{N})$ be the Sobolev ellipsoid $\Theta = \{\theta \in \ell^2(\mathbb{N}): \sum_{k = 1}^{\infty} k^{2s} \theta_k^2 \leq M\}$. The function class $\{f = \sum_{k = 1}^{\infty} \theta_k \psi_k: \theta \in \Theta\}$ is \textcolor{red}{equivalent to} the \emph{periodic Sobolev space} $H_{\mathrm{per}}^s([0,1])$, defined as
\begin{equation}
\label{eqn:periodic_sobolev_space}
H_{\mathrm{per}}^s([0,1]) = \Bigl\{f \in H^s([0,1],M): D^jf(0) = D^jf(1)~~\textrm{for $j = 0,\ldots, s - 1$} \Bigr\}
\end{equation}
It is known that when $P$ is uniform, the eigenvectors $v_k$ converge to $\psi_k$ as $n \to \infty, \varepsilon \to 0$, and it is therefore natural that they accurately approximate only periodic functions $f_0 \in H^s([0,1])$. Although the zero-trace boundary condition is somewhat more restrictive than mere periodicity, the point is that some kind of boundary condition is inherently necessary in order to obtain sharp rates for spectral series estimators. For simplicity, we will stick to the specific boundary condition~\eqref{eqn:zero_trace}. 

\subsection{Minimax Rates}
\label{subsec:minimax_rates_sobolev}
We now turn to reviewing the minimax estimation and goodness-of-fit testing rates over Sobolev classes. We will always be interested in measuring loss in mean-squared error, that is in squared $\Leb^2$-norm. In this case, standard non-parametric minimax rates hold under some regularity conditions on $\mc{X}$ and $P$. Specifically, we will assume the following.\footnote{Assumption~\ref{asmp:sobolev_radius} ensures that we get a non-parametric rate. If $M \lesssim n^{-1/2}$ then both the estimation and testing rates become the parametric $n^{-1}$.}
\begin{enumerate}[label=(A\arabic*)]
	\item 
	\label{asmp:domain}
	The domain $\mc{X}$ is an open connected subset of $\Rd$, with Lipschitz boundary.
	\item
	\label{asmp:density}
	The design distribution $P$ admits a density $p$ with respect to Lebesgue measure. The density $p$ is bounded away from $0$ and $\infty$: there exists $\rho \geq 1$ such that
	\begin{equation*}
	0 < \frac{1}{\rho} \leq p(x) \leq \rho < \infty,~~\textrm{for all $x \in \mc{X}$}
	\end{equation*}
	\item
	\label{asmp:sobolev_radius}
	The radius $M$ of the Sobolev class satisfies $M = M(n) \gtrsim n^{-1/2}$.
\end{enumerate}
Under assumptions~\ref{asmp:domain}-\ref{asmp:sobolev_radius}, the minimax estimation rate over $\mc{H} = H^s(\mc{X};M)$ or $\mc{H} = H_0^s(\mc{X};M)$) is (see e.g. \citet{tsybakov2008_book})
\begin{equation}
\label{eqn:minimax_estimation_rate}
\inf_{\wh{f}} \sup_{f_0 \in \mc{H}} \|f_0 - f\|_P^2 \asymp M^{2d/(2s + d)}n^{-2s/(2s + d)};
\end{equation}
here the infimum is over estimators $\wh{f}$. 

In the goodness-of-fit testing problem, we ask for a test function---formally, a Borel measurable function $\phi$ that takes values in $\{0,1\}$--- which can distinguish between the hypotheses
\begin{equation}
\mathbf{H}_0: f_0 = f_0^{\star}, ~~\textrm{versus}~~ \mathbf{H}_a: f_0 \in \mc{H} \setminus \{f_0^{\star}\}.
\end{equation} 
Typically, the null hypothesis $f_0 = f_0^{\star} \in \mc{F}$ reflects the absence of interesting structure, and $\mc{F} \setminus  \{f_0^{\star}\}$ is a set of smooth departures from this null. To fix ideas, as in \citet{ingster2009} we focus on the problem of \emph{signal detection} in Sobolev spaces, where $f_0^{\star} = 0$ and $\mc{H}$ is a Sobolev ball, $\mc{H} = H^s(\Xset,M)$ or $\mc{H} = H_0^s(\mc{X};M)$ is a Sobolev ball; this is without loss of generality.

The minimax critical radius is the smallest value of $\sigma$ such that some level-${\alpha}$ test $\phi$ has power at least $1 - \alpha$ over all $\mc{H}_{\sigma} := \mc{H} \cap \{f: \|f\|_{\Leb^2(P)} \geq \sigma\}$:
\begin{equation*}
\sigma(\mc{H}) := \inf\Biggl\{\sigma > 0: \inf_{\phi} \biggl[ \sup_{f_0 \in \mc{H}_{\sigma}} \Ebb_{f_0}[1 - \phi]\biggr] \leq \alpha\Biggr\}
\end{equation*} 
where in the above the infimum is over all level-$\alpha$ tests $\phi$, and $\Ebb_{f_0}[\cdot]$ is the expectation under the regression function $f_0$. Testing whether a regression function $f_0$ is equal to $0$ is an easier problem than estimating $f_0$, and so the minimax testing critical radius over $\mc{H}$ is much smaller than the minimax estimation rate \citep{ingster2009}:
\begin{equation}
\label{eqn:sobolev_space_testing_critical_radius}
\sigma^2(\mc{H}) \asymp M^{2d/(4s + d)}n^{-4s/(4s + d)}~~\textrm{for $1 \leq d < 4s$.}
\end{equation}
When $4s \geq d$ the functions in $H^s(\Xset)$ are very irregular---formally speaking $H^s(\Xset)$ does not \textcolor{red}{continuously} embed into $\Leb^4(\Xset)$ when $4s \geq d$---and the minimax testing rates in this regime are unknown.

\textcolor{red}{(TODO)}
\begin{itemize}
	\item Is there a better citation than \citet{tsybakov2008_book} for the minimax rate of estimation~\eqref{eqn:minimax_estimation_rate}? Tsybakov considers the univariate case with fixed, equispaced design.
	\item I copy pasted the testing part directly from our AISTATS paper, then made a few necessary changes. Is that bad?
	\item I have stated the rate in~\eqref{eqn:sobolev_space_testing_critical_radius} over $\mc{H}$. In reality,~\citep{ingster2009} make some kind of boundary assumptions and I cannot remember exactly what. Is it ok to state it as is, or do I need to be more precise?
\end{itemize}

\subsection{Spectral series methods for Sobolev Spaces}
Spectral series estimators---and tests using their $\Leb^2$-norm---are particularly designed for regression in Sobolev spaces. Let $f_1,f_2,\ldots$ be the unit norm eigenfunctions of $\Delta_P$. The classical order-$K$ spectral series estimator $\wt{f}$ and test statistic $\wt{T}$ are
\begin{equation*}
\wt{f} = \sum_{k = 1}^{K} \dotp{Y}{f_k}_n f_k~~\textrm{and}~~\wt{T} = \|\wt{f}\|_P^2.
\end{equation*}
The estimator $\wt{f}$ and a test based on $\wt{T}$ are minimax rate-optimal over $H_0^s(\mc{X})$. This fact is intimately related to the series representation of periodic Sobolev spaces, e.g.~\eqref{eqn:periodic_sobolev_space}, whereby the model~\eqref{eqn:model} can be formally reduced to the Gaussian sequence model (see e.g. \citep{tsybakov08,gine16}). 

Unfortunately, in practice the estimator $\wt{f}$ is usually unworkable. Typically one does not have direct knowledge of the design distribution $P$, and even if $P$ is known, the eigenfunctions of $\Delta_P$ are rarely easy to compute. As an alternative, suppose instead we had access only to eigenfunctions $f_1',f_2',\ldots$ of the unweighted Laplace-Beltrami operator $\Delta_{\nu}$. Then letting $F_K = \mathrm{span}\{f_1,\ldots,f_K\}$, the least-squares estimator and test statistic
\begin{equation*}
\wt{f}_{\mathrm{LS}} := \argmin_{f \in F_K} \|Y - f\|_n^2,\textrm{and}~~\wt{T}_{\mathrm{LS}} = \|\wt{f}\|_{\nu}^2
\end{equation*}
are still rate-optimal over $H_0^s(P)$. However, this is not a totally satisfactory fix. For one thing, the least squares approach still requires that we know $\Delta$, and is not well suited for the case where $\mc{X}$ is a manifold, and $\Delta$ is the manifold Laplace-Beltrami operator. Additionally, diagonalizing even the unweighted Laplace-Beltrami operator $\Delta$ is quite difficult for all but a few special domains, such as $\mc{X} = [0,1]^d$. The general message is that in order to be minimax optimal, classical spectral series and least-squares estimators require an unrealistic amount of information on the design, and can be implemented only in selective situations. 

In contrast, Laplacian eigenmaps regression uses the eigenvectors of the neighborhood graph Laplacian $L_{n,\varepsilon}$ as features. The neighborhood graph Laplacian can be constructed (basically) without any knowledge of $P$ or $\mc{X}$, and Laplacian eigenmaps is thus a practicable method for regression. Moreover, the eigenvectors of $L_{n,\varepsilon}$ are approximations to the eigenfunctions $f_1,f_2,\ldots$ of $\Delta_P$---as we will discuss momentarily---so that $\wh{f}$ and $\wh{T}$ can in turn be viewed as approximations to the $\wt{f}$ and $\wt{T}$, respectively. As this view suggests, Laplacian eigenmaps naturally incurs some additional error compared to traditional spectral series estimators---this is the price we pay for using an approximation of $\Delta_P$. Fortunately, the general message of this paper is that despite this extra error, Laplacian eigenmaps retains many of the favorable optimality properties of its classical spectral series counterparts while freeing us from requiring unrealistic knowledge of $\Delta_P$. 

\subsection{Our contributions}
The following items summarize our major results. In all of them we assume~\ref{asmp:domain} and~\ref{asmp:density} hold, but not that any of $P$, $\mc{X}$, $\Delta_P$ or $\Delta$ are known. Put $r := s - 1$.
\begin{itemize}
	\item \textbf{Minimax optimal estimation.} For any $f_0 \in H^1(\Xset;M)$, with high probability $\norm{\wh{f} - f_0}_{n}^2 \lesssim M^{2d/(2 + d)}n^{-2/(2 + d)}$.
	\item \textbf{Minimax optimal testing.}
	A test constructed using $\wh{T}$ has non-trivial power whenever $f_0 \in H^1(\Xset;M)$ satisfies $\norm{f_0}_{P}^2 \gtrsim M^{2d/(2 + d)}n^{-4/(4 + d)}$ and $d < 4$.
	\item \textbf{Higher-order smoothness conditions.} When the regression function $f_0 \in H_0^s(\Xset;M)$ and additionally the density $p \in C^{r}(\mc{X};M)$, with high probability the error $\norm{\wh{f} - f_0}_{n}^2 \lesssim M^{2d/(2s + d)}n^{-2s/(2s + d)}$. Additionally, a test constructed using $\wh{T}$ has non-trivial power whenever $f_0 \in H_0^s(\Xset;M)$ satisfies $\norm{f_0}_{P}^2 \gtrsim M^{2d/(4s + d)}n^{-4s/(4s + d)}$ and $d < 4$.
	\item \textbf{Manifold adaptivity.}
	If $\mc{X} \subset \Rd$ is a manifold of dimension $m < d$, and if $H^s(\mc{X})$ for $s \leq 4$, then each of the aforementioned rates hold with $d$ replaced by $m$.
\end{itemize}
The above estimation results are stated with respect to in-sample mean squared error, meaning the loss is measured in the empirical squared-norm $\|\cdot\|_n^2$.\footnote{To state the obvious: this is very different than measuring the \emph{training error} $\|Y - \wh{f}\|_n^2$.} This is natural, as Laplacian eigenmaps is defined only at the observed design points $X_1,\ldots,X_n$. That being said, often one would like an upper bound on the prediction risk, or equivalently the out-of-sample loss $\|\cdot\|_P^2$. We therefore also introduce a simple, intuitive way of extending $\wh{f}$ to a function defined over all of $\mc{X}$, and show that this extension is minimax rate-optimal with respect to error measured in $L^2(P)$ norm.

\subsection{Related Work}
\begin{itemize}
	\item \textcolor{red}{(TODO)} Summarize what is known on fixed graphs.
	\item \textcolor{red}{Belkin, Slepcev} show that for a fixed $k$, the eigenvector $v_k$ converges to the eigenfunction $u_k$, in the sense that \textcolor{red}{(...)}. \textcolor{red}{Shi, Burago, Garcia Trillos, Calder, Cheng} build on this, giving finite sample bounds, rates of convergence, and making statements uniform over $1 \leq k \leq k_{\max}$. This justifies our previous discussion, in which we treat Laplacian eigenmaps as an approximation of spectral series methods. However, we emphasize that the minimax optimality of $\wh{f}$ or a test based on $\wh{T}$ does not follow straightforwardly from such results. Instead our analysis proceeds along different lines, which we detail in  Section~\ref{sec:analysis}.
	\item There has been limited analysis of the estimator $\wh{f}$. \cite{zhou2011} consider a similar estimator, but in the semi-supervised setting where the number of unlabeled points grows to infinity. In this case the estimator reduces to a classical spectral series estimator, and there is no error incurred by approximating $\Delta_P$. \cite{lee2016} consider a similar estimator---using eigenvectors of a different normalization of the Laplacian $L$---in both the supervised and semi-supervised setups, but get suboptimal rates for the supervised problem. As far as we know, there has been no analysis of the test statistic $\wh{T}$ in the random design framework which we study.
	\item \textcolor{red}{(TODO)}: Alternatively, \cite{trillos2020} \textcolor{blue}{(Green 2021)} use the graph Laplacian operator to induce a penalty over functions $f: \{\bf X\} \to \Reals$, and study a regularized estimator estimator, which minimizes the penalized loss
	\begin{equation*}
	\|Y - f\|_n^2 + \lambda \dotp{L_{n,\varepsilon}f}{f}_n.
	\end{equation*}
	\textcolor{blue}{(Green 2021)} show that the resulting estimator is (nearly)-minimax optimal, but only for $s = 1$ and $d \leq 4$. In contrast, the Laplacian eigenmaps estimator is optimal for all $s$ and $d$. 
\end{itemize}

\section{Minimax Optimality of Laplacian Eigenmaps}
\label{sec:minimax_optimal_laplacian_eigenmaps}

In this section, we will show that the estimator $\wh{f}$, and a test using the statistic $\wh{T}$, achieve optimal estimation and goodness-of-fit testing rates over Sobolev classes. We will divide our statements based on whether $f_0$ belongs to the first order Sobolev class ($s = 1$) or a higher-order Sobolev class ($s > 1$), since the details of the two settings are somewhat different. Throughout this section, we will always assume~\ref{asmp:domain} and~\ref{asmp:density}.

\subsection{First-order Sobolev classes}
\label{sec:first_order_sobolev_classes}
To begin, we handle the case where $f_0 \in H^1(P)$. We show that $\wh{f}$ and a test based on $\wh{T}$ are minimax optimal, for all values of $d$, and under no additional assumptions on the data generating process, i.e. on either $P$ or $f_0$. 

\paragraph{Estimation.} When the graph radius $\varepsilon$ and number of eigenvectors $K$ are chosen appropriately, the estimator $\wh{f}$ has (in-sample) optimal risk (up to constant factors) $\|\wh{f} - f_0\|_n^2 \lesssim n^{-2/(2 + d)}$. 
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{3}
	\item 
	\label{asmp:parameters_estimation_fo} 
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy the following inequalities:
	\begin{equation}\\
	\label{eqn:radius_fo} 
	C_0 \biggl(\frac{\log n}{n}\biggr)^{1/d} \leq \varepsilon \leq \min\{i_0,K^{-1/d}\},
	\end{equation}
	and 
	\begin{equation}
	\label{eqn:eigenvector_estimation_fo} 
	c_1 (M^2 n)^{2d/(2 + d)}\leq K \leq C_1 (M^2 n)^{2d/(2 + d)}.
	\end{equation}
\end{enumerate}
\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_fo}
	Suppose $f_0 \in H^1(P,M)$. If the Laplacian eigenmaps estimator $\wh{f}$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_estimation_fo}, then the following statement holds for any $\delta \in (0,1)$: with probability at least $1 - \delta - \textcolor{red}{(???)}$,
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_fo}
	\|\wh{f} - f_0\|_n^2 \leq \frac{C}{\delta}M^{2d/(2 + d)}n^{-2/(2 + d)}
	\end{equation}
\end{theorem}
Some remarks:
\begin{itemize}
	\item Regarding the restrictions on hyperparameters: the lower bound on $\varepsilon$ is the connectivity threshold, the smallest length scale at which the resulting graph will still, with high probability, be connected. On the other hand, as we will see in Section~\ref{subsec:analysis}, the upper bound on $\varepsilon$ is needed to ensure that the graph eigenvalue $\lambda_K \asymp \lambda_K(\Delta_P) \asymp K^{2/d}$. Finally, the restriction $K \asymp (M^2n)^{2d/(2 + d)}$ is chosen  to optimally trade-off bias and variance. Together, these restrictions suggest that the 
	\item \textbf{(1.1) How to tune hyper-parameters.} We note that the ranges~\eqref{eqn:radius_fo} and~\eqref{eqn:eigenvector_estimation_fo} depend on unknown quantities such as the dimension $d$ and radius of the Sobolev ball $M$. In practice, one typically tunes hyper-parameters by sample-splitting or cross-validation. However, because the estimator $\wh{f}$ is defined only in-sample, such approaches cannot be straightforwardly applied to select the graph radius $\varepsilon$, or number of eigenvectors $K$. We return to this issue upon considering out-of-sample extensions of $\wh{f}$ later, in Section~\ref{sec:out_of_sample}.
	\item \textbf{(1.2) The dependence on failure probability can be improved.} The upper bound on in-sample risk given by Equation~\eqref{eqn:laplacian_eigenmaps_estimation_fo} depends on the pre-factor $C/\delta$, and holds with probability $1 - \delta - \textcolor{red}{(?)}$. When $f_0 \in C^1(\mc{X};M)$, we can improve these dependencies, changing the pre-factor in~\eqref{eqn:laplacian_eigenmaps_estimation_fo} to $C(1 + 1/\delta)$, and showing that the bound holds with probability at least $1 - C\delta^2/n$. 
\end{itemize}

\textbf{(2) Laplacian eigenmaps is an optimal test.} Consider the test $\varphi = \1\{\wh{T} \geq t_{\delta}\}$, where $t_{\delta}$ is the threshold
\begin{equation*}
t_{\delta} := \frac{K}{n} + \frac{2K}{\sqrt{n}}.
\end{equation*}
The choice of $t_{\delta}$ guarantees that $\varphi$ is a level-$\delta$ test. Under appropriate choices of $\varepsilon$ and $K$, the test $\varphi$ has non-negligible power (up to constant factors) against alternatives separated the null by at least $\|f_0\|_{P}^2 \gtrsim M^{2d/(4 + d)}n^{-4/(4 + d)}$, whenever $d < 4$. 

\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{4}
	\item 
	\label{asmp:parameters_testing_fo}
	The graph radius $\varepsilon$ and the number of eigenvectors $K$ satisfy~\eqref{eqn:radius_fo}. Additionally,
	\begin{equation}
	\label{eqn:eigenvector_testing_fo}
	c_2 (M^2 n)^{2d/(4 + d)}\leq K \leq C_2 (M^2 n)^{2d/(4 + d)}.
	\end{equation}
\end{enumerate}
Let $p(d) = \min\{1,2/d\}$.
\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_fo}
	Suppose $f_0 \in H^1(\mc{X};M)$. If the Laplacian eigenmaps test $\varphi$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_testing_fo}, then the following statement holds for any $\delta \in (0,1)$: if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_fo}
	\|f_0\|_P^2 \geq \frac{C}{\delta} \max\biggl\{\frac{1}{\delta}M^{2d/(4 + d)}n^{-4/(4 + d)}, M n^{-p(d)}\biggr\},
	\end{equation}
	then $\varphi$ has Type II error upper bounded by 
	\begin{equation}
	\Pbb_{f_0}(\varphi = 0) \leq \delta + \textcolor{red}{(...)}
	\end{equation}
\end{theorem}
Some remarks:
\begin{itemize}
	\item \textbf{(2.1) When $d \geq 4$, the testing problem is different.} When $d \geq 4$, it is not the case that $H^1(\mc{X})$ continuously embeds into $\Leb^4(\mc{X})$, and as far we know the optimal testing rates over $H^1(\mc{X})$ are not known. On the other hand, if we explicitly assume $f_0 \in \Leb^4(\mc{X})$, then simply taking $T = \|Y\|_n^2$ achieves the optimal rates.
	\item \textbf{(2.2) Both of the remarks after Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} also apply to testing.}
\end{itemize}

\subsection{Higher-order Sobolev classes}
\label{sec:higher_order_sobolev_classes}
In this section, we now assume that the regression function displays some higher-order regularity, $f_0 \in H_0^s(\mc{X})$. We point out that on a graph there is no natural analogue to higher-order derivatives. For this reason it is not at all obvious that graph-based methods \emph{can} adapt to higher-order smoothness. Remarkably, we show that this is in fact the case. Under appropriate choices of $\varepsilon$ and $K$, the estimator $\wh{f}$ has (in-sample) optimal error (up to constant factors) $\|\wh{f} - f_0\|_n^2 \lesssim n^{-2s/(2s + d)}$. 

\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp:parameters_estimation_ho}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy
	\begin{equation*}
	C_0 \cdot \max\biggl\{\biggl(\frac{\log}{n}\biggr)^{1/d}, n^{-1/(2r + d)}\biggr\} \leq \varepsilon \leq \min\{i_0, K^{-1/d}\}
	\end{equation*}
	and
	\begin{equation*}
	K \asymp (M^2n)^{2d/(2s + d)}
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_ho}
	Under appropriate conditions, with probability at least $1 - \delta$,
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_ho}
	\|\wh{f} - f_0\|_n^2 \lesssim n^{-2s/(2s + d)}
	\end{equation}
\end{theorem}
Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho}, in combination with Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo}, implies that in the flat Euclidean setting Laplacian eigenmaps is a rate-optimal estimator over Sobolev classes, for all scalings of $s$ and $d$. Some other remarks:
\begin{itemize}
	\item \textbf{(1.1) Optimal rates, no RKHS.} It is worth pointing out that we do not require that the regularity of the Sobolev space satisfy $2s > d$, a condition often seen in the literature. In the ``sub-critical'' regime $2s < d$, the Sobolev space $H^s(\mc{X})$ is quite irregular. For instance, it is not a Reproducing Kernel Hilbert Space, nor does it embed into any H\"{o}lder space. As a result, certain versions of the nonparametric regression problem are ill-posed---for instance when loss is measured in $\Leb^{\infty}$ norm, or when the design points $\{X_1,\ldots,X_n\}$ are assumed to be fixed. Likewise, certain estimators are ``off the table'', most notably RKHS-based methods such as smoothing splines. Nevertheless, for random design regression with error measured in $\Leb^2(P)$-norm, spectral series estimators obtain the minimax rates for all values of $s$ and $d$. Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho} shows that the same is true with respect to Laplacian eigenmaps, with error measured in $\Leb^2(P_n)$ norm.
	\item \textbf{(1.2) The zero-trace condition.} The zero-trace condition can likely be weakened. However, requiring some boundary condition on $f_0$ and its first through $(k -1 )$th-order derivatives is unavoidable. \textcolor{red}{Explain why: graphs are implicitly boundaryless objects, and the eigenvectors of $L$ converge to those eigenvectors of $\Delta_P$ which satisfy boundary conditions. Repeat that this is a standard assumption made when analyzing spectral series estimators}.
	\item \textbf{(1.3) The bounded norm and smooth density conditions.} We require that $f_0$ be bounded in the Sobolev norm $\|f_0\|_{H^s(\mc{X})}$---as opposed to the semi-norm $|f_0|_{H^s(\mc{X})}$--- and that $p \in C^{s - 1}(\mc{X})$. Both requirements are related and fundamental, as we shall now explain. As previously mentioned, $L$ converges to a weighted-version of the Laplace operator $\Delta_P$ (also known as the Fokker-Planck) operator. The operator $\Delta_P$ includes both first and second-order derivatives,
	\begin{equation}
	\label{eqn:fokker_planck_1}
	\Delta_Pf= \frac{-1}{p} \mathrm{div}(p^2 \nabla f) = \Delta f - 2\frac{\nabla p^{\top} \nabla f}{p},
	\end{equation}
	and therefore a function $f \in H_0^s(\mc{X})$ has finite energy $\|\Delta_P f\|_{P}^2$ only if the first-order partial derivatives of $f$ and $p$ are bounded. \textcolor{red}{This discussion---and in particular the last sentence---are obviously not quite correct. This is in part because you realize now that the requirements are not in fact fundamental. Instead, the requirement should merely be that $\|\Delta_P^kf\|_{\Leb^2(P)}^2$ is bounded---here $k = s/2$ and we assume $s$ is even---and the upper bounds should be in terms of this semi-norm.}.
	\item \textbf{(1.4) Both of the remarks after Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} continue to apply.}
\end{itemize}

\textbf{(1.4) Rates for testing with higher-order smoothness assumptions.} For testing, we can get the optimal rate $n^{-4s/(4s + d)}$ when $d \leq 4$. When $4 < d \leq 4s$, we get the rate~\textcolor{red}{(...)} Recall that the rate $n^{-4s/(4s + d)}$ is optimal whenever $d \leq 4s$. Thus when $4s > d$, our upper bounds do not imply that $\varphi$ is an optimal test. We do not know whether this is due to looseness in our proof techniques, or a problem with the test itself, and leave this for future work.

\subsection{Analysis}
\label{subsec:analysis}

\textbf{(1) Bias and variance terms, for estimation and testing}. We analyze the estimation error of $\wh{f}$, and the testing error of $\wh{\varphi}$, by first conditioning on the design points $X$ and deriving \emph{design-dependent} bias and variance terms. For estimation, we have that with high probability, conditional on the design $X$,
\begin{equation*}
\|\wh{f} - f_0\|_n^2 \leq C\frac{\dotp{L^s f_0}{f_0}_n}{\lambda_{\kappa}^s} + \frac{K}{n}
\end{equation*}
For testing, we have~\textcolor{red}{(...)}.

The semi-norm $\dotp{L^s f_0}{f_0}_n$ and eigenvalue $\lambda_{K}$ are random variables that depend the random design points $X_1,\ldots,X_n$. It remains to establish suitable upper and lower bounds on these quantities. 

\textbf{(2) Upper bound on the semi-norm. } Proposition~\ref{prop:graph_seminorm} gives a sufficient upper bound on $\dotp{L^s f_0}{f_0}_n$. Take $\beta := s - 1$. 
\begin{proposition}
	\label{prop:graph_seminorm} 
	Suppose $f \in H_0^s(\mc{X})$, that $p \in C^{\beta}(\mc{X})$, and that $r \gtrsim n^{-1/d}$. Then with probability at least $1 - \delta$, it holds that 
	\begin{equation}
	\label{eqn:graph_seminorm_1}
	\dotp{L^s f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^s} \|p\|_{C^{\beta}} + \textcolor{red}{(?)}
	\end{equation}
	Therefore, if additionally $r \gtrsim n^{-1/(2\beta + d)}$, then 
	\begin{equation}
	\label{eqn:graph_seminorm_2}
	\dotp{L^s f}{f}_n \leq \frac{C}{\delta} \|f\|_{H^s} \|p\|_{C^{\beta}}
	\end{equation}
	When $s = 1$, the results hold for $f \in H^1(\mc{X})$ and any density $p$ that satisfies Assumption~\ref{asmp:density}.
\end{proposition}
The proof of Proposition~\ref{prop:graph_seminorm} involves two steps. In the first step, we show that with probability at least $1 - \delta$, the graph semi-norm $\dotp{L^s f}{f}_n$ is upper bounded by a non-local continuum energy $E_{\varepsilon}^{(s)}(f)$, defined as
\begin{equation*}
E_{\varepsilon}^{(s)}(f) := \dotp{L_{P,\varepsilon}^sf}{f}_{P}
\end{equation*}
where $L_{P,\varepsilon}$ is a non-local approximation to $\Delta_P$, 
\begin{equation}
\label{eqn:nonlocal_laplacian}
L_{P,\varepsilon}f(x) := \frac{1}{\varepsilon^{d + 2}}\int_{\mc{X}}\bigl(f(z) - f(x)\bigr) \eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,dP(x).
\end{equation}
In the second step, we show that $E_{\varepsilon}^{(s)}(f)$ is in turn upper bounded by a constant times $D_s(f) := \|f\|_{H^s(P)}^2$. Together, these steps establish Proposition~\ref{prop:graph_seminorm}.

When $s = 1$, similar results have been shown in \textcolor{red}{(?)}. On the other hand, much less work has been done in the $s > 1$ case, where the analysis is more complicated. For one thing, we note that when $s > 1$ the graph energy $\dotp{L^s f}{f}_n$ is itself a biased estimate of the non-local energy $E_{\varepsilon}^{(s)}(f)$. This bias term is the second term on the right hand side of~\eqref{eqn:graph_seminorm_1}, and is neglible only when $r \gtrsim n^{-1/(2r + d)}$. On the other hand, in order to give an estimate of $E_{\varepsilon}^{(s)}(f)$ in terms of the Sobolev norm $D_s(f)$, we first show that $L_{P,\varepsilon}^jf(x) \approx \Delta_P^jf(x)$, for $j = s/2$ when $s$ is odd and $j = (s - 1)/2$ when $s$ is even. In words, we show that a $j$th-order iterated difference operator approximates a $2j$-th (or $2j + 1$-th) order differential operator.  In to establish this fact, we require that on the one hand $p \in C^{r}(\mc{X};M)$, and on the other hand $f$ be zero-trace.

\textbf{(3) Lower bound on the eigenvalue.} On the other hand, recent work \textcolor{red}{(Burago14, Shi16, Garcia Trillos 18)} has analyzed the convergence of $\lambda_{k}$ towards $\lambda_{k}(\Delta_P)$. They provide explicit bounds on the relative error $|\lambda_{k} - \lambda_{k}(\Delta_P)|/\lambda_{k}(\Delta_P)$, showing that the relative error is small for sufficiently large $n$, small $\varepsilon$, and any $1 \leq k \leq n$ that is not too large relative to $n$. These results are actually stronger than are necessary to establish Theorems~\ref{thm:laplacian_eigenmaps_estimation_fo}-\ref{thm:laplacian_eigenmaps_estimation_ho}---in order to get rate-optimality, we need only show that $\lambda_{K}/\lambda_K(P)$ is $\Omega_P(1)$---but unfortunately they all assume $P$ is supported on a manifold without boundary. We will instead use the results of \textcolor{red}{(Green 2021)}, whose assumptions match our own, and who give a weaker bound on $\lambda_k/\lambda_k(\Delta_P)$ that will nevertheless suffice for our purposes. 

\begin{proposition}
	\label{prop:graph_eigenvalue}
	\textcolor{red}{(TODO)}
	With probability at least $1 - \textcolor{red}{(?)}$,
	\begin{equation*}
	\lambda_k \geq c \cdot \min\Bigl\{\lambda_k(\Delta_P), \frac{1}{r^{2}} \Bigr\}
	\end{equation*}
\end{proposition}
By Weyl's Law, $c k^{2/d} \leq \lambda_{k}(\Delta_P) \leq Ck^{2/d}$, and as a result $\lambda_{K} \geq C\lambda_{K}(\Delta_P)$ as long as $r \geq \kappa^{-1/d}$. 

\textbf{(1.2) Traps we avoid.} It is tempting to use recent results regarding spectral convergence to analyze the estimator $\wh{f}$ and test $\wh{T}$. This suffers from an accumulation of error problem. 

It is also key that we directly analyze the semi-norm $\dotp{L^s f}{f}_n$, rather than invoking the pointwise convergence of $L^{s}f \to \Delta_P^{s}f$ to obtain a bound on $\dotp{L^s f}{f}_n$. Recall that we assume $f$ has bounded derivatives up to (and including) order $s$. This suffices to gain control of the semi-norm $\dotp{L^s f}{f}_n$, because $\dotp{L^s f}{f}_n$ approximates the $\Leb^2$ norm of an order $s$ differential operator, and so we have exactly as many derivatives as we need. On the other hand, $L^s$ approximates an order $2s$ differential operator, and $\Delta_P^sf$ is only a well-defined limiting object if we assume $f$ has order $2s$ derivatives. 

\textcolor{red}{(TODO)}
\begin{itemize}
	\item Possibly turn the first part of the discussion after Proposition~\ref{prop:graph_seminorm} into a small proof.
\end{itemize}


\section{Manifold Adaptivity}
\label{sec:manifold_adaptivity}

The optimal rates for nonparametric regression suffer from a bona fide curse of dimensionality, and are thus difficult to reconcile with the practical success of many methods for nonparametric regression. One explanation that resolves this seeming paradox is the \emph{manifold hypothesis}, where the design points $X$ are assumed to lie on or near a manifold $\mc{X}$ of intrinsic dimension $m \ll d$. Under the manifold hypothesis nonparametric regression is demonstrably easier, and in particular it is known \citep{bickel2007,ariascastro2018} that in this setting the optimal rates over $H^s(\Xset)$ scale like $n^{-2s/(2s + m)}$ (for estimation) and $n^{-4s/(4s + m)}$ (for testing). 

On the other hand, a theory has been developed~\citep{belkin03,belkin05,niyogi2013} establishing the the neighborhood graph $G_{n,r}$ can ``learn'' the manifold $\Xset$ in various senses, so long as $\Xset$ is locally linear. In this section, we contribute to this line of work by showing that under the manifold hypothesis, Laplacian eigenmaps achieve the sharper minimax estimation and testing rates. First, we give some relevant background on Riemmanian manifolds, and Sobolev classes defined upon them.

\subsection{Riemmanian manifolds}
Throughout this section, we will assume that the support $\mc{X}$ of the distribution $P$ is a manifold of dimension $m$ embedded in $\Rd$. We give to $\mc{X}$ the Riemannian structure induced by the ambient space $\Rd$, and denote the resulting volume form by $d\mu$.  We will also assume some regularity conditions on $\mc{X}$ and $P$. Recall that the injectivity radius of $\mc{X}$ is the maximum value of $\delta$ such that the exponential map $\exp_x: B_m(0,\delta) \subset T_x(\mc{X}) \to B_{\mc{X}}(x,\delta) \subset \mc{X}$ is a diffeomorphism for all $x \in \mc{X}$.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{4}
	\item 
	\label{asmp:domain_manifold} The manifold $\mc{X}$ is closed, connected, smooth and boundaryless. Additionally, the injectivity radius of $\mc{X}$ is at least $i_0 > 0$.
	\item 
	\label{asmp:density_manifold} The distribution $P$ admits a density $p$ with respect to the volume form $d\mu$. The density is bounded away from $0$ and $\infty$,
	\begin{equation*}
	0 < \frac{1}{\rho} < p(x) \leq \rho < \infty
	\end{equation*}
	for all $x \in \mc{X}$.
\end{enumerate}
The definition of the Sobolev space $H^s(P)$, and the normed ball $H^s(P;M)$ are the same, mutatis mutandi, as when $\mc{X}$ is full dimensional. 

\subsection{Error rates under the manifold hypothesis}
We now show that Laplacian eigenmaps achieves the faster minimax rates under the manifold hypothesis, when the regression function $f_0 \in H^s(P;M)$ and $s \leq 4$. This section will proceed in a similar fashion to Section~\ref{sec:higher_order_sobolev_classes}, except with the ambient dimension $d$ replaced by intrinsic dimension $m$. 

\paragraph{Estimation.}
To obtain a minimax optimal estimator, we choose the graph radius $\varepsilon$ and number of eigenvectors $K$ as in~\ref{asmp:parameters_estimation_ho}, except with ambient dimension $d$ replaced by the intrinsic dimension $m$.

\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp:parameters_estimation_manifold}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy
	\begin{equation}
	\label{eqn:radius_estimation_manifold}
	C_0 \cdot \max\biggl\{\biggl(\frac{\log}{n}\biggr)^{1/m}, n^{-1/(2r + m)}\biggr\} \leq \varepsilon \leq \min\{i_0, K^{-1/m}\}
	\end{equation}
	and
	\begin{equation*}
	K \asymp (M^2n)^{2m/(2s + m)}
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_estimation_manifold}
	Suppose $f_0 \in H^s(\mc{X},M)$, for $s \leq 4$. Under appropriate conditions, with probability at least $1 - \delta - \textcolor{red}{(?)}$,
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_estimation_manifold}
	\|\wh{f} - f_0\|_n^2 \lesssim M^{2m/(2s + m)}n^{-2s/(2s + m)}
	\end{equation}
\end{theorem}

\paragraph{Testing.}
To construct a minimax optimal test using $\wh{T}$, we choose $\varepsilon$ and $K$ as in~\ref{asmp:parameters_testing_fo}, except with the ambient dimension $d$ replaced by the intrinsic dimension $m$.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp:parameters_testing_manifold}
	The graph radius $\varepsilon$ and number of eigenvectors $K$ satisfy~\eqref{eqn:radius_estimation_manifold}, for $r = 0$. Additionally, there exist constants $c_2$ and $C_2$ such that
	\begin{equation*}
	c_2 (M^2n)^{2m/(4s + m)} \leq K \leq C_2 (M^2n)^{2m/(4s + m)}
	\end{equation*}
\end{enumerate}

\begin{theorem}
	\label{thm:laplacian_eigenmaps_testing_manifold}
	Suppose $f_0 \in H^1(\mc{X};M)$. If the Laplacian eigenmaps test $\varphi$ is computed with parameters $\varepsilon$ and $K$ that satisfy~\ref{asmp:parameters_testing_manifold}, then the following statement holds for any $\delta \in (0,1)$: if $f_0$ satisfies
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_testing_criticalradius_manifold}
	\|f_0\|_P^2 \geq \frac{C}{\delta} \max\biggl\{\frac{1}{\delta}M^{2m/(4 + m)}n^{-4/(4 + m)}, M n^{-p(m)}\biggr\},
	\end{equation}
	then $\varphi$ has Type II error upper bounded by 
	\begin{equation}
	\Pbb_{f_0}(\varphi = 0) \leq \delta + \textcolor{red}{(...)}
	\end{equation}
\end{theorem}

\begin{itemize}
	\item \textbf{Proof strategy.} The proofs of Theorems~\ref{thm:laplacian_eigenmaps_estimation_manifold} and~\ref{thm:laplacian_eigenmaps_testing_manifold} follow very similarly to the full-dimensional setting. The difference is that when $\mc{X}$ is a manifold with intrinsic dimension $m$, we can prove analogous results to Propositions~\ref{prop:graph_seminorm} and~\ref{prop:graph_eigenvalue}, but with the ambient dimension $d$ replaced by the intrinsic dimension $m$. 
	\item \textbf{Restriction on number of derivatives.} Unlike in the full-dimensional case, our upper bound on the estimation error of $\wh{f}$ matches the minimax rate $n^{-2s/(2s + m)}$ only when $s \leq 4$. At a high level, thinking of the graph $G$ as an estimate of the manifold $\mc{X}$, we incur some error by using Euclidean distance rather than geodesic distance to form the edges of $G$. By contrast, in the full-dimensional setting the Euclidean metric exactly coincides with the geodesic distance for all points $x,z \in \mc{X}$ that are sufficiently close to each other and far from the boundary of $\mc{X}$). This extra error incurred in the manifold setting dominates when $s > 4$. We give a more detailed explanation of this phenomenon in Appendix~\ref{sec:manifold_proofs}.
	\item \textbf{Comparison with traditional spectral series estimators.} Traditional spectral series estimators achieve the minimax rate for all values of $s$ and $m$. On the other hand, as we have already discussed, these estimators assume a priori knowledge of the domain $\mc{X}$, which is arguably a particularly unrealistic assumption when $\mc{X}$ is assumed to be a manifold. It is not clear whether this gap between spectral series and Laplacian eigenmaps estimators---or more generally, between estimators which assume the manifold is known, and those which do not---is real, or a product of loose upper bounds. 
\end{itemize}

\textcolor{red}{(TODO)}
\begin{itemize}
	\item \textcolor{red}{Give a reference regarding the optimality of spectral series estimators on known manifolds.}
\end{itemize}

\section{Out-of-sample error}
\label{sec:out_of_sample}
To summarize, Sections~\ref{sec:minimax_optimal_laplacian_eigenmaps} and~\ref{sec:manifold_adaptivity} show that $\wh{f}$ is a minimax optimal estimator over Sobolev spaces. However, as mentioned previously we have measured loss \emph{in-sample}---that is, measured in $\Leb^2(P_n)$ norm---whereas \emph{out-of-sample} error---error measured in $L^2(P)$ norm---is the more typical metric in the random design setup.

Of course, the Laplacian eigenmaps estimator is only defined at the observed design points $X_1,\ldots,X_n$, and to measure its error in $L^2(P)$ norm we must first extend it to the rest of $\Xset$. We propose a simple method, kernel smoothing, to do the job. The method can applied to any estimator defined at the design points, including Laplacian eigenmaps, and we show that a smoothed version $\wt{f}$ of our original estimator $\wh{f}$ has optimal $L^2(P)$ error. For simplicity, in this section we stick to the full-dimensional setting, but everything can be adapted to the manifold setting in a straightforward way.

\paragraph{Extension by kernel smoothing.}
We now formally define our approach to extending by kernel smoothing. For a kernel function $\psi(\cdot): [0,\infty) \to (-\infty,+\infty)$, bandwidth $h > 0$, and a distribution $Q$, the \emph{Nadaraya-Watson kernel smoother} $T_{\varepsilon,Q}$ is given by
\begin{equation*}
\bigl(T_{h,Q}f)(x) := \frac{1}{d_Q(x)} \int_{\Omega} f(z)\psi\biggl(\frac{\|z - x\|}{h}\biggr) \,dQ(z)
\end{equation*}
where $d_Q(x) := \int_{\Omega} \psi\bigl(\|z - x\|/\varepsilon\bigr) \,dQ(z)$. We denote $T_{\varepsilon,P_n}$ by $T_{\varepsilon,n}$, and $d_{P_n}$ by $d_n$. 

The estimated function $\wt{f}$ we will consider is a kernel smoother passed over $\wh{f}$, meaning
\begin{equation}
\label{eqn:kernel_smoother_laplacian_eigenmaps}
\wt{f}(x) := \bigl(T_{h,n}\wh{f}\bigr)(x).
\end{equation}
It is useful to make a few comments about $\wt{f}$. First of all, while in principle~\eqref{eqn:kernel_smoother_laplacian_eigenmaps} makes sense for any $x \in \Rd$, we will always think of $\wt{f}$ as a function on $\mc{X}$. Additionally, note we may use the same kernel for smoothing as was used to construct the graph, i.e. take $\psi = \eta$. Likewise, we can set $h = \varepsilon$. However, we will see that for theoretical purposes these are not always the best choices. 

\paragraph{Out-of-sample error of kernel smoothed Laplacian Eigenmaps.}

In Lemma~\ref{lem:kernel_smoothing_laplacian_eigenmaps}, we consider an arbitrary estimator $\wc{f} \in L^2(P_n)$. We show that the out-of-sample error $\|T_{n,h}\wc{f} - f_0\|_P^2$ can be upper bounded by three terms--- (a constant times) the in-sample error of $\wc{f} - f_0$, and bias and variance terms associated with a noiseless version of kernel smoothing. We shall assume the following conditions on $\psi$.
\begin{enumerate}[label=(K\arabic*)]
	\setcounter{enumi}{1}
	\item
	\label{asmp:kernel}
	The kernel function $\psi$ is supported on a subset of $[0,1]$, with $\psi(1/2) > 0$. Additionally, $\psi$ is Lipschitz continuous on $[0,1]$. Finally, for convenience, we will assume that $\psi$ is normalized such that
	\begin{equation*}
	\int_{-\infty}^{\infty} \psi(|s|) \,ds = 1.
	\end{equation*}
\end{enumerate}
\begin{lemma}
	\label{lem:kernel_smoothing_laplacian_eigenmaps}
	Suppose $\wc{f} \in L^2(P_n)$, $f_0 \in H^1(\mc{X})$ and $p \in C^1(\mc{X})$. Under Assumptions~\ref{asmp:domain}, \ref{asmp:density}, and~\ref{asmp:kernel}, it holds that
	\begin{equation}
	\label{eqn:kernel_smoothing_laplacian_eigenmaps}
	\|T_{n,h}\wc{f} - f_0\|_P^2 \leq C\biggl(\|\wc{f} - f_0\|_n^2 + \frac{1}{\delta} \cdot \frac{h^2}{nh^d} |f|_{H^1(\mc{X})}^2 + \frac{1}{\delta}\|T_{h,P}f_0 - f_0\|_P^2\biggr),
	\end{equation}
	with probability at least $1 - \delta - Ch^d\exp\{-Cnh^d\}$. 
\end{lemma}
Notice that the variance term in the above is smaller than the typical variance term for kernel smoothing of noisy data, by a factor of $h^2$. The bias term, on the other hand, is standard (at least when $f_0$ is assumed to be H\"{o}lder smooth), and can be tightly bounded using Taylor expansion when $\psi$ is an order-$s$ kernel. Recall that $\psi$ is an order-$s$ kernel if
\begin{equation*}
\int_{-\infty}^{\infty} \psi(|s|) = 1,~~\int_{-\infty}^{\infty} t^j \psi(|t|) \,dt = 0~~\textrm{for $j = 1,\ldots,s - 1$},~~\textrm{and}~~\int_{-\infty}^{\infty} t^s \psi(|t|) \,dt < \infty.
\end{equation*}
\begin{lemma}
	\label{lem:kernel_smoothing_bias}
	Suppose the domain $\mc{X}$, the distribution $P$, and the kernel $\psi$  satisfy~\ref{asmp:domain}, \ref{asmp:density}, and~\ref{asmp:kernel}, respectively. 
	\begin{itemize}
		\item If $f_0 \in H^1(P)$, it follows that
		\begin{equation*}
		\|T_{h,P}f_0 - f_0\|_P^2 \leq C h^{2} |f|_{H^1(P)}^2.
		\end{equation*}
		\item If $f_0 \in H_0^{s}(P)$, $p \in C^{r}(\mc{X})$ for $r = s - 1$, and $\psi$ is an order-$s$ kernel, it follows that
		\begin{equation*}
		\|T_{h,P}f_0 - f_0\|_P^2 \leq C h^{2s} |f|_{H^s(\mc{X})}^2.
		\end{equation*}
	\end{itemize}
\end{lemma}
Choosing $h \asymp n^{-1/(2r + d)}$ for $r = s - 1$ balances the kernel smoothing bias and variance terms in~\eqref{eqn:kernel_smoothing_laplacian_eigenmaps}. Then, applying the bounds in Theorem~\ref{thm:laplacian_eigenmaps_estimation_fo} (when $s = 1$) or Theorem~\ref{thm:laplacian_eigenmaps_estimation_ho} (when $s > 1$), we conclude that $\wt{f}$ is an optimal estimator, error measured in $L^2(P)$ norm.

Some remarks:
\begin{itemize}
	\item \textbf{Tuning parameters by sample-splitting.}
	\item \textbf{Other extensions.} Comment that there does not exist any canonical way to extend Laplacian based estimators such as $\wh{\theta}$ to an ambient domain. Point out that there exist various proposals to extend eigenvectors in the literature, such as $1$-nearest-neighbor regression (piecewise constant extrapolation over Voronoi cells), convolution with a specialized kernel, or Nystr\"{o}m extension. Explain that each of these is designed for a highly specialized purpose, such as suitability for mathematical analysis or computational efficiency, and in any case are suggested only for eigenvectors rather than a regression estimate. State explicitly that we choose kernel smoothing for its simplicity, ubiquity, and strong theoretical properties under our assumptions, but that considerations of extensions for specifically tailored to Laplacian based estimators could be of interest.
	\item \textbf{Comparison with kernel smoothing.}
\end{itemize}


\subsection{Why not just do kernel smoothing?}
On the other hand, the allusion to kernel smoothing raises a very natural question: why should we prefer to first compute $\wh{\theta}$ and then smooth it out, as opposed to simply performing kernel smoothing over the original responses $Y$?

We now answer this question, by \textcolor{red}{(...)}.

\section{Experiments}
\label{sec:experiments}

\textcolor{red}{(TODO)}: Come up with a set of experiments that might be interesting to run. Run them by Ryan and Siva, so that you can prioritize.

\section{Discussion}
\label{sec:discussion}

\textbf{(1) Future items:}
\begin{itemize}
	\item \textbf{Other properties of spectral series estimators.}
\end{itemize}


\bibliographystyle{plainnat}
\bibliography{../../graph_regression_bibliography} 

\appendix

\input{appendix.tex}

\end{document}