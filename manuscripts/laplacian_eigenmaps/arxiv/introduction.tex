\documentclass{article}

%%% Begin Ryan's template
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{times}          % times font

\usepackage[round]{natbib}
\usepackage{amssymb,amsmath,amsthm,bbm}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim,float,url,dsfont}
\usepackage{graphicx,subfigure,psfrag}
\usepackage{algorithm,algorithmic}
\usepackage{mathtools,enumitem}
\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}
\usepackage{multirow}

% Theorems and such
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

% Assumption
\newtheorem*{assumption*}{\assumptionnumber}
\providecommand{\assumptionnumber}{}
\makeatletter
\newenvironment{assumption}[2]{
	\renewcommand{\assumptionnumber}{Assumption #1#2}
	\begin{assumption*}
		\protected@edef\@currentlabel{#1#2}}
	{\end{assumption*}}
\makeatother

% Widebar
\makeatletter
\newcommand*\rel@kern[1]{\kern#1\dimexpr\macc@kerna}
\newcommand*\widebar[1]{%
	\begingroup
	\def\mathaccent##1##2{%
		\rel@kern{0.8}%
		\overline{\rel@kern{-0.8}\macc@nucleus\rel@kern{0.2}}%
		\rel@kern{-0.2}%
	}%
	\macc@depth\@ne
	\let\math@bgroup\@empty \let\math@egroup\macc@set@skewchar
	\mathsurround\z@ \frozen@everymath{\mathgroup\macc@group\relax}%
	\macc@set@skewchar\relax
	\let\mathaccentV\macc@nested@a
	\macc@nested@a\relax111{#1}%
	\endgroup
}
\makeatother

% Min and max
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\st}{\mathop{\mathrm{subject\,\,to}}}

% Shortcuts
\def\R{\mathbb{R}}

%%% End Ryan's template

%%% Begin Alden's additions
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\Reals}{\mathbb{R}} % Same thing as Ryan's \R
\newcommand{\Rd}{\Reals^d}
\newcommand{\wb}[1]{\widebar{#1}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\bj}{{\bf j}}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wc}{0}{mathx}{"71}
%%% End Alden's Additions

\begin{document}
	\begin{center} {\Large{\bf{Convergence Rates of Principal Components Regression with Laplacian Eigenmaps}}}
		
		\vspace*{.3cm}
		
		{\large{
				\begin{center}
					Alden Green~~~~~ Sivaraman Balakrishnan~~~~~ Ryan J. Tibshirani\\
					\vspace{.2cm}
				\end{center}
				
				
				\begin{tabular}{c}
					Department of Statistics and Data Science \\
					Carnegie Mellon University
				\end{tabular}
				
				\vspace*{.2in}
				
				\begin{tabular}{c}
					\texttt{\{ajgreen,siva,ryantibs\}@stat.cmu.edu}
				\end{tabular}
		}}
		
		\vspace*{.2in}
		
		\today
		\vspace*{.2in}
	\end{center}

	\section{Introduction}

	Laplacian Eigenmaps (LE)~\citep{belkin03a} is a method for nonlinear dimensionality reduction and data representation. Given scattered data points $\{X_1,\ldots,X_n\} \subset \Reals^d$, LE maps each $X_i$ to a vector $(v_{i,1},\ldots,v_{i,K})$ according to the following steps.
	\begin{enumerate}
		\item First, LE forms a \emph{neighborhood graph} $G = (V,W)$ over the points $\{X_1,\ldots,X_n\}$. The graph $G$ is an undirected, weighted graph, with vertices $V = \{X_1,\ldots,X_n\}$, and weighted edges $W_{ij}$ which correspond to the proximity between points $X_i$ and $X_j$, say in Euclidean distance.(For a formal definition see Section~\ref{})
		\item Next, LE forms an (unweighted) \emph{graph Laplacian} matrix $L \in \Reals^{n \times n}$, a symmetric and diagonally dominant matrix with diagonal elements $L_{ii} = \sum_{j = 1}^{n} W_{ij}$, and off-diagonal elements $L_{ij} = -W_{ij}$. 
		\item Finally, LE takes the eigendecomposition $L = \sum_{k = 1}^{n} \lambda_k v_k v_k^{\top}$, and outputs the vectors $(v_{1,i},\ldots,v_{K,i}) \in \Reals^K$ for each $i = 1,\ldots,n$.
	\end{enumerate} 
	
	A natural way to use LE is by taking the vectors $\{(v_{i,1},\ldots,v_{i,K})\}_{i = 1}^{n}$ to be features in a downstream supervised learning algorithm. In this paper, we study a simple method along these lines: Principal Components Regression with Laplacian-Eigenmaps (PCR-LE), a method for nonparametric regression which operates by running ordinary least squares (OLS) using the features learned by LE. Given pairs of design points and responses $(X_1,Y_1),\ldots, (X_n,Y_n)$, PCR-LE computes an estimate $\wh{f} \in \Reals^n$,
	\begin{equation}
	\label{eqn:pcr-le}
	\wh{f} := \argmin_{f \in \mathrm{span}\{v_1,\ldots,v_K\}} \|{\bf Y} - f\|_2^2 = V_K V_K^{\top} {\bf Y},
	\end{equation}
	where ${\bf Y} = (Y_1,\ldots,Y_n) \in \Reals^n$ is the vector of responses, $\|\cdot\|_2$ denotes the usual Euclidean norm in $\Reals^n$, and the equality in \eqref{eqn:pcr-le} is due to the orthogonality of eigenvectors with respect to Euclidean inner product. 
	
	LE has been practically very successful, and by now has been used for various statistical tasks such as spectral clustering, manifold learning, level-set estimation, semi-supervised learning, etc. At this point there exists a rich literature \citep{koltchinskii2000,belkin07,vonluxburg2008,burago2014,shi2015,singer2017,garciatrillos18,trillos2019, calder2019, cheng2021,dunson2021} explaining this practical success from a theoretical perspective. Loosely speaking, these works model the design points as being independent samples from a distribution $P$ with density $p$, and show that in this case the eigenvectors of the graph Laplacian $L$ are good empirical approximations of population-level objects. These population-level objects are the eigenfunctions $\psi_k$---meaning solutions, along with eigenvalues $\rho_k$, to the equation $\Delta_P \psi_k = \rho_k \psi_k$--- of the density-weighted Laplacian operator
	\begin{equation}
	\label{eqn:density-weighted-laplace}
	\Delta_Pf := -\frac{1}{p}~ \mathrm{div}(p^2 \nabla f).
	\end{equation}  
	(Here $\mathrm{div}$ stands for the divergence operator, and $\nabla$ for the gradient.) These eigenfunctions in turn characterize various interesting structural aspects of $p$, such as the location and number of high- and low-density regions, the shape and intrinsic dimension of its support, and so forth.
	
	These aforementioned works explain what kind of representation LE learns, and the accuracy with which it learns this representation. However, this theory does not address the convergence rates of PCR-LE. That is the major question we answer in this paper. We adopt the classical model of nonparametric regression with random design: we observe independent pairs $(X_1,Y_1),\ldots,(X_n,Y_n)$ of design points and responses, where the design points $\{X_1,\ldots,X_n\}$ are sampled independently from an unknown distribution $P$ supported on $\mc{X} \subseteq \Rd$, and the responses follow a signal plus Gaussian noise model,
	\begin{equation}
	\label{eqn:model}
	Y_i = f_0(X_i) + w_i, \quad w_i \sim N(0,1),
	\end{equation}
	with noise $w_i$ independent of $X_i$. The task is to learn the regression function $f_0$, which is unknown but assumed to belong to a Sobolev space $H^s(\mc{X})$. We consider two settings: one where $\mc{X}$ is a full-dimensional domain, and the other where $\mc{X}$ is a low-dimensional submanifold of $\Rd$. In each setting, we derive upper bounds which imply that the PCR-LE estimate $\wh{f}$, and a test using the statistic $T = \|\wh{f}\|_2^2$, are statistically optimal methods for two classical problems in nonparametric regression: estimation and goodness-of-fit testing. 
	
	At first glance, the optimality of PCR-LE is somewhat surprising. As a function of the sample size $n$, known rates of convergence of LE are much slower than the minimax-optimal rates of convergence over $H^s(\mc{X})$ (roughly speaking $n^{-2s/(2s + d)}$ for estimation and $n^{-4s/(4s + d)}$ for testing). This is not due to suboptimal theory for LE, but rather reflects a fundamental fact: regression using learned features does not rely on accurately learning the features. It is essential to employ modes of analysis which exploit this fact in order to derive sharp rates of convergence for PCR-LE. We explain this in more detail after summarizing our main results.
	
	\paragraph{Sobolev spaces and spectral series regression.}
	To analyze PCR-LE, we work in a classical situation where the regression function is assumed to belong to a (Hilbert-)Sobolev space. For an open domain $\mc{X} \subseteq \Rd$, the Sobolev space $H^s(\mc{X})$ consists of all functions $f \in L^2(\mc{X})$ which are $s$-times weakly differentiable, with all order-$s$ partial derivatives $D^{\alpha}f \in L^2(\mc{X})$. We study regression over Sobolev spaces in part because generally speaking, the minimax rates are well-understood; as mentioned before, as a function of the sample size $n$ they are $n^{-2s/(2s + d)}$ for estimation, and $n^{-4s/(4s + d)}$ for testing. For this reason, regression over Sobolev spaces is a good setting in which to see whether PCR-LE measures up to simpler and more classical minimax-optimal approaches, which have strong theoretical guarantees but are less often used in practice.
	
	Moreover, we view PCR-LE as being particularly well-suited for regression over Sobolev spaces, due to their close connection with \emph{spectral series regression}. Spectral series regression computes empirical Fourier coefficients and truncates to the lowest frequency eigenfunctions, producing the estimate
	\begin{equation}
	\label{eqn:population-level_spectral_series}
	\wt{a}_k = \frac{1}{n}\sum_{i = 1}^{n} Y_i \psi_k(X_i), \quad \wt{f}(x) = \sum_{k = 1}^{K} \wt{a}_k \psi_k(x).
	\end{equation} 
	Under appropriate boundary conditions, the Sobolev spaces $H^s(\mc{X})$ can roughly be characterized as consisting of those functions $f = \sum_{k} a_k \psi_k \in L^2(\mc{X})$ for which the generalized Fourier coefficients $\{a_k\}_{k = 1}^{\infty}$ satisfy the decay condition $\sum_{k} a_k \rho_k^2 < \infty$. Heuristically, this decay condition justifies the truncation in~\eqref{eqn:population-level_spectral_series}---since the truncation incurs only a limited amount of bias for any $f_0 \in H^s(\mc{X})$---and for this reason spectral series regression over Sobolev spaces has been studied since \textcolor{red}{(?)}, shown to be optimal for estimation by \textcolor{red}{(?)}, and for goodness-of-fit testing by \textcolor{red}{(?)}.
	
	PCR-LE serves as an empirical approximation to spectral series regression, since the eigenvectors $v_k$ are empirical approximations to eigenfunctions $\psi_k$. Viewed in this light, a major advantage of PCR-LE is that it operates independently of the design distribution $P$. In the contrast, the spectral series estimator defined in~\eqref{eqn:population-level_spectral_series} relies on diagonalizing the density-weighted Laplacian $\Delta_P$. In our context, where $P$ is treated as unknown,~\eqref{eqn:population-level_spectral_series} must be viewed as an oracle method, and to emphasize this  we henceforth refer to~\eqref{eqn:population-level_spectral_series} as \emph{population-level spectral series regression}. On the other hand, naturally PCR-LE incurs some extra error by using an empirical approximation to the underlying basis $\{\psi_k\}$, and our work shows that in many cases, this extra error is small enough that it does not change the overall rate of convergence.
	
	\subsection{Main contributions}
	Summarized succinctly, our main contribution is to theoretically analyze nonparametric regression with PCR-LE and establish upper bounds which imply that, in many cases, this method achieves optimal rates of convergence over Sobolev spaces.
	
	\paragraph{Rates of convergence: population-level spectral series regression.}
	As we have already mentioned, the minimax-optimal rates over Sobolev spaces are generally well-known, as are upper bounds for population-level spectral series methods which match these rates. However, we could not find precisely stated results applying to our setting, where in particular
	\begin{enumerate}
		\item We consider subcritical Sobolev spaces $H^s(\mc{X})$, where the smoothness parameter $s$ satisfies $s < d/2$ and so $H^s(\mc{X})$ does not continuously embed into the space of continuous functions $C^0(\mc{X})$, and 
		\item We consider general design distributions $P$, which may satisfy certain regularity conditions but are not limited to being, say, the uniform distribution over $[0,1]^d$. 
	\end{enumerate}
	For completeness, we analyze population-level spectral series methods in this general setting, and establish upper bounds showing that such methods converge at the ``usual'' rates of $n^{-2s/(2s + d)}$ for estimation and $n^{-4s/(4s + d)}$ for testing. This analysis relies heavily on certain asymptotic properties of the continuum eigenfunctions $\psi_k$ and eigenvalues $\rho_k$, which hold for quite general second-order differential operators $\mc{L}$ including the density-weighted Laplacian $\mc{L} = \Delta_P$.
	 
	\paragraph{Rates of convergence: PCR-LE.}
	The rest of our results consist of various upper bounds on the rates of convergence for the PCR-LE estimator $\wh{f}$, and a test using the statistic $T = \|\wh{f}\|_2^2$. These upper bounds quantify two important properties of PCR-LE: first, that it can take advantage of smooth higher-order derivatives, and second that it can adapt to low intrinsic dimension of the design distribution, each in an optimal manner. We first model the design distribution $P$ as having support $\mc{X}$ which is a full-dimensional set in $\Rd$. In this case, our main contributions are as follows:
	\begin{itemize}
		\item  Over a ball in the Sobolev space $H^{s}(\mc{X})$, we establish that the PCR-LE estimator $\wh{f}$ has in-sample mean-squared error of at most on the order of $n^{-2s/(2s + d)}$, for any number of derivatives $s \in \mathbb{N}$ and dimension $d$. 
		\item We show that a test based on the statistic $\|\wh{f}\|_2^2$ has a squared critical radius on the order of $n^{-4s/(4s + d)}$, for any number of derivatives $s \in \mathbb{N}$ and dimension $d \in \{1,2,3,4\}$. 
	\end{itemize}
	We then consider the behavior of PCR-LE when the data satisfies a \emph{manifold hypothesis}, meaning the design distribution is supported on an (unknown) domain $\mc{X}$ which is a submanifold of $\Rd$ of intrinsic dimension $m \in \mathbb{N}, m < d$. In this case, our upper bounds imply that:
	\begin{itemize}
		\item When $f_0 \in H^s(\mc{X})$ the PCR-LE estimator has in-sample mean squared error of at most $n^{-2s/(2s + m)}$, when $s \in \{1,2,3\}$ and for any $m \in \mathbb{N}$. 
		\item A test based on the statistic $\|\wh{f}\|_2^2$ has a squared critical radius on the order of $n^{-4s/(4s + m)}$, when $s \in \{1,2,3\}$ and $m \in \{1,2,3,4\}$.
	\end{itemize}
	We note that under the manifold hypothesis, to the best of our knowledge the minimax rates for Sobolev spaces $H^s(\mc{X})$ under the assumption of random design with design distribution supported on an \emph{unknown} manifold have not been worked out. Our upper bounds confirm that these rates are the same as for regression over a known manifold, at least for the values of $s$ and $m$ mentioned above.
	
	In all these cases, our bounds also depend optimally on the radius $M$ of the Sobolev ball under consideration. However, for some values of $s$ (number of derivatives) and $d$ (dimension), there do exist gaps between our upper bounds on the error of PCR-LE and the minimax rates. Although we do not give corresponding lower bounds verifying the tightness of our analysis, we believe these gaps reflect the true behavior of the method rather than some looseness in our analysis, and we comment more on this at relevant parts in the text. For completeness, we summarize all of our upper bounds---those which match the minimax rates, and those which do not---in Tables~\ref{tbl:estimation_rates} and~\ref{tbl:testing_rates}.
	\begin{table}
		\begin{center}
			\begin{tabular}{p{.2\textwidth} | p{.14\textwidth} p{.12\textwidth} }
				Smoothness order & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
				\hline
				$s \leq 3$ & ${\bf n^{-2s/(2s + d)}}$ & ${\bf n^{-2s/(2s + m)}}$ \\
				$s > 3$  & ${\bf n^{-2s/(2s + d)}}$ & $n^{-6/(6 + m)}$
			\end{tabular}
		\end{center}
		\caption{Summary of Laplacian eigenmaps estimation rates over Sobolev balls. Bold font marks minimax optimal rates. In each case, rates hold for all $d \in \mathbb{N}$ (under Model~\ref{def:model_flat_euclidean}), and for all $m \in \mathbb{N}, 1 < m < d$ (under Model~\ref{def:model_manifold}). Although we suppress it for simplicity, in all cases when the Laplacian eigenmaps estimator is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal.}
		\label{tbl:estimation_rates}
	\end{table}
	
	\begin{table}
		\begin{center}
			\begin{tabular}{p{.175\textwidth} p{.175\textwidth} | p{.14\textwidth} p{.12\textwidth} }
				Smoothness order & Dimension & Flat Euclidean (Model~\ref{def:model_flat_euclidean}) & Manifold (Model~\ref{def:model_manifold}) \\
				\hline
				\multirow{2}{*}{$s = 1$} & $\dim(\mc{X}) < 4$ & ${\bf n^{-4s/(4s + d)}}$ & ${\bf n^{-4s/(4s + m)}}$ \\
				& $\dim(\mc{X}) \geq 4$ & ${\bf n^{-1/2}}$ & ${\bf n^{-1/2}}$ \\
				\hline
				\multirow{3}{*}{$s = 2$ or $3$} & $\dim(\mc{X}) \leq 4$  & ${\bf n^{-4s/(4s + d)}}$ & ${\bf n^{-4s/(4s + m)}}$ \\
				& $4 <\dim(\mc{X}) < 4s$  & $n^{-2s/(2(s - 1) + d)}$ & $n^{-2s/(2(s - 1) + m)}$\\
				& $\dim(\mc{X}) \geq 4s$ & ${\bf n^{-1/2}}$ & ${\bf n^{-1/2}}$ \\
				\hline
				\multirow{3}{*}{$s > 3$} & $\dim(\mc{X}) \leq 4$ & ${\bf n^{-4s/(4s + d)}}$ & $n^{-12/(12 + d)}$ \\
				& $4 < \dim(\mc{X}) < 4s$ & $n^{-2s/(2(s - 1) + d)}$ & $n^{-6/(4 + m)}$ \\
				& $\dim(\mc{X}) \geq 4s$ & ${\bf n^{-1/2}}$ & ${\bf n^{-1/2}}$ \\
			\end{tabular}
		\end{center}
		\caption{Summary of Laplacian eigenmaps testing rates over Sobolev balls. Bold font marks minimax optimal rates. Rates when $d > 4s$ assume that $f_0 \in L^4(\mc{X})$, and depend on $\|f_0\|_{L^4(\mc{X})}$. Although we suppress it for simplicity, in all cases when othe Laplacian eigenmaps test is optimal, the dependence of the error rate on the radius $M$ of the Sobolev ball is also optimal.}
		\label{tbl:testing_rates}
	\end{table}
	
	\paragraph{Technical contributions.}
	One of our primary technical contributions is a strategy for analyzing regression methods using learned features which leads to sharp rates of convergence. 
	
	At an abstract level PCR-LE is a two-stage algorithm: the first stage (LE) uses the observed design points $\{X_1,\ldots,X_n\}$ to learn a feature representation ($X_i \mapsto (v_{1,i},\ldots,v_{K,i})$) which is an empirical approximation to some ideal population-level representation ($X_i \mapsto (\psi_{1}(X_i),\ldots,\psi_{K}(X_i))$);  the second stage then applies a simple method (OLS) to the learned features, and obtains an estimate. At first blush, one might expect the error for PCR-LE to be likewise decomposed into two parts: first, the error with which the features are learned (i.e. error due to sampling variation in $X$), and the second being error with which, given ideal population-level features, the function is learned (i.e. error due to noise in $Y$.) 
	
	Crucially, our analysis \emph{does not} work in this way. This is important because, as already mentioned, all known upper bounds on the error with which the learned features (Laplacian eigenvectors) approximate their population-level limits (continuum Laplacian eigenfunctions) are much larger than the minimax rates of convergence over $H^s(\mc{X})$. For example,
	\begin{itemize}
		\item The best known rates of convergence for graph Laplacian eigenvectors (due to~\cite{cheng2021}) are
		\begin{equation}
		\label{eqn:eigenvector_convergence}
		\max_{1 \leq k \leq K}\|\sqrt{n}v_k - \psi_k\|_n^2 \leq C_{K} n^{-2/(4 + d)},
		\end{equation}
		where the constant $C_K$ depends on $K$ in an unknown manner.
		\item The dependence of $C_K$ on $K$ is relevant for our interests where $K$ is growing with $n$. The earlier works of \cite{burago2014,trillos2019} give upper bounds which, although implying weaker rates of convergence as a function of $n$, do keep track of $C_K$, and suggest that it should depend on the inverse of the spectral gap 
		\begin{equation}
		\label{eqn:spectral_gap}
		C_K = \frac{C}{\rho_{K + 1} - \rho_K} \geq C K^{1 - 2/d},
		\end{equation}
		where $C$ is a constant depending only on the dimension $d$, and the latter inequality follows from the Weyl's Law scaling $\rho_K \asymp K^{2/d}$.
	\end{itemize}
	Although these upper bounds may not reflect the true rate of convergence of graph Laplacian eigenvectors---this is still an active area of research, and no lower bounds are known---it seems very unlikely that the true rate matches even the estimation minimax rate $n^{-2s/(2s + d)}$, which after all approaches the dimension-free rate $1/n$ for large values of $s$. 
	
	Instead of relying on convergence of eigenvectors to eigenfunctions, our analysis proceeds via a bias-variance decomposition at the level of the graph. Focusing for simplicity on estimation, this is 
	\begin{equation}
	\label{eqn:bias_variance_estimation}
	\|f_0 - \wh{f}\|_n^2 \leq 2\bigl(\|f_0 - V_KV_K^{\top}f_0\|_n^2 + \|V_KV_K^{\top}({\bf Y} - f_0)\|_n^2\bigr) 
	\end{equation}
	The second term in~\eqref{eqn:bias_variance_estimation} is the variance, and as usual for OLS estimates depends only on the degrees of freedom $\mathrm{df}(\wh{f}) = \mathrm{tr}(V_KV_K^{\top}) = K$. More surprisingly, the first term (squared-bias) can also be upper bounded without appealing to the eigenfunctions $\psi_k$. Letting $S = L^{\dagger}$ be the pseudo-inverse of the graph Laplacian,
	\begin{equation}
	\begin{aligned}
	\|(I - V_KV_K^{\top})f_0\|_n^2 & = \|(I - V_KV_K^{\top})S^{s/2}L^{s/2}f_0\|_n^2 \leq \|(I - V_KV_K^{\top})S^{s/2}\|_{op}^2 \|L^{s/2}f_0\|_n^2 = \frac{f_0^{\top} L^s f_0}{n \lambda_{K + 1}^{s}}.
	\end{aligned}
	\end{equation}
	Thus we obtain an upper bound on the in-sample sample mean squared error $\|\wh{f} - f_0\|_n^2$ that is \emph{independent of the error with which graph Laplacian eigenvectors $v_k$ approximate population-level eigenfunctions $\psi_k$}. 
	
	Instead, our upper bound on the error of PCR-LE is determined by a pair of graph functionals: the quadratic form $f_0^{\top}L^s f_0$, and the graph Laplacian eigenvalue $\lambda_{K + 1}$. Theoretically speaking, this brings two advantages over directly analyzing convergence of eigenvectors to eigenfunctions. First, the graph functionals can be more tightly controlled than the eigenvectors $v_k$; for example, the error  Moreover, in order to obtain rates of convergence we do not require that these functionals themselves converge to population-level limits, but only that they be stochastically bounded on the right order. To derive our ultimate upper bounds we use some existing results regarding neighborhood graph Laplacians, and prove some new ones which may be of independent interest. 
		
	A last observation regarding the difference between feature learning and regression using learned features. To balance the bias and variance terms in~\eqref{eqn:bias_variance_estimation}, we will end up using $K(n) := n^{d/(2s + d)}$ eigenvectors. Although this is the usual choice for spectral series regression over order-$s$ Sobolev spaces, in the context of PCR-LE it leads to a striking conclusion. Namely, examining~\eqref{eqn:eigenvector_convergence} and~\eqref{eqn:spectral_gap}, we see that for all dimensions $d \geq 3$, known upper bounds on $\|\sqrt{n} v_{K(n)} - \psi_{K(n)}\|_n^2$ do not converge to $0$ as $n \to \infty$. In words, it is not merely the case that PCR-LE converges at a faster rate than LE itself, but that PCR-LE can profitably use eigenvectors for regression which may be inconsistent estimates of their population-level limits. 
	
	To summarize, our work demonstrates, broadly speaking, that regression using learned features can be analyzed independent of the accuracy of the features themselves. Regression using learned features is a general and widely applied paradigm, and we believe this observation may have consequences outside of its application to PCR-LE in this work.
	
	\subsection{Related work}
	
	Much of the work regarding regression using neighborhood graph Laplacians deals with \emph{semi-supervised learning}, where in addition to the labeled data $(X_1,Y_1),\ldots,(X_n,Y_n)$ one observes unlabeled points $(X_{n + 1},\ldots,X_{N})$. The landmark paper of \cite{zhu2003semisupervised} proposed to interpolate the observed values by~\emph{harmonic extension}, i.e. compute $L_N$ the Laplacian matrix corresponding to a graph formed over all design points $X_1,\ldots,X_N$, and then solve the constrained problem
	\begin{equation*}
	\minimize_{f \in \Reals^N} f^{\top} L_N f \quad \mathrm{subject\,\,to}~~~ f_i = Y_i~~\textrm{for $i = 1,\ldots,n$.}
	\end{equation*}
	Conventional wisdom says that harmonic extension is sensible only when the responses are noiseless, $Y_i = f_0(X_i)$, and that in the noisy setting one should instead solve the penalized formulation
	\begin{equation}
	\label{eqn:graph_laplacian_regularization_ssl}
	\wc{f} = \argmin_{f \in \Reals^N} \|{\bf Y} - f\|_n^2 + \lambda f^{\top} L_N f.
	\end{equation}
	Notwithstanding their intuitive appeal, both the constrained and penalized problems have issues when $d > 1$ and $n/N \to 0$~\citep{nadler09,calder2019b, calder2020}, and in certain cases the estimates are degenerate, meaning they are ``spiky'' at labeled data points and close to constant everywhere else. One solution to this problem is to instead use Laplacian Eigenmaps for semi-supervised learning (SSL-LE), i.e. compute the eigendecomposition $L_N = \sum_{k = 1}^{N} \lambda_k u_k u_k^{\top}$ and solve the problem
	\begin{equation}
	\label{eqn:laplacian_eigenmaps_ssl}
	\wb{f} := \argmin_{f \in \mathrm{span}\{u_1,\ldots,u_K\}} \|{\bf Y} - f\|_n^2.
	\end{equation}
	\cite{zhou2011,lee2016} analyze SSL-LE in a particular asymptotic regime where $n$ is held fixed while $N \to \infty$, and show that it achieves minimax-optimal rates of convergence over $H^s(\mc{X})$ as a function of $n$, the number of labeled points. Of course, in this regime the relevant eigenvectors of $L_n$ all converge to their continuum limits, meaning
	\begin{equation*}
	\lim_{N \to \infty} \max_{1 \leq K \leq K(n)} \|\sqrt{N} u_k - \psi_k\|_N^2 = 0,
	\end{equation*}
	and consequently $\lim_{N \to \infty} \|\wb{f} - \wt{f}\|_N^2 = 0$. Thus the analysis of SSL-LE  reduces to that of population-level spectral series regression. The supervised setting (where $N = n$) we consider in this work is very different, and analyzing PCR-LE necessitates an entirely different approach, as explained in the preceding section.
	
	There has been much less work, relatively speaking, regarding regression with neighborhood graph Laplacians in the supervised setting, which is the focus of this work. \citet{lee2016} also analyze a variant of PCR-LE, but derive suboptimal rates of convergence. \citet{trillos2020} study nonparametric regression via graph Laplacian regularization, where the estimator is computed by solving~\eqref{eqn:graph_laplacian_regularization_ssl}, but with no unlabeled data. They establish the upper bound $\max_{i = 1,\ldots,n}|\wc{f}(X_i) - f_0(X_i)| \leq C n^{-2/(2 + d)}$ under the assumption $f_0 \in C^2(\mc{X})$, which is slower than the minimax rate $n^{-4/(4 + d)}$ for this class. In a previous paper~\citep{green2021}, we (the authors) also considered nonparametric regression via graph Laplacian regularization, when the regression function $f_0 \in H^1(\mc{X})$ and loss is measured using in-sample mean squared error. We found that graph Laplacian regularization is consistent as $n \to \infty$, and optimal (using mean-squared error) over the first-order Sobolev space $H^1(\mc{X})$ when $d \in \{1,2,3,4\}$. However, graph Laplacian regularization neither takes advantage of smooth higher-order derivatives, nor is it provably optimal over $H^1(\mc{X})$ for all dimensions $d$. One of our motivations for considering PCR-LE was to find an estimator which addressed these deficiencies. In this work we indeed find that PCR-LE has much stronger optimality properties than graph Laplacian regularization. 
	
	Most work on supervised learning using graphs adopts a \emph{fixed design} perspective, treating the design points $X_1 = x_1,\ldots,X_n = x_n$ as vertices of a fixed graph, and carrying out inference with respect to the conditional mean vector $(f_0(x_1),\ldots,f_0(x_n))$. In this setting, matching upper and lower bounds have been established that certify the optimality of graph-based methods for estimation \citep{wang2016,hutter2016,sadhanala16,sadhanala17,kirichenko2017,kirichenko2018}) and testing \citep{sharpnack2010identifying,sharpnack2013b,sharpnack2013,sharpnack2015} over different ``function'' classes (in quotes because these classes really model the $n$-dimensional vector of evaluations). This setting is quite general, because the graph need not be a geometric graph defined on a vertex set which belongs to Euclidean space. On the other hand, depending on the data collection process, it may be unnatural to model the design points as being a priori fixed, and the estimand as being a vector which exhibits a discrete notion of ``smoothness'' over this fixed design. Instead, we adopt the \emph{random design} perspective, and seek to estimate a function that we assume exhibits a more classical notion of smoothness. 
	
	
% Bibliography
\bibliographystyle{plainnat}
\bibliography{../../../graph_regression_bibliography} 
	
\end{document}