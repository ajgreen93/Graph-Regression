\noindent 

\section{Graph-dependent error bounds}
\label{sec:fixed_graph_error_bounds}
In this section, we adopt the fixed design perspective; or equivalently, condition on $X_i = x_i$ for $i = 1,\ldots,n$. Let $G = \bigl([n],W\bigr)$ be a fixed graph on $\{1,\ldots,n\}$ with Laplacian matrix $L = \sum_{k = 1}^{n}\lambda_k v_k v_k^{\top}$. The randomness thus all comes from the responses 
\begin{equation}
\label{eqn:fixed_graph_regression_model}
Y_i = f_{0}(x_i) + \varepsilon_i
\end{equation}
where the noise variables $\varepsilon_i$ are independent $N(0,1)$. In the rest of this section, we will mildly abuse notation and write $f_0 = (f_0(x_1),\ldots,f_0(x_n)) \in \Reals^n$. We will also write ${\bf Y} = (Y_1,\ldots,Y_n)$.

Let $\wh{T} = \sum_{k = 1}^{K} \dotp{{\bf Y}}{v_k}_n^2$, and let $\varphi = \1\{\wh{T} \geq t_a\}$. In the following Lemma, we upper bound the Type I and Type II error of the test $\varphi$.

\begin{lemma}
	Suppose we observe $(Y_1,x_1),\ldots,(Y_n,x_n)$ according to~\eqref{eqn:fixed_graph_regression_model}.
	\begin{itemize}
		\item If $f_0 = 0$, then $\Ebb_0[\varphi] \leq a$.
		\item Suppose $f_0 \neq 0$ satisfies
		\begin{equation*}
		\sum_{k = 1}^{K} \dotp{f_0}{v_k}_n^2 \geq 16\frac{\sqrt{2K}}{n}\biggl[\sqrt{\frac{1}{a}} + \frac{1}{b}\biggr].
		\end{equation*}
		Then $\Ebb_{f_0}[1 - \phi] \leq b$.
	\end{itemize}
\end{lemma}

\section{Graph Sobolev semi-norm, flat Euclidean domain}
\label{sec:graph_quadratic_form_euclidean}
In this section we prove Proposition~\ref{prop:graph_seminorm_ho}. The proposition will follow from several intermediate results.
\begin{enumerate}
	\item~In Section~\ref{subsec:decomposition_graph_seminorm}, we show that
	\begin{equation}
	\label{pf:graph_seminorm_ho_1}
	\dotp{L_{n,\varepsilon}^sf}{f}_n \leq \frac{1}{\delta} \dotp{L_{P,\varepsilon}^sf}{f}_{P} + \frac{C\varepsilon^2}{\delta n\varepsilon^{2 + d}}M^2.
	\end{equation}
	with probability at least $1 - 2\delta$. 
	
	We term the first term on the right hand side the \emph{non-local Sobolev semi-norm}, as it is a kernelized approximation to the Sobolev semi-norm $\dotp{\Delta_P^sf}{f}_{P}$. The second term on the right hand side is a pure bias term, which as we will see is negligible compared to the non-local Sobolev semi-norm as long as $\varepsilon \ll n^{-1/(2(s -1 + d))}$. 
	\item~In Section~\ref{subsec:approximation_error_nonlocal_laplacian}, we show that when $x$ is sufficiently in the interior of $\mc{X}$, then $L_{P,\varepsilon}^kf(x)$ is a good approximation to $\Delta_P^kf(x)$, as long as $f \in H^{s}(\mc{X})$ and $p \in C^{s - 1}(\mc{X})$ for some $s \geq 2k + 1$. 
	\item~In Section~\ref{subsec:boundary_behavior_nonlocal_laplacian}, we show that when $x$ is sufficiently near the boundary of $\mc{X}$, then $L_{P,\varepsilon}^kf(x)$ is close to $0$, as long as $f \in H_0^{s}(\mc{X})$ for some $s > 2k$.
	\item~In Section~\ref{subsec:estimate_nonlocal_seminorm}, we use the results of the preceding two sections to show that if $f \in H_0^s(\mc{X};M)$ and $p \in C^{s - 1}(\mc{X})$, there exists a constant $C$ which does not depend on $f$ such that
	\begin{equation}
	\label{pf:graph_seminorm_ho_2}
	\dotp{L_{P,\varepsilon}^sf}{f}_{P} \leq CM^2.
	\end{equation}
\end{enumerate}
Finally, in Section~\ref{subsec:integrals} we provide some assorted estimates used in Sections~\ref{subsec:decomposition_graph_seminorm}. 

\paragraph{Proof (of Proposition~\ref{prop:graph_seminorm_ho}).}
Proposition~\ref{prop:graph_seminorm_ho} follows immediately from~\eqref{pf:graph_seminorm_ho_1} and~\eqref{pf:graph_seminorm_ho_2}. \qed

One note regarding notation: suppose a function $g \in H^{\ell}(U)$, where $\ell \in \mathbb{N}$ and $U$ is an open set. Let $V$ be another open set, compactly contained within $U$. Then we will use the notation $g \in H^{\ell}(V)$ to mean that the restriction $\restr{g}{V}$ of $g$ to $V$ belongs to $H^{\ell}(V)$.

\subsection{Decomposition of graph Sobolev semi-norm}
\label{subsec:decomposition_graph_seminorm}

In Lemma~\textcolor{red}{(?)}, we show that the graph Sobolev semi-norm (V-statistic) decomposes into an unbiased estimate of the non-local Sobolev semi-norm (U-statistic), and a pure bias term. We also establish that the pure bias term will be small (in expectation) relative to the U-statistic whenever $\varepsilon$ is sufficiently small.
\begin{lemma}
	\label{lem:graph_seminorm_bias}
	For any $f \in L^2(\mc{X})$, the graph Sobolev semi-norm satisfies
	\begin{equation}
	\label{eqn:graph_seminorm_bias_1}
	\dotp{L_{n,\varepsilon}^sf}{f}_{n} = U_{n,\varepsilon}^{(s)}(f) + B_{n,\varepsilon}^{(s)}(f),
	\end{equation}
	such that $\mathbb{E}[U_{n,\varepsilon}^{(s)}(f)] = (n - s - 1)!/n! \cdot \dotp{L_{P,\varepsilon}^sf}{f}_P$. If additionally $f \in H^1(\mc{X};M)$ and $\varepsilon \geq n^{-1/d}$, then the bias term $B_{n,\varepsilon}^{(s)}(f)$ satisfies
	\begin{equation}
	\label{eqn:graph_seminorm_bias_2}
	\mathbb{E}\bigl[|B_{n,\varepsilon}^{(s)}(f)|\bigr] \leq \frac{C\varepsilon^2}{\delta n\varepsilon^{2 + d}}M^2.
	\end{equation}
\end{lemma}
Then~\ref{pf:graph_seminorm_ho_1} follows immediately from Lemma~\ref{lem:graph_seminorm_bias}, by Markov's inequality.
\paragraph{Proof (of Lemma~\ref{lem:graph_seminorm_bias}).}
We begin by introducing some notation. We will use bold notation $\bj = (j_1,\ldots,j_s)$ for a vector of indices where $j_i \in [n]$ for each $i$. We write $[n]^s$ for the collection of all such vectors, and $(n)^s$ for the subset of such vectors with no repeated indices. Finally, we write $D_if$ for a kernelized difference operator,
\begin{equation*}
D_if(x) := \bigl(f(x) - f(X_i)\bigr) \eta\biggl(\frac{\|X_i - x\|}{\varepsilon}\biggr),
\end{equation*}
and we let $D_{\bj}f(x) := \bigl(D_{j_1}\circ \cdots \circ D_{j_s}f\bigr)(x)$.

With this notation in hand, it is easy to represent $\dotp{L_{n,\varepsilon}^sf}{f}_{n}$ as the sum of a U-statistic and a bias term,
\begin{align*}
\dotp{L_{n,\varepsilon}^sf}{f}_{n} & = \frac{1}{n} \sum_{i = 1}^{n} L_{n,\varepsilon}^sf(X_i) \cdot f(X_i) \\
& = \underbrace{\frac{1}{n^{s + 1}\varepsilon^{s(d + 2)}} \sum_{{i\bf j} \in (n)^{s + 1}}D_{\bj}f(X_i) \cdot f(X_i)}_{=:U_{n,\varepsilon}^{(s)}(f)} + \underbrace{\frac{1}{n^{s + 1}\varepsilon^{s(d + 2)}} \sum_{\substack{i\bj \in \\ [n]^{s + 1}\setminus (n)^{s + 1}}} D_{\bj}f(X_i) \cdot f(X_i)}_{=:B_{n,\varepsilon}^{(s)}(f)}
\end{align*}
When the indices of $i\bj$ are all distinct, it follows straightforwardly from the law of iterated expectation that
\begin{equation*}
\mathbb{E}[ D_{\bj}f(X_i) \cdot f(X_i)] = \varepsilon^{s(d + 2)}\mathbb{E}[L_{P,\varepsilon}^sf(X_i) \cdot f(X_i)] = \dotp{L_{P,\varepsilon}^sf}{f}_{P}, 
\end{equation*}
which in turn implies $\mathbb{E}[U_{n,\varepsilon}^{(s)}(f)] = (n - s - 1)!/n! \cdot \dotp{L_{P,\varepsilon}^sf}{f}_P$. 

It remains to show~\eqref{eqn:graph_seminorm_bias_2}. By adding and subtracting $f(X_{\bj_1})$, we obtain by symmetry that
\begin{equation*}
\sum_{\substack{i\bj \in \\ [n]^{s + 1}\setminus (n)^{s + 1}}} D_{\bj}f(X_i) \cdot f(X_i) = \frac{1}{2} \cdot \sum_{\substack{i\bj \in \\ [n]^{s + 1}\setminus (n)^{s + 1}}} D_{\bj}f(X_i) \cdot \bigl(f(X_i) - f(X_{\bj_1})\bigr),
\end{equation*}
and consequently
\begin{equation*}
\Ebb\Bigl[\sum_{\substack{i\bj \in \\ [n]^{s + 1}\setminus (n)^{s + 1}}} D_{\bj}f(X_i) \cdot f(X_i)\Bigr] \leq \frac{1}{2} \cdot \sum_{\substack{i\bj \in \\ [n]^{s + 1}\setminus (n)^{s + 1}}} \Ebb\Bigl[\bigl|D_{\bj}f(X_i)\bigr| \cdot \bigl|f(X_i) - f(X_{\bj_1})\bigr|\Bigr].
\end{equation*}
In Lemma~\textcolor{red}{(?)}, we show that if $f \in H^1(\mc{X};M)$, then for any $i\bj \in [n]^{s + 1}$ which contains a total of $k + 1$ distinct indices, 
\begin{equation*}
\Ebb\Bigl[\bigl|D_{\bj}f(X_i)\bigr| \cdot \bigl|f(X_i) - f(X_{\bj_1})\bigr|\Bigr] \leq C_1 \varepsilon^{2 + kd} M^2.
\end{equation*}
This shows us that the expectation of $|B_{n,\varepsilon}^s(f)|$ can bounded from above by the sum over several different terms, as follows:
\begin{align*}
\Ebb\Bigl[|B_{n,\varepsilon}^s(f)|\Bigr] & \leq C_1\frac{\varepsilon^2}{n\varepsilon^{2s}}M^2 \sum_{\substack{i\bj \in \\ [n]^{s + 1}\setminus (n)^{s + 1}}} \frac{1}{(n\varepsilon^d)^s}  \varepsilon^{(|i\bj| - 1)d} \\
& \leq C_1\frac{\varepsilon^2}{n\varepsilon^{2s}}M^2  \sum_{k = 1}^{s - 1} \frac{(n\varepsilon^d)^k}{(n\varepsilon^d)^s}n.
\end{align*}
Finally, we note that by assumption $n\varepsilon^d \geq 1$, so that in the above sum the factor of $(n\varepsilon^d)^k$ is largest when $k = s- 1$. We conclude that
\begin{equation*}
\Ebb\Bigl[|B_{n,\varepsilon}^s(f)|\Bigr] \leq C_1 (s - 1) \frac{\varepsilon^2}{n\varepsilon^{2s + d}}M^2,
\end{equation*}
which is the desired result.

\subsection{Approximation error of non-local Laplacian}
\label{subsec:approximation_error_nonlocal_laplacian}

In this section, we establish the convergence $L_{P,\varepsilon}^kf \to \sigma_{\eta}^k\Delta_P^kf$ as $\varepsilon \to 0$. More precisely, we give an upper bound on the squared difference between $L_{P,\varepsilon}^kf$ and  $\sigma_{\eta}^k\Delta_P^kf$ as a function of $\varepsilon$. The bound holds for all $x \in \mc{X}_{k\varepsilon}$, and $f \in H^{s}(\mc{X})$, as long as $s \geq 2k + 1$.  \textcolor{red}{(TODO): Make sure that $\sigma_{\eta}$ is defined somewhere.}
\begin{lemma}
	\label{lem:approximation_error_nonlocal_laplacian}
	Assume Model~\ref{def:model_flat_euclidean}. Let $s \in \mathbb{N} \setminus \{0,1\}$, suppose that $f \in H^s(\mc{X};M)$, and if $s > 1$ suppose that $p \in C^{s - 1}(\mc{X})$. Let $L_{P,\varepsilon}$ be define with respect to a kernel $\eta$ that satisfies~\ref{asmp:kernel_flat_euclidean}. Then there exist constants $C_1$ and $C_2$ that do not depend on $f$, such that each of the following statements hold.
	\begin{itemize}
		\item If $s$ is odd and $k = (s - 1)/2$, then
		\begin{equation}
		\label{eqn:approximation_error_nonlocal_laplacian_1}
		\|L_{P,\varepsilon}^kf - \Delta_P^kf\|_{L^2(\mc{X}_{k\varepsilon})} \leq C_1 M \varepsilon
		\end{equation}
		\item If $s$ is even and $k = (s - 2)/2$, then
		\begin{equation}
		\label{eqn:approximation_error_nonlocal_laplacian_2}
		\|L_{P,\varepsilon}^kf - \Delta_P^kf\|_{L^2(\mc{X}_{k\varepsilon})} \leq C_2 M \varepsilon^2.
		\end{equation}
	\end{itemize}
\end{lemma}
We remark that when $k = 1$ and $f \in C^3(\mc{X})$ or $C^4(\mc{X})$, statements of this kind are well known \textcolor{red}{(references)}, and indeed stronger results---with $L^{\infty}(\mc{X})$ norm replacing $L^2(\mc{X})$ norm---hold. When dealing with the iterated Laplacian, and functions $f$ which are regular only in the Sobolev sense, the proof is somewhat more lengthy, but the spirit of the result is largely the same.
 
\paragraph{Proof (of Lemma~\ref{lem:approximation_error_nonlocal_laplacian}).}
Throughout this proof, we shall assume that $f$ and $p$ are smooth functions, meaning they belong to $C^{\infty}(\mc{X})$. This is without loss of generality, since $C^{\infty}(\mc{X})$ is dense in both $H^s(\mc{X})$ and $C^{s - 1}(\mc{X})$, and since both sides of the inequalities~\eqref{eqn:approximation_error_nonlocal_laplacian_1} and~\eqref{eqn:approximation_error_nonlocal_laplacian_2} are continuous with respect to $\|\cdot\|_{H^s(\mc{X})}$ and $\|\cdot\|_{C^{s - 1}(\mc{X})}$ norms.

We will actually prove a more general set of statements than contained in Lemma~\ref{lem:approximation_error_nonlocal_laplacian}, more general in the sense that they give estimates for all $k$, rather than simply the particular choices of $k$ given above. In particular, we will prove that the following two statements hold for any $s \in \mathbb{N}$ and any $k \in \mathbb{N} \setminus \{0\}$. 
\begin{itemize}
	\item If $k \geq s/2$, then for every $x \in \mc{X}_{k\varepsilon}$, 
	\begin{equation}
	\label{pf:approximation_error_nonlocal_laplacian_0}
	L_{P,\varepsilon}^kf(x) = g_s(x) \varepsilon^{s - 2k}
	\end{equation}
	for a function $g_s$ that satisfies
	\begin{equation}
	\label{pf:approximation_error_nonlocal_laplacian_0.5}
	\|g_s\|_{L^2(\mc{X}_{k\varepsilon})} \leq C \|p\|_{C^{q}(\mc{X})}^k M 
	\end{equation}
	where $q = 1$ if $s =0$ or $s = 1$, and otherwise $q = s - 1$. 
	\item If $k < s/2$, then for every $x \in \mc{X}_{k\varepsilon}$,
	\begin{equation}
	\label{pf:approximation_error_nonlocal_laplacian_1}
	L_{P,\varepsilon}^kf(x) = \sigma_{\eta}^k \cdot \Delta_{P}^kf(x) + \sum_{j = 1}^{\floor{(s - 1)/2} - k} g_{2(j + k)}(x)\varepsilon^{2j} + g_{s}(x) \varepsilon^{s - 2k}.
	\end{equation}
	for functions $g_j$ that satisfy
	\begin{equation}
	\label{pf:approximation_error_nonlocal_laplacian_1.5}
	\|g_j\|_{H^{s - j}(\mc{X}_{k\varepsilon})} \leq C \|p\|_{C^{s - 1}(\mc{X})}^k M.
	\end{equation}
\end{itemize}
In the statement above, recall that $H^0(\mc{X}_{k\varepsilon}) = L^2(\mc{X}_{k\varepsilon})$. Additionally, note that we may speak of the pointwise behavior of derivatives of $f$ because we have assumed that $f$ is a smooth function. Observe that~\eqref{eqn:approximation_error_nonlocal_laplacian_1} follows upon taking $k = \floor{(s - 1)/2}$ in~\eqref{pf:approximation_error_nonlocal_laplacian_1}, whence we have
\begin{equation*}
\bigl(L_{P,\varepsilon}^kf(x) - \sigma_{\eta}^k \Delta_{P}^kf(x)\bigr)^2 = \varepsilon^2 \bigl(g_s(x)\bigr)^2
\end{equation*}
for some $g_s \in \Leb^2(\mc{X}_{k\varepsilon},C \cdot M \cdot \|p\|_{C^{s - 1}(\mc{X})})$, and integrating over $\mc{X}_{k\varepsilon}$ gives the desired result. \eqref{eqn:approximation_error_nonlocal_laplacian_2} follows from~\eqref{pf:approximation_error_nonlocal_laplacian_1} in an identical fashion. 

It thus remains establish~\eqref{pf:approximation_error_nonlocal_laplacian_1}, and~\eqref{pf:approximation_error_nonlocal_laplacian_0} which is an important part of proving~\eqref{pf:approximation_error_nonlocal_laplacian_1}. We will do so by induction on $k$. Note that throughout, we will let $g_j$ refer to functions which may change from line to line, but which always satisfy~\eqref{pf:approximation_error_nonlocal_laplacian_1.5}. 

\underline{\textit{Proof of~\eqref{pf:approximation_error_nonlocal_laplacian_0} and~\eqref{pf:approximation_error_nonlocal_laplacian_1}, base case.}}

We begin with the base case, where $k = 1$. Again, we point out that although desired result is known when $s = 3$ or $s = 4$, and $f$ is regular in the H\"{o}lder sense, we require estimates for all $s \in \mathbb{N}$ when $f$ is regular in the Sobolev sense.

When $s = 0$, the inequality~\eqref{pf:approximation_error_nonlocal_laplacian_0} is implied by Lemma~\ref{lem:l2estimate_nonlocal_laplacian}.  When $s \geq 1$, we proceed using Taylor expansion. For any $x \in \mc{X}_{\varepsilon}$, we have that $B(x,\varepsilon) \subseteq \mc{X}$. Thus for any $x' \in B(x,\varepsilon)$, we may take an order $s$ Taylor expansion of $f$ around $x' = x$, and an order $q$ Taylor expansion of $p$ around $x' = x$, where $q = 1$ if $s = 1$, and otherwise $q = s - 1$. (See Section~\ref{subsec:taylor_expansion} for a review of the notation we use for Taylor expansions, as well as some properties that we make use of shortly.) This allows us to express $L_{P,\varepsilon}f(x)$ as the sum of three terms,
\begin{align*}
L_{P,\varepsilon}f(x) & = \frac{1}{\varepsilon^{d + 2}}\sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{q - 1}\frac{1}{j_1!j_2!}  \int_{\mc{X}} \bigl(d_x^{j_1}f\bigr)(x' - x) \bigl(d_x^{j_2}p\bigr)(x' - x) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \quad + \\
& \quad \frac{1}{\varepsilon^{d + 2}}\sum_{j = 1}^{s - 1} \frac{1}{j!} \int_{\mc{X}} \bigl(d_x^jf\bigr)(x' - x)  r_{x'}^{q}(x;p) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \quad  + \\
& \quad \frac{1}{\varepsilon^{d + 2}} \int_{\mc{X}} r_{x'}^j(x;f) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \\
& := G_1(x) + G_2(x) + G_3(x).
\end{align*}
We now separately consider each of $G_1(x),G_2(x)$ and $G_3(x)$. We will establish that if $s = 1$ or $s = 2$, then $G_1(x) = 0$, and otherwise if $s \geq 3$ that
\begin{equation*}
G_1(x) = \sigma_{\eta}\Delta_Pf(x) + \sum_{j = 1}^{\floor{(s - 1)/2} - 1}g_{2(j + 1)}(x)\varepsilon^{2j} + g_{s}(x)\varepsilon^{s - 2}.
\end{equation*}
On the other hand, we will establish that if $s = 1$ then $G_2(x) = 0$, and otherwise for $s \geq 2$
\begin{equation*}
\|G_2\|_{\Leb^2(\mc{X}_{\varepsilon})} \leq C \varepsilon^{s - 2} M \|p\|_{C^{s - 1}(\mc{X})};
\end{equation*}
this same estimate will hold for $G_3$ for all $s \geq 1$. Together these will imply~\eqref{pf:approximation_error_nonlocal_laplacian_0} and~\eqref{pf:approximation_error_nonlocal_laplacian_1}. 

\emph{Estimate on $G_1(x)$.}
If $s = 1$, then $s - 1 = 0$, and so $G_1(x) = 0$. We may therefore suppose $s \geq 2$. In this case, a change of variables gives
\begin{align}
L_{P,\varepsilon}f(x) & = \frac{1}{\varepsilon^{d + 2}} \sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{q - 1} \frac{1}{j_1!j_2!}  \int_{\mc{X}} \bigl(d_x^{j_1}f\bigr)(x' - x) \bigl(d_x^{j_2}p\bigr)(x' - x) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \nonumber \\
& = \sum_{j_1 = 1}^{s - 1} \sum_{j_2 = 0}^{q - 1} \frac{\varepsilon^{j_1 + j_2 - 2}}{j_1!j_2!}  \underbrace{\int_{B(0,1)} d_x^{j_1}f(z) d_x^{j_2}p(z) \eta(\|z\|) \,dz}_{:= g_{j_1,j_2}(x)} \label{pf:approximation_error_nonlocal_laplacian_3}
\end{align}
The nature of $g_{j_1,j_2}(x)$ depends on the sum $j_1 + j_2$. Since $d_x^{j_1}f d_x^{j_2}$ is an order $j_1 + j_2$ (multivariate) monomial, we have (see Section~\ref{subsec:taylor_expansion}) that whenever $j_1 + j_2$ is odd,
\begin{equation*}
g_{j_1,j_2}(x) = \int_{\mc{X}} d_x^{j_1}f(z) d_x^{j_2}p(z) \eta(\|z\|) \,dz = 0.
\end{equation*}
In particular this is the case when $j_1 = 1$ and $j_2 = 0$, which means that if $s = 2$, then $G_1(x) = 0$. On the other hand if $s \geq 3$, then the lowest order term in~\eqref{pf:approximation_error_nonlocal_laplacian_3} is when $j_1 + j_2 = 2$, so that either $j_1 = 1$ and $j_2 = 1$, or $j_1 = 2$ and $j_2 = 0$. It follows that
\begin{align*}
g_{1,1}(x) + \frac{1}{2}g_{2,0}(x) & = \int_{\mc{X}} d_x^{1}f(z) d_x^{1}p(z) \eta(\|z\|) \,dz + \frac{p(x)}{2} \int_{\mc{X}} d_x^{2}f(z) \eta(\|z\|) \,dz \\
& = \sum_{i_1 = 1}^{d} \sum_{i_2 = 1}^{d} D^{e_{i_1}}f(x) D^{e_{i_2}}p(x) \int_{\mc{X}} z^{e_{i_1} + e_{i_2}} \eta(\|z\|) \,dz + \frac{p(x)}{2} \sum_{i_1 = 1}^{d} \sum_{i_2 = 1}^{d}D^{e_{i_2}+e_{i_2}}f(x)\int_{\mc{X}} z^{e_{i_1} + e_{i_2}} \eta(\|z\|) \,dz\\
& = \sum_{i = 1}^{d} D^{e_{i}}f(x) D^{e_{i}}p(x) \int_{\mc{X}} z^2 \eta(\|z\|) \,dz + \frac{p(x)}{2} \sum_{i = 1}^{d} D^{2e_{i}}f(x)\int_{\mc{X}} z^2 \eta(\|z\|) \,dz\\ 
& = \sigma_{\eta}\Delta_Pf(x).
\end{align*}
Now it remains only to deal with the higher-order terms, where $j_1 + j_2 > 2$, and where it suffices to show that each function $g_{j_1,j_2}$ satisfies~\eqref{pf:approximation_error_nonlocal_laplacian_1.5} for $j = \min\{j_1 + j_2 - 2,s - 2\}$. Now it is helpful to write $g_{j_1,j_2}$ using multi-index notation, 
\begin{align*}
g_{j_1,j_2}(x) = \sum_{|\alpha_1| = j_1} \sum_{|\alpha_2| = j_2} D^{\alpha_1}f(x) D^{\alpha_2}p(x) \int_{B(0,1)} z^{\alpha_1 + \alpha_2} \eta(\|z\|) \,dz,
\end{align*}
where we note that $|\int_{B(0,1)} z^{\alpha_1 + \alpha_2} \eta(\|z\|) \,dz| < \infty$ for all $\alpha_1, \alpha_2$, by the assumption that $\eta$ is Lipschitz on its support. Finally, by H\"{o}lder's inequality we have that
\begin{align*}
\|D^{\alpha_1}f D^{\alpha_2}p\|_{H^{s - (j + 2)}(\mc{X})} & \leq \|D^{\alpha_1}f\|_{H^{s - (j + 2)}(\mc{X})} \|D^{\alpha_2}p\|_{C^{s - (j + 2)}(\mc{X})} \\
& \leq \|D^{\alpha_1}f\|_{H^{s - j_1}(\mc{X})} \|D^{\alpha_2}p\|_{C^{s - (j_2 + 1)}(\mc{X})} \\
& \leq M \cdot \|p\|_{C^{s - 1}(\mc{X})},
\end{align*}
and summing over all $|\alpha_1| = j_1$ and $|\alpha_2| = j_2$ establishes that $g_{j_1,j_2}$ satisfies~\eqref{pf:approximation_error_nonlocal_laplacian_1.5}.

\emph{Estimate on $G_2(x)$.}
Note immediately that $G_2(x) = 0$ if $s = 1$. Otherwise if $s \geq 2$, then $q = s - 1$.Recalling that $|r_{x + z\varepsilon}^{s - 1}(x; p)| \leq C\varepsilon^{s - 1}\|p\|_{C^{s - 1}(\mc{X})}$ for any $z \in B(0,1)$, and that $d_x^jf(\cdot)$ is a $j$-homogeneous function, we have that
\begin{align}
|G_2(x)| & \leq \frac{1}{\varepsilon^{d + 2}}\sum_{j = 1}^{s - 1} \frac{1}{j!}\int_{\mc{X}} \Bigl|\bigl(d_x^{j}f\bigr)(x' - x)\Bigr| \cdot \Bigl|r_{x'}^{s - 1}(x;p)\Bigr| \cdot \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \nonumber \\
& = \frac{1}{\varepsilon^2} \sum_{j = 1}^{s - 1} \frac{1}{j!}\int_{B(0,1)} \Bigl|\bigl(d_x^{j}f\bigr)(\varepsilon z)\Bigr| \cdot |r_{x + z\varepsilon}^{s - 1}(x;p)| \cdot \eta(\|z\|) \,dz \nonumber \\
& \leq C\varepsilon^{s - 2}\|p\|_{C^{s - 1}(\mc{X})} \sum_{j = 1}^{s - 1} \frac{1}{j!} \int_{B(0,1)} \Bigl|\bigl(d_x^{j}f\bigr)(z)\Bigr| \cdot \eta(\|z\|) \,dz \label{pf:approximation_error_nonlocal_laplacian_4}.
\end{align}
Furthermore, for each $j = 1,\ldots,s - 1$ convolution of $d_x^jf$ with $\eta$ only decreases the $\Leb^2(\mc{X}_{\varepsilon})$ norm, meaning
\begin{equation}
\label{pf:approximation_error_nonlocal_laplacian_5}
\begin{aligned}
\int_{\mc{X}_{\varepsilon}} \biggl(\int_{B(0,1)} \Bigl|\bigl(d_x^{j}f\bigr)(z)\Bigr| \cdot \eta(\|z\|) \,dz\biggr)^2 \,dx & \leq \int_{\mc{X}_{\varepsilon}} \biggl(\int_{B(0,1)} \Bigl|\bigl(d_x^jf\bigr)(z)\Bigr|^2 \eta(\|z\|)\,dz \biggr) \cdot \biggl(\int_{B(0,1)} \eta(\|z\|) \,dz \biggr) \,dx \\
& \leq \int_{B(0,1)} \int_{\mc{X}_{\varepsilon}} \Bigl[\bigl(d^jf\bigr)(x)\Bigr]^2 \eta(\|z\|) \,dx  \,dz \\
& \leq \|d^jf\|_{\Leb^2(\mc{X_{\varepsilon}})}^2.
\end{aligned}
\end{equation}
In the above, we have used both that $|d_x^jf(z)| \leq |d^jf(x)|$ for all $z \in B(0,1)$, and that the kernel is normalized so that $\int \eta(\|z\|) \,dz = 1$. 
Combining this with~\eqref{pf:approximation_error_nonlocal_laplacian_4}, we conclude that
\begin{align*}
\int_{\mc{X}_{\varepsilon}} |G_2(x)|^2 \,dx & \leq C \Bigl(\varepsilon^{s - 2}\|p\|_{C^{s - 1}(\mc{X})}\Bigr)^2 \sum_{j = 1}^{s - 1} \int_{\mc{X}_{\varepsilon}}\biggl(\frac{1}{j!} \int_{B(0,1)} \Bigl|\bigl(d_x^{j}f\bigr)(z)\Bigr| \cdot \Bigl|\eta(\|z\|)\Bigr| \,dz\biggr)^2 \,dx \\
& \leq C \Bigl(\varepsilon^{s - 2}\|p\|_{C^{s - 1}(\mc{X})}\Bigr)^2 \sum_{j = 1}^{s - 1} \|d^ju\|_{\Leb^2(\mc{X_{\varepsilon}})}^2,
\end{align*}
establishing the desired estimate.

\emph{Estimate on $G_3(x)$.}
We begin with a pointwise upper bound on $|J_3(x)|$, 
\begin{align*}
|G_3(x)| & \leq \frac{p_{\max}}{\varepsilon^{2 + d}} \int_{\mc{X}} \bigl|r_{x'}^s(x;f)\bigr| \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \\
& = \frac{p_{\max}}{\varepsilon^2} \int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;f)\bigr| \eta(\|z\|) \,dz,
\end{align*}
and applying the Cauchy-Schwarz inequality, we deduce a pointwise upper bound on $|G_3(x)|^2$,
\begin{align*}
|G_3(x)|^2 & \leq \biggl(\frac{p_{\max}}{\varepsilon^2}\biggr)^2 \cdot \biggl(\int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr|^2 \eta(\|z\|)\,dz\biggr) \cdot \biggl(\int_{B(0,1)} \eta(\|z\|) \,dz\biggr) \\
& \leq \biggl(\frac{p_{\max}}{\varepsilon^2}\biggr)^2 \int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;u)\bigr|^2 \eta(\|z\|) \,dz.
\end{align*}
Applying this pointwise over all $x \in \mc{X}_{\varepsilon}$ and integrating, we obtain
\begin{align*}
\int_{\mc{X}_{\varepsilon}} |G_3(x)|^2 \,dx & \leq \biggl(\frac{p_{\max}}{\varepsilon^2}\biggr)^2 \int_{\mc{X}_{\varepsilon}} \int_{B(0,1)} \bigl|r_{x + \varepsilon z}^s(x;f)\bigr|^2 \eta(\|z\|) \,dz \,dx \\
& = \biggl(\frac{p_{\max}}{\varepsilon^2}\biggr)^2 \int_{B(0,1)} \int_{\mc{X}_{\varepsilon}} \bigl|r_{x + \varepsilon z}^s(x;f)\bigr|^2 \eta(\|z\|) \,dx \,dz \\
& \leq \biggl(\frac{p_{\max}\varepsilon^s}{\varepsilon^2}\biggr)^2  \|d^sf\|_{L^2(\mc{X}_{\varepsilon})}^2,
\end{align*}
with the last inequality following from~\eqref{eqn:sobolev_remainder_term}. Noting that $p_{\max} = \|p\|_{C^0(\mc{X})} \leq \|p\|_{C^{s - 1}(\mc{X})}$, we see that this is a sufficient bound on $\|G_3\|_{\Leb^2(\mc{X}_{\varepsilon})}$.

\underline{\textit{Proof of~\eqref{pf:approximation_error_nonlocal_laplacian_0} and~\eqref{pf:approximation_error_nonlocal_laplacian_1}, induction step.}}
We now assume that~\eqref{pf:approximation_error_nonlocal_laplacian_0} and~\eqref{pf:approximation_error_nonlocal_laplacian_1} hold for all order up to some $k$, and show that they then hold for order $k + 1$ as well. The proof is relatively straightforward, once we introduce a bit of notation. Namely, for any $\ell,j \in \mathbb{N}$ such that $1 \leq j \leq \ell \leq$, we will use $g_j^{\ell}$ to refer to a function satisfying
\begin{equation}
\label{pf:approximation_error_nonlocal_laplacian_6}
\|g_j^{\ell}\|_{H^{\ell - j}(\mc{X}_{(k + 1)\varepsilon})} \leq C \|p\|_{C^{q}(\mc{X})}^{k + 1} M.
\end{equation}
Note that $g_j^{\ell}(x) = g_{(s - \ell) + j}(x)$, so that $g_j^{s}(x) = g_j(x)$. As before, the functions $g_j^{\ell}$ may change from line to line, but will always satisfy~\eqref{pf:approximation_error_nonlocal_laplacian_6}. We immediately illustrate the purpose of this notation. Suppose $g \in H^{\ell}(\mc{X}_{k\varepsilon}; C \|p\|_{C^{q}(\mc{X})}^k M)$ for some $\ell \leq s$. If $\ell \leq 2$, then by the inductive hypothesis, it follows that for any $x \in \mc{X}_{(k + 1)\varepsilon}$
\begin{equation}
\label{pf:approximation_error_nonlocal_laplacian_7}
L_{P,\varepsilon}g(x) = g_{\ell}^{\ell}(x) \varepsilon^{\ell - 2}.
\end{equation} 
On the other hand if $2 < \ell \leq s$, then by the inductive hypothesis, it follows that for any $x \in \mc{X}_{(k + 1)\varepsilon}$,
\begin{equation}
\label{pf:approximation_error_nonlocal_laplacian_8}
L_{P,\varepsilon}g(x) = \sigma_{\eta} \Delta_Pg(x) + \sum_{j = 1}^{\floor{(\ell - 1)/2} - 1} g_{2j + 2}^{\ell}(x) \varepsilon^{2j} + g_{\ell}^{\ell}(x) \varepsilon^{\ell - 2}.
\end{equation}

\emph{Proof of \eqref{pf:approximation_error_nonlocal_laplacian_0}.} If $s \leq 2(k + 1)$, then by the inductive hypothesis it follows that for all $x \in \mc{X}_{k\varepsilon}$, we have $L_{P,\varepsilon}^kf(x) = g_{s}(x) \cdot \varepsilon^{s - 2k}$, for some $g_s \in L^2(\mc{X}_{k\varepsilon}, C\|p\|_{C^{s - 1}(\mc{X})}^k M)$. Note that we may know more about $L_P^kf(x)$ than simply that it is bounded in $L^2$-norm, but a bound in $L^2$-norm suffices. In particular, from such a bound along with~\eqref{pf:approximation_error_nonlocal_laplacian_7} we deduce that for any $x \in \mc{X}_{(k + 1)\varepsilon}$,
\begin{equation}
\label{pf:approximation_error_nonlocal_laplacian_8.5}
L_{P,\varepsilon}^{k + 1}f(x) = \bigl(L_{P,\varepsilon} \circ L_{P,\varepsilon}^k f)(x)= L_{P,\varepsilon} g_s(x)\varepsilon^{s - 2k} = g_{s}^{s}(x) \varepsilon^{s - 2(k + 1)},
\end{equation}
establishing~\eqref{pf:approximation_error_nonlocal_laplacian_0}. 

\emph{Proof of \eqref{pf:approximation_error_nonlocal_laplacian_1}.} If $s > 2(k + 1)$, then by the inductive hypothesis we have that for all $x \in \mc{X}_{k\varepsilon}$, 
\begin{equation*}
L_{P,\varepsilon}^kf(x) = \sigma_{\eta}^k \Delta_P^kf(x) + \sum_{j = 1}^{\floor{(s - 1)/2} - k} g_{2(j + k)}(x) \varepsilon^{2j} + g_s(x) \varepsilon^{s - 2k}.
\end{equation*}
Thus for any $x \in \mc{X}_{(k + 1)\varepsilon}$, 
\begin{equation*}
L_{P,\varepsilon}^{k + 1}f(x) = \bigl(L_{P,\varepsilon} \circ L_{P,\varepsilon}^k f\bigr)(x) = \sigma_{\eta}^k L_{P,\varepsilon}\Delta_P^kf(x) + \sum_{j = 1}^{\floor{(s - 1)/2} - k} L_{P,\varepsilon}g_{2(j + k)}(x) \varepsilon^{2j} + L_{P,\varepsilon}g_s(x) \varepsilon^{s - 2k}
\end{equation*}
There are three terms on the right hand side of this equality, and we now analyze each separately.
\begin{enumerate}
	\item Noting that $\Delta_P^kf \in H^{s - 2k}(\mc{X}; C\|p\|_{C^{s - 1}(\mc{X})}^kM)$, we use~\eqref{pf:approximation_error_nonlocal_laplacian_8} to derive that
	\begin{align}
	L_{P,\varepsilon}\Delta_P^kf(x) & = \sigma_{\eta} \Delta_P^{k + 1}f(x) + \sum_{j = 1}^{(s - 2k - 1)/2 - } g_{2j + 2}^{s - 2k}(x)\varepsilon^{2j} + g_{s - 2k}^{s - 2k}(x) \varepsilon^{s - 2k - 2} \nonumber \\
	& = \sigma_{\eta} \Delta_P^{k + 1}f(x) + \sum_{j = 1}^{(s - 1)/2 - (k + 1)} g_{2(k + 1 + j)}(x)\varepsilon^{2j} + g_{s}(x) \varepsilon^{s - 2(k + 1)}, \label{pf:approximation_error_nonlocal_laplacian_9}
	\end{align}
	where in the second equality we have simply used the fact $g_j^{\ell}(x) = g_{(s - \ell) + j}(x)$ to rewrite the equation.
	\item Suppose $j < \floor{(s - 1)/2} - k$. Then we use~\eqref{pf:approximation_error_nonlocal_laplacian_8} to derive that
	\begin{align*}
	L_{P,\varepsilon}g_{2(j + k)}(x) & = \sigma_{\eta}\Delta_P g_{2(j + k)}(x) + \sum_{i = 1}^{\floor{(s - 2j - 2k - 1)/2} - 1} g_{2(i + 1)}^{s - 2(j + k)}(x)\varepsilon^{2i} + g_{s - 2(j + k)}^{s - 2(j + k)}(x) \varepsilon^{s - 2(j + k + 1)} \\
	& = g_{2(j + k + 1)}(x) + \sum_{i = 1}^{\floor{(s - 1)/2} - (j + k + 1)} g_{2(i + j + k + 1)}(x)\varepsilon^{2i} + g_{s}(x) \varepsilon^{s - 2(j + k + 1)},
	\end{align*}
	where in the second equality we have again used $g_j^{\ell}(x) = g_{(s - \ell) + j}(x)$, and also written $\sigma_{\eta} \Delta_Pf = g_{2}^{s - 2(j + k)} = g_{2(j + k + 1)}$, since the particular dependence on the Laplacian $\Delta_P$ will not matter. From here, multiplying by $\varepsilon^{2j}$, we conclude that
	\begin{align}
	\varepsilon^{2j} L_{P,\varepsilon}g_{2(j + k)}(x) & = g_{2(j + k + 1)}(x) \varepsilon^{2j} + \sum_{i = 1}^{\floor{(s - 1)/2} - (j + k + 1)} g_{2(i + j + k + 1)}(x)\varepsilon^{2(i + j)} + g_{s}(x) \varepsilon^{s - 2(k + 1)} \nonumber \\ 
	& = g_{2(j + k + 1)}(x) \varepsilon^{2j} + \sum_{m = 1}^{\floor{(s - 1)/2} - (k + 1)}  g_{2(m + k + 1)}(x)\varepsilon^{2m} + g_{s}(x) \varepsilon^{s - 2(k + 1)} \label{pf:approximation_error_nonlocal_laplacian_10},
	\end{align}
	with the second equality following upon changing variables to $m = i + j$. 
	
	On the other hand if $j = \floor{(s - 1)/2} - k$, then the calculation is much simpler,
	\begin{equation}
	\label{pf:approximation_error_nonlocal_laplacian_11}
	\varepsilon^{2j} L_{P,\varepsilon}g_{2(j + k)}(x) = g_{s - 2(j + k)}^{s - 2(j + k)}(x) \varepsilon^{2j} \varepsilon^{s - 2(j + k) - 2} = g_s(x) \varepsilon^{s - 2(k + 1)}.
	\end{equation}
	\item Finally, it follows immediately from~\eqref{pf:approximation_error_nonlocal_laplacian_8} that
	\begin{equation}
	\label{pf:approximation_error_nonlocal_laplacian_12}
	L_{P,\varepsilon}g_s(x) \varepsilon^{s - 2k} = g_{s}(x) \varepsilon^{s - 2(k + 1)}.
	\end{equation}
\end{enumerate}
Plugging~\eqref{pf:approximation_error_nonlocal_laplacian_9}-\eqref{pf:approximation_error_nonlocal_laplacian_12} back into~\eqref{pf:approximation_error_nonlocal_laplacian_8.5} proves the claim.

\subsection{Boundary behavior of non-local Laplacian}
\label{subsec:boundary_behavior_nonlocal_laplacian}

In Lemma~\ref{lem:approximation_error_nonlocal_laplacian_boundary}, we establish that if $f$ is Sobolev smooth of order $s > 2k$ and zero-trace, then near the boundary of $\mc{X}$ the non-local Laplacian $L_{P,\varepsilon}^kf$ is close to $0$ in the $\Leb^2$-sense.
\begin{lemma}
	\label{lem:approximation_error_nonlocal_laplacian_boundary}
	Assume Model~\ref{def:model_flat_euclidean}. Let $s,k \in \mathbb{N}$. Suppose that $f \in H_0^{s}(\mc{X};M)$. Then there exist numbers $c,C > 0$ that do not depend on $M$, such that for all $\varepsilon < c$, 
	\begin{equation*}
	\|L_{P,\varepsilon}^kf\|_{L^2(\partial_{k\varepsilon}\mc{X})}^2 \leq C \varepsilon^{2(s - 2k)}M^2.
	\end{equation*}
\end{lemma}

\paragraph{Proof (of Lemma~\ref{lem:approximation_error_nonlocal_laplacian_boundary})}
Applying Lemma~\ref{lem:l2estimate_nonlocal_laplacian}, we have that
\begin{equation*}
\|L_{P,\varepsilon}^kf\|_{L^2(\partial_{k\varepsilon}(\mc{X}))}^2 \leq \frac{(Cp_{\max})^{2}}{\varepsilon^4} \|L_{P,\varepsilon}^{k - 1}f\|_{L^2(\partial_{k\varepsilon}(\mc{X}))}^2 \leq \cdots \leq \frac{(Cp_{\max})^{2}}{\varepsilon^{4k}} \|f\|_{L^2(\partial_{k\varepsilon}(\mc{X}))}^2   
\end{equation*}
Thus it remains to show that for all $\varepsilon < c$,
\begin{equation}
\label{pf:approximation_error_nonlocal_laplacian_boundary_0}
\|f\|_{L^2(\partial_{k\varepsilon}(\mc{X}))}^2 = \int_{\partial_{k\varepsilon}(\mc{X})} \bigl(f(x)\bigr)^2 \,dx \leq C_1 \varepsilon^{2s} \|f\|_{H^s(\mc{X})}^2.
\end{equation}
We will build to~\eqref{pf:approximation_error_nonlocal_laplacian_boundary_0} by a series of intermediate steps, following the same rough structure as the proof of Theorem 18.1 in \citet{leoni2017}. For simplicity, we will take $k = 1$; the exact same proof applies to the general case upon assuming $\varepsilon < c/k$.

\underline{\textit{Step 1: Local Patch.}}
To begin, we assume that for some $c_0 > 0$ and a Lipschitz mapping $\phi: \Reals^{d - 1} \to [-c_0,c_0]$, we have that $f \in C_c^{\infty}(U_{\phi}(c_0))$, where 
\begin{equation*}
U_{\phi}(c_0) = \Bigl\{y \in Q(0,c_0): \phi(y_{-d}) \leq y_d\Bigr\}, 
\end{equation*}
and here $Q(0,c_0)$ is the $d$-dimensional cube of side length $c_0$, centered at $0$. We will show that for all $0 < \varepsilon < c_0$, and for the tubular neighborhood $V_{\phi}(\varepsilon) = \{y \in Q(0,c_0): \phi(y_{-d}) \leq y_d \leq \phi(y_{-d}) + \varepsilon\}$, we have that
\begin{equation*}
\int_{V_{\phi}(\varepsilon)} |f(x)|^2 \,dx \leq C\varepsilon^{2s} \|f\|_{H^s(U_{\phi}(c_0))}^2.
\end{equation*}
For a given $y = (y',y_d) \in V_{\phi}(\varepsilon)$, let $y_0 = (y',\phi(y'))$. Taking the Taylor expansion of $f(y)$ around $y = y_0$ because $u$ is compactly supported in $V_{\phi}$ it follows that,
\begin{align*}
f(y) & = f(y_0) + \sum_{j = 1}^{s - 1} \frac{1}{j!} D^{je_d}f(y_0) \bigl(y_d - \phi(y')\bigr)^j + \frac{1}{(s - 1)!}\int_{\phi(y')}^{y_d} (1 - t)^{s - 1} D^{se_d}f(y',z) \bigl(y_d - z\bigr)^{s - 1} \,dz \Longrightarrow\\
|f(y)| & \leq C\varepsilon^{s - 1}\int_{\phi(y')}^{y_d} \bigl|D^{se_d}f(y',z)\bigr| \,dz. 
\end{align*}
Consequently, by squaring both sides and applying Cauchy-Schwarz, we have that
\begin{equation*}
|f(y)|^2 \leq C\varepsilon^{2(s - 1)} \biggl(\int_{\phi(y')}^{y_d} \bigl|D^{se_d}f(y',z)\bigr| \,dz\biggr)^2 \leq C\varepsilon^{2s - 1} \int_{\phi(y')}^{y_d} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dz.
\end{equation*}
Applying this bound for each $y \in V_{\phi}(\varepsilon)$, and then integrating, we obtain
\begin{align}
\int_{V_{\phi}(\varepsilon)} |f(y)|^2 \,dy & \leq \int_{Q_{d - 1}(c_0)} \int_{\phi(y')}^{\phi(y') + \varepsilon} |f(y',y_d)|^2 \,dy_d \,dy' \nonumber \\
& \leq C\varepsilon^{2s - 1}\int_{Q_{d - 1}(c_0)}  \int_{\phi(y')}^{\phi(y') + \varepsilon} \int_{\phi(y')}^{y_d} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dz \,dy_d \,dy' \label{pf:approximation_error_nonlocal_laplacian_boundary_1}
\end{align}
where we have written $Q_{d - 1}(0,c_0)$ for the $d - 1$ dimensional cube of side length $c_0$, centered at $0$. Exchanging the order of the inner two integrals then gives
\begin{align*}
\int_{\phi(y')}^{\phi(y') + \varepsilon} \int_{\phi(y')}^{y_d} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dz \,dy_d & = \int_{\phi(y')}^{\phi(y') + \varepsilon} \int_{z}^{\varepsilon} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dy_d \,dz \\
& \leq C \varepsilon \int_{\phi(y')}^{\phi(y') + \varepsilon} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dz \\
& \leq C \varepsilon \int_{\phi(y')}^{c_0} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dz.
\end{align*}
Finally, plugging back into~\eqref{pf:approximation_error_nonlocal_laplacian_boundary_1}, we conclude that
\begin{equation*}
\int_{V_{\phi}(\varepsilon)} |f(y)|^2 \,dy \leq C \varepsilon^{2s} \int_{Q_{d - 1}(0,c_0)} \int_{\phi(y')}^{c_0} \bigl|D^{se_d}f(y',z)\bigr|^2 \,dz \,dy' \leq C \varepsilon^{2s} |u|_{H^s(U_{\phi}(c_0))}^2.
\end{equation*}

\underline{\textit{Step 2: Rigid motion of local patch.}} Now, suppose that at a point $x_0 \in \partial \mc{X}$, there exists a rigid motion $T: \Rd \to \Rd$ for which $T(x_0) = 0$, and a number $C_0$ such that for all $\varepsilon \cdot C_0 \leq c_0$, 
\begin{equation*}
T\bigl(Q_{T}(x_0,c_0) \cap \partial_{\varepsilon}\mc{X}\bigr) \subseteq V_{\phi}\bigl(C_0\varepsilon\bigr) \quad\textrm{and}\quad T\bigl(Q_T(x_0,c_0) \cap \mc{X}\bigr) = U_{\phi}(c_0).
\end{equation*}
Here $Q_{T}(x_0,c_0))$ is a (not necessarily coordinate-axis-aligned) cube of side length $c_0)$, centered at $x_0$. Define $v(y) := f(T^{-1}(y))$ for $y \in U_{\phi}(c_0)$. If $u \in C_c^{\infty}(\mc{X})$, then $v \in C_c^{\infty}(U_{\phi}(c_0))$, and moreover $\|v\|_{H^s(U_{\phi}(c_0))}^2 = \|f\|_{H^s(Q_{T}(x_0,c_0) \cap \mc{X})}^2$. Therefore, using the upper bound that we derived in Step 1,
\begin{equation*}
\int_{V_{\phi}(C_0 \cdot \varepsilon)} |v(y)|^2 \,dy \leq C \varepsilon^{2s} \|v\|_{H^s(U_{\phi}(c_0))}^2,
\end{equation*}
we conclude that
\begin{align*}
\int_{Q_{T}(x_0,c_0) \cap \partial_{\varepsilon}\mc{X}} |f(x)|^2 \,dx & = \int_{T(Q_T(x_0,c_0)) \cap \partial_{\varepsilon}\mc{X})} |v(y)|^2 \,dy \\
& \leq \int_{V_{\phi}(C_0 \cdot \varepsilon)} |v(y)|^2 \,dy \\
& \leq C \varepsilon^{2s} \|v\|_{H^s(U_{\phi}(c_0))}^2 = C \varepsilon^{2s} \|f\|_{H^s(Q_{T}(x_0,c_0)) \cap \mc{X})}^2 \leq C \varepsilon^{2s} \|f\|_{H^s(\mc{X})}^2.
\end{align*}

\underline{\textit{Step 3: Lipschitz domain}}.
Finally, we deal with the case where $\mc{X}$ is assumed to be an open, bounded subset of $\Rd$, with Lipschitz boundary. In this case, at every $x_0 \in \partial \mc{X}$, there exists a rigid motion $T_{x_0}: \Rd \to \Rd$ such that $T_{x_0}(x_0) = 0$, a number $c_0(x_0)$, a Lipschitz function $\phi_{x_0}:\Reals^{d - 1} \to [-c_0,c_0]$, and a number $C_0(x_0)$, such that for all $\varepsilon \cdot C_0(x_0) \leq c_0(x_0)$,
\begin{equation*}
T\bigl(Q_{T}(x_0,c_0(x_0)) \cap \partial_{\varepsilon}\mc{X}\bigr) \subseteq V_{\phi}\bigl(C_0(x_0) \cdot \varepsilon\bigr) \quad\textrm{and}\quad T\bigl(Q_T(x_0,c_0(x_0)) \cap \mc{X}\bigr) = U_{\phi}(c_0(x_0)).
\end{equation*}
Therefore for every $x_0 \in \partial \mc{X}$, it follows from the previous step that
\begin{equation*}
\int_{Q_{T_{x_0}}(x_0,c_0(x_0)) \cap \partial_{\varepsilon}\mc{X}} |f(x)|^2 \,dx \leq C(x_0) \varepsilon^{2s} \|f\|_{H^s(\mc{X})}^2,
\end{equation*}
where on the right hand side $C(x_0)$ is a constant that may depend on $x_0$, but not on $u$ or $\varepsilon$.

We conclude by taking a collection of cubes that covers $\partial_{\varepsilon}\mc{X}$ for all $\epsilon$ sufficiently small. First, we note that by a compactness argument there exists a finite subset of the collection of cubes $\{Q_{T_{x_0}}(x_0,c_0(x_0)/2): x_0 \in \partial\mc{X} \}$ which covers $\partial \mc{X}$, say $Q_{T_{x_1}}(x_1,c_0(x_1)/2),\ldots, Q_{T_{x_N}}(x_N,c_0(x_N)/2)$. Then, for any $\varepsilon \leq \min_{i = 1,\ldots,N} c_0(x_i)/2$, it follows from the triangle inequality that
\begin{equation*}
\partial_{\varepsilon}\mc{X} \subseteq \bigcup_{i = 1}^{N} Q_{T_{x_i}}(x_i, c_0(x_i)).
\end{equation*}
As a result,
\begin{equation*}
\int_{\partial_{\varepsilon}\mc{X}} |f(x)|^2 \leq \sum_{i = 1}^{N} \int_{Q_{T_{x_i}}(x_i, c_0(x_i)) \cap \partial_{\varepsilon}(\mc{X})} |f(x)|^2 \leq  \varepsilon^{2s} \|f\|_{H^s(\mc{X})}^2 \sum_{i = 1}^{N}C_0(x_i),
\end{equation*}
which proves the claim of~\eqref{pf:approximation_error_nonlocal_laplacian_boundary_0}.

\subsection{Estimate of non-local Sobolev seminorm}
\label{subsec:estimate_nonlocal_seminorm}

Now, we use the results of the preceding two sections to prove~\eqref{pf:graph_seminorm_ho_2}. We will divide our analysis in two cases, depending on whether $s$ is odd or even, but before we do this we state some facts that will be applicable to both cases. First, we  recall that $L_{P,\varepsilon}$ is self-adjoint in $L^2(P)$, meaning $\dotp{L_{P,\varepsilon}f}{g}_{P} = \dotp{f}{L_{P,\varepsilon}g}_{P}$ for all $f, g \in L^2(P)$. We will also use the fact that when $s = 1$, the non-local seminorm is a \emph{non-local Dirichlet energy},
\begin{equation*}
\dotp{L_{P,\varepsilon}f}{f}_{P} = \frac{1}{\varepsilon^{d + 2}}\int_{\mc{X}} \int_{\mc{X}} \bigl(f(x) - f(x')\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \,dP(x) =: E_{P,\varepsilon}(f;\mc{X}).
\end{equation*}
Finally, we recall a result of \textcolor{blue}{(Green 21)}: there exist constants $c_0$ and $C_0$ which do not depend on $M$, such that for all $\varepsilon < c_0$ and for any $f \in H^1(\mc{X};M)$,
\begin{equation}
\label{pf:estimate_nonlocal_seminorm_1}
E_{P,\varepsilon}(f;\mc{X}) \leq C_0 M^2.
\end{equation}
\paragraph{Case 1: $s$ odd.}
Suppose $s$ is odd, so that $s \geq 3$. Taking $k = (s - 1)/2$, we use the self-adjointness of $L_{P,\varepsilon}$ to relate the non-local semi-norm $\dotp{L_{P,\varepsilon}^sf}{f}_{P}$ to a non-local Dirichlet energy,
\begin{equation*}
\dotp{L_{P,\varepsilon}^sf}{f}_P = \dotp{L_{P,\varepsilon}^{k + 1}f}{L_{P,\varepsilon}^{k}f}_P = E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\mc{X}).
\end{equation*}
We now separate this energy into integrals over $\mc{X}_{k\varepsilon}$ and $\partial_{k\varepsilon}(\mc{X})$,
\begin{align}
E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\mc{X}) & = \frac{1}{\varepsilon^{d + 2}}\Biggl\{\int_{\mc{X}_{k\varepsilon}} \int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}^kf(x) - L_{P,\varepsilon}^kf(x')\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \,dP(x) \nonumber \\
& \quad + \int_{\partial_{k\varepsilon}\mc{X}} \int_{\partial_{k\varepsilon}\mc{X}} \bigl(L_{P,\varepsilon}^kf(x) - L_{P,\varepsilon}^kf(x')\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \,dP(x)\Biggr\} \nonumber \\
& := E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\mc{X}_{k\varepsilon}) + E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\partial_{k\varepsilon}\mc{X}) \label{pf:estimate_nonlocal_seminorm_1.5}
\end{align}
and upper bound each energy separately. For the first term, we add and substract $\sigma_{\eta}^k\Delta_P^kf(x)$ and $\sigma_{\eta}^k\Delta_P^kf(x')$ within the integrand, then use the triangle inequality and \textcolor{red}{the symmetry between $x$ and $x'$} to deduce that
\begin{equation}
\label{pf:estimate_nonlocal_seminorm_2}
E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\mc{X}_{k\varepsilon}) \leq 3 \sigma_{\eta}^{2k} E_{P,\varepsilon}(\Delta_P^kf;\mc{X}_{k\varepsilon}) + \frac{2}{\varepsilon^{d + 2}}\int_{\mc{X}_{k\varepsilon}} \int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}^kf(x) - \sigma_{\eta}^k \Delta_P^kf(x)\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \,dP(x).
\end{equation}
Noticing that $\Delta_P^kf \in H^1(\mc{X};\|p\|_{C^{s - 1}(\mc{X})}^kM)$, we use~\eqref{pf:estimate_nonlocal_seminorm_1} to conclude that $E_{P,\varepsilon}(\Delta_P^kf;\mc{X}_{k\varepsilon}) \leq C_0M^2$. On the other hand, it follows from Assumption~\ref{asmp:kernel_flat_euclidean} and~\eqref{eqn:approximation_error_nonlocal_laplacian_1} that
\begin{align*}
\frac{2}{\varepsilon^{d + 2}}\int_{\mc{X}_{k\varepsilon}} \int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}^kf(x) - \sigma_{\eta}^k \Delta_P^kf(x)\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \,dP(x) & \leq \frac{2p_{\max}}{\varepsilon^{2}}\int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}^kf(x) - \sigma_{\eta}^k \Delta_P^kf(x)\bigr)^2 \,dP(x) \\
& \leq C_1M^2.
\end{align*}
Plugging these two bounds into~\eqref{pf:estimate_nonlocal_seminorm_2} gives the desired upper bound on $E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\mc{X}_{k\varepsilon})$. 

For the second term in~\eqref{pf:estimate_nonlocal_seminorm_1.5}, we apply Lemmas~\ref{lem:dirichlet_estimate_nonlocal_laplacian} and~\ref{lem:approximation_error_nonlocal_laplacian_boundary} and conclude that,
\begin{equation*}
E_{P,\varepsilon}(L_{P,\varepsilon}^{k};\partial_{k\varepsilon}\mc{X}) \leq \frac{4p_{\max}^2}{\varepsilon^2}\|L_{P,\varepsilon}^kf\|_{L^2(\partial_{k\varepsilon}\mc{X})} \leq C M^2.
\end{equation*}
\paragraph{Case 2: $s$ even.}
If $s \in \mathbb{N}$ is even, $s \geq 2$, then letting $k = (s - 2)/2$, the  self-adjointness of $L_{P,\varepsilon}$ implies
\begin{equation*}
\dotp{L_{P,\varepsilon}^sf}{f}_P = \|L_{P,\varepsilon}^{k + 1}f\|_P^2.
\end{equation*}
As in the first case, we divide the integral up into the interior region $\mc{X}_{k\varepsilon}$ and the boundary region $\partial_{k\varepsilon}\mc{X}$,
\begin{equation}
\label{pf:estimate_nonlocal_seminorm_3}
\|L_{P,\varepsilon}^{k + 1}f\|_P^2 \leq p_{\max} \|L_{P,\varepsilon}^{k + 1}f\|_{\Leb^2(\mc{X})}^2 \leq p_{\max}\biggl\{\int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}^{k + 1}f(x)\bigr)^2 \,dP(x) + \int_{\partial_{k\varepsilon}\mc{X}} \bigl(L_{P,\varepsilon}^{k + 1}f(x)\bigr)^2 \,dP(x)\biggr\},
\end{equation}
and upper bound each term separately. For the first term, adding and subtracting $\sigma_{\eta}^k \Delta_P^kf(x)$ gives
\begin{align*}
\int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}^{k + 1}f(x)\bigr)^2 \,dP(x) & \leq 2\int_{\mc{X}_{k\varepsilon}} \bigl(L_{P,\varepsilon}\Delta_P^kf(x)\bigr)^2 \,dP(x)  + 2 \int_{\mc{X}_{k\varepsilon}} \Bigl(L_{P,\varepsilon}\bigl(L_{P,\varepsilon}^kf- \sigma_{\eta}\Delta_P^kf\bigr)(x)\Bigr)^2 \,dP(x) \\
& \overset{(i)}{\leq} CM^2  + 2 \int_{\mc{X}_{k\varepsilon}} \Bigl(L_{P,\varepsilon}\bigl(L_{P,\varepsilon}^kf- \sigma_{\eta}\Delta_P^kf\bigr)(x)\Bigr)^2 \,dP(x) \\
& \overset{(ii)}{\leq} CM^2  + \frac{Cp_{\max}^2}{\varepsilon^{2}} \|L_{P,\varepsilon}^kf- \sigma_{\eta}\Delta_P^kf\|_{L^2(\mc{X}_{k\varepsilon})}^2 \\
& \overset{(iii)}{\leq} CM^2,
\end{align*}
with $(i)$ following from~\eqref{pf:approximation_error_nonlocal_laplacian_0} since $\Delta_P^kf \in H^2(\mc{X};M\|p\|_{C^{s - 1}(\mc{X})}^l)$, $(ii)$ following from Lemma~\ref{lem:l2estimate_nonlocal_laplacian}, and $(iii)$ following from~\eqref{eqn:approximation_error_nonlocal_laplacian_2}.

Then Lemma~\ref{lem:approximation_error_nonlocal_laplacian_boundary} shows that the second term in~\eqref{pf:estimate_nonlocal_seminorm_3} satisfies
\begin{equation*}
\int_{\partial_{k\varepsilon}\mc{X}} \bigl(L_{P,\varepsilon}^{k + 1}f(x)\bigr)^2 \,dP(x) \leq CM^2.
\end{equation*}

\subsection{Assorted integrals}
\label{subsec:integrals}

\textcolor{red}{(TODO): The language ``...for a Borel set $U \subseteq \mc{X}$...'' should be checked.}

\begin{lemma}
	\label{lem:l2estimate_nonlocal_laplacian}
	Assume Model~\ref{def:model_flat_euclidean}. Suppose $f \in L^2(U;M)$ for a Borel set $U \subseteq \mc{X}$, and let $L_{P,\varepsilon}$ be defined with respect to a kernel $\eta$ that satisfies~\ref{asmp:kernel_flat_euclidean}. Then there exists a constant $C$ which does not depend on $f$ or $M$ such that
	\begin{equation}
	\label{eqn:l2estimate_nonlocal_laplacian}
	\|L_{P,\varepsilon}f\|_{\Leb^2(U)} \leq \frac{2 p_{\max}}{\varepsilon^2} \|f\|_{L^2(U)}
	\end{equation}
\end{lemma}

\begin{lemma}
	\label{lem:dirichlet_estimate_nonlocal_laplacian}
	Assume Model~\ref{def:model_flat_euclidean}. Suppose $f \in L^2(U;M)$ for a Borel set $U \subseteq \mc{X}$, and let $L_{P,\varepsilon}$ be defined with respect to a kernel $\eta$ that satisfies~\ref{asmp:kernel_flat_euclidean}. Then there exists a constant $C$ which does not depend on $f$ or $M$ such that
	\begin{equation}
	\label{eqn:dirichlet_estimate_nonlocal_laplacian}
	E_{P,\varepsilon}(f;U) \leq \frac{4 p_{\max}^2}{\varepsilon^2} \|f\|_{L^2(U)}^2
	\end{equation}
\end{lemma}

\begin{lemma}
	\label{lem:graph_seminorm_bias2}
	Assume Model~\ref{def:model_flat_euclidean}. Suppose $f \in H^1(\mc{X};M)$, and let $D_if$ be defined with respect to a kernel $\eta$ that satisfies~\ref{asmp:kernel_flat_euclidean}. Then there exists a constant $C$ which does not depend on $f$ or $M$, such that for any $i \in [n]$ and $\bj \in [n]^s$,
	\begin{equation*}
	\Ebb\bigl[|D_{\bj}f(X_i)| \cdot |f(X_i) - f(X_{\bj_1})|\bigr] \leq C \varepsilon^{2 + dk}M^2,
	\end{equation*}
	where $k + 1$ is the number of distinct indices in $i\bj$. 
\end{lemma}

\paragraph{Proof (of Lemma~\ref{lem:l2estimate_nonlocal_laplacian}).}
We fix a version of $f \in \Leb^2(U)$, so that we may speak of its pointwise values.

At a given point $x \in U$, we can upper bound $|L_{P,\varepsilon}f(x)|^2$ using the Cauchy-Schwarz inequality as follows,
\begin{align*}
|L_{P,\varepsilon}f(x)|^2 & \leq \biggl(\frac{p_{\max}}{\varepsilon^{2 + d}}\biggr)^2 \Biggl(\int_U \bigl(|f(x')| + |f(x)|\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx'\Biggr)^2 \\
& \leq \biggl(\frac{p_{\max}}{\varepsilon^{2 + d}}\biggr)^2 \Biggl(\int_U \bigl(|f(x')| + |f(x)|\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \cdot \int \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \Biggr) \\
& = \frac{p_{\max}^2}{\varepsilon^{4 + d}} \int_U \bigl(|f(x')| + |f(x)|\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx'.
\end{align*}
The equality follows by the assumption $\int_{\Rd} \eta(\|z\|)\,dx = 1$ in~\ref{asmp:kernel_flat_euclidean}. Integrating over all $x \in U$, it follows from the triangle inequality that
\begin{align}
\|L_{P,\varepsilon}\|_{\Leb^2(U)}^2 & \leq \frac{2 p_{\max}^2}{\varepsilon^{4 + d}} \int_{U} \int_{U} \bigl(|f(x')|^2 + |f(x)|^2\bigr) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \,dx \nonumber \\
& \leq \frac{2 p_{\max}^2}{\varepsilon^{4 + d}} \int_{U} \int_{U} \bigl(|f(x')|^2 + |f(x)|^2\bigr) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \,dx. \label{pf:l2estimate_nonlocal_laplacian_1}
\end{align}
Finally, using Fubini's Theorem we determine that
\begin{equation}
\int_{U} \int_{U} \bigl(|f(x')|^2 + |f(x)|^2\bigr) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \,dx = 2 \int_{U} \int_{U} |f(x)|^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx \leq 2 \varepsilon^d \int_{U} |f(x)|^2 \,dx = 2\varepsilon^d \|f\|_{\Leb^2(U)}^2,
\label{pf:l2estimate_nonlocal_laplacian_2}
\end{equation}
and by combining~\eqref{pf:l2estimate_nonlocal_laplacian_1} and~\eqref{pf:l2estimate_nonlocal_laplacian_2} we conclude that
\begin{equation*}
\|L_{P,\varepsilon}\|_{\Leb^2(U)}^2 \leq \frac{4p_{\max}^2}{\varepsilon^4} \|f\|_{\Leb^2(U)}^2. 
\end{equation*}

\paragraph{Proof (of Lemma~\ref{lem:dirichlet_estimate_nonlocal_laplacian}).}
We have
\begin{equation*}
E_{P,\varepsilon}(f) = \frac{1}{\varepsilon^{2 + d}} \int_{U} \int_{U} \bigl(f(x) - f(x')\bigr)^2 \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dP(x') \,dP(x) \leq \frac{2p_{\max}^2}{\varepsilon^{2 + d}} \int_{U} \int_{U} \bigl(|f(x)|^2 + |f(x')|^2\bigr) \eta\biggl(\frac{\|x' - x\|}{\varepsilon}\biggr) \,dx' \,dx,
\end{equation*}
and the claim follows from~\eqref{pf:l2estimate_nonlocal_laplacian_2}.

\paragraph{Proof (of Lemma~\ref{lem:graph_seminorm_bias2}).}
Let $G_{n,\varepsilon}[X_{i\bj}]$ be the subgraph induced by vertices $X_i, X_{\bj_1},\ldots,X_{\bj_s}$. We make two observations. First, in order for $|D_{\bj}f(X_i)| \cdot |f(X_i)  - f(X_j)|$ to be non-zero, it must be the case that the subgraph $G_{n,\varepsilon}[X_{i\bj}]$ is connected. Second, noting that for any indices $i$ and $j$,
\begin{equation*}
|D_{ij}f(x)| \leq \Bigl(|D_jf(X_i)| + |D_jf(x)|\Bigr)\|\eta\|_{\infty}, 
\end{equation*}
a straightforward inductive argument implies that 
\begin{equation*}
|D_{\bj}f(X_i)| \leq s\|\eta\|_{\infty}^s \sum_{j \in i\bj} |D_{\bj_s}f(X_j)|.
\end{equation*}
Combining these two observations, we reduce the task to upper bounding the product of two (first-order) differences,
\begin{align*}
\mathbb{E}\Bigl[ |D_{\bj}f(X_i)| |f(X_i) - f(X_{\bj_1})| \Bigr] & = \mathbb{E}\Bigl[ |D_{\bj}f(X_i)| |f(X_i) - f(X_{\bj_1})| \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\} \Bigr] \\
& \leq s\|\eta\|_{\infty}^s \sum_{j \in i\bj} \mathbb{E}\Bigl[ |D_{\bj_s}f(X_j)| \cdot |f(X_i) - f(X_{\bj_1})| \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\}   \Bigr] \\
& \leq s\|\eta\|_{\infty}^s \sum_{j \in i\bj} \mathbb{E}\Bigl[ |f(X_j) - f(X_{\bj_s})| \cdot |f(X_i) - f(X_{\bj_1})| \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\}   \Bigr] 
\end{align*}
Next, from the Cauchy-Schwarz inequality we have that for any $j \in \bj$,
\begin{align*}
& \mathbb{E}\Bigl[ |f(X_j) - f(X_{\bj_s})| \cdot |f(X_i) - f(X_{\bj_1})| \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\}   \Bigr] \\
& \quad \leq \sqrt{\mathbb{E}\Bigl[ |f(X_j) - f(X_{\bj_s})|^2 \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\} \Bigr]} \cdot \sqrt{\mathbb{E}\Bigl[ |f(X_j) - f(X_{\bj_s})|^2 \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\} \Bigr]} \\
& \quad = \mathbb{E}\Bigl[ |f(X_j) - f(X_{i})|^2 \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\} \Bigr],
\end{align*}
with the equality following since each $X_i$ are identically distributed. Marginalizing out the contribution of all indices in $\bj$ not equal to $i$ or $j$ gives
\begin{align}
\mathbb{E}\Bigl[ |f(X_j) - f(X_{i})|^2 \cdot \1\bigl\{G_{n,\varepsilon}[X_{i\bj}]~~\textrm{is connected.} \bigr\} \Bigr] & \leq \bigl((s + 1)p_{\max}\nu_d\varepsilon^d\bigr)^{|i\bj\setminus\{j \cup i\}|} \cdot \mathbb{E}\Bigl[ |f(X_j) - f(X_{i})|^2 \1\{\|X_i - X_j\| \leq \varepsilon\}\Bigr] \nonumber \\
& \leq \bigl((s + 1)p_{\max}\nu_d\varepsilon^d\bigr)^{|i\bj\setminus\{j \cup i\}|} \cdot p_{\max}^2 \nu_d \varepsilon^{2 + d} M^2 \label{pf:graph_seminorm_bias2_1}
\end{align}
with the second inequality following from the proof of Lemma~1 in \textcolor{blue}{(Green 2021).} Finally, we notice that $|i\bj\setminus \{i \cup j\}| + 1 = k$, so that~\eqref{pf:graph_seminorm_bias2_1} gives the desired result.

\section{Graph quadratic form, manifold domain}
\label{sec:graph_quadratic_form_manifold}

\paragraph{Discussion of requirements that $s \leq 3$.}
Recall from our discussion in Section~\ref{subsec:analysis} that an essential step in the proof of Lemma~\textcolor{red}{(?)}---which establishes a high-probability upper bound on $\dotp{L_{n,\varepsilon}^sf_0}{f_0}_n$---is relating the iterated non-local operator $L_{P,\varepsilon}^j$ to the weighted Laplace-Beltrami operator $\Delta_P^j$, for $j = \floor{s/2}$. Suppose for the moment $s = 3$, and $f \in C^3(\mc{X};1)$ and $p$ is uniform. In this case $\floor{s/2} = 1$, and the relation between $L_{P,\varepsilon}$ to $\Delta_P$ has been worked out by \textcolor{red}{(Burago2013, GarciaTrillos2019, Calder2019, GarciaTrillos2020)}. This relation is accomplished by means of an intermediary nonlocal operator, as described by the following chain of estimates:
\begin{align*}
L_{P,\varepsilon} f(x) & = \frac{1}{\varepsilon^{m + 2}}\int_{\mc{X}} \bigl(f(z) - f(x)\bigr)\eta\biggl(\frac{\|z - x\|}{\varepsilon}\biggr) \,d\mu(z) \\
& \overset{(i)}{=} \frac{1}{\varepsilon^{m + 2}} \int_{\mc{X}} \bigl(f(z) - f(x)\bigr)\eta\biggl(\frac{d_{\mc{X}}(z,x)}{\varepsilon}\biggr) \,d\mu(z) + O(\varepsilon) \\
& = \frac{1}{\varepsilon^{m + 2}} \int_{T_x(\mc{X})} \Bigl(f\bigl(\exp_x(v)\bigr) - f\bigl(\exp_x(0)\bigr)\Bigr)\eta\biggl(\frac{d_{\mc{X}}\bigl(\exp_x(v),\exp_x(0)\bigr)}{\varepsilon}\biggr) J_x(v) \,dv + O(\varepsilon) \\
& \overset{(ii)}{=} \frac{1}{\varepsilon^{m + 2}} \int_{T_x(\mc{X})} \Bigl(f\bigl(\exp_x(v)\bigr) - f\bigl(\exp_x(0)\bigr)\Bigr)\eta\biggl(\frac{d_{\mc{X}}\bigl(\exp_x(v),\exp_x(0)\bigr)}{\varepsilon}\biggr) \,dv + O(\varepsilon) \\
& = \frac{1}{\varepsilon^{m + 2}} \int_{T_x(\mc{X})} \Bigl(f\bigl(\exp_x(v)\bigr) - f\bigl(\exp_x(0)\bigr)\Bigr)\eta\biggl(\frac{\|v\|}{\varepsilon}\biggr) \,dv + O(\varepsilon) \\
& \overset{(iii)}{=} \Delta_Pf(x) + O(\varepsilon)
\end{align*}
Here, $d_{\mc{X}}(\cdot,\cdot)$ is the geodesic distance on $\mc{X}$, $J_x(v)$ denotes the Jacobian of the exponential map $\exp_x$ evaluated at $v \in T_x(\mc{M})$, and $h = f \circ \exp_x$. Then $(i)$ follows because $\bigl|d_{\mc{X}}(x,z)  - \|z - x\|\bigr| = O(\varepsilon^3)$ and $\eta$ is a Lipschitz function with bounded Lipschitz constant, $(ii)$ follows because the Jacobian $J_x(v)$ satisfies $|J_x(v)| = 1 + O(\varepsilon^2)$ for all $v \in B(0,\varepsilon) \subset T_x(\mc{X})$, and $(iii)$ follows since the function $h$ admits the Taylor expansion
\begin{equation*}
h(v) = h(0) + \dotp{\nabla h(0)}{v} + \frac{1}{2} \dotp{\nabla^2 h(0) v}{v} + O(\varepsilon^3),~~\textrm{for all $v \in B(0,\varepsilon) \subset T_x(\mc{X})$.}
\end{equation*}
Then, using the upper bound on $\max_{i = 1,\ldots,n}|L_{n,\varepsilon}f(X_i) - L_{P,\varepsilon}(X_i)| = O_\Pbb((n\varepsilon^{m + 4})^{-1})$ given in \textcolor{red}{(?)}, and the fact that $\Delta_Pf \in C^1(\mc{X};1)$, we have that,
\begin{align*}
\dotp{L_{n,\varepsilon}^3f}{f}_n & = \frac{1}{n^2 \varepsilon^{m + 2}} \sum_{i,j = 1}^{n} \bigl(L_{n,\varepsilon}f(X_i) - L_{n,\varepsilon}f(X_j)\bigr)^2 \eta\biggl(\frac{\|X_i - X_j\|}{\varepsilon}\biggr) \\
& = \frac{1}{n^2 \varepsilon^{m + 2}} \sum_{i,j = 1}^{n} \bigl(L_{P,\varepsilon}f(X_i) - L_{P,\varepsilon}f(X_j)\bigr)^2 \eta\biggl(\frac{\|X_i - X_j\|}{\varepsilon}\biggr) + O_{\Pbb}\biggl(\frac{1}{n\varepsilon^{m + 4}}\biggr) \\
& = \frac{1}{n^2 \varepsilon^{m + 2}} \sum_{i,j = 1}^{n} \bigl(\Delta_Pf(X_i) - \Delta_Pf(X_j)\bigr)^2 \eta\biggl(\frac{\|X_i - X_j\|}{\varepsilon}\biggr) + O_{\Pbb}\biggl(\frac{1}{n\varepsilon^{m + 4}}\biggr) + O_{\Pbb}(1) \\
& = O_{\Pbb}\biggl(\frac{1}{n\varepsilon^{m + 4}}\biggr) + O_{\Pbb}(1) = O_{\Pbb}(1),
\end{align*}
with the last equality following when $\varepsilon = \Omega(n^{-1/(m + 4)})$.

On the other hand when $s = 4$, and thus $j = 2$, we have only that
\begin{equation*}
L_{P,\varepsilon}^2f(x) = L_{P,\varepsilon}(\Delta_Pf + O(\varepsilon)) = \Delta_P^2f(x) + O(1/\varepsilon),
\end{equation*}
which is useless when $\varepsilon \to 0$. The bottom line is that in the manifold case, the $O(\varepsilon^3)$ error incurred by going between Euclidean norm and geodesic distance is negligible only when $j = 1$. 

\section{Lower bound on empirical norm}
\label{sec:empirical_norm}
In this Section we prove Proposition~\ref{prop:empirical_norm_sobolev} (in Section~\ref{subsec:empirical_norm_sobolev}). We also prove an analogous result when $\mc{X}$ is a manifold as in Model~\ref{def:model_manifold} (in Section~\ref{subsec:empirical_norm_sobolev_manifold}), and we prove some sharper estimates in the case where the function is bounded in H\"{o}lder norm, in Section~\ref{subsec:empirical_norm_holder}.

\subsection{Proof of Proposition~\ref{prop:empirical_norm_sobolev}}
\label{subsec:empirical_norm_sobolev}
\textcolor{red}{(TODO)}

\subsection{Analysis of empirical norm in manifold setting}
\label{subsec:empirical_norm_soboolev_manifold}

\subsection{Better bounds assuming H\"{o}lder smoothness}
\label{subsec:empirical_norm_holder}

\section{Analysis of kernel smoothing}
\label{subsec:kernel_smoothing}
In this section we prove Lemma~\ref{lem:kernel_smoothing_laplacian_eigenmaps} (in Section~\ref{subsec:pf_kernel_smoothing_laplacian_eigenmaps}) and Lemma~\ref{lem:kernel_smoothing_bias} (in Section~\ref{subsec:pf_kernel_smoothing_bias}). In Section~\ref{subsec:eigenmaps_beats_kernel_smoothing}, we give a sequence $\{p^{(n)}(x), f_0^{(n)}(x)\}_{n \in \mathbb{N}}$ for which the estimator $T_{h,n}\wc{f}$ (kernel extension of Laplacian eigenmaps) dominates direct kernel smoothing of the responses, in the sense that
\begin{equation*}
\lim_{n \to \infty} \sup_{h'} \frac{\|T_{h_n,n}\wc{f} - f_0\|_P^2}{\|T_{h',n}Y - f_0\|_P^2 } = 0,\quad \textrm{with probability $1$.}
\end{equation*}

\subsection{Proof of Lemma~\ref{lem:kernel_smoothing_laplacian_eigenmaps}}
\label{subsec:pf_kernel_smoothing_laplacian_eigenmaps}

\subsection{Proof of Lemma~\ref{lem:kernel_smoothing_bias}}
\label{subsec:pf_kernel_smoothing_bias}

\subsection{Comparison of Laplacian eigenmaps to kernel smoothing}
\label{subsec:eigenmaps_beats_kernel_smoothing}

\textcolor{red}{(TODO)}
\begin{itemize}
	\item \textcolor{red}{Mention the relationship between the estimation problem under consideration and classification with a margin (Tsybakov noise) condition.}
\end{itemize}

\section{Miscellaneous}
Here we give some helpful Lemmas used at various points in the above proofs. We also prove that spectral sparsification does not change the rate of convergence of $\wh{f}$. Finally, we review notation and relevant facts regarding Taylor expansion.

\subsection{Taylor expansion}
\label{subsec:taylor_expansion}
Let $u$ be a function which is $s$ times continuously differentiable at all $x \in \mc{X}$, for $k \in \mathbb{N}\setminus\{0\}$. Suppose that for some $h > 0$, $x \in \mc{X}_{h}$ and $x' \in B(x,h)$. We write the order-$s$ Taylor expansion of $u(x')$ around $x' = x$ as
\begin{equation*}
u(x') = u(x) + \sum_{j = 1}^{s - 1} \frac{1}{j!}\bigl(d_x^{j}u\bigr)(x' - x) + r_{x'}^{s}(x;u)
\end{equation*}
For notational convenience we have adopted the convention that $\sum_{j = 1}^{0} a_j = 0$. For a given $z \in \Rd$ and $j$-times differentiable function $f: \mc{X} \to \Reals$, we have used the notation $\bigl(d_x^jf\bigr)(z) := \sum_{\abs{\alpha} = j} D^{\alpha}f(x) z^{\alpha}$. Thus $\bigl(d_x^{j}f\bigr)(z)$ is a degree-$j$ polynomial---and so a $j$-homogeneous function---in $z$, meaning for any $t \in \Reals$,
\begin{equation*}
\bigl(d_x^{j}f\bigr)(tz) = t^{j} \cdot \bigl(d_x^{j}f\bigr)(z).
\end{equation*}
The remainder term $r_{x'}$ is given by
\begin{equation*}
r_{x'}^s(x;f) = \frac{1}{(j - 1)!} \int_{0}^{1}(1 - t)^{j - 1} \bigl(d_{x + t(x' - x)}^{s}f\bigr)(x' - x) \,dt,
\end{equation*}
where we point out that the integral makes sense because $x + t(x' - x) \in B(x,h) \subseteq \mc{X}$. We now give estimates on the remainder term in both sup-norm and $L^2(\mc{X}_{h})$ norm, each of which hold for any $z \in B(0,1)$. In sup-norm, we have that 
\begin{equation*}
\sup_{x \in \mc{X}_{h}}|r_{x + hz}^j(x;f)| \leq C h^j \|f\|_{C^j(\mc{X})},
\end{equation*}
whereas in $L^2(\mc{X}_{h})$ norm we have,
\begin{equation}
\label{eqn:sobolev_remainder_term}
\int_{\mc{X}_{h}} \bigl|r_{x + thz}^j(x;f)\bigr|^2 \,dx \leq h^{2j} \int_{\mc{X}_{h}} \int_{0}^{1} |d_{x + thz}^jf(z)|^2 \,dt \,dx \leq h^{2j} \|d^jf\|_{\Leb^2(\mc{X})}^2.
\end{equation}

Finally, we recall some facts regarding the interaction between smoothing kernels and polynomials.  Let $q_j(z)$ be an arbitrary degree-$j$ (multivariate) polynomial. If $\eta$ is a radially symmetric kernel and $j$ is odd, then by symmetry it follows that
\begin{equation*}
\int_{B(0,1)} q_j(z) \eta(\|z\|) \,dz = 0.
\end{equation*}
On the other hand, if $\psi$ is an order-$s$ kernel for some $s > j$, then by converting to polar coordinates we can verify that
\begin{equation*}
\int_{B(0,1)} q_j(z) \eta(\|z\|) \,dz = 0.
\end{equation*}

\textcolor{red}{(TODO): Check with Ryan and Siva that (i) they believe these facts, and (ii) these facts do not require additional justification.}

 