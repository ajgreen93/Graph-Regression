\documentclass{beamer}

% Packages
\usepackage{bm}

%% Theme related.
\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{frametitle}[default][center]
\setbeamertemplate{itemize items}[circle]
\usefonttheme[onlymath]{serif}


%% Math
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\wt}[1]{\widetilde{#1}}
\DeclareMathOperator*{\argmin}{argmin}

%% Font
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\title{Laplacian Smoothing on Neighborhood Graphs}

\author{Alden Green}
\institute{Department of Statistics and Data Science \\
		   Carnegie Mellon Unviersity}
\date{}

\begin{document}
	
\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Basic problem setup}
\alert{Nonparametric regression, random design}: We observe $(X_1,Y_1),\ldots,(X_n,Y_n)$, where $X_1,\ldots,X_n$ are sampled independently from $P$, and
\begin{equation*}
Y_i = f_0(X_i) + \varepsilon_i, \quad \varepsilon_i \sim N(0,1).
\end{equation*}
Goal is to learn the unknown regression function $f_0$, which is assumed to be \alert{Sobolev} smooth, i.e.
\begin{equation*}
\|f_0\|_{H^1}^2 := \int \|\nabla f_0(x)\|^2 \,dx \quad \text{is small.}
\end{equation*}
Either \alert{estimate} $f_0$ with some $\hat{f}$, or \alert{test} whether $f_0 = 0$.
\end{frame}

\begin{frame}{Laplacian smoothing}
Laplacian smoothing solves a discrete, penalized least squares problem. 
\begin{enumerate}
	\item  Form a \alert{neighborhood graph} $G$ over $X_1,\ldots,X_n$, with weighted edges $W_{i,j} = K(\|X_i - X_j\|/r)$. The \alert{graph Laplacian} $L$ is defined by
	\begin{equation*}
	(Lf)_i = \sum_{i = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr) W_{i,j}.
	\end{equation*}
	\item The \alert{Laplacian smoothing} estimator is
	\begin{equation*}
	\wh{f} = \argmin_{f \in \Reals^n} \|Y - f\|_n^2 + \frac{\lambda}{2} f^{\top} L f, \end{equation*}
	with corresponding test statistic $\wh{T} = \|\wh{f}\|_n^2$.
\end{enumerate}
\textcolor{red}{(TODO): Simple picture here.}
\end{frame}

\begin{frame}{Motivation}
Why Laplacian smoothing?
\begin{itemize}
	\item \emph{Computational ease.} Sparse graph, fast solvers.
	\item \emph{Generality.} Makes sense whenever one can form a graph $G$. 
	\item \emph{Weak supervision.} Naturally extends to semi- and unsupervised problems.
\end{itemize}
Intuition: when $n$ large, $r$ small and $f$ smooth, \textcolor{blue}{(Bousquet et al.)} show
\begin{equation*}
f^T L f = \frac{1}{2}\sum_{i,j = 1}^{n} \bigl(f(X_i) - f(X_j)\bigr)^2 W_{i,j} \approx n^2 r^{d + 2} \int \|\nabla f\|_2^2 \,dx.
\end{equation*}
Seemingly $\wh{f}$ is a noisy approximation of a very classical estimator $\wt{f}$: aka \alert{smoothing spline} (1d), or \alert{thin-plate spline} ($d > 1$), defined as
\begin{equation*}
\widetilde{f} = \argmin_{f \in H^1} \|Y - f\|_n^2 + \lambda \int \|\nabla f(x)\|_2^2 \,dx.
\end{equation*}
Similarly $\wh{T}$ approximates $\wt{T} = \|\wt{f}\|_n^2$. 
\end{frame}

\begin{frame}{Main Question}
In $1d$, smoothing splines achieve the minimax optimal rate \blue{$n^{-2/3}$} (estimation) or \blue{$n^{-4/5}$} (testing).
\begin{block}{{\bf Question}}
	Does Laplacian smoothing enjoy the same optimality properties as smoothing/thin plate splines?
\end{block}

\begin{block}{{\bf Answer}}
	They actually do better! When properly tuned:
	\begin{itemize}
		\item Laplacian smoothing has error of \blue{$n^{-2/3}$} (estimation) and \blue{$n^{-4/5}$} (testing) when $d = 1$, matching smoothing splines.
		\item Laplacian smoothing also achieves optimal error \blue{$n^{-2/(2 + d)}$} (estimation) and \blue{$n^{-4/(4 + d)}$} (testing) when $d = 2,3,4$ (up to $\log(n)$ factors when $d = 4$).
		\item On the other hand $\wt{f}$ is \alert{not even well-defined} when $d > 1$. Well known in spline theory community.
	\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Summary of Main Results}

{\large Estimation rates, assuming $f_0 \in H^1(M)$}
\begin{table}
	\begin{center}
		\begin{tabular}{p{.25\textwidth} | p{.25\textwidth} p{.25\textwidth} }
			Dimension & Laplacian smoothing & Thin-plate splines \\
			\hline
			$d = 1$ & \blue{${n^{-2/3}}$} & \blue{${n^{-2/3}}$} \\
			$d = 2,3$ & \blue{${n^{-2/(2 + d)}}$} & \red{NA} \\
			$d = 4$ & $\blue{{n^{-1/3}}} (\log n)^{1/3}$ & \red{NA} \\
			$d \geq 5$  & $(\log n/n)^{4/(3d)}$ & \red{NA} \\
		\end{tabular}
	\end{center}
\end{table}

{\large Testing rates, assuming $f_0 \in H^1(M)$ ($d < 4$) or $f_0 \in L^4(M)$ ($d \geq 4$)}
\begin{table}
	\begin{center}
		\begin{tabular}{p{.25\textwidth} | p{.25\textwidth} p{.25\textwidth} }
			Dimension & Laplacian smoothing & Thin-plate splines \\
			\hline
			$d = 1$ & \blue{${n^{-4/5}}$} & \blue{${n^{-4/5}}$} \\
			$d = 2,3$ & \blue{${n^{-4/(4 + d)}}$} & \red{NA} \\
			$d \geq 4$ & $\blue{{n^{-1/2}}}$ & \red{NA} \\
		\end{tabular}
	\end{center}
\end{table}
\end{frame}

\begin{frame}{Explanation: Failure of Thin-plate Splines}
Well-posedness of smoothing splines (when $d = 1$) vs. ill-posedness of thin-plate splines ($d \geq 2$) explained by \alert{Sobolev Embedding Theorem}.
\begin{itemize}
	\item In $1d$, every function in $H^1$ is continuous. 
	\item When $d \geq 2$, this is no longer the case.
\end{itemize}
As a result the criterion underlying $\wt{f}$ is \red{not continuous} with respect to $H^1$ norm. I can construct sequences of ``bump functions'' $f_1,f_2,\ldots$ such that
\begin{equation*}
f_k(X_i) = Y_i \quad \text{for all $k$, but} \quad \|f_k\|_{H^1} \to 0.
\end{equation*}
Implication: For any $\delta > 0$, there exists a function $f$ which satisfies
\begin{equation*}
f(X_i) = Y_i \quad \text{for all $i = 1,\ldots,n$, yet} \int\|\nabla f(x)\|^2 \,dx < \delta. 
\end{equation*} 
\end{frame}

\begin{frame}{Explanation: Success of Laplacian Smoothing}
Roughly, discretization is a blessing.

\end{frame}


\begin{frame}{What about $d > 4$?}
content...
\end{frame}

\begin{frame}{Extensions}

\end{frame}

\begin{frame}{Analysis}
\end{frame}



\end{document}
